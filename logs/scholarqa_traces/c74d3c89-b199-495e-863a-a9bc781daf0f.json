{
    "query": "What are the most effective methods for detecting pre-training data contamination in large language models, and how do current benchmarks like WIKIMIA evaluate these methods?",
    "user_id": "lib_user",
    "task_id": "c74d3c89-b199-495e-863a-a9bc781daf0f",
    "timestamp": "2025-06-23T23:17:00.997882",
    "n_retrieval": 256,
    "n_retrieved": 258,
    "n_candidates": 28,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.41722500000000007,
    "decomposed_query": {
        "rewritten_query": "Most effective methods for detecting pre-training data contamination in large language models, and how current benchmarks like WIKIMIA evaluate these methods.",
        "keyword_query": "detecting pre-training data contamination large language models benchmarks WIKIMIA",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009933,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Probing Language Models for Pre-training Data Detection",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 13,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.01333, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294376388",
                    "name": "Zhenhua Liu"
                },
                {
                    "authorId": "1914586128",
                    "name": "Tong Zhu"
                },
                {
                    "authorId": "2186374155",
                    "name": "Chuanyuan Tan"
                },
                {
                    "authorId": "2304460083",
                    "name": "Haonan Lu"
                },
                {
                    "authorId": "2330946427",
                    "name": "Bing Liu"
                },
                {
                    "authorId": "2265943980",
                    "name": "Wenliang Chen"
                }
            ],
            "abstract": "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories. Our experiments demonstrate that our method outperforms all baselines, and achieves state-of-the-art performance on both WikiMIA and ArxivMIA, with additional experiments confirming its efficacy (Our code and dataset are available at https://github.com/zhliu0106/probing-lm-data).",
            "corpus_id": 270217411,
            "sentences": [
                {
                    "corpus_id": "270217411",
                    "title": "Probing Language Models for Pre-training Data Detection",
                    "text": "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories. Our experiments demonstrate that our method outperforms all baselines, and achieves state-of-the-art performance on both WikiMIA and ArxivMIA, with additional experiments confirming its efficacy (Our code and dataset are available at https://github.com/zhliu0106/probing-lm-data).",
                    "score": 0.670974503995851,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9775390625
                },
                {
                    "corpus_id": "270217411",
                    "title": "Probing Language Models for Pre-training Data Detection",
                    "text": "Our contributions can be summarized as follows:\n\n\u2022 We propose a novel pre-training data detection method that utilizes the probing technique.To the best of our knowledge, this is the first work to examine LLMs' internal activations to determine whether a given text was included in the pre-training data.\n\n\u2022 We propose ArxivMIA, a new benchmark in the academic domain to assess pre-training data detection methods.With a low duplication rate and the inherent complexity of texts, ArxivMIA presents a more challenging scenario and highlights the need for more sophisticated pre-training data detection methods.\n\n\u2022 We conduct extensive experiments on WikiMIA and ArxivMIA benchmarks.In addition, we also evaluate various detection methods on a downstream task datasets contamination challenge.Through comprehensive experimentation, we demonstrate that our proposed method outperforms all baselines, and achieves state-of-the-art performance.",
                    "score": 0.6578752520405002,
                    "section_title": "Introduction",
                    "char_start_offset": 4195,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 47
                        },
                        {
                            "start": 49,
                            "end": 141
                        },
                        {
                            "start": 141,
                            "end": 304
                        },
                        {
                            "start": 306,
                            "end": 414
                        },
                        {
                            "start": 414,
                            "end": 609
                        },
                        {
                            "start": 611,
                            "end": 681
                        },
                        {
                            "start": 681,
                            "end": 791
                        },
                        {
                            "start": 791,
                            "end": 939
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.962890625
                },
                {
                    "corpus_id": "270217411",
                    "title": "Probing Language Models for Pre-training Data Detection",
                    "text": "In summary, this paper investigates the pre-training data detection problem in large language models.We propose a simple and effective approach that determines whether a target text has been included in a model's pre-training dataset by analyzing the internal activations using the probe technique.Additionally, we introduce a more challenging benchmark, ArxivMIA.The experiments demonstrate that our method outperforms all baselines across various benchmarks, achieving SOTA performance.\n\nWe further analyze the impact of target model size and the number of training data on our method.Additionally, we validate the effectiveness of our approach through a downstream task datasets contamination detection challenge.Future work could extend our methods to larger model scales or apply them to multi-modal models.",
                    "score": 0.6906253556787894,
                    "section_title": "Conclusion",
                    "char_start_offset": 20510,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 101
                        },
                        {
                            "start": 101,
                            "end": 298
                        },
                        {
                            "start": 298,
                            "end": 364
                        },
                        {
                            "start": 364,
                            "end": 488
                        },
                        {
                            "start": 490,
                            "end": 587
                        },
                        {
                            "start": 587,
                            "end": 716
                        },
                        {
                            "start": 716,
                            "end": 812
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.955078125
                },
                {
                    "corpus_id": "270217411",
                    "title": "Probing Language Models for Pre-training Data Detection",
                    "text": "Large language models (LLMs) trained on massive corpora of texts demonstrate extraordinary abilities to understand, reason, and generate following natural language instructions (Brown et al., 2020;Anil et al., 2023).Meanwhile, the open-source of LLMs has significantly contributed to the advancement and collaborative development within the LLM community (Zhang et al., 2022;Touvron et al., 2023b;Biderman et al., 2023;Bai et al., 2023;Team, 2023;LLaMA-MoE Team, 2023).Despite this progress, the lack of transparency raises ethical and legal questions, particularly about the use of potentially private data sourced from the internet, and threatens the reliability of benchmark evaluations due to the risk of leaking test data.Therefore, determining if certain texts have been utilized during the pre-training phase of the target LLM becomes a critical task.\n\nRecent efforts to detect pre-training data in LLMs have attracted significant attention.Several studies have been proposed to investigate dataset contamination, including prompting LLMs to generate data-specific examples or using statistical methods to detect contamination in test sets (Sainz et al., 2023;Golchin and Surdeanu, 2023;Oren et al., 2023).Concurrently, Membership Inference Attacks (MIAs) in Natural Language Processing have been extensively explored for their potential to discern whether specific data was used in LLMs' pre-training (Carlini et al., 2021;Mireshghallah et al., 2022;Mattern et al., 2023;Shi et al., 2023).The above solutions have achieved a certain success.However, they all rely on the model's superficial features, such as generated texts or loss metrics, which may not always be reliable indicators of pretraining data.\n\nDifferent from these conventional approaches, we propose a simple yet effective pre-training data detection method that utilizes the probing technique to examine the model's internal activations.This approach is based on the assumption: Texts that have been seen during the model's pre-training phase are represented differently in the model's internal activations compared to texts that have not been seen, so we could train a linear probe classifier to distinguish them.",
                    "score": 0.6120032974542478,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 216
                        },
                        {
                            "start": 216,
                            "end": 469
                        },
                        {
                            "start": 469,
                            "end": 727
                        },
                        {
                            "start": 727,
                            "end": 858
                        },
                        {
                            "start": 860,
                            "end": 948
                        },
                        {
                            "start": 948,
                            "end": 1213
                        },
                        {
                            "start": 1213,
                            "end": 1497
                        },
                        {
                            "start": 1497,
                            "end": 1549
                        },
                        {
                            "start": 1549,
                            "end": 1714
                        },
                        {
                            "start": 1716,
                            "end": 1911
                        },
                        {
                            "start": 1911,
                            "end": 2188
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 397,
                            "end": 419,
                            "matchedPaperCorpusId": "257921893"
                        },
                        {
                            "start": 1409,
                            "end": 1431,
                            "matchedPaperCorpusId": "229156229"
                        },
                        {
                            "start": 1431,
                            "end": 1458,
                            "matchedPaperCorpusId": "247315260"
                        },
                        {
                            "start": 1458,
                            "end": 1479,
                            "matchedPaperCorpusId": "258967264"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94921875
                }
            ],
            "relevance_judgement": 0.9775390625,
            "relevance_judgment_input_expanded": "# Title: Probing Language Models for Pre-training Data Detection\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Zhenhua Liu, Tong Zhu, Chuanyuan Tan, Haonan Lu, Bing Liu, Wenliang Chen\n## Abstract\nLarge Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories. Our experiments demonstrate that our method outperforms all baselines, and achieves state-of-the-art performance on both WikiMIA and ArxivMIA, with additional experiments confirming its efficacy (Our code and dataset are available at https://github.com/zhliu0106/probing-lm-data).\n## Introduction\nLarge language models (LLMs) trained on massive corpora of texts demonstrate extraordinary abilities to understand, reason, and generate following natural language instructions (Brown et al., 2020;Anil et al., 2023).Meanwhile, the open-source of LLMs has significantly contributed to the advancement and collaborative development within the LLM community (Zhang et al., 2022;Touvron et al., 2023b;Biderman et al., 2023;Bai et al., 2023;Team, 2023;LLaMA-MoE Team, 2023).Despite this progress, the lack of transparency raises ethical and legal questions, particularly about the use of potentially private data sourced from the internet, and threatens the reliability of benchmark evaluations due to the risk of leaking test data.Therefore, determining if certain texts have been utilized during the pre-training phase of the target LLM becomes a critical task.\n\nRecent efforts to detect pre-training data in LLMs have attracted significant attention.Several studies have been proposed to investigate dataset contamination, including prompting LLMs to generate data-specific examples or using statistical methods to detect contamination in test sets (Sainz et al., 2023;Golchin and Surdeanu, 2023;Oren et al., 2023).Concurrently, Membership Inference Attacks (MIAs) in Natural Language Processing have been extensively explored for their potential to discern whether specific data was used in LLMs' pre-training (Carlini et al., 2021;Mireshghallah et al., 2022;Mattern et al., 2023;Shi et al., 2023).The above solutions have achieved a certain success.However, they all rely on the model's superficial features, such as generated texts or loss metrics, which may not always be reliable indicators of pretraining data.\n\nDifferent from these conventional approaches, we propose a simple yet effective pre-training data detection method that utilizes the probing technique to examine the model's internal activations.This approach is based on the assumption: Texts that have been seen during the model's pre-training phase are represented differently in the model's internal activations compared to texts that have not been seen, so we could train a linear probe classifier to distinguish them.\n...\nOur contributions can be summarized as follows:\n\n\u2022 We propose a novel pre-training data detection method that utilizes the probing technique.To the best of our knowledge, this is the first work to examine LLMs' internal activations to determine whether a given text was included in the pre-training data.\n\n\u2022 We propose ArxivMIA, a new benchmark in the academic domain to assess pre-training data detection methods.With a low duplication rate and the inherent complexity of texts, ArxivMIA presents a more challenging scenario and highlights the need for more sophisticated pre-training data detection methods.\n\n\u2022 We conduct extensive experiments on WikiMIA and ArxivMIA benchmarks.In addition, we also evaluate various detection methods on a downstream task datasets contamination challenge.Through comprehensive experimentation, we demonstrate that our proposed method outperforms all baselines, and achieves state-of-the-art performance.\n\n## Conclusion\nIn summary, this paper investigates the pre-training data detection problem in large language models.We propose a simple and effective approach that determines whether a target text has been included in a model's pre-training dataset by analyzing the internal activations using the probe technique.Additionally, we introduce a more challenging benchmark, ArxivMIA.The experiments demonstrate that our method outperforms all baselines across various benchmarks, achieving SOTA performance.\n\nWe further analyze the impact of target model size and the number of training data on our method.Additionally, we validate the effectiveness of our approach through a downstream task datasets contamination detection challenge.Future work could extend our methods to larger model scales or apply them to multi-modal models.",
            "reference_string": "[270217411 | Liu et al. | 2024 | Citations: 13]"
        },
        {
            "title": "ConStat: Performance-Based Contamination Detection in Large Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 53,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.16281, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268310707",
                    "name": "Jasper Dekoninck"
                },
                {
                    "authorId": "2116235329",
                    "name": "Mark Niklas M\u00fcller"
                },
                {
                    "authorId": "1736447",
                    "name": "Martin T. Vechev"
                }
            ],
            "abstract": "Public benchmarks play an essential role in the evaluation of large language models. However, data contamination can lead to inflated performance, rendering them unreliable for model comparison. It is therefore crucial to detect contamination and estimate its impact on measured performance. Unfortunately, existing detection methods can be easily evaded and fail to quantify contamination. To overcome these limitations, we propose a novel definition of contamination as artificially inflated and non-generalizing benchmark performance instead of the inclusion of benchmark samples in the training data. This perspective enables us to detect any model with inflated performance, i.e., performance that does not generalize to rephrased samples, synthetic samples from the same distribution, or different benchmarks for the same task. Based on this insight, we develop ConStat, a statistical method that reliably detects and quantifies contamination by comparing performance between a primary and reference benchmark relative to a set of reference models. We demonstrate the effectiveness of ConStat in an extensive evaluation of diverse model architectures, benchmarks, and contamination scenarios and find high levels of contamination in multiple popular models including Mistral, Llama, Yi, and the top-3 Open LLM Leaderboard models.",
            "corpus_id": 270063170,
            "sentences": [
                {
                    "corpus_id": "270063170",
                    "title": "ConStat: Performance-Based Contamination Detection in Large Language Models",
                    "text": "Public benchmarks play an essential role in the evaluation of large language models. However, data contamination can lead to inflated performance, rendering them unreliable for model comparison. It is therefore crucial to detect contamination and estimate its impact on measured performance. Unfortunately, existing detection methods can be easily evaded and fail to quantify contamination. To overcome these limitations, we propose a novel definition of contamination as artificially inflated and non-generalizing benchmark performance instead of the inclusion of benchmark samples in the training data. This perspective enables us to detect any model with inflated performance, i.e., performance that does not generalize to rephrased samples, synthetic samples from the same distribution, or different benchmarks for the same task. Based on this insight, we develop ConStat, a statistical method that reliably detects and quantifies contamination by comparing performance between a primary and reference benchmark relative to a set of reference models. We demonstrate the effectiveness of ConStat in an extensive evaluation of diverse model architectures, benchmarks, and contamination scenarios and find high levels of contamination in multiple popular models including Mistral, Llama, Yi, and the top-3 Open LLM Leaderboard models.",
                    "score": 0.5786821727158388,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97705078125
                },
                {
                    "corpus_id": "270063170",
                    "title": "ConStat: Performance-Based Contamination Detection in Large Language Models",
                    "text": "As large language models (LLMs) become increasingly effective at a wide range of tasks, many companies and research institutions compete to develop better models [2,5,28,35].To facilitate this development, a variety of benchmarks have been proposed that allow a standardized in-depth comparison of model performance across diverse tasks [15,16,26,32].\n\nData Contamination Modern LLMs are trained on vast amounts of internet-sourced data, raising the risk of unintentionally including benchmark samples in the training set.Such data contamination can lead to artificially inflated benchmark performance that does not accurately reflect a model's true ability to generalize to unseen tasks.However, model providers argue that the impact of this contamination on model performance is negligible [2,14,35] and the enormous size of current training sets almost guarantees contamination to some extent.This casts doubt on the relevance of this traditional definition of contamination in the context of LLMs.This Work: A New Perspective on Data Contamination We propose a new perspective on contamination, defining it based on its effect on model performance rather than its cause.Specifically, we define contamination as artificially inflated, non-generalizing performance, i.e., we say a model is contaminated if and only if its performance relative to other models is significantly higher on the original benchmark than on a similar reference benchmark.This definition captures the essence of the  contamination problem, i.e., performance measurements becoming unreliable for model comparisons.Furthermore, it enables principled detection methods that are robust against evasion attacks by malicious providers as this would require generalizing performance improvements.\n\nTraditional Contamination Detection Existing contamination detection methods [18,23,24,30,33,36,39,40,48] aim to detect the inclusion of benchmark samples in the training data as a measure of contamination.However, these approaches show limited success, cannot quantify the contamination's effect on model performance, and have to make strict assumptions about the contamination process, making them easy to evade [17].\n\nThis Work: A Statistical Test for Contamination In contrast, we leverage our novel performancebased definition of data contamination to propose a statistical contamination test called CONSTAT, illustrated in Fig. 1.",
                    "score": 0.7070551416024942,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 174,
                            "end": 351
                        },
                        {
                            "start": 353,
                            "end": 522
                        },
                        {
                            "start": 522,
                            "end": 688
                        },
                        {
                            "start": 688,
                            "end": 896
                        },
                        {
                            "start": 896,
                            "end": 1001
                        },
                        {
                            "start": 1001,
                            "end": 1174
                        },
                        {
                            "start": 1174,
                            "end": 1449
                        },
                        {
                            "start": 1449,
                            "end": 1590
                        },
                        {
                            "start": 1590,
                            "end": 1766
                        },
                        {
                            "start": 1768,
                            "end": 1974
                        },
                        {
                            "start": 1974,
                            "end": 2187
                        },
                        {
                            "start": 2189,
                            "end": 2404
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 341,
                            "end": 344,
                            "matchedPaperCorpusId": "239998651"
                        },
                        {
                            "start": 344,
                            "end": 347,
                            "matchedPaperCorpusId": "221516475"
                        },
                        {
                            "start": 347,
                            "end": 350,
                            "matchedPaperCorpusId": "237532606"
                        },
                        {
                            "start": 1858,
                            "end": 1861,
                            "matchedPaperCorpusId": "258967264"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95703125
                }
            ],
            "relevance_judgement": 0.97705078125,
            "relevance_judgment_input_expanded": "# Title: ConStat: Performance-Based Contamination Detection in Large Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Jasper Dekoninck, Mark Niklas M\u00fcller, Martin T. Vechev\n## Abstract\nPublic benchmarks play an essential role in the evaluation of large language models. However, data contamination can lead to inflated performance, rendering them unreliable for model comparison. It is therefore crucial to detect contamination and estimate its impact on measured performance. Unfortunately, existing detection methods can be easily evaded and fail to quantify contamination. To overcome these limitations, we propose a novel definition of contamination as artificially inflated and non-generalizing benchmark performance instead of the inclusion of benchmark samples in the training data. This perspective enables us to detect any model with inflated performance, i.e., performance that does not generalize to rephrased samples, synthetic samples from the same distribution, or different benchmarks for the same task. Based on this insight, we develop ConStat, a statistical method that reliably detects and quantifies contamination by comparing performance between a primary and reference benchmark relative to a set of reference models. We demonstrate the effectiveness of ConStat in an extensive evaluation of diverse model architectures, benchmarks, and contamination scenarios and find high levels of contamination in multiple popular models including Mistral, Llama, Yi, and the top-3 Open LLM Leaderboard models.\n## Introduction\nAs large language models (LLMs) become increasingly effective at a wide range of tasks, many companies and research institutions compete to develop better models [2,5,28,35].To facilitate this development, a variety of benchmarks have been proposed that allow a standardized in-depth comparison of model performance across diverse tasks [15,16,26,32].\n\nData Contamination Modern LLMs are trained on vast amounts of internet-sourced data, raising the risk of unintentionally including benchmark samples in the training set.Such data contamination can lead to artificially inflated benchmark performance that does not accurately reflect a model's true ability to generalize to unseen tasks.However, model providers argue that the impact of this contamination on model performance is negligible [2,14,35] and the enormous size of current training sets almost guarantees contamination to some extent.This casts doubt on the relevance of this traditional definition of contamination in the context of LLMs.This Work: A New Perspective on Data Contamination We propose a new perspective on contamination, defining it based on its effect on model performance rather than its cause.Specifically, we define contamination as artificially inflated, non-generalizing performance, i.e., we say a model is contaminated if and only if its performance relative to other models is significantly higher on the original benchmark than on a similar reference benchmark.This definition captures the essence of the  contamination problem, i.e., performance measurements becoming unreliable for model comparisons.Furthermore, it enables principled detection methods that are robust against evasion attacks by malicious providers as this would require generalizing performance improvements.\n\nTraditional Contamination Detection Existing contamination detection methods [18,23,24,30,33,36,39,40,48] aim to detect the inclusion of benchmark samples in the training data as a measure of contamination.However, these approaches show limited success, cannot quantify the contamination's effect on model performance, and have to make strict assumptions about the contamination process, making them easy to evade [17].\n\nThis Work: A Statistical Test for Contamination In contrast, we leverage our novel performancebased definition of data contamination to propose a statistical contamination test called CONSTAT, illustrated in Fig. 1.",
            "reference_string": "[270063170 | Dekoninck et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 54,
            "influential_citation_count": 16,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.02936, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267788653",
                    "name": "Jingyang Zhang"
                },
                {
                    "authorId": "2156016815",
                    "name": "Jingwei Sun"
                },
                {
                    "authorId": "2119236605",
                    "name": "Eric C. Yeats"
                },
                {
                    "authorId": "2294877492",
                    "name": "Ouyang Yang"
                },
                {
                    "authorId": "2211526996",
                    "name": "Martin Kuo"
                },
                {
                    "authorId": "2265652686",
                    "name": "Jianyi Zhang"
                },
                {
                    "authorId": "2297270173",
                    "name": "Hao k Yang"
                },
                {
                    "authorId": "2294901591",
                    "name": "Hai Li"
                }
            ],
            "abstract": "The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. Despite improved performance, existing methods (including the state-of-the-art, Min-K%) are mostly developed upon simple heuristics and lack solid, reasonable foundations. In this work, we propose a novel and theoretically motivated methodology for pre-training data detection, named Min-K%++. Specifically, we present a key insight that training samples tend to be local maxima of the modeled distribution along each input dimension through maximum likelihood training, which in turn allow us to insightfully translate the problem into identification of local maxima. Then, we design our method accordingly that works under the discrete distribution modeled by LLMs, whose core idea is to determine whether the input forms a mode or has relatively high probability under the conditional categorical distribution. Empirically, the proposed method achieves new SOTA performance across multiple settings. On the WikiMIA benchmark, Min-K%++ outperforms the runner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the more challenging MIMIR benchmark, it consistently improves upon reference-free methods while performing on par with reference-based method that requires an extra reference model.",
            "corpus_id": 268889777,
            "sentences": [
                {
                    "corpus_id": "268889777",
                    "title": "Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models",
                    "text": "The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. Despite improved performance, existing methods (including the state-of-the-art, Min-K%) are mostly developed upon simple heuristics and lack solid, reasonable foundations. In this work, we propose a novel and theoretically motivated methodology for pre-training data detection, named Min-K%++. Specifically, we present a key insight that training samples tend to be local maxima of the modeled distribution along each input dimension through maximum likelihood training, which in turn allow us to insightfully translate the problem into identification of local maxima. Then, we design our method accordingly that works under the discrete distribution modeled by LLMs, whose core idea is to determine whether the input forms a mode or has relatively high probability under the conditional categorical distribution. Empirically, the proposed method achieves new SOTA performance across multiple settings. On the WikiMIA benchmark, Min-K%++ outperforms the runner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the more challenging MIMIR benchmark, it consistently improves upon reference-free methods while performing on par with reference-based method that requires an extra reference model.",
                    "score": 0.6875784343455109,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.974609375
                }
            ],
            "relevance_judgement": 0.974609375,
            "relevance_judgment_input_expanded": "# Title: Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models\n# Venue: arXiv.org\n# Authors: Jingyang Zhang, Jingwei Sun, Eric C. Yeats, Ouyang Yang, Martin Kuo, Jianyi Zhang, Hao k Yang, Hai Li\n## Abstract\nThe problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. Despite improved performance, existing methods (including the state-of-the-art, Min-K%) are mostly developed upon simple heuristics and lack solid, reasonable foundations. In this work, we propose a novel and theoretically motivated methodology for pre-training data detection, named Min-K%++. Specifically, we present a key insight that training samples tend to be local maxima of the modeled distribution along each input dimension through maximum likelihood training, which in turn allow us to insightfully translate the problem into identification of local maxima. Then, we design our method accordingly that works under the discrete distribution modeled by LLMs, whose core idea is to determine whether the input forms a mode or has relatively high probability under the conditional categorical distribution. Empirically, the proposed method achieves new SOTA performance across multiple settings. On the WikiMIA benchmark, Min-K%++ outperforms the runner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the more challenging MIMIR benchmark, it consistently improves upon reference-free methods while performing on par with reference-based method that requires an extra reference model.\n",
            "reference_string": "[268889777 | Zhang et al. | 2024 | Citations: 54]"
        },
        {
            "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges",
            "venue": "International Conference on Computational Linguistics",
            "year": 2024,
            "reference_count": 21,
            "citation_count": 8,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.09927, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2298040858",
                    "name": "Vinay Samuel"
                },
                {
                    "authorId": "2261321156",
                    "name": "Yue Zhou"
                },
                {
                    "authorId": "2261285492",
                    "name": "Henry Peng Zou"
                }
            ],
            "abstract": "As large language models achieve increasingly impressive results, questions arise about whether such performance is from generalizability or mere data memorization. Thus, numerous data contamination detection methods have been proposed. However, these approaches are often validated with traditional benchmarks and early-stage LLMs, leaving uncertainty about their effectiveness when evaluating state-of-the-art LLMs on the contamination of more challenging benchmarks. To address this gap and provide a dual investigation of SOTA LLM contamination status and detection method robustness, we evaluate five contamination detection approaches with four state-of-the-art LLMs across eight challenging datasets often used in modern LLM evaluation. Our analysis reveals that (1) Current methods have non-trivial limitations in their assumptions and practical applications; (2) Notable difficulties exist in detecting contamination introduced during instruction fine-tuning with answer augmentation; and (3) Limited consistencies between SOTA contamination detection techniques. These findings highlight the complexity of contamination detection in advanced LLMs and the urgent need for further research on robust and generalizable contamination evaluation. Our code is available at https://github.com/vsamuel2003/data-contamination.",
            "corpus_id": 272689919,
            "sentences": [
                {
                    "corpus_id": "272689919",
                    "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges",
                    "text": "Detecting training data through probability inference and reconstruction has long been a wellestablished approach (Shokri et al., 2017;Carlini et al., 2021). Recently, the challenge of data contamination in large language models has garnered significant attention due to its potential to skew model evaluation and misrepresent true performance. Sainz et al. (2023) highlighted the risks of contamination, particularly emphasizing that while test set contamination invalidates benchmarks, contamination in training and validation sets is less concerning unless zero or few-shot learning claims are made. Contamination is most likely during the pre-training phase, where massive text corpora are scraped with minimal curation. Additionally, Balloccu et al. (2024) found that 42% of papers evaluating models such as GPT-3.5 and GPT-4 contained leaked data, affecting millions of instances, further underscoring the widespread impact of contamination. \n\nSeveral methods have been proposed to detect data contamination, focusing on either log probability analysis or prompting-based techniques. Log probability-based methods, such as those developed by Oren et al. (2024) and Shi et al. (2023), assess the likelihood of data being present in a model's training set. In contrast, prompting-based approaches by Golchin and Surdeanu (2024b) and Golchin and Surdeanu (2024a) directly query the model to detect contamination. However, these methods have primarily been tested on traditional benchmarks and early-stage models, leaving a gap in understanding their effectiveness on more recent, advanced LLMs and complex datasets. \n\nMoreover, much of the existing research assumes contamination occurs during pre-training, overlooking the potential impact of instruction  fine-tuning, which is increasingly used to enhance LLM capabilities. This stage can introduce variations in the data that are not adequately tested by current methods. Our work seeks to address these gaps by evaluating a broader range of detection techniques across diverse benchmarks and models, with particular attention to more challenging datasets and the instruction fine-tuning phase.",
                    "score": 0.7969932817188794,
                    "section_title": "Related Work",
                    "char_start_offset": 5247,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 157
                        },
                        {
                            "start": 158,
                            "end": 344
                        },
                        {
                            "start": 345,
                            "end": 602
                        },
                        {
                            "start": 603,
                            "end": 724
                        },
                        {
                            "start": 725,
                            "end": 947
                        },
                        {
                            "start": 950,
                            "end": 1089
                        },
                        {
                            "start": 1090,
                            "end": 1260
                        },
                        {
                            "start": 1261,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1618
                        },
                        {
                            "start": 1621,
                            "end": 1828
                        },
                        {
                            "start": 1829,
                            "end": 1927
                        },
                        {
                            "start": 1928,
                            "end": 2150
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1148,
                            "end": 1166,
                            "matchedPaperCorpusId": "264490730"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9736328125
                },
                {
                    "corpus_id": "272689919",
                    "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges",
                    "text": "We examine five distinct approaches to detecting data contamination in LLMs, including three state-of-the-art techniques from ICLR (2023-2024) and two exploratory prompt-based approaches. Two approaches are based on sequence probabilities and require access to model parameters. Figure 1 illustrates the visual overview of each approach. For each method, we also note the limitations we identified during our examination. The overview of these methods are illustrated in Figure 1. \n\n\u2022 Min-K% Prob (Shi et al., 2023) assesses whether a text was in an LLM's pre-training data by calculating the average log-likelihood of the k% lowestprobability tokens, with a high result suggesting the text's presence in the training data. Limitations: (1) The authors report AUC based on the proposed WiKiMIA dataset, in which they regarded data events before the model release as contaminated data. Such a strong assumption on the ground truth may require more justification. (2) They did not provide the threshold to determine the value of min-K%-prob in the paper since they claim they can use AUC; however, in real-world settings, we do not always have the oracle to determine AUC -instead, we need a metric for determining whether arbitrary datasets are contaminated. (3) The code is not available. \n\n\u2022 Canonical Order Statistical Testing (Oren et al., 2024) identifies contamination in a pre-training dataset by checking if the model shows a preference for the canonical order of examples over random shuffling. This preference is tested by comparing their log probabilities, with results aggregated across datasets to ensure a low false positive rate. Limitations: When an individual data example's length is long, the combination of sample/shards/permutations in the setup can be costly. \n\n\u2022 Token Completion Overlap Score (Golchin and Surdeanu, 2024b) detects contamination by prompting the LLM with a dataset name, partition type, and a random initial segment of a reference instance. If the LLM's output closely matches the latter part of the reference, the instance is flagged as contaminated.",
                    "score": 0.5236540627570234,
                    "section_title": "Evaluated Methods and Limitations",
                    "char_start_offset": 9487,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 278
                        },
                        {
                            "start": 279,
                            "end": 337
                        },
                        {
                            "start": 338,
                            "end": 421
                        },
                        {
                            "start": 422,
                            "end": 480
                        },
                        {
                            "start": 483,
                            "end": 723
                        },
                        {
                            "start": 724,
                            "end": 884
                        },
                        {
                            "start": 885,
                            "end": 961
                        },
                        {
                            "start": 962,
                            "end": 1257
                        },
                        {
                            "start": 1258,
                            "end": 1288
                        },
                        {
                            "start": 1291,
                            "end": 1502
                        },
                        {
                            "start": 1503,
                            "end": 1643
                        },
                        {
                            "start": 1644,
                            "end": 1780
                        },
                        {
                            "start": 1783,
                            "end": 1979
                        },
                        {
                            "start": 1980,
                            "end": 2090
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1329,
                            "end": 1348,
                            "matchedPaperCorpusId": "264490730"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.970703125
                },
                {
                    "corpus_id": "272689919",
                    "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges",
                    "text": "Such variations may cause difficulties in contamination detection yet are rarely considered when evalu-ating the effectiveness of contamination detection methods. Lastly, while various detection methods have demonstrated effectiveness (with a particular set of datasets and LLMs of their choice), there is a lack of a comprehensive crosscomparison to assess the consistency and reliability of these techniques, particularly for more recent LLMs, benchmarks, and training paradigms. \n\nTo bridge these gaps, we evaluate five distinct data contamination detection approaches, including three state-of-the-art methods recently published at ICLR, a simple prompting method based on token perturbation, and our proposed pilot prompt-based method, which queries the LLM about its knowledge of the original order of data points. Our study covers eight benchmarks, including six challenging datasets frequently used to evaluate modern LLMs and two traditional benchmarks. We apply these methods with four language models: GPT-4 (OpenAI, 2023), Claude 3 Sonnet, LLaMA-3-Chat (70B) (AI@Meta, 2024), and LLaMA-2-Chat (70B) (Touvron et al., 2023). To provide a gold standard for assessing the effectiveness of these methods, we create an oracle using LLaMA-2 (70B) by intentionally contaminating the model with varying portions of the six challenging benchmarks in the format of instruction fine-tuning with answer augmentation. This setup allows us to observe the performance of the five detection methods given the known contamination status. By these setups, we seek to answer the following research questions: i. Are the latest state-of-the-art LLMs, which consistently achieve higher performance, contaminated with these challenging benchmarks? What do the detection methods indicate? ii. Can these methods detect contamination that occurred during instruction fine-tuning with data variations instead of the original format in pretraining? iii. Do different \"well-accepted\" detection methods corroborate each other's findings for a given dataset? Do they yield inconsistent results? \n\nOur experimental results and analysis reveal several critical findings about current data contamination detection in LLMs: First, all existing methods have limitations in their underlying assumptions or practical applications.",
                    "score": 0.5242581553961256,
                    "section_title": "Introduction",
                    "char_start_offset": 2186,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 481
                        },
                        {
                            "start": 484,
                            "end": 820
                        },
                        {
                            "start": 821,
                            "end": 962
                        },
                        {
                            "start": 963,
                            "end": 1134
                        },
                        {
                            "start": 1135,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1531
                        },
                        {
                            "start": 1532,
                            "end": 1736
                        },
                        {
                            "start": 1737,
                            "end": 1776
                        },
                        {
                            "start": 1777,
                            "end": 1780
                        },
                        {
                            "start": 1781,
                            "end": 1932
                        },
                        {
                            "start": 1933,
                            "end": 1937
                        },
                        {
                            "start": 1938,
                            "end": 2039
                        },
                        {
                            "start": 2040,
                            "end": 2075
                        },
                        {
                            "start": 2078,
                            "end": 2304
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96142578125
                }
            ],
            "relevance_judgement": 0.9736328125,
            "relevance_judgment_input_expanded": "# Title: Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges\n# Venue: International Conference on Computational Linguistics\n# Authors: Vinay Samuel, Yue Zhou, Henry Peng Zou\n## Abstract\nAs large language models achieve increasingly impressive results, questions arise about whether such performance is from generalizability or mere data memorization. Thus, numerous data contamination detection methods have been proposed. However, these approaches are often validated with traditional benchmarks and early-stage LLMs, leaving uncertainty about their effectiveness when evaluating state-of-the-art LLMs on the contamination of more challenging benchmarks. To address this gap and provide a dual investigation of SOTA LLM contamination status and detection method robustness, we evaluate five contamination detection approaches with four state-of-the-art LLMs across eight challenging datasets often used in modern LLM evaluation. Our analysis reveals that (1) Current methods have non-trivial limitations in their assumptions and practical applications; (2) Notable difficulties exist in detecting contamination introduced during instruction fine-tuning with answer augmentation; and (3) Limited consistencies between SOTA contamination detection techniques. These findings highlight the complexity of contamination detection in advanced LLMs and the urgent need for further research on robust and generalizable contamination evaluation. Our code is available at https://github.com/vsamuel2003/data-contamination.\n## Introduction\nSuch variations may cause difficulties in contamination detection yet are rarely considered when evalu-ating the effectiveness of contamination detection methods. Lastly, while various detection methods have demonstrated effectiveness (with a particular set of datasets and LLMs of their choice), there is a lack of a comprehensive crosscomparison to assess the consistency and reliability of these techniques, particularly for more recent LLMs, benchmarks, and training paradigms. \n\nTo bridge these gaps, we evaluate five distinct data contamination detection approaches, including three state-of-the-art methods recently published at ICLR, a simple prompting method based on token perturbation, and our proposed pilot prompt-based method, which queries the LLM about its knowledge of the original order of data points. Our study covers eight benchmarks, including six challenging datasets frequently used to evaluate modern LLMs and two traditional benchmarks. We apply these methods with four language models: GPT-4 (OpenAI, 2023), Claude 3 Sonnet, LLaMA-3-Chat (70B) (AI@Meta, 2024), and LLaMA-2-Chat (70B) (Touvron et al., 2023). To provide a gold standard for assessing the effectiveness of these methods, we create an oracle using LLaMA-2 (70B) by intentionally contaminating the model with varying portions of the six challenging benchmarks in the format of instruction fine-tuning with answer augmentation. This setup allows us to observe the performance of the five detection methods given the known contamination status. By these setups, we seek to answer the following research questions: i. Are the latest state-of-the-art LLMs, which consistently achieve higher performance, contaminated with these challenging benchmarks? What do the detection methods indicate? ii. Can these methods detect contamination that occurred during instruction fine-tuning with data variations instead of the original format in pretraining? iii. Do different \"well-accepted\" detection methods corroborate each other's findings for a given dataset? Do they yield inconsistent results? \n\nOur experimental results and analysis reveal several critical findings about current data contamination detection in LLMs: First, all existing methods have limitations in their underlying assumptions or practical applications.\n\n## Related Work\nDetecting training data through probability inference and reconstruction has long been a wellestablished approach (Shokri et al., 2017;Carlini et al., 2021). Recently, the challenge of data contamination in large language models has garnered significant attention due to its potential to skew model evaluation and misrepresent true performance. Sainz et al. (2023) highlighted the risks of contamination, particularly emphasizing that while test set contamination invalidates benchmarks, contamination in training and validation sets is less concerning unless zero or few-shot learning claims are made. Contamination is most likely during the pre-training phase, where massive text corpora are scraped with minimal curation. Additionally, Balloccu et al. (2024) found that 42% of papers evaluating models such as GPT-3.5 and GPT-4 contained leaked data, affecting millions of instances, further underscoring the widespread impact of contamination. \n\nSeveral methods have been proposed to detect data contamination, focusing on either log probability analysis or prompting-based techniques. Log probability-based methods, such as those developed by Oren et al. (2024) and Shi et al. (2023), assess the likelihood of data being present in a model's training set. In contrast, prompting-based approaches by Golchin and Surdeanu (2024b) and Golchin and Surdeanu (2024a) directly query the model to detect contamination. However, these methods have primarily been tested on traditional benchmarks and early-stage models, leaving a gap in understanding their effectiveness on more recent, advanced LLMs and complex datasets. \n\nMoreover, much of the existing research assumes contamination occurs during pre-training, overlooking the potential impact of instruction  fine-tuning, which is increasingly used to enhance LLM capabilities. This stage can introduce variations in the data that are not adequately tested by current methods. Our work seeks to address these gaps by evaluating a broader range of detection techniques across diverse benchmarks and models, with particular attention to more challenging datasets and the instruction fine-tuning phase.\n\n## Evaluated Methods and Limitations\nWe examine five distinct approaches to detecting data contamination in LLMs, including three state-of-the-art techniques from ICLR (2023-2024) and two exploratory prompt-based approaches. Two approaches are based on sequence probabilities and require access to model parameters. Figure 1 illustrates the visual overview of each approach. For each method, we also note the limitations we identified during our examination. The overview of these methods are illustrated in Figure 1. \n\n\u2022 Min-K% Prob (Shi et al., 2023) assesses whether a text was in an LLM's pre-training data by calculating the average log-likelihood of the k% lowestprobability tokens, with a high result suggesting the text's presence in the training data. Limitations: (1) The authors report AUC based on the proposed WiKiMIA dataset, in which they regarded data events before the model release as contaminated data. Such a strong assumption on the ground truth may require more justification. (2) They did not provide the threshold to determine the value of min-K%-prob in the paper since they claim they can use AUC; however, in real-world settings, we do not always have the oracle to determine AUC -instead, we need a metric for determining whether arbitrary datasets are contaminated. (3) The code is not available. \n\n\u2022 Canonical Order Statistical Testing (Oren et al., 2024) identifies contamination in a pre-training dataset by checking if the model shows a preference for the canonical order of examples over random shuffling. This preference is tested by comparing their log probabilities, with results aggregated across datasets to ensure a low false positive rate. Limitations: When an individual data example's length is long, the combination of sample/shards/permutations in the setup can be costly. \n\n\u2022 Token Completion Overlap Score (Golchin and Surdeanu, 2024b) detects contamination by prompting the LLM with a dataset name, partition type, and a random initial segment of a reference instance. If the LLM's output closely matches the latter part of the reference, the instance is flagged as contaminated.",
            "reference_string": "[272689919 | Samuel et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Detecting Pretraining Data from Large Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 64,
            "citation_count": 201,
            "influential_citation_count": 51,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.16789, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2254168373",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2218438150",
                    "name": "Anirudh Ajith"
                },
                {
                    "authorId": "67284811",
                    "name": "Mengzhou Xia"
                },
                {
                    "authorId": "108053318",
                    "name": "Yangsibo Huang"
                },
                {
                    "authorId": "2261780806",
                    "name": "Daogao Liu"
                },
                {
                    "authorId": "3443287",
                    "name": "Terra Blevins"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                }
            ],
            "abstract": "Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous methods. We apply Min-K% Prob to two real-world scenarios, copyrighted book detection, and contaminated downstream example detection, and find it a consistently effective solution.",
            "corpus_id": 264451585,
            "sentences": [
                {
                    "corpus_id": "264451585",
                    "title": "Detecting Pretraining Data from Large Language Models",
                    "text": "Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous methods. We apply Min-K% Prob to two real-world scenarios, copyrighted book detection, and contaminated downstream example detection, and find it a consistently effective solution.",
                    "score": 0.6231854425943014,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9736328125
                },
                {
                    "corpus_id": "264451585",
                    "title": "Detecting Pretraining Data from Large Language Models",
                    "text": "We present a pre-training data detection dataset WIKIMIA and a new approach MIN-K% PROB. Our approach uses the intuition that trained data tends to contain fewer outlier tokens with very low probabilities compared to other baselines. Additionally, we verify the effectiveness of our approach in real-world setting, we perform two case studiies: detecting dataset contamination and published book detection. For dataset contamination, we observe empirical results aligning with theoretical predictions about how detection difficulty changes with dataset size, example frequency, and learning rate. Most strikingly, our book detection experiments provide strong evidence that GPT-3 models may have been trained on copyrighted books. System You are a helpful assistant in evaluating the similarity between two outputs generated by two different AI chatbots. Your goal is to rate the similarity between the two outputs based on a scale of 1 to 5, with 1 being highly dissimilar and 5 being highly similar.",
                    "score": 0.5521633793230777,
                    "section_title": "CONCLUSION",
                    "char_start_offset": 34800,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 88
                        },
                        {
                            "start": 89,
                            "end": 233
                        },
                        {
                            "start": 234,
                            "end": 406
                        },
                        {
                            "start": 407,
                            "end": 596
                        },
                        {
                            "start": 597,
                            "end": 730
                        },
                        {
                            "start": 731,
                            "end": 854
                        },
                        {
                            "start": 855,
                            "end": 1001
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.951171875
                },
                {
                    "corpus_id": "264451585",
                    "title": "Detecting Pretraining Data from Large Language Models",
                    "text": "Chowdhery et al. (2022) further improves this by deeming an example contaminated if 70% of its 8-grams appear in the training data. Touvron et al. (2023b) builds on these methods by extending the framework to tokenized inputs and judging a token to be contaminated if it appears in any token n-gram longer than 10 tokens. However, their methods require access to retraining corpora, which is largely unavailable for recent model releases. Other approaches try to detect contamination without access to pretraining corpora. Sainz et al. (2023) simply prompts ChatGPT to generate examples from a dataset by providing the dataset's name and split. They found that the models generate verbatim instances from NLP datasets. Golchin & Surdeanu (2023) extends this framework to extract more memorized instances by incorporating partial instance content into the prompt. While these methods study contamination in closed-sourced models, they can not determine contamination on an instance level.",
                    "score": 0.46186137106523967,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 33798,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 321
                        },
                        {
                            "start": 322,
                            "end": 438
                        },
                        {
                            "start": 439,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 644
                        },
                        {
                            "start": 645,
                            "end": 718
                        },
                        {
                            "start": 719,
                            "end": 862
                        },
                        {
                            "start": 863,
                            "end": 987
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93701171875
                },
                {
                    "corpus_id": "264451585",
                    "title": "Detecting Pretraining Data from Large Language Models",
                    "text": "As the scale of language model (LM) training corpora has grown, model developers (e.g, GPT-4 (Brown et al., 2020a) and LLaMA 2 (Touvron et al., 2023b)) have become reluctant to disclose the full composition or sources of their data. This lack of transparency poses critical challenges to scientific model evaluation and ethical deployment. Critical private information may be exposed during pretraining; previous work showed that LLMs generated excerpts from copyrighted books (Chang et al., 2023) and personal emails (Mozes et al., 2023), potentially infringing upon the legal rights of original content creators and violating their privacy. Additionally, Sainz et al. (2023); Magar & Schwartz (2022); Narayanan (2023) showed that the pretraining corpus may inadvertently include benchmark evaluation data, making it difficult to assess the effectiveness of these models. \n\nIn this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM with no knowledge of its pretraining data, can we determine if the model was pretrained on the text? We present a benchmark, WIKIMIA, and an approach, MIN-K% PROB, for pretraining data detection. This problem is an instance of Membership Inference Attacks (MIAs), which was initially proposed by Shokri et al. (2016). Recent work has studied fine-tuning data detection (Song & Shmatikov, 2019;Shejwalkar et al., 2021;Mahloujifar et al., 2021) as an MIA problem. However, adopting these methods to detect the pertaining data of contemporary large LLMs presents two unique technical challenges: First, unlike fine-tuning which usually runs for multiple epochs, pretraining uses a much larger dataset but exposes each instance only once, significantly Figure 1: Overview of MIN-K% PROB. To determine whether a text X is in the pretraining data of a LLM such as GPT, MIN-K% PROB first gets the probability for each token in X, selects the k% tokens with minimum probabilities and calculates their average log likelihood.",
                    "score": 0.5303737253086556,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 232
                        },
                        {
                            "start": 233,
                            "end": 339
                        },
                        {
                            "start": 340,
                            "end": 642
                        },
                        {
                            "start": 643,
                            "end": 872
                        },
                        {
                            "start": 875,
                            "end": 1092
                        },
                        {
                            "start": 1093,
                            "end": 1187
                        },
                        {
                            "start": 1188,
                            "end": 1309
                        },
                        {
                            "start": 1310,
                            "end": 1453
                        },
                        {
                            "start": 1454,
                            "end": 1775
                        },
                        {
                            "start": 1776,
                            "end": 2008
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1288,
                            "end": 1308,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1361,
                            "end": 1385,
                            "matchedPaperCorpusId": "159041003"
                        },
                        {
                            "start": 1385,
                            "end": 1409,
                            "matchedPaperCorpusId": "245222525"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93359375
                }
            ],
            "relevance_judgement": 0.9736328125,
            "relevance_judgment_input_expanded": "# Title: Detecting Pretraining Data from Large Language Models\n# Venue: International Conference on Learning Representations\n# Authors: Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke S. Zettlemoyer\n## Abstract\nAlthough large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous methods. We apply Min-K% Prob to two real-world scenarios, copyrighted book detection, and contaminated downstream example detection, and find it a consistently effective solution.\n## INTRODUCTION\nAs the scale of language model (LM) training corpora has grown, model developers (e.g, GPT-4 (Brown et al., 2020a) and LLaMA 2 (Touvron et al., 2023b)) have become reluctant to disclose the full composition or sources of their data. This lack of transparency poses critical challenges to scientific model evaluation and ethical deployment. Critical private information may be exposed during pretraining; previous work showed that LLMs generated excerpts from copyrighted books (Chang et al., 2023) and personal emails (Mozes et al., 2023), potentially infringing upon the legal rights of original content creators and violating their privacy. Additionally, Sainz et al. (2023); Magar & Schwartz (2022); Narayanan (2023) showed that the pretraining corpus may inadvertently include benchmark evaluation data, making it difficult to assess the effectiveness of these models. \n\nIn this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM with no knowledge of its pretraining data, can we determine if the model was pretrained on the text? We present a benchmark, WIKIMIA, and an approach, MIN-K% PROB, for pretraining data detection. This problem is an instance of Membership Inference Attacks (MIAs), which was initially proposed by Shokri et al. (2016). Recent work has studied fine-tuning data detection (Song & Shmatikov, 2019;Shejwalkar et al., 2021;Mahloujifar et al., 2021) as an MIA problem. However, adopting these methods to detect the pertaining data of contemporary large LLMs presents two unique technical challenges: First, unlike fine-tuning which usually runs for multiple epochs, pretraining uses a much larger dataset but exposes each instance only once, significantly Figure 1: Overview of MIN-K% PROB. To determine whether a text X is in the pretraining data of a LLM such as GPT, MIN-K% PROB first gets the probability for each token in X, selects the k% tokens with minimum probabilities and calculates their average log likelihood.\n\n## RELATED WORK\nChowdhery et al. (2022) further improves this by deeming an example contaminated if 70% of its 8-grams appear in the training data. Touvron et al. (2023b) builds on these methods by extending the framework to tokenized inputs and judging a token to be contaminated if it appears in any token n-gram longer than 10 tokens. However, their methods require access to retraining corpora, which is largely unavailable for recent model releases. Other approaches try to detect contamination without access to pretraining corpora. Sainz et al. (2023) simply prompts ChatGPT to generate examples from a dataset by providing the dataset's name and split. They found that the models generate verbatim instances from NLP datasets. Golchin & Surdeanu (2023) extends this framework to extract more memorized instances by incorporating partial instance content into the prompt. While these methods study contamination in closed-sourced models, they can not determine contamination on an instance level.\n\n## CONCLUSION\nWe present a pre-training data detection dataset WIKIMIA and a new approach MIN-K% PROB. Our approach uses the intuition that trained data tends to contain fewer outlier tokens with very low probabilities compared to other baselines. Additionally, we verify the effectiveness of our approach in real-world setting, we perform two case studiies: detecting dataset contamination and published book detection. For dataset contamination, we observe empirical results aligning with theoretical predictions about how detection difficulty changes with dataset size, example frequency, and learning rate. Most strikingly, our book detection experiments provide strong evidence that GPT-3 models may have been trained on copyrighted books. System You are a helpful assistant in evaluating the similarity between two outputs generated by two different AI chatbots. Your goal is to rate the similarity between the two outputs based on a scale of 1 to 5, with 1 being highly dissimilar and 5 being highly similar.",
            "reference_string": "[264451585 | Shi et al. | 2023 | Citations: 201]"
        },
        {
            "title": "PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.18326, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2288559437",
                    "name": "Huixuan Zhang"
                },
                {
                    "authorId": "2297335276",
                    "name": "Yun Lin"
                },
                {
                    "authorId": "2288537941",
                    "name": "Xiaojun Wan"
                }
            ],
            "abstract": "Large language models (LLMs) are known to be trained on vast amounts of data, which may unintentionally or intentionally include data from commonly used benchmarks. This inclusion can lead to cheatingly high scores on model leaderboards, yet result in disappointing performance in real-world applications. To address this benchmark contamination problem, we first propose a set of requirements that practical contamination detection methods should follow. Following these proposed requirements, we introduce PaCoST, a Paired Confidence Significance Testing to effectively detect benchmark contamination in LLMs. Our method constructs a counterpart for each piece of data with the same distribution, and performs statistical analysis of the corresponding confidence to test whether the model is significantly more confident under the original benchmark. We validate the effectiveness of PaCoST and apply it on popular open-source models and benchmarks. We find that almost all models and benchmarks we tested are suspected contaminated more or less. We finally call for new LLM evaluation methods.",
            "corpus_id": 270737802,
            "sentences": [
                {
                    "corpus_id": "270737802",
                    "title": "PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models",
                    "text": "Large language models (LLMs) are known to be trained on vast amounts of data, which may unintentionally or intentionally include data from commonly used benchmarks. This inclusion can lead to cheatingly high scores on model leaderboards, yet result in disappointing performance in real-world applications. To address this benchmark contamination problem, we first propose a set of requirements that practical contamination detection methods should follow. Following these proposed requirements, we introduce PaCoST, a Paired Confidence Significance Testing to effectively detect benchmark contamination in LLMs. Our method constructs a counterpart for each piece of data with the same distribution, and performs statistical analysis of the corresponding confidence to test whether the model is significantly more confident under the original benchmark. We validate the effectiveness of PaCoST and apply it on popular open-source models and benchmarks. We find that almost all models and benchmarks we tested are suspected contaminated more or less. We finally call for new LLM evaluation methods.",
                    "score": 0.6176002090990811,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9736328125
                },
                {
                    "corpus_id": "270737802",
                    "title": "PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models",
                    "text": "Large Language Models (LLMs) have brought about a paradigm shift in the domain of natural language processing, yielding notable enhancements across various evaluation benchmarks (Wang et al., 2019) and demonstrating proficiency in professional examinations (OpenAI, 2023). These advancements primarily stem from extensive training on vast and diverse datasets sourced from multiple origins. However, the substantial volume of data has given rise to significant concerns regarding benchmark contamination, where benchmarks for LLM evaluation are inadvertently or deliberately included in model training. This contamination presents considerable obstacles in accurately gauging the capabilities of LLMs. \n\nWhile efforts are being made to address this issue by removing benchmarks from training datasets and conducting contamination studies, these endeavors face numerous limitations (Brown et al., 2020a;Zhang et al., 2024;Wei et al., 2022;Chowdhery et al., 2022). These limitations include narrow focus on specific benchmarks and reliance on the trustworthiness of vendors. Moreover, the competitive dynamics within the field, coupled with copyright considerations, have resulted in recent model releases lacking accompanying contamination studies (OpenAI, 2023). Hence, there is an urgent necessity for independent methods to audit LLMs for the presence of benchmark datasets, eliminating the dependence on model providers' cooperation. \n\nSimultaneously, there has been a growing interest in heuristic membership inference algorithms designed to reverse-engineer aspects of the training dataset (Carlini et al., 2021a;Mattern et al., 2023), thereby providing insights into potential test set contamination (Sainz et al., 2023a;Golchin and Surdeanu, 2023b). Despite their promise, these heuristic approaches often lack definitive proof of contamination and tend to rely on assumptions that may be too stringent. Moreover, the majority of these methods concentrate less on detecting benchmark contamination. As elaborated in Section 3.1, inherent challenges, such as the need for lengthy trained segments and the necessity of establishing thresholds, impede the adaptation of previous methods for detecting benchmark contamination. \n\nIn this study, we introduce a novel approach named PaCoST (Paired Confidence Significance Testing) designed for the detection of benchmark contamination in open-source LLMs.",
                    "score": 0.7135075576675742,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 272
                        },
                        {
                            "start": 273,
                            "end": 390
                        },
                        {
                            "start": 391,
                            "end": 602
                        },
                        {
                            "start": 603,
                            "end": 701
                        },
                        {
                            "start": 704,
                            "end": 962
                        },
                        {
                            "start": 963,
                            "end": 1072
                        },
                        {
                            "start": 1073,
                            "end": 1262
                        },
                        {
                            "start": 1263,
                            "end": 1436
                        },
                        {
                            "start": 1439,
                            "end": 1756
                        },
                        {
                            "start": 1757,
                            "end": 1910
                        },
                        {
                            "start": 1911,
                            "end": 2005
                        },
                        {
                            "start": 2006,
                            "end": 2229
                        },
                        {
                            "start": 2232,
                            "end": 2405
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 881,
                            "end": 902,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1618,
                            "end": 1639,
                            "matchedPaperCorpusId": "258967264"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94140625
                },
                {
                    "corpus_id": "270737802",
                    "title": "PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models",
                    "text": "In this study, we introduce a novel approach named PaCoST (Paired Confidence Significance Testing) designed for the detection of benchmark contamination in open-source LLMs. Our method entails a three-step statistical analysis, capable of identifying benchmarks within the model's training data. Specifically, our approach involves constructing counterparts for each data instance with similar distribution, followed by statistical analysis of corresponding confidence scores to ascertain whether the model exhibits significantly higher confidence when presented with original benchmarks. We operate under the assumption that the model tends to demonstrate greater confidence when responding to questions it has been trained on. To validate our method rigorously, we conduct a series of controlled experiments. \n\nSubsequently, we employ PaCoST across a diverse array of publicly accessible LLMs, scrutinizing various benchmarks to reveal contamination outcomes. Our experimental observations indicate that, across the board, there are suspicions of contamination to varying degrees in both models and benchmarks. Consequently, we advocate for the adoption of a benchmark-free evaluation approach as a means to mitigate this contamination issue. \n\nOur contributions can be summarized as follows: \n\n\u2022 We propose several properties which a good benchmark contamination detection method should satisfy. \n\n\u2022 We introduce a simple yet effective method PaCoST to detect benchmark contamination in LLMs and validate its effectiveness and stability. \n\n\u2022 We conduct experiments on popular opensource LLMs and benchmarks and find suspected contamination on almost all tested models and benchmarks. \n\n2 Related Works",
                    "score": 0.47592382574891234,
                    "section_title": "Introduction",
                    "char_start_offset": 2247,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 173
                        },
                        {
                            "start": 174,
                            "end": 295
                        },
                        {
                            "start": 296,
                            "end": 588
                        },
                        {
                            "start": 589,
                            "end": 728
                        },
                        {
                            "start": 729,
                            "end": 810
                        },
                        {
                            "start": 813,
                            "end": 961
                        },
                        {
                            "start": 962,
                            "end": 1112
                        },
                        {
                            "start": 1113,
                            "end": 1244
                        },
                        {
                            "start": 1247,
                            "end": 1294
                        },
                        {
                            "start": 1297,
                            "end": 1398
                        },
                        {
                            "start": 1401,
                            "end": 1540
                        },
                        {
                            "start": 1543,
                            "end": 1686
                        },
                        {
                            "start": 1689,
                            "end": 1704
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9404296875
                }
            ],
            "relevance_judgement": 0.9736328125,
            "relevance_judgment_input_expanded": "# Title: PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Huixuan Zhang, Yun Lin, Xiaojun Wan\n## Abstract\nLarge language models (LLMs) are known to be trained on vast amounts of data, which may unintentionally or intentionally include data from commonly used benchmarks. This inclusion can lead to cheatingly high scores on model leaderboards, yet result in disappointing performance in real-world applications. To address this benchmark contamination problem, we first propose a set of requirements that practical contamination detection methods should follow. Following these proposed requirements, we introduce PaCoST, a Paired Confidence Significance Testing to effectively detect benchmark contamination in LLMs. Our method constructs a counterpart for each piece of data with the same distribution, and performs statistical analysis of the corresponding confidence to test whether the model is significantly more confident under the original benchmark. We validate the effectiveness of PaCoST and apply it on popular open-source models and benchmarks. We find that almost all models and benchmarks we tested are suspected contaminated more or less. We finally call for new LLM evaluation methods.\n## Introduction\nLarge Language Models (LLMs) have brought about a paradigm shift in the domain of natural language processing, yielding notable enhancements across various evaluation benchmarks (Wang et al., 2019) and demonstrating proficiency in professional examinations (OpenAI, 2023). These advancements primarily stem from extensive training on vast and diverse datasets sourced from multiple origins. However, the substantial volume of data has given rise to significant concerns regarding benchmark contamination, where benchmarks for LLM evaluation are inadvertently or deliberately included in model training. This contamination presents considerable obstacles in accurately gauging the capabilities of LLMs. \n\nWhile efforts are being made to address this issue by removing benchmarks from training datasets and conducting contamination studies, these endeavors face numerous limitations (Brown et al., 2020a;Zhang et al., 2024;Wei et al., 2022;Chowdhery et al., 2022). These limitations include narrow focus on specific benchmarks and reliance on the trustworthiness of vendors. Moreover, the competitive dynamics within the field, coupled with copyright considerations, have resulted in recent model releases lacking accompanying contamination studies (OpenAI, 2023). Hence, there is an urgent necessity for independent methods to audit LLMs for the presence of benchmark datasets, eliminating the dependence on model providers' cooperation. \n\nSimultaneously, there has been a growing interest in heuristic membership inference algorithms designed to reverse-engineer aspects of the training dataset (Carlini et al., 2021a;Mattern et al., 2023), thereby providing insights into potential test set contamination (Sainz et al., 2023a;Golchin and Surdeanu, 2023b). Despite their promise, these heuristic approaches often lack definitive proof of contamination and tend to rely on assumptions that may be too stringent. Moreover, the majority of these methods concentrate less on detecting benchmark contamination. As elaborated in Section 3.1, inherent challenges, such as the need for lengthy trained segments and the necessity of establishing thresholds, impede the adaptation of previous methods for detecting benchmark contamination. \n\nIn this study, we introduce a novel approach named PaCoST (Paired Confidence Significance Testing) designed for the detection of benchmark contamination in open-source LLMs.\n...\nIn this study, we introduce a novel approach named PaCoST (Paired Confidence Significance Testing) designed for the detection of benchmark contamination in open-source LLMs. Our method entails a three-step statistical analysis, capable of identifying benchmarks within the model's training data. Specifically, our approach involves constructing counterparts for each data instance with similar distribution, followed by statistical analysis of corresponding confidence scores to ascertain whether the model exhibits significantly higher confidence when presented with original benchmarks. We operate under the assumption that the model tends to demonstrate greater confidence when responding to questions it has been trained on. To validate our method rigorously, we conduct a series of controlled experiments. \n\nSubsequently, we employ PaCoST across a diverse array of publicly accessible LLMs, scrutinizing various benchmarks to reveal contamination outcomes. Our experimental observations indicate that, across the board, there are suspicions of contamination to varying degrees in both models and benchmarks. Consequently, we advocate for the adoption of a benchmark-free evaluation approach as a means to mitigate this contamination issue. \n\nOur contributions can be summarized as follows: \n\n\u2022 We propose several properties which a good benchmark contamination detection method should satisfy. \n\n\u2022 We introduce a simple yet effective method PaCoST to detect benchmark contamination in LLMs and validate its effectiveness and stability. \n\n\u2022 We conduct experiments on popular opensource LLMs and benchmarks and find suspected contamination on almost all tested models and benchmarks. \n\n2 Related Works",
            "reference_string": "[270737802 | Zhang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Benchmark Data Contamination of Large Language Models: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 164,
            "citation_count": 56,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.04244, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153079650",
                    "name": "Cheng Xu"
                },
                {
                    "authorId": "2304954016",
                    "name": "Shuhao Guan"
                },
                {
                    "authorId": "2304952028",
                    "name": "Derek Greene"
                },
                {
                    "authorId": "2266906186",
                    "name": "Mohand-Tahar Kechadi"
                }
            ],
            "abstract": "The rapid development of Large Language Models (LLMs) like GPT-4, Claude-3, and Gemini has transformed the field of natural language processing. However, it has also resulted in a significant issue known as Benchmark Data Contamination (BDC). This occurs when language models inadvertently incorporate evaluation benchmark information from their training data, leading to inaccurate or unreliable performance during the evaluation phase of the process. This paper reviews the complex challenge of BDC in LLM evaluation and explores alternative assessment methods to mitigate the risks associated with traditional benchmarks. The paper also examines challenges and future directions in mitigating BDC risks, highlighting the complexity of the issue and the need for innovative solutions to ensure the reliability of LLM evaluation in real-world applications.",
            "corpus_id": 270285708,
            "sentences": [
                {
                    "corpus_id": "270285708",
                    "title": "Benchmark Data Contamination of Large Language Models: A Survey",
                    "text": "Magar and Schwartz [96] presented a method to detect contaminated data in downstream tasks, they tested pre-training BERT models on corpora that include Wikipedia and labeled downstream datasets, then fine-tuning them on relevant tasks, and then detected BDC by comparing the performance of model-generated content from \"seen\" and \"unseen\" evaluation datasets.Their experiments reveal that while some models do exploit contaminated data, others merely memorize them without exploitation.The study shows that the level of memorization and exploitation is influenced by factors such as the number of data duplications and model size.\n\nDong et al. [34] focused on the distribution of generated content, they proposed a novel method, CDD (Contamination Detection via output Distribution), which uses the output distribution of LLMs to detect BDC.They also introduce TED (Trustworthy Evaluation via output Distribution), which corrects the output distribution to mitigate the effects of contamination.Through extensive experiments, they demonstrate that CDD can significantly improve contamination detection over existing methods and TED can reduce performance inflation due to contamination.The paper also presents two new benchmarks, DetCon and ComiEval, for assessing BDC and mitigation methods.Their findings reveal that popular models like ChatGPT are susceptible to BDC, emphasizing the need for more reliable evaluation methods.\n\nDifferent from the distribution, Li [89] proposed a novel method to detect BDC in language model evaluation without requiring access to the full training set.The technique uses perplexity to measure the extent of contamination, providing evidence of significant memorization in recent foundation models across various benchmarks.The study reveals that while reading comprehension and summarisation benchmarks show signs of contamination, multiple-choice benchmarks appear less affected.This method allows for a more accessible and less computationally intensive way to audit language models for contamination, ensuring more reliable evaluations.\n\nAn alternative interesting perspective is to focus on the order of content generated by LLMs.Oren et al. [108] presented a method to detect test set contamination in language models without needing access to the model's pre-training data or weights.The authors used a statistical test to identify contamination by comparing the likelihood of a benchmark dataset's canonical ordering against a shuffled version.",
                    "score": 0.6677295648908108,
                    "section_title": "Comparison-based Methods",
                    "char_start_offset": 33001,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 360
                        },
                        {
                            "start": 360,
                            "end": 487
                        },
                        {
                            "start": 487,
                            "end": 631
                        },
                        {
                            "start": 633,
                            "end": 842
                        },
                        {
                            "start": 842,
                            "end": 996
                        },
                        {
                            "start": 996,
                            "end": 1187
                        },
                        {
                            "start": 1187,
                            "end": 1293
                        },
                        {
                            "start": 1293,
                            "end": 1430
                        },
                        {
                            "start": 1432,
                            "end": 1590
                        },
                        {
                            "start": 1590,
                            "end": 1761
                        },
                        {
                            "start": 1761,
                            "end": 1918
                        },
                        {
                            "start": 1918,
                            "end": 2077
                        },
                        {
                            "start": 2079,
                            "end": 2172
                        },
                        {
                            "start": 2172,
                            "end": 2328
                        },
                        {
                            "start": 2328,
                            "end": 2489
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 19,
                            "end": 23,
                            "matchedPaperCorpusId": "247475929"
                        },
                        {
                            "start": 2184,
                            "end": 2189,
                            "matchedPaperCorpusId": "264490730"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97265625
                },
                {
                    "corpus_id": "270285708",
                    "title": "Benchmark Data Contamination of Large Language Models: A Survey",
                    "text": "In each case, contamination can potentially lead to distorted performance scores that do not accurately reflect the model's true capabilities.The severity of BDC, as categorized into Semantic Level, Information Level, Data Level, and Label Level, increases as we move closer to full exposure of the benchmark data.The complexity of detecting and mitigating BDC inversely correlates with the severity of exposure, making it a challenging problem to address.The primary sources of BDC are the large-scale pre-training datasets used in training LLMs, which due to their diversity and complexity, can inadvertently introduce the risk of BDC.Highlighting the potential scenarios of BDC occurrence across seven prevalent LLM tasks underscores the critical necessity of addressing this issue for precise model evaluation and performance enhancement in the domain of NLP. 3 BDC DETECTION TECHNIQUES Efficient identification of BDC in evaluation benchmarks represents a fundamental aspect in ensuring the reliability and integrity of LLMs, while also providing the basis for developing effective mitigation strategies.This section provides a comprehensive review of literature related to the detection of BDC.We categorize the methodologies into two distinct strategies: matching-based and comparison-based, discussed in Sections 3.1 and 3.2, respectively.Note that certain investigations incorporate elements from both strategies.Such instances are allocated to the category deemed more comprehensive or preferred by the authors in question.All reviewed work on BDC detection is summarized in Table 2.",
                    "score": 0.5135352967828853,
                    "section_title": "Related Tasks",
                    "char_start_offset": 23354,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 142
                        },
                        {
                            "start": 142,
                            "end": 314
                        },
                        {
                            "start": 314,
                            "end": 456
                        },
                        {
                            "start": 456,
                            "end": 637
                        },
                        {
                            "start": 637,
                            "end": 1109
                        },
                        {
                            "start": 1109,
                            "end": 1200
                        },
                        {
                            "start": 1200,
                            "end": 1347
                        },
                        {
                            "start": 1347,
                            "end": 1422
                        },
                        {
                            "start": 1422,
                            "end": 1533
                        },
                        {
                            "start": 1533,
                            "end": 1593
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96435546875
                },
                {
                    "corpus_id": "270285708",
                    "title": "Benchmark Data Contamination of Large Language Models: A Survey",
                    "text": "The Pile Similarly, as shown in Figure 1, Deng et al. [30] proposed two novel methods to detect potential overlaps between evaluation benchmarks and pre-training corpora, tailored for both open-source and proprietary LLMs.They introduced a retrieval-based system and a Testset Slot Guessing (TS-Guessing) protocol, which involves masking incorrect answers in multiple-choice questions and prompting the model to fill in the gaps.Their findings indicated that commercial LLMs, including ChatGPT and GPT-4, can guess missing options in benchmark tests with a high level of accuracy.\n\nGolchin and Surdeanu [46] presented another novel approach, the Data Contamination Quiz (DCQ), to detect and quantify BDC in LLMs.The DCQ is a series of multiple-choice questions with three perturbed versions of each dataset instance, including only word-level changes.The LLM's ability to identify the original instance among the perturbed ones indicates potential exposure to the data during pre-training.Tested on GPT-3.5/4, the DCQ demonstrated higher contamination , Vol. 1, No. 1, Article .Publication date: June 2024.levels than other methods and effectively bypassed safety filters designed to prevent the generation of copyrighted content.\n\nGolchin and Surdeanu [47] presented a technique that combines instance-level identification using guided instruction prompts with partition-level assessment through overlap score comparison and classifier-based detection.The approach achieved high accuracy rates, between 92% and 100%, across seven datasets.The authors also found specific cases of contamination in popular datasets, such as AG News, WNLI, and XSum, when tested with GPT-4.\n\nLi et al. [91] presented a comprehensive report on BDC across over 15 LLMs and six multiplechoice QA benchmarks.They introduced an open-source pipeline to conduct contamination analysis on customized data and models.Their research uncovered varying degrees of contamination, ranging from 1% to 45%, and demonstrated that contamination does not always correlate with improved model performance.Interestingly, larger models may benefit more from contaminated test sets than smaller ones, with significant accuracy boosts observed on certain benchmarks.",
                    "score": 0.5216721388191621,
                    "section_title": "Pretrain Corpus",
                    "char_start_offset": 27303,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 222
                        },
                        {
                            "start": 222,
                            "end": 429
                        },
                        {
                            "start": 429,
                            "end": 580
                        },
                        {
                            "start": 582,
                            "end": 712
                        },
                        {
                            "start": 712,
                            "end": 851
                        },
                        {
                            "start": 851,
                            "end": 989
                        },
                        {
                            "start": 989,
                            "end": 1078
                        },
                        {
                            "start": 1078,
                            "end": 1106
                        },
                        {
                            "start": 1106,
                            "end": 1230
                        },
                        {
                            "start": 1232,
                            "end": 1453
                        },
                        {
                            "start": 1453,
                            "end": 1540
                        },
                        {
                            "start": 1540,
                            "end": 1672
                        },
                        {
                            "start": 1674,
                            "end": 1786
                        },
                        {
                            "start": 1786,
                            "end": 1890
                        },
                        {
                            "start": 1890,
                            "end": 2067
                        },
                        {
                            "start": 2067,
                            "end": 2224
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1253,
                            "end": 1257,
                            "matchedPaperCorpusId": "260925501"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.923828125
                }
            ],
            "relevance_judgement": 0.97265625,
            "relevance_judgment_input_expanded": "# Title: Benchmark Data Contamination of Large Language Models: A Survey\n# Venue: arXiv.org\n# Authors: Cheng Xu, Shuhao Guan, Derek Greene, Mohand-Tahar Kechadi\n## Abstract\nThe rapid development of Large Language Models (LLMs) like GPT-4, Claude-3, and Gemini has transformed the field of natural language processing. However, it has also resulted in a significant issue known as Benchmark Data Contamination (BDC). This occurs when language models inadvertently incorporate evaluation benchmark information from their training data, leading to inaccurate or unreliable performance during the evaluation phase of the process. This paper reviews the complex challenge of BDC in LLM evaluation and explores alternative assessment methods to mitigate the risks associated with traditional benchmarks. The paper also examines challenges and future directions in mitigating BDC risks, highlighting the complexity of the issue and the need for innovative solutions to ensure the reliability of LLM evaluation in real-world applications.\n## Related Tasks\nIn each case, contamination can potentially lead to distorted performance scores that do not accurately reflect the model's true capabilities.The severity of BDC, as categorized into Semantic Level, Information Level, Data Level, and Label Level, increases as we move closer to full exposure of the benchmark data.The complexity of detecting and mitigating BDC inversely correlates with the severity of exposure, making it a challenging problem to address.The primary sources of BDC are the large-scale pre-training datasets used in training LLMs, which due to their diversity and complexity, can inadvertently introduce the risk of BDC.Highlighting the potential scenarios of BDC occurrence across seven prevalent LLM tasks underscores the critical necessity of addressing this issue for precise model evaluation and performance enhancement in the domain of NLP. 3 BDC DETECTION TECHNIQUES Efficient identification of BDC in evaluation benchmarks represents a fundamental aspect in ensuring the reliability and integrity of LLMs, while also providing the basis for developing effective mitigation strategies.This section provides a comprehensive review of literature related to the detection of BDC.We categorize the methodologies into two distinct strategies: matching-based and comparison-based, discussed in Sections 3.1 and 3.2, respectively.Note that certain investigations incorporate elements from both strategies.Such instances are allocated to the category deemed more comprehensive or preferred by the authors in question.All reviewed work on BDC detection is summarized in Table 2.\n\n## Pretrain Corpus\nThe Pile Similarly, as shown in Figure 1, Deng et al. [30] proposed two novel methods to detect potential overlaps between evaluation benchmarks and pre-training corpora, tailored for both open-source and proprietary LLMs.They introduced a retrieval-based system and a Testset Slot Guessing (TS-Guessing) protocol, which involves masking incorrect answers in multiple-choice questions and prompting the model to fill in the gaps.Their findings indicated that commercial LLMs, including ChatGPT and GPT-4, can guess missing options in benchmark tests with a high level of accuracy.\n\nGolchin and Surdeanu [46] presented another novel approach, the Data Contamination Quiz (DCQ), to detect and quantify BDC in LLMs.The DCQ is a series of multiple-choice questions with three perturbed versions of each dataset instance, including only word-level changes.The LLM's ability to identify the original instance among the perturbed ones indicates potential exposure to the data during pre-training.Tested on GPT-3.5/4, the DCQ demonstrated higher contamination , Vol. 1, No. 1, Article .Publication date: June 2024.levels than other methods and effectively bypassed safety filters designed to prevent the generation of copyrighted content.\n\nGolchin and Surdeanu [47] presented a technique that combines instance-level identification using guided instruction prompts with partition-level assessment through overlap score comparison and classifier-based detection.The approach achieved high accuracy rates, between 92% and 100%, across seven datasets.The authors also found specific cases of contamination in popular datasets, such as AG News, WNLI, and XSum, when tested with GPT-4.\n\nLi et al. [91] presented a comprehensive report on BDC across over 15 LLMs and six multiplechoice QA benchmarks.They introduced an open-source pipeline to conduct contamination analysis on customized data and models.Their research uncovered varying degrees of contamination, ranging from 1% to 45%, and demonstrated that contamination does not always correlate with improved model performance.Interestingly, larger models may benefit more from contaminated test sets than smaller ones, with significant accuracy boosts observed on certain benchmarks.\n\n## Comparison-based Methods\nMagar and Schwartz [96] presented a method to detect contaminated data in downstream tasks, they tested pre-training BERT models on corpora that include Wikipedia and labeled downstream datasets, then fine-tuning them on relevant tasks, and then detected BDC by comparing the performance of model-generated content from \"seen\" and \"unseen\" evaluation datasets.Their experiments reveal that while some models do exploit contaminated data, others merely memorize them without exploitation.The study shows that the level of memorization and exploitation is influenced by factors such as the number of data duplications and model size.\n\nDong et al. [34] focused on the distribution of generated content, they proposed a novel method, CDD (Contamination Detection via output Distribution), which uses the output distribution of LLMs to detect BDC.They also introduce TED (Trustworthy Evaluation via output Distribution), which corrects the output distribution to mitigate the effects of contamination.Through extensive experiments, they demonstrate that CDD can significantly improve contamination detection over existing methods and TED can reduce performance inflation due to contamination.The paper also presents two new benchmarks, DetCon and ComiEval, for assessing BDC and mitigation methods.Their findings reveal that popular models like ChatGPT are susceptible to BDC, emphasizing the need for more reliable evaluation methods.\n\nDifferent from the distribution, Li [89] proposed a novel method to detect BDC in language model evaluation without requiring access to the full training set.The technique uses perplexity to measure the extent of contamination, providing evidence of significant memorization in recent foundation models across various benchmarks.The study reveals that while reading comprehension and summarisation benchmarks show signs of contamination, multiple-choice benchmarks appear less affected.This method allows for a more accessible and less computationally intensive way to audit language models for contamination, ensuring more reliable evaluations.\n\nAn alternative interesting perspective is to focus on the order of content generated by LLMs.Oren et al. [108] presented a method to detect test set contamination in language models without needing access to the model's pre-training data or weights.The authors used a statistical test to identify contamination by comparing the likelihood of a benchmark dataset's canonical ordering against a shuffled version.",
            "reference_string": "[270285708 | Xu et al. | 2024 | Citations: 56]"
        },
        {
            "title": "CAP: Data Contamination Detection via Consistency Amplification",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 35,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.15005, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284551755",
                    "name": "Yi Zhao"
                },
                {
                    "authorId": "2282848438",
                    "name": "Jing Li"
                },
                {
                    "authorId": "2327143577",
                    "name": "Linyi Yang"
                }
            ],
            "abstract": "Large language models (LLMs) are widely used, but concerns about data contamination challenge the reliability of LLM evaluations. Existing contamination detection methods are often task-specific or require extra prerequisites, limiting practicality. We propose a novel framework, Consistency Amplification-based Data Contamination Detection (CAP), which introduces the Performance Consistency Ratio (PCR) to measure dataset leakage by leveraging LM consistency. To the best of our knowledge, this is the first method to explicitly differentiate between fine-tuning and contamination, which is crucial for detecting contamination in domain-specific models. Additionally, CAP is applicable to various benchmarks and works for both white-box and black-box models. We validate CAP's effectiveness through experiments on seven LLMs and four domain-specific benchmarks. Our findings also show that composite benchmarks from various dataset sources are particularly prone to unintentional contamination. Codes will be publicly available soon.",
            "corpus_id": 273502439,
            "sentences": [
                {
                    "corpus_id": "273502439",
                    "title": "CAP: Data Contamination Detection via Consistency Amplification",
                    "text": "Large language models (LLMs) are widely used, but concerns about data contamination challenge the reliability of LLM evaluations. Existing contamination detection methods are often task-specific or require extra prerequisites, limiting practicality. We propose a novel framework, Consistency Amplification-based Data Contamination Detection (CAP), which introduces the Performance Consistency Ratio (PCR) to measure dataset leakage by leveraging LM consistency. To the best of our knowledge, this is the first method to explicitly differentiate between fine-tuning and contamination, which is crucial for detecting contamination in domain-specific models. Additionally, CAP is applicable to various benchmarks and works for both white-box and black-box models. We validate CAP's effectiveness through experiments on seven LLMs and four domain-specific benchmarks. Our findings also show that composite benchmarks from various dataset sources are particularly prone to unintentional contamination. Codes will be publicly available soon.",
                    "score": 0.5601844106222117,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97265625
                }
            ],
            "relevance_judgement": 0.97265625,
            "relevance_judgment_input_expanded": "# Title: CAP: Data Contamination Detection via Consistency Amplification\n# Venue: arXiv.org\n# Authors: Yi Zhao, Jing Li, Linyi Yang\n## Abstract\nLarge language models (LLMs) are widely used, but concerns about data contamination challenge the reliability of LLM evaluations. Existing contamination detection methods are often task-specific or require extra prerequisites, limiting practicality. We propose a novel framework, Consistency Amplification-based Data Contamination Detection (CAP), which introduces the Performance Consistency Ratio (PCR) to measure dataset leakage by leveraging LM consistency. To the best of our knowledge, this is the first method to explicitly differentiate between fine-tuning and contamination, which is crucial for detecting contamination in domain-specific models. Additionally, CAP is applicable to various benchmarks and works for both white-box and black-box models. We validate CAP's effectiveness through experiments on seven LLMs and four domain-specific benchmarks. Our findings also show that composite benchmarks from various dataset sources are particularly prone to unintentional contamination. Codes will be publicly available soon.\n",
            "reference_string": "[273502439 | Zhao et al. | 2024 | Citations: 1]"
        },
        {
            "title": "How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 37,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.00678, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2162122419",
                    "name": "Hyeong Kyu Choi"
                },
                {
                    "authorId": "2282532776",
                    "name": "Maxim Khanov"
                },
                {
                    "authorId": "2346637624",
                    "name": "Hongxin Wei"
                },
                {
                    "authorId": "2300129169",
                    "name": "Yixuan Li"
                }
            ],
            "abstract": "Dataset contamination, where evaluation datasets overlap with pre-training corpora, inflates performance metrics and undermines the reliability of model evaluations. Measuring dataset contamination thus becomes essential to ensure that performance evaluations genuinely reflect a model's ability to generalize to unseen data, rather than relying on memorized examples. To address this problem, we propose Kernel Divergence Score (KDS), a novel method that evaluates dataset contamination by computing the divergence between the kernel similarity matrix of sample embeddings, before and after fine-tuning on the benchmark dataset. Leveraging the insight that fine-tuning affects unseen samples more significantly than seen ones, KDS provides a reliable measure of contamination. Through extensive experiments on controlled contamination scenarios, KDS demonstrates a near-perfect correlation with contamination levels and outperforms existing baselines. Additionally, we perform comprehensive ablation studies to analyze the impact of key design choices, providing deeper insights into the components and effectiveness of KDS. These ablations highlight the importance of leveraging fine-grained kernel-based information and confirm the reliability of the proposed framework across diverse datasets and settings. Code is released in https://github.com/deeplearning-wisc/kernel-divergence-score.",
            "corpus_id": 276094401,
            "sentences": [
                {
                    "corpus_id": "276094401",
                    "title": "How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence",
                    "text": "When a large language model (LLM) performs remarkably well on a benchmark, can we confidently attribute its success to true generalization-or is it simply a reflection of what the model has already seen during pre-training? The reality is, we often don't know. Beneath the surface of those impressive performance scores lies a critical vulnerability: dataset contamination, a phenomenon where evaluation datasets overlap with the pretraining data of the model (Golchin & Surdeanu, 2024). This overlap artificially inflates reported performance metrics, obscures true generalization capabilities, and raises critical concerns about the reliability of benchmark evaluations. This brings us to a pressing and underexplored question: How can we measure the degree of dataset contamination? \n\nAddressing this question is crucial to ensuring that performance evaluations genuinely reflect a model's ability to generalize to unseen data, rather than benefiting from overlap with pretraining data. To formalize the problem, we aim to develop a scoring function S : (D, M) \u2192 R, that takes a benchmark dataset D as input and produces a score indicative of its relative contamination level with respect to the given model M. A higher score corresponds to a greater contamination level. Such a score is valuable because researchers can use it to rank multiple benchmarks and prioritize the less contaminated ones, enabling more informed comparisons and reliable evaluation. For the score to be reliable, we argue that the scoring function must satisfy two essential properties: monotonicity, which ensures that the score exhibits a positive correlation with the contamination level, and consistency, which means that the score remains stable across independently sampled subsets with the same contamination rate. \n\nTo measure dataset contamination, we introduce the Kernel Divergence Score (KDS), which computes the divergence of the kernel similarity matrix of sample embeddings before and after fine-tuning on the benchmark dataset. By analyzing changes in the kernel similarity matrix, KDS captures how fine-tuning reshapes the embeddings for seen and unseen data, providing a more holistic and nuanced perspective on dataset contamination. This approach is motivated by the fact that fine-tuning has a more significant effect on the Pre-trained LLM Fine-tuned LLM",
                    "score": 0.5207482324615604,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 223
                        },
                        {
                            "start": 224,
                            "end": 260
                        },
                        {
                            "start": 261,
                            "end": 487
                        },
                        {
                            "start": 488,
                            "end": 672
                        },
                        {
                            "start": 673,
                            "end": 785
                        },
                        {
                            "start": 788,
                            "end": 989
                        },
                        {
                            "start": 990,
                            "end": 1274
                        },
                        {
                            "start": 1275,
                            "end": 1461
                        },
                        {
                            "start": 1462,
                            "end": 1800
                        },
                        {
                            "start": 1803,
                            "end": 2022
                        },
                        {
                            "start": 2023,
                            "end": 2231
                        },
                        {
                            "start": 2232,
                            "end": 2355
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 460,
                            "end": 486,
                            "matchedPaperCorpusId": "260925501"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.970703125
                }
            ],
            "relevance_judgement": 0.970703125,
            "relevance_judgment_input_expanded": "# Title: How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence\n# Venue: arXiv.org\n# Authors: Hyeong Kyu Choi, Maxim Khanov, Hongxin Wei, Yixuan Li\n## Abstract\nDataset contamination, where evaluation datasets overlap with pre-training corpora, inflates performance metrics and undermines the reliability of model evaluations. Measuring dataset contamination thus becomes essential to ensure that performance evaluations genuinely reflect a model's ability to generalize to unseen data, rather than relying on memorized examples. To address this problem, we propose Kernel Divergence Score (KDS), a novel method that evaluates dataset contamination by computing the divergence between the kernel similarity matrix of sample embeddings, before and after fine-tuning on the benchmark dataset. Leveraging the insight that fine-tuning affects unseen samples more significantly than seen ones, KDS provides a reliable measure of contamination. Through extensive experiments on controlled contamination scenarios, KDS demonstrates a near-perfect correlation with contamination levels and outperforms existing baselines. Additionally, we perform comprehensive ablation studies to analyze the impact of key design choices, providing deeper insights into the components and effectiveness of KDS. These ablations highlight the importance of leveraging fine-grained kernel-based information and confirm the reliability of the proposed framework across diverse datasets and settings. Code is released in https://github.com/deeplearning-wisc/kernel-divergence-score.\n## Introduction\nWhen a large language model (LLM) performs remarkably well on a benchmark, can we confidently attribute its success to true generalization-or is it simply a reflection of what the model has already seen during pre-training? The reality is, we often don't know. Beneath the surface of those impressive performance scores lies a critical vulnerability: dataset contamination, a phenomenon where evaluation datasets overlap with the pretraining data of the model (Golchin & Surdeanu, 2024). This overlap artificially inflates reported performance metrics, obscures true generalization capabilities, and raises critical concerns about the reliability of benchmark evaluations. This brings us to a pressing and underexplored question: How can we measure the degree of dataset contamination? \n\nAddressing this question is crucial to ensuring that performance evaluations genuinely reflect a model's ability to generalize to unseen data, rather than benefiting from overlap with pretraining data. To formalize the problem, we aim to develop a scoring function S : (D, M) \u2192 R, that takes a benchmark dataset D as input and produces a score indicative of its relative contamination level with respect to the given model M. A higher score corresponds to a greater contamination level. Such a score is valuable because researchers can use it to rank multiple benchmarks and prioritize the less contaminated ones, enabling more informed comparisons and reliable evaluation. For the score to be reliable, we argue that the scoring function must satisfy two essential properties: monotonicity, which ensures that the score exhibits a positive correlation with the contamination level, and consistency, which means that the score remains stable across independently sampled subsets with the same contamination rate. \n\nTo measure dataset contamination, we introduce the Kernel Divergence Score (KDS), which computes the divergence of the kernel similarity matrix of sample embeddings before and after fine-tuning on the benchmark dataset. By analyzing changes in the kernel similarity matrix, KDS captures how fine-tuning reshapes the embeddings for seen and unseen data, providing a more holistic and nuanced perspective on dataset contamination. This approach is motivated by the fact that fine-tuning has a more significant effect on the Pre-trained LLM Fine-tuned LLM",
            "reference_string": "[276094401 | Choi et al. | 2025 | Citations: 4]"
        },
        {
            "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 76,
            "citation_count": 28,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.15043, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2164113313",
                    "name": "Zhuohao Yu"
                },
                {
                    "authorId": "2287659901",
                    "name": "Chang Gao"
                },
                {
                    "authorId": "2286328804",
                    "name": "Wenjin Yao"
                },
                {
                    "authorId": "2108024279",
                    "name": "Yidong Wang"
                },
                {
                    "authorId": "145235149",
                    "name": "Wei Ye"
                },
                {
                    "authorId": "2273553706",
                    "name": "Jindong Wang"
                },
                {
                    "authorId": "2249681654",
                    "name": "Xing Xie"
                },
                {
                    "authorId": "2250437942",
                    "name": "Yue Zhang"
                },
                {
                    "authorId": "1705434",
                    "name": "Shikun Zhang"
                }
            ],
            "abstract": "Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered\"interactor\"role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KIEval's effectiveness and generalization. We also reveal that data contamination brings no contribution or even negative effect to models' real-world applicability and understanding, and existing contamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning.",
            "corpus_id": 267897557,
            "sentences": [
                {
                    "corpus_id": "267897557",
                    "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models",
                    "text": "Data contamination refers to the inclusion of information in the training set of models that provides insights into the test set of a benchmark dataset, and then evaluated in the same benchmark. Recently, the AI community has become increasingly concerned (Schaeffer, 2023;Zhou et al., 2023;Oren et al., 2023) about data contamination in LLMs. Detecting data contamination, a form of Membership Inference Attack (MIA), poses challenges for large language models (LLMs) due to their training on vast corpora and the difficulty of conducting ablation studies (Shi et al., 2023). To detect such contamination of LLMs, Wei et al. (2023) suggested comparing average loss values between training and test datasets, while Shi et al. (2023) introduced Min-K% Prob based on loss values to identify texts used in training. Our experiments show these methods are effective for pre-training but not for detecting contamination during fine-tuning. Zhu et al. (2023) leveraged DAG to dynamically generate evaluation data in reasoning tasks to avoid contamination. In comparison, KIEval only requires access to output texts of evaluated models and detects data contamination through evaluating its ability to generalize and utilize knowledge as well as generative ability, which requires a deeper understanding of knowledge instead of mere memorization of the answers. Moreover, our experiments suggest that KIEval is resilient to data contamination, offering a reliable means to discern whether models have been trained on test sets. This makes it a valuable tool for complementing traditional benchmarks, providing a more nuanced understanding of a model's exposure to and handling of contaminated data.",
                    "score": 0.6110612497335517,
                    "section_title": "Addressing Data Contamination of LLMs",
                    "char_start_offset": 7866,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 194
                        },
                        {
                            "start": 195,
                            "end": 343
                        },
                        {
                            "start": 344,
                            "end": 576
                        },
                        {
                            "start": 577,
                            "end": 812
                        },
                        {
                            "start": 813,
                            "end": 934
                        },
                        {
                            "start": 935,
                            "end": 1049
                        },
                        {
                            "start": 1050,
                            "end": 1353
                        },
                        {
                            "start": 1354,
                            "end": 1519
                        },
                        {
                            "start": 1520,
                            "end": 1690
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.962890625
                }
            ],
            "relevance_judgement": 0.962890625,
            "relevance_judgment_input_expanded": "# Title: KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, Shikun Zhang\n## Abstract\nAutomatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered\"interactor\"role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KIEval's effectiveness and generalization. We also reveal that data contamination brings no contribution or even negative effect to models' real-world applicability and understanding, and existing contamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning.\n## Addressing Data Contamination of LLMs\nData contamination refers to the inclusion of information in the training set of models that provides insights into the test set of a benchmark dataset, and then evaluated in the same benchmark. Recently, the AI community has become increasingly concerned (Schaeffer, 2023;Zhou et al., 2023;Oren et al., 2023) about data contamination in LLMs. Detecting data contamination, a form of Membership Inference Attack (MIA), poses challenges for large language models (LLMs) due to their training on vast corpora and the difficulty of conducting ablation studies (Shi et al., 2023). To detect such contamination of LLMs, Wei et al. (2023) suggested comparing average loss values between training and test datasets, while Shi et al. (2023) introduced Min-K% Prob based on loss values to identify texts used in training. Our experiments show these methods are effective for pre-training but not for detecting contamination during fine-tuning. Zhu et al. (2023) leveraged DAG to dynamically generate evaluation data in reasoning tasks to avoid contamination. In comparison, KIEval only requires access to output texts of evaluated models and detects data contamination through evaluating its ability to generalize and utilize knowledge as well as generative ability, which requires a deeper understanding of knowledge instead of mere memorization of the answers. Moreover, our experiments suggest that KIEval is resilient to data contamination, offering a reliable means to discern whether models have been trained on test sets. This makes it a valuable tool for complementing traditional benchmarks, providing a more nuanced understanding of a model's exposure to and handling of contaminated data.",
            "reference_string": "[267897557 | Yu et al. | 2024 | Citations: 28]"
        },
        {
            "title": "Proving Test Set Contamination in Black Box Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 38,
            "citation_count": 146,
            "influential_citation_count": 11,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.17623, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "153163779",
                    "name": "Yonatan Oren"
                },
                {
                    "authorId": "2261737865",
                    "name": "Nicole Meister"
                },
                {
                    "authorId": "22193324",
                    "name": "Niladri S. Chatterji"
                },
                {
                    "authorId": "8759332",
                    "name": "Faisal Ladhak"
                },
                {
                    "authorId": "2244446164",
                    "name": "Tatsunori Hashimoto"
                }
            ],
            "abstract": "Large language models are trained on vast amounts of internet data, prompting concerns and speculation that they have memorized public benchmarks. Going from speculation to proof of contamination is challenging, as the pretraining data used by proprietary models are often not publicly accessible. We show that it is possible to provide provable guarantees of test set contamination in language models without access to pretraining data or model weights. Our approach leverages the fact that when there is no data contamination, all orderings of an exchangeable benchmark should be equally likely. In contrast, the tendency for language models to memorize example order means that a contaminated language model will find certain canonical orderings to be much more likely than others. Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples. We demonstrate that our procedure is sensitive enough to reliably prove test set contamination in challenging situations, including models as small as 1.4 billion parameters, on small test sets of only 1000 examples, and datasets that appear only a few times in the pretraining corpus. Using our test, we audit five popular publicly accessible language models for test set contamination and find little evidence for pervasive contamination.",
            "corpus_id": 264490730,
            "sentences": [
                {
                    "corpus_id": "264490730",
                    "title": "Proving Test Set Contamination in Black Box Language Models",
                    "text": "We evaluate our statistical test on a 1.4 billion parameter language model trained on a combination of Wikipedia and a curated set of canary test sets. Our test is sensitive enough to identify test sets with as few as 1000 examples, and sometimes even appearing only twice in the pretraining corpus. In the case of higher duplication counts, such as datasets appearing 10 or more times, we obtain vanishingly small p-values on our test. Finally, we run our test on four commonly used, public language models to study the behavior of our test on language models in the wild and find little evidence of pervasive and strong test set contamination. \n\nWe summarize our contributions below. \n\n\u2022 Demonstrating the use of exchangability as a way to provably identify test set contamination using only log probability queries. \n\n\u2022 Construction of an efficient and powerful sharded hypothesis test for test set contamination. \n\n\u2022 Empirical demonstration of black-box detection of contamination for small datasets that appear few times during pretraining. \n\nOur three contributions suggest that black-box identification of test set contamination is practical and further improvements in the power of the tests may allow us to regularly audit language models in the wild for test set contamination. To encourage the development of new provable guarantees for test set contamination, we release our pretrained models as a benchmark for developing future statistical tests.1 .",
                    "score": 0.5310019991518748,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 4291,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 299
                        },
                        {
                            "start": 300,
                            "end": 436
                        },
                        {
                            "start": 437,
                            "end": 645
                        },
                        {
                            "start": 648,
                            "end": 685
                        },
                        {
                            "start": 688,
                            "end": 818
                        },
                        {
                            "start": 821,
                            "end": 916
                        },
                        {
                            "start": 919,
                            "end": 1045
                        },
                        {
                            "start": 1048,
                            "end": 1287
                        },
                        {
                            "start": 1288,
                            "end": 1463
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95849609375
                },
                {
                    "corpus_id": "264490730",
                    "title": "Proving Test Set Contamination in Black Box Language Models",
                    "text": "We test for these differences in log probabilities, and aggregate them across the dataset to provide false positive rate guarantees. \n\n2019; Mattern et al., 2023) as well as provide some evidence for test set contamination (Sainz et al., 2023;Golchin & Surdeanu, 2023). However, the heuristic nature of these methods limits their usefulness, as these methods cannot elevate speculation about a suspected instance of test set contamination into an irrefutable proof of contamination. \n\nIn this work, we show it is possible to go beyond heuristics and provide provable guarantees of test set contamination in black box language models. More specifically, we provide a statistical test that can identify the presence of a benchmark in the pre-training dataset of a language model with provable false positive rate guarantees and without access to the model's training data or weights. \n\nTo achieve these guarantees, we exploit the fact that many datasets have a property known as exchangeability, where the order of examples in the dataset can be shuffled without affecting its joint distribution. Our key insight is that if a language model shows a preference for any particular ordering of the dataset -such as a canonical ordering that appears in publicly available repositoriesthis violates exchangeability and can only occur by observing the dataset during training (Figure 1). We leverage this insight to propose a set of tests that compares the language model's log probability on the 'canonical' ordering (taken from public repositories) to the log probability on a dataset with shuffled examples, and flag a dataset if the two log probabilities have statistically significant differences. \n\nUsing these ideas, we propose a computationally efficient and statistically powerful test for contamination which shards the dataset into smaller segments and performs a series of log probability comparisons within each shard. We prove that this sharded test provides control over the false positive rate, enables computationally efficient parallel tests, and substantially improves the power of the test for small p-values. \n\nWe evaluate our statistical test on a 1.4 billion parameter language model trained on a combination of Wikipedia and a curated set of canary test sets.",
                    "score": 0.533460344047913,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 2167,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 132
                        },
                        {
                            "start": 135,
                            "end": 269
                        },
                        {
                            "start": 270,
                            "end": 482
                        },
                        {
                            "start": 485,
                            "end": 633
                        },
                        {
                            "start": 634,
                            "end": 881
                        },
                        {
                            "start": 884,
                            "end": 1094
                        },
                        {
                            "start": 1095,
                            "end": 1379
                        },
                        {
                            "start": 1380,
                            "end": 1694
                        },
                        {
                            "start": 1697,
                            "end": 1923
                        },
                        {
                            "start": 1924,
                            "end": 2121
                        },
                        {
                            "start": 2124,
                            "end": 2275
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 141,
                            "end": 162,
                            "matchedPaperCorpusId": "258967264"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9462890625
                }
            ],
            "relevance_judgement": 0.95849609375,
            "relevance_judgment_input_expanded": "# Title: Proving Test Set Contamination in Black Box Language Models\n# Venue: International Conference on Learning Representations\n# Authors: Yonatan Oren, Nicole Meister, Niladri S. Chatterji, Faisal Ladhak, Tatsunori Hashimoto\n## Abstract\nLarge language models are trained on vast amounts of internet data, prompting concerns and speculation that they have memorized public benchmarks. Going from speculation to proof of contamination is challenging, as the pretraining data used by proprietary models are often not publicly accessible. We show that it is possible to provide provable guarantees of test set contamination in language models without access to pretraining data or model weights. Our approach leverages the fact that when there is no data contamination, all orderings of an exchangeable benchmark should be equally likely. In contrast, the tendency for language models to memorize example order means that a contaminated language model will find certain canonical orderings to be much more likely than others. Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples. We demonstrate that our procedure is sensitive enough to reliably prove test set contamination in challenging situations, including models as small as 1.4 billion parameters, on small test sets of only 1000 examples, and datasets that appear only a few times in the pretraining corpus. Using our test, we audit five popular publicly accessible language models for test set contamination and find little evidence for pervasive contamination.\n## INTRODUCTION\nWe test for these differences in log probabilities, and aggregate them across the dataset to provide false positive rate guarantees. \n\n2019; Mattern et al., 2023) as well as provide some evidence for test set contamination (Sainz et al., 2023;Golchin & Surdeanu, 2023). However, the heuristic nature of these methods limits their usefulness, as these methods cannot elevate speculation about a suspected instance of test set contamination into an irrefutable proof of contamination. \n\nIn this work, we show it is possible to go beyond heuristics and provide provable guarantees of test set contamination in black box language models. More specifically, we provide a statistical test that can identify the presence of a benchmark in the pre-training dataset of a language model with provable false positive rate guarantees and without access to the model's training data or weights. \n\nTo achieve these guarantees, we exploit the fact that many datasets have a property known as exchangeability, where the order of examples in the dataset can be shuffled without affecting its joint distribution. Our key insight is that if a language model shows a preference for any particular ordering of the dataset -such as a canonical ordering that appears in publicly available repositoriesthis violates exchangeability and can only occur by observing the dataset during training (Figure 1). We leverage this insight to propose a set of tests that compares the language model's log probability on the 'canonical' ordering (taken from public repositories) to the log probability on a dataset with shuffled examples, and flag a dataset if the two log probabilities have statistically significant differences. \n\nUsing these ideas, we propose a computationally efficient and statistically powerful test for contamination which shards the dataset into smaller segments and performs a series of log probability comparisons within each shard. We prove that this sharded test provides control over the false positive rate, enables computationally efficient parallel tests, and substantially improves the power of the test for small p-values. \n\nWe evaluate our statistical test on a 1.4 billion parameter language model trained on a combination of Wikipedia and a curated set of canary test sets.\n...\nWe evaluate our statistical test on a 1.4 billion parameter language model trained on a combination of Wikipedia and a curated set of canary test sets. Our test is sensitive enough to identify test sets with as few as 1000 examples, and sometimes even appearing only twice in the pretraining corpus. In the case of higher duplication counts, such as datasets appearing 10 or more times, we obtain vanishingly small p-values on our test. Finally, we run our test on four commonly used, public language models to study the behavior of our test on language models in the wild and find little evidence of pervasive and strong test set contamination. \n\nWe summarize our contributions below. \n\n\u2022 Demonstrating the use of exchangability as a way to provably identify test set contamination using only log probability queries. \n\n\u2022 Construction of an efficient and powerful sharded hypothesis test for test set contamination. \n\n\u2022 Empirical demonstration of black-box detection of contamination for small datasets that appear few times during pretraining. \n\nOur three contributions suggest that black-box identification of test set contamination is practical and further improvements in the power of the tests may allow us to regularly audit language models in the wild for test set contamination. To encourage the development of new provable guarantees for test set contamination, we release our pretrained models as a benchmark for developing future statistical tests.1 .",
            "reference_string": "[264490730 | Oren et al. | 2023 | Citations: 146]"
        },
        {
            "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 44,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.10880, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316591285",
                    "name": "Hengxiang Zhang"
                },
                {
                    "authorId": "2266803682",
                    "name": "Songxin Zhang"
                },
                {
                    "authorId": "2283306681",
                    "name": "Bingyi Jing"
                },
                {
                    "authorId": "2325203633",
                    "name": "Hongxin Wei"
                }
            ],
            "abstract": "In the era of large language models (LLMs), detecting pretraining data has been increasingly important due to concerns about fair evaluation and ethical risks. Current methods differentiate members and non-members by designing scoring functions, like Perplexity and Min-k%. However, the diversity and complexity of training data magnifies the difficulty of distinguishing, leading to suboptimal performance in detecting pretraining data. In this paper, we first explore the benefits of unseen data, which can be easily collected after the release of the LLM. We find that the perplexities of LLMs shift differently for members and non-members, after fine-tuning with a small amount of previously unseen data. In light of this, we introduce a novel and effective method termed Fine-tuned Score Deviation(FSD), which improves the performance of current scoring functions for pretraining data detection. In particular, we propose to measure the deviation distance of current scores after fine-tuning on a small amount of unseen data within the same domain. In effect, using a few unseen data can largely decrease the scores of all non-members, leading to a larger deviation distance than members. Extensive experiments demonstrate the effectiveness of our method, significantly improving the AUC score on common benchmark datasets across various models.",
            "corpus_id": 273350935,
            "sentences": [
                {
                    "corpus_id": "273350935",
                    "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
                    "text": "The results demonstrate that our method can significantly improve the performance of existing methods based on scoring functions. For example, our method improves the AUC score of the best baseline method Min-k%, increasing it from 0.62 to 0.91 on WikiMIA under the OPT-6.7B model. Moreover, our method can also improve the TPR@5%FPR score of baseline methods. For example, our method improves the TPR@5%FPR score of the detection method using perplexity, increasing it from 0.10 to 0.81 on ArXivTection under the LLaMA-7B model. \n\nOur main contributions are as follows: \n\n\u2022 We analyze the limitations of existing methods based on scoring functions for pretraining data detection. The significant overlap in metric score distribution between seen data and unseen data results in the inferior performance of detection methods. \n\n\u2022 We propose Fine-tuned Score Deviation (FSD), a novel and effective method for detecting pretraining data from large language models. The core idea is to enlarge the gap between members and non-members by exposing the LLM to a few unseen data. \n\n\u2022 We empirically show that FSD can improve the performance of existing methods based on scoring functions for pretraining data detection, through extensive experiments conducted on various benchmark datasets with diverse LLMs.",
                    "score": 0.5798717204927225,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 3914,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 129
                        },
                        {
                            "start": 130,
                            "end": 274
                        },
                        {
                            "start": 275,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 360
                        },
                        {
                            "start": 361,
                            "end": 529
                        },
                        {
                            "start": 532,
                            "end": 570
                        },
                        {
                            "start": 573,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 825
                        },
                        {
                            "start": 828,
                            "end": 962
                        },
                        {
                            "start": 963,
                            "end": 1072
                        },
                        {
                            "start": 1075,
                            "end": 1301
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95556640625
                },
                {
                    "corpus_id": "273350935",
                    "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
                    "text": "Thus, it is important to assess the leakage of benchmark data into pretraining data (Zhou et al., 2023). On the one hand, model developers can remove evaluation benchmark data from training data by retrieval-based methods with access to pertaining data (Ravaut et al., 2024;Chowdhery et al., 2023). Specifically, those methods employ n-gram tokenization and string-matching for detecting data contamination (Brown et al., 2020;Touvron et al., 2023b;Team et al., 2023;Radford et al., 2019). On the other hand, researchers utilize prompting techniques (Golchin & Surdeanu, 2024), performance analysis (Ye et al., 2024;Debenedetti et al., 2024), model likelihood (Oren et al., 2024;Shi et al., 2024;Xu et al., 2024b) to detect potential contamination without access to the training data. Our work focuses on pretraining data detection, an area that is similar to data contamination. Different from data contamination detection, our FSD can also be applied to the detection of copyrighted resources in real-world scenarios.",
                    "score": 0.5152441020817581,
                    "section_title": "A RELATED WORK",
                    "char_start_offset": 30092,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 104
                        },
                        {
                            "start": 105,
                            "end": 298
                        },
                        {
                            "start": 299,
                            "end": 489
                        },
                        {
                            "start": 490,
                            "end": 784
                        },
                        {
                            "start": 785,
                            "end": 879
                        },
                        {
                            "start": 880,
                            "end": 1019
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 274,
                            "end": 297,
                            "matchedPaperCorpusId": "247951931"
                        },
                        {
                            "start": 407,
                            "end": 427,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 467,
                            "end": 488,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 550,
                            "end": 576,
                            "matchedPaperCorpusId": "260925501"
                        },
                        {
                            "start": 616,
                            "end": 641,
                            "matchedPaperCorpusId": "261697333"
                        },
                        {
                            "start": 660,
                            "end": 679,
                            "matchedPaperCorpusId": "264490730"
                        },
                        {
                            "start": 679,
                            "end": 696,
                            "matchedPaperCorpusId": "264451585"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95361328125
                },
                {
                    "corpus_id": "273350935",
                    "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
                    "text": "Pretraining data detection, which is an increasingly important topic for large language models, relates to a large amount of literature on membership inference attacks and data contamination. We discuss some of the relevant works to ours in two directions below. \n\nMembership Inference Attacks Our work mainly studies how to detect a given example in the pretraining data, which is consistent with the objective of membership inference attacks (MIAs) (Shokri et al., 2017;Truex et al., 2019). This task aims to determine whether a given data point is a member of training data. Metric-based attack methods, such as loss (Yeom et al., 2018), entropy (Salem et al., 2019), confidence (Liu et al., 2019) and gradient (Liu et al., 2023), infer membership of data by comparing the calculated metric value with a preset threshold. Previous works have generalized metric-based methods to large language models (Duan et al., 2024;Xie et al., 2024;Zhang et al., 2024;Mattern et al., 2023), by calculating the based-likelihood metric (e.g., perplexity) for membership inference. Recent works apply MIAs to pretraining data detection by designing likelihood-based scoring functions to measure the membership of data (Shi et al., 2024;Ye et al., 2024). In this work, we analyze the limitations of existing scoring functions for pretraining data detection, and design an effective method to improve their performance. In particular, this work is the first to explore the importance of collecting unseen data in pretraining data detection. \n\nData Contamination Data contamination has been studied in the literature (Xu et al., 2024a;Magar & Schwartz, 2022;Balloccu et al., 2024), where training data may inadvertently include evaluation benchmark data, resulting in unauthentic evaluation results. Thus, it is important to assess the leakage of benchmark data into pretraining data (Zhou et al., 2023).",
                    "score": 0.5976767665193571,
                    "section_title": "A RELATED WORK",
                    "char_start_offset": 28308,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 191
                        },
                        {
                            "start": 192,
                            "end": 262
                        },
                        {
                            "start": 265,
                            "end": 492
                        },
                        {
                            "start": 493,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 824
                        },
                        {
                            "start": 825,
                            "end": 1068
                        },
                        {
                            "start": 1069,
                            "end": 1240
                        },
                        {
                            "start": 1241,
                            "end": 1404
                        },
                        {
                            "start": 1405,
                            "end": 1525
                        },
                        {
                            "start": 1528,
                            "end": 1783
                        },
                        {
                            "start": 1784,
                            "end": 1888
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 451,
                            "end": 472,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 472,
                            "end": 491,
                            "matchedPaperCorpusId": "86836429"
                        },
                        {
                            "start": 620,
                            "end": 639,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 649,
                            "end": 669,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 682,
                            "end": 700,
                            "matchedPaperCorpusId": "86836429"
                        },
                        {
                            "start": 714,
                            "end": 732,
                            "matchedPaperCorpusId": "264179931"
                        },
                        {
                            "start": 958,
                            "end": 979,
                            "matchedPaperCorpusId": "258967264"
                        },
                        {
                            "start": 1205,
                            "end": 1223,
                            "matchedPaperCorpusId": "264451585"
                        },
                        {
                            "start": 1619,
                            "end": 1642,
                            "matchedPaperCorpusId": "247475929"
                        },
                        {
                            "start": 1642,
                            "end": 1664,
                            "matchedPaperCorpusId": "267499939"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92138671875
                },
                {
                    "corpus_id": "273350935",
                    "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
                    "text": "In the era of large language models (LLMs), detecting pretraining data has been increasingly important due to concerns about fair evaluation and ethical risks. Current methods differentiate members and non-members by designing scoring functions, like Perplexity and Min-k%. However, the diversity and complexity of training data magnifies the difficulty of distinguishing, leading to suboptimal performance in detecting pretraining data. In this paper, we first explore the benefits of unseen data, which can be easily collected after the release of the LLM. We find that the perplexities of LLMs shift differently for members and non-members, after fine-tuning with a small amount of previously unseen data. In light of this, we introduce a novel and effective method termed Fine-tuned Score Deviation(FSD), which improves the performance of current scoring functions for pretraining data detection. In particular, we propose to measure the deviation distance of current scores after fine-tuning on a small amount of unseen data within the same domain. In effect, using a few unseen data can largely decrease the scores of all non-members, leading to a larger deviation distance than members. Extensive experiments demonstrate the effectiveness of our method, significantly improving the AUC score on common benchmark datasets across various models.",
                    "score": 0.5560194042558733,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92041015625
                }
            ],
            "relevance_judgement": 0.95556640625,
            "relevance_judgment_input_expanded": "# Title: Fine-tuning can Help Detect Pretraining Data from Large Language Models\n# Venue: arXiv.org\n# Authors: Hengxiang Zhang, Songxin Zhang, Bingyi Jing, Hongxin Wei\n## Abstract\nIn the era of large language models (LLMs), detecting pretraining data has been increasingly important due to concerns about fair evaluation and ethical risks. Current methods differentiate members and non-members by designing scoring functions, like Perplexity and Min-k%. However, the diversity and complexity of training data magnifies the difficulty of distinguishing, leading to suboptimal performance in detecting pretraining data. In this paper, we first explore the benefits of unseen data, which can be easily collected after the release of the LLM. We find that the perplexities of LLMs shift differently for members and non-members, after fine-tuning with a small amount of previously unseen data. In light of this, we introduce a novel and effective method termed Fine-tuned Score Deviation(FSD), which improves the performance of current scoring functions for pretraining data detection. In particular, we propose to measure the deviation distance of current scores after fine-tuning on a small amount of unseen data within the same domain. In effect, using a few unseen data can largely decrease the scores of all non-members, leading to a larger deviation distance than members. Extensive experiments demonstrate the effectiveness of our method, significantly improving the AUC score on common benchmark datasets across various models.\n## INTRODUCTION\nThe results demonstrate that our method can significantly improve the performance of existing methods based on scoring functions. For example, our method improves the AUC score of the best baseline method Min-k%, increasing it from 0.62 to 0.91 on WikiMIA under the OPT-6.7B model. Moreover, our method can also improve the TPR@5%FPR score of baseline methods. For example, our method improves the TPR@5%FPR score of the detection method using perplexity, increasing it from 0.10 to 0.81 on ArXivTection under the LLaMA-7B model. \n\nOur main contributions are as follows: \n\n\u2022 We analyze the limitations of existing methods based on scoring functions for pretraining data detection. The significant overlap in metric score distribution between seen data and unseen data results in the inferior performance of detection methods. \n\n\u2022 We propose Fine-tuned Score Deviation (FSD), a novel and effective method for detecting pretraining data from large language models. The core idea is to enlarge the gap between members and non-members by exposing the LLM to a few unseen data. \n\n\u2022 We empirically show that FSD can improve the performance of existing methods based on scoring functions for pretraining data detection, through extensive experiments conducted on various benchmark datasets with diverse LLMs.\n\n## A RELATED WORK\nPretraining data detection, which is an increasingly important topic for large language models, relates to a large amount of literature on membership inference attacks and data contamination. We discuss some of the relevant works to ours in two directions below. \n\nMembership Inference Attacks Our work mainly studies how to detect a given example in the pretraining data, which is consistent with the objective of membership inference attacks (MIAs) (Shokri et al., 2017;Truex et al., 2019). This task aims to determine whether a given data point is a member of training data. Metric-based attack methods, such as loss (Yeom et al., 2018), entropy (Salem et al., 2019), confidence (Liu et al., 2019) and gradient (Liu et al., 2023), infer membership of data by comparing the calculated metric value with a preset threshold. Previous works have generalized metric-based methods to large language models (Duan et al., 2024;Xie et al., 2024;Zhang et al., 2024;Mattern et al., 2023), by calculating the based-likelihood metric (e.g., perplexity) for membership inference. Recent works apply MIAs to pretraining data detection by designing likelihood-based scoring functions to measure the membership of data (Shi et al., 2024;Ye et al., 2024). In this work, we analyze the limitations of existing scoring functions for pretraining data detection, and design an effective method to improve their performance. In particular, this work is the first to explore the importance of collecting unseen data in pretraining data detection. \n\nData Contamination Data contamination has been studied in the literature (Xu et al., 2024a;Magar & Schwartz, 2022;Balloccu et al., 2024), where training data may inadvertently include evaluation benchmark data, resulting in unauthentic evaluation results. Thus, it is important to assess the leakage of benchmark data into pretraining data (Zhou et al., 2023).\n...\nThus, it is important to assess the leakage of benchmark data into pretraining data (Zhou et al., 2023). On the one hand, model developers can remove evaluation benchmark data from training data by retrieval-based methods with access to pertaining data (Ravaut et al., 2024;Chowdhery et al., 2023). Specifically, those methods employ n-gram tokenization and string-matching for detecting data contamination (Brown et al., 2020;Touvron et al., 2023b;Team et al., 2023;Radford et al., 2019). On the other hand, researchers utilize prompting techniques (Golchin & Surdeanu, 2024), performance analysis (Ye et al., 2024;Debenedetti et al., 2024), model likelihood (Oren et al., 2024;Shi et al., 2024;Xu et al., 2024b) to detect potential contamination without access to the training data. Our work focuses on pretraining data detection, an area that is similar to data contamination. Different from data contamination detection, our FSD can also be applied to the detection of copyrighted resources in real-world scenarios.",
            "reference_string": "[273350935 | Zhang et al. | 2024 | Citations: 1]"
        },
        {
            "title": "A Comprehensive Survey of Contamination Detection Methods in Large Language Models",
            "venue": "",
            "year": 2024,
            "reference_count": 137,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.00699, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "14038850",
                    "name": "Mathieu Ravaut"
                },
                {
                    "authorId": "2064493724",
                    "name": "Bosheng Ding"
                },
                {
                    "authorId": "1689176705",
                    "name": "Fangkai Jiao"
                },
                {
                    "authorId": "2258571998",
                    "name": "Hailin Chen"
                },
                {
                    "authorId": "2155447436",
                    "name": "Xingxuan Li"
                },
                {
                    "authorId": "2091437375",
                    "name": "Ruochen Zhao"
                },
                {
                    "authorId": "2084609980",
                    "name": "Chengwei Qin"
                },
                {
                    "authorId": "2267728986",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                }
            ],
            "abstract": "With the rise of Large Language Models (LLMs) in recent years, abundant new opportunities are emerging, but also new challenges, among which contamination is quickly becoming critical. Business applications and fundraising in Artificial Intelligence (AI) have reached a scale at which a few percentage points gained on popular question-answering benchmarks could translate into dozens of millions of dollars, placing high pressure on model integrity. At the same time, it is becoming harder and harder to keep track of the data that LLMs have seen; if not impossible with closed-source models like GPT-4 and Claude-3 not divulging any information on the training set. As a result, contamination becomes a major issue: LLMs' performance may not be reliable anymore, as the high performance may be at least partly due to their previous exposure to the data. This limitation jeopardizes real capability improvement in the field of NLP, yet, there remains a lack of methods on how to efficiently detect contamination. In this paper, we survey all recent work on contamination detection with LLMs, analyzing their methodologies and use cases to shed light on the appropriate usage of contamination detection methods. Our work calls the NLP research community's attention into systematically taking into account contamination bias in LLM evaluation.",
            "corpus_id": 268819579,
            "sentences": [
                {
                    "corpus_id": "268819579",
                    "title": "A Comprehensive Survey of Contamination Detection Methods in Large Language Models",
                    "text": "The tokens sequence is scored by averaging these token-level cross-entropy scores over the first occurence of each token. Contamination is assessed when this final score is above a pre-defined threshold. Min-K%++ (Zhang et al., 2024a) proposes another extension of the idea of Min-K% Prob: rather than simply getting next-token predicted probabilities, the authors propose a score which subtracts the expected log-probability and divides the score by the variance; in other words, normalizing the initial log-probability. The sequence scoring mechanism is then identical to the one of Min-K% Prob. Formally, noting the expected probability over the next token \n\nand its corresponding standard deviation \n\nThe motivation of the score built by Min-K%++ is to assess whether an input form a mode, after observing that because of maximum likelihood training, training samples often become local maxima in the modeled distribution along each input dimension (here in the context of LLM, dimensions are tokens). Min-K%++ reaches state-of-the-art on WikiMIA, outperforming Min-K% Prob by up to 10 points. \n\nLi (2023a) also work on token-level probabilities and compare perplexity on benchmark samples against memorized and clean baselines. The study finds significant memorization in recent models on popular reading comprehension and summarization benchmarks, while multiple-choice benchmarks show less evidence of contamination. This method provides a tool for the community to conduct rigorous contamination analysis, enabling more accurate and reliable model evaluation. Dong et al. (2024) propose two novel likelihoodbased contamination detection methodologies: CDD (Contamination Detection via output Distribution) and TED (Trustworthy Evaluation via output Distribution). CDD detects data contamination by observing the peakedness in the LLM's output distribution in a black-box manner. It represents a significant improvement over existing approaches, offering average relative improvements of 21.8%-30.2% in terms of Accuracy, F1 Score, and AUC metrics. TED corrects the LLM's output distribution to mitigate the impact of data contamination on evaluation metrics, significantly reducing performance improvements attributed to data contamination across various scenarios and contamination degrees.",
                    "score": 0.572679917621492,
                    "section_title": "Likelihood",
                    "char_start_offset": 34345,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 121
                        },
                        {
                            "start": 122,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 521
                        },
                        {
                            "start": 522,
                            "end": 597
                        },
                        {
                            "start": 598,
                            "end": 659
                        },
                        {
                            "start": 662,
                            "end": 702
                        },
                        {
                            "start": 705,
                            "end": 1005
                        },
                        {
                            "start": 1006,
                            "end": 1097
                        },
                        {
                            "start": 1100,
                            "end": 1232
                        },
                        {
                            "start": 1233,
                            "end": 1423
                        },
                        {
                            "start": 1424,
                            "end": 1567
                        },
                        {
                            "start": 1568,
                            "end": 1771
                        },
                        {
                            "start": 1772,
                            "end": 1886
                        },
                        {
                            "start": 1887,
                            "end": 2055
                        },
                        {
                            "start": 2056,
                            "end": 2299
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.955078125
                }
            ],
            "relevance_judgement": 0.955078125,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Survey of Contamination Detection Methods in Large Language Models\n# Venue: \n# Authors: Mathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen, Xingxuan Li, Ruochen Zhao, Chengwei Qin, Caiming Xiong, Shafiq R. Joty\n## Abstract\nWith the rise of Large Language Models (LLMs) in recent years, abundant new opportunities are emerging, but also new challenges, among which contamination is quickly becoming critical. Business applications and fundraising in Artificial Intelligence (AI) have reached a scale at which a few percentage points gained on popular question-answering benchmarks could translate into dozens of millions of dollars, placing high pressure on model integrity. At the same time, it is becoming harder and harder to keep track of the data that LLMs have seen; if not impossible with closed-source models like GPT-4 and Claude-3 not divulging any information on the training set. As a result, contamination becomes a major issue: LLMs' performance may not be reliable anymore, as the high performance may be at least partly due to their previous exposure to the data. This limitation jeopardizes real capability improvement in the field of NLP, yet, there remains a lack of methods on how to efficiently detect contamination. In this paper, we survey all recent work on contamination detection with LLMs, analyzing their methodologies and use cases to shed light on the appropriate usage of contamination detection methods. Our work calls the NLP research community's attention into systematically taking into account contamination bias in LLM evaluation.\n## Likelihood\nThe tokens sequence is scored by averaging these token-level cross-entropy scores over the first occurence of each token. Contamination is assessed when this final score is above a pre-defined threshold. Min-K%++ (Zhang et al., 2024a) proposes another extension of the idea of Min-K% Prob: rather than simply getting next-token predicted probabilities, the authors propose a score which subtracts the expected log-probability and divides the score by the variance; in other words, normalizing the initial log-probability. The sequence scoring mechanism is then identical to the one of Min-K% Prob. Formally, noting the expected probability over the next token \n\nand its corresponding standard deviation \n\nThe motivation of the score built by Min-K%++ is to assess whether an input form a mode, after observing that because of maximum likelihood training, training samples often become local maxima in the modeled distribution along each input dimension (here in the context of LLM, dimensions are tokens). Min-K%++ reaches state-of-the-art on WikiMIA, outperforming Min-K% Prob by up to 10 points. \n\nLi (2023a) also work on token-level probabilities and compare perplexity on benchmark samples against memorized and clean baselines. The study finds significant memorization in recent models on popular reading comprehension and summarization benchmarks, while multiple-choice benchmarks show less evidence of contamination. This method provides a tool for the community to conduct rigorous contamination analysis, enabling more accurate and reliable model evaluation. Dong et al. (2024) propose two novel likelihoodbased contamination detection methodologies: CDD (Contamination Detection via output Distribution) and TED (Trustworthy Evaluation via output Distribution). CDD detects data contamination by observing the peakedness in the LLM's output distribution in a black-box manner. It represents a significant improvement over existing approaches, offering average relative improvements of 21.8%-30.2% in terms of Accuracy, F1 Score, and AUC metrics. TED corrects the LLM's output distribution to mitigate the impact of data contamination on evaluation metrics, significantly reducing performance improvements attributed to data contamination across various scenarios and contamination degrees.",
            "reference_string": "[268819579 | Ravaut et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Detecting Benchmark Contamination Through Watermarking",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 37,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.17259, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283934407",
                    "name": "Tom Sander"
                },
                {
                    "authorId": "2322442744",
                    "name": "Pierre Fernandez"
                },
                {
                    "authorId": "2290845753",
                    "name": "Saeed Mahloujifar"
                },
                {
                    "authorId": "2283933043",
                    "name": "A. Durmus"
                },
                {
                    "authorId": "2290241478",
                    "name": "Chuan Guo"
                }
            ],
            "abstract": "Benchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect ``radioactivity'', \\ie traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, e.g. $p$-val $=10^{-3}$ for +5$\\%$ on ARC-Easy.",
            "corpus_id": 276575227,
            "sentences": [
                {
                    "corpus_id": "276575227",
                    "title": "Detecting Benchmark Contamination Through Watermarking",
                    "text": "Benchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect ``radioactivity'', \\ie traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, e.g. $p$-val $=10^{-3}$ for +5$\\%$ on ARC-Easy.",
                    "score": 0.5700390612355188,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.955078125
                },
                {
                    "corpus_id": "276575227",
                    "title": "Detecting Benchmark Contamination Through Watermarking",
                    "text": "Benchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect \"radioactivity\", i.e., traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, e.g., p-val = 10 \u22123 for +5% on ARC-Easy. Alice is a benchmark provider and wants to make sure that contamination on her benchmark can be detected with high confidence. Before release, she rephrases the original benchmark dataset while embedding a non-intrusive LLM watermark. This rephrasing does not change the utility of the benchmark. Bob decides to train a model. The benchmark may contaminate Bob's model during training, either intentionally or unintentionally. Alice can give statistical evidence if her benchmark was used in training.",
                    "score": 0.6145940037479881,
                    "section_title": "body",
                    "char_start_offset": 1,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 194
                        },
                        {
                            "start": 195,
                            "end": 283
                        },
                        {
                            "start": 284,
                            "end": 419
                        },
                        {
                            "start": 420,
                            "end": 592
                        },
                        {
                            "start": 593,
                            "end": 801
                        },
                        {
                            "start": 802,
                            "end": 998
                        },
                        {
                            "start": 999,
                            "end": 1125
                        },
                        {
                            "start": 1126,
                            "end": 1233
                        },
                        {
                            "start": 1234,
                            "end": 1295
                        },
                        {
                            "start": 1296,
                            "end": 1325
                        },
                        {
                            "start": 1326,
                            "end": 1425
                        },
                        {
                            "start": 1426,
                            "end": 1500
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9501953125
                }
            ],
            "relevance_judgement": 0.955078125,
            "relevance_judgment_input_expanded": "# Title: Detecting Benchmark Contamination Through Watermarking\n# Venue: arXiv.org\n# Authors: Tom Sander, Pierre Fernandez, Saeed Mahloujifar, A. Durmus, Chuan Guo\n## Abstract\nBenchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect ``radioactivity'', \\ie traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, e.g. $p$-val $=10^{-3}$ for +5$\\%$ on ARC-Easy.\n## body\nBenchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect \"radioactivity\", i.e., traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, e.g., p-val = 10 \u22123 for +5% on ARC-Easy. Alice is a benchmark provider and wants to make sure that contamination on her benchmark can be detected with high confidence. Before release, she rephrases the original benchmark dataset while embedding a non-intrusive LLM watermark. This rephrasing does not change the utility of the benchmark. Bob decides to train a model. The benchmark may contaminate Bob's model during training, either intentionally or unintentionally. Alice can give statistical evidence if her benchmark was used in training.",
            "reference_string": "[276575227 | Sander et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 53,
            "citation_count": 108,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.08493",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.08493, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "65754049",
                    "name": "Shahriar Golchin"
                },
                {
                    "authorId": "1760868",
                    "name": "M. Surdeanu"
                }
            ],
            "abstract": "Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in measuring LLMs' real effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination at the instance level; using this information, our approach then assesses wider contamination at the partition level. To estimate contamination of individual instances, we employ\"guided instruction:\"a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or nearly matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset partition as contaminated if the average overlap score with the reference instances (as measured by ROUGE-L or BLEURT) is statistically significantly better with the completions from guided instruction compared to a\"general instruction\"that does not include the dataset and partition name. The second idea marks a dataset partition as contaminated if a classifier based on GPT-4 with few-shot in-context learning prompt marks multiple generated completions as exact/near-exact matches of the corresponding reference instances. Our best method achieves an accuracy between 92% and 100% in detecting if an LLM is contaminated with seven datasets, containing train and test/validation partitions, when contrasted with manual evaluation by human experts. Further, our findings indicate that GPT-4 is contaminated with AG News, WNLI, and XSum datasets.",
            "corpus_id": 260925501,
            "sentences": [
                {
                    "corpus_id": "260925501",
                    "title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models",
                    "text": "Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in measuring LLMs' real effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination at the instance level; using this information, our approach then assesses wider contamination at the partition level. To estimate contamination of individual instances, we employ\"guided instruction:\"a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or nearly matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset partition as contaminated if the average overlap score with the reference instances (as measured by ROUGE-L or BLEURT) is statistically significantly better with the completions from guided instruction compared to a\"general instruction\"that does not include the dataset and partition name. The second idea marks a dataset partition as contaminated if a classifier based on GPT-4 with few-shot in-context learning prompt marks multiple generated completions as exact/near-exact matches of the corresponding reference instances. Our best method achieves an accuracy between 92% and 100% in detecting if an LLM is contaminated with seven datasets, containing train and test/validation partitions, when contrasted with manual evaluation by human experts. Further, our findings indicate that GPT-4 is contaminated with AG News, WNLI, and XSum datasets.",
                    "score": 0.5781448779947007,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95458984375
                },
                {
                    "corpus_id": "260925501",
                    "title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models",
                    "text": "The rise of Transformer networks (Vaswani et al. 2017) has spurred the development of large language models (LLMs), marking a new epoch in Natural Language Processing (NLP). This shift has led to an extensive range of LLMs (Touvron et al. 2023a;b;Biderman et al. 2023;K\u00f6pf et al. 2023;Chung et al. 2022;Penedo et al. 2023, inter-alia) which excel in various professional and academic benchmarks (Bang et al. 2023;Bubeck et al. 2023). Their superior performance is primarily attributed to the massive web data consumed by these billion/trillion-parameter LLMs during training. However, the impressive LLM performance observed on many downstream tasks (e.g., summarization, natural language inference, text classification) may be inflated due to data contamination, i.e., the presence of test data from these downstream tasks in the pre-training data of LLMs. Guaranteeing lack of contamination is not trivial due to two potential sources of contamination: directly from ingesting the official version of a dataset (easier to control), and indirectly through duplicated data found somewhere on the web (nearly impossible to control). 2 The potential of data contamination is especially relevant for closed models such as the GPT-3/3.5 family (Brown et al. 2020) and GPT-4 (OpenAI 2023;Bubeck et al. 2023), and, needless to say, raises questions on the validity of evaluations and benchmarks conducted so far (Chang et al. 2023;Zhu et al. 2023;Bordt & von Luxburg 2023;Ray 2023;Penedo et al. 2023). \n\nTo address this issue, we propose an inexpensive and robust approach to detect data contamination for a given dataset partition automatically. Importantly, our approach functions under two realistic assumptions: (a) we lack direct access to the pre-training data of the LLMs, and (b) we have limited computational resources. Intuitively, our method starts by identifying potential contamination in individual instances that are drawn from a small random sample of the corresponding dataset partition (we use samples of 10 instances in this work).",
                    "score": 0.7568301187910232,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 173
                        },
                        {
                            "start": 174,
                            "end": 433
                        },
                        {
                            "start": 434,
                            "end": 575
                        },
                        {
                            "start": 576,
                            "end": 857
                        },
                        {
                            "start": 858,
                            "end": 1133
                        },
                        {
                            "start": 1134,
                            "end": 1495
                        },
                        {
                            "start": 1498,
                            "end": 1640
                        },
                        {
                            "start": 1641,
                            "end": 1822
                        },
                        {
                            "start": 1823,
                            "end": 2044
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1240,
                            "end": 1259,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1466,
                            "end": 1475,
                            "matchedPaperCorpusId": "258157875"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.921875
                }
            ],
            "relevance_judgement": 0.95458984375,
            "relevance_judgment_input_expanded": "# Title: Time Travel in LLMs: Tracing Data Contamination in Large Language Models\n# Venue: International Conference on Learning Representations\n# Authors: Shahriar Golchin, M. Surdeanu\n## Abstract\nData contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in measuring LLMs' real effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination at the instance level; using this information, our approach then assesses wider contamination at the partition level. To estimate contamination of individual instances, we employ\"guided instruction:\"a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or nearly matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset partition as contaminated if the average overlap score with the reference instances (as measured by ROUGE-L or BLEURT) is statistically significantly better with the completions from guided instruction compared to a\"general instruction\"that does not include the dataset and partition name. The second idea marks a dataset partition as contaminated if a classifier based on GPT-4 with few-shot in-context learning prompt marks multiple generated completions as exact/near-exact matches of the corresponding reference instances. Our best method achieves an accuracy between 92% and 100% in detecting if an LLM is contaminated with seven datasets, containing train and test/validation partitions, when contrasted with manual evaluation by human experts. Further, our findings indicate that GPT-4 is contaminated with AG News, WNLI, and XSum datasets.\n## INTRODUCTION\nThe rise of Transformer networks (Vaswani et al. 2017) has spurred the development of large language models (LLMs), marking a new epoch in Natural Language Processing (NLP). This shift has led to an extensive range of LLMs (Touvron et al. 2023a;b;Biderman et al. 2023;K\u00f6pf et al. 2023;Chung et al. 2022;Penedo et al. 2023, inter-alia) which excel in various professional and academic benchmarks (Bang et al. 2023;Bubeck et al. 2023). Their superior performance is primarily attributed to the massive web data consumed by these billion/trillion-parameter LLMs during training. However, the impressive LLM performance observed on many downstream tasks (e.g., summarization, natural language inference, text classification) may be inflated due to data contamination, i.e., the presence of test data from these downstream tasks in the pre-training data of LLMs. Guaranteeing lack of contamination is not trivial due to two potential sources of contamination: directly from ingesting the official version of a dataset (easier to control), and indirectly through duplicated data found somewhere on the web (nearly impossible to control). 2 The potential of data contamination is especially relevant for closed models such as the GPT-3/3.5 family (Brown et al. 2020) and GPT-4 (OpenAI 2023;Bubeck et al. 2023), and, needless to say, raises questions on the validity of evaluations and benchmarks conducted so far (Chang et al. 2023;Zhu et al. 2023;Bordt & von Luxburg 2023;Ray 2023;Penedo et al. 2023). \n\nTo address this issue, we propose an inexpensive and robust approach to detect data contamination for a given dataset partition automatically. Importantly, our approach functions under two realistic assumptions: (a) we lack direct access to the pre-training data of the LLMs, and (b) we have limited computational resources. Intuitively, our method starts by identifying potential contamination in individual instances that are drawn from a small random sample of the corresponding dataset partition (we use samples of 10 instances in this work).",
            "reference_string": "[260925501 | Golchin et al. | 2023 | Citations: 108]"
        },
        {
            "title": "A Taxonomy for Data Contamination in Large Language Models",
            "venue": "CONDA",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.08716, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2220962946",
                    "name": "Medha Palavalli"
                },
                {
                    "authorId": "2138301112",
                    "name": "Amanda Bertsch"
                },
                {
                    "authorId": "1762110",
                    "name": "Matthew R. Gormley"
                }
            ],
            "abstract": "Large language models pretrained on extensive web corpora demonstrate remarkable performance across a wide range of downstream tasks. However, a growing concern is data contamination, where evaluation datasets may unintentionally be contained in the pretraining corpus, inflating model performance. Decontamination, the process of detecting and removing such data, is a potential solution; yet these contaminants may originate from altered versions of the test set, evading detection during decontamination. How different types of contamination impact the performance of language models on downstream tasks is not fully understood. We present a taxonomy that categorizes the various types of contamination encountered by LLMs during the pretraining phase and identify which types pose the highest risk. We analyze the impact of contamination on two key NLP tasks\u2014summarization and question answering\u2014revealing how different types of contamination influence task performance during evaluation.",
            "corpus_id": 271097946,
            "sentences": [
                {
                    "corpus_id": "271097946",
                    "title": "A Taxonomy for Data Contamination in Large Language Models",
                    "text": "Large language models pretrained on extensive web corpora demonstrate remarkable performance across a wide range of downstream tasks. However, a growing concern is data contamination, where evaluation datasets may unintentionally be contained in the pretraining corpus, inflating model performance. Decontamination, the process of detecting and removing such data, is a potential solution; yet these contaminants may originate from altered versions of the test set, evading detection during decontamination. How different types of contamination impact the performance of language models on downstream tasks is not fully understood. We present a taxonomy that categorizes the various types of contamination encountered by LLMs during the pretraining phase and identify which types pose the highest risk. We analyze the impact of contamination on two key NLP tasks\u2014summarization and question answering\u2014revealing how different types of contamination influence task performance during evaluation.",
                    "score": 0.5921622548499555,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95166015625
                },
                {
                    "corpus_id": "271097946",
                    "title": "A Taxonomy for Data Contamination in Large Language Models",
                    "text": "Methods with access to pretraining data Early research on LLM data contamination primarily employed methods akin to high-order n-gram overlap detection between pretraining and evaluation data (Radford et al., 2019a;Brown et al., 2020;Wei et al., 2021;Touvron et al., 2023). Tools for qualitative analysis on large-scale corpora (such as Data Portraits (Marone and Durme, 2023) and the ROOTS Search Tool (Piktus et al., 2023)) have further increased the practicality of this type of contamination detection. However, these approaches have several limitations: they remain fairly computationally expensive, assume access to pretraining data, and generally can only detect contamination when a cluster of several test set examples co-occur (as most methods leverage data sketching (Broder, 1997) tools that are only effective for sequences above a certain length). Yang et al. (2023) proposes an LLM-based decontamination method, which leverages embedding similarity search followed by evaluation with a strong language model (e.g. GPT-4), to identify and mitigate contamination. This is computationally costly but can identify noisy contamination Methods without access to pretraining data Some approaches are capable of detecting contamination without direct access to pretraining data, but assume that the test data has not been modified or distributed across the pretraining corpus. These methods leverage metadata from the dataset to detect contamination, e.g. by leveraging dataset ordering (Sainz et al., 2023b) or the assignment of examples to specific data splits (Golchin and Surdeanu, 2023). Golchin and Surdeanu (2024) introduce the Data Contamination Quiz, a streamlined method that efficiently detects and estimates verbatim contamination in LLMs by crafting multiple choice questions that prompt a model to correctly dataset-specific content among similar but noisy alternatives. \n\nChang et al. (2023a) detect contamination of books (which serve as inputs for many long-context evaluation datasets) using domain specific featuresa name cloze test and a publication-year evaluation.",
                    "score": 0.5626161182366429,
                    "section_title": "Detecting Data Contamination",
                    "char_start_offset": 10132,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 273
                        },
                        {
                            "start": 274,
                            "end": 506
                        },
                        {
                            "start": 507,
                            "end": 861
                        },
                        {
                            "start": 862,
                            "end": 1028
                        },
                        {
                            "start": 1029,
                            "end": 1076
                        },
                        {
                            "start": 1077,
                            "end": 1383
                        },
                        {
                            "start": 1384,
                            "end": 1599
                        },
                        {
                            "start": 1600,
                            "end": 1891
                        },
                        {
                            "start": 1894,
                            "end": 2093
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94970703125
                },
                {
                    "corpus_id": "271097946",
                    "title": "A Taxonomy for Data Contamination in Large Language Models",
                    "text": "Our analysis highlights the importance of data format, with models performing better when pretraining data matches the evaluation format. We also observe task-specific effects, with certain contamination methods benefiting particular tasks more than others. Additionally, we find that some latestage pretraining contamination can actually be unhelpful to downstream performance, if it occurs in a substantially different format from the downstream task. Our findings underscore gaps in current decontamination practices, which primarily focus on full-dataset-level contamination and are often unable to detect approximate or noisy contamination. \n\nWe demonstrate that different types of contamination can have variable effects on model performance, highlighting the need for careful consideration during training and evaluation. With the creation of our taxonomy, we hope to promote standardization regarding the definition and categories of contamination within the research community, facilitating clear communication and collaboration, while also enabling precise detection and mitigation of contamination in pretraining data. We recommend researchers decontaminating pretraining corpora for LLMs prioritize developing techniques that address noisy evaluation data, while also ensuring rigorous scrutiny to prevent any shuffled or interleaved evaluation data from inadvertently persisting in the pretraining data. It is not enough to merely remove instances of the full test dataset in the pretraining corpus; fragments or noised versions of the test set can also inflate performance. We hope our work inspires future work on detecting and mitigating specific types of contamination.",
                    "score": 0.45732318312132936,
                    "section_title": "Conclusion",
                    "char_start_offset": 22752,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 137
                        },
                        {
                            "start": 138,
                            "end": 257
                        },
                        {
                            "start": 258,
                            "end": 453
                        },
                        {
                            "start": 454,
                            "end": 645
                        },
                        {
                            "start": 648,
                            "end": 828
                        },
                        {
                            "start": 829,
                            "end": 1129
                        },
                        {
                            "start": 1130,
                            "end": 1416
                        },
                        {
                            "start": 1417,
                            "end": 1587
                        },
                        {
                            "start": 1588,
                            "end": 1686
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9443359375
                }
            ],
            "relevance_judgement": 0.95166015625,
            "relevance_judgment_input_expanded": "# Title: A Taxonomy for Data Contamination in Large Language Models\n# Venue: CONDA\n# Authors: Medha Palavalli, Amanda Bertsch, Matthew R. Gormley\n## Abstract\nLarge language models pretrained on extensive web corpora demonstrate remarkable performance across a wide range of downstream tasks. However, a growing concern is data contamination, where evaluation datasets may unintentionally be contained in the pretraining corpus, inflating model performance. Decontamination, the process of detecting and removing such data, is a potential solution; yet these contaminants may originate from altered versions of the test set, evading detection during decontamination. How different types of contamination impact the performance of language models on downstream tasks is not fully understood. We present a taxonomy that categorizes the various types of contamination encountered by LLMs during the pretraining phase and identify which types pose the highest risk. We analyze the impact of contamination on two key NLP tasks\u2014summarization and question answering\u2014revealing how different types of contamination influence task performance during evaluation.\n## Detecting Data Contamination\nMethods with access to pretraining data Early research on LLM data contamination primarily employed methods akin to high-order n-gram overlap detection between pretraining and evaluation data (Radford et al., 2019a;Brown et al., 2020;Wei et al., 2021;Touvron et al., 2023). Tools for qualitative analysis on large-scale corpora (such as Data Portraits (Marone and Durme, 2023) and the ROOTS Search Tool (Piktus et al., 2023)) have further increased the practicality of this type of contamination detection. However, these approaches have several limitations: they remain fairly computationally expensive, assume access to pretraining data, and generally can only detect contamination when a cluster of several test set examples co-occur (as most methods leverage data sketching (Broder, 1997) tools that are only effective for sequences above a certain length). Yang et al. (2023) proposes an LLM-based decontamination method, which leverages embedding similarity search followed by evaluation with a strong language model (e.g. GPT-4), to identify and mitigate contamination. This is computationally costly but can identify noisy contamination Methods without access to pretraining data Some approaches are capable of detecting contamination without direct access to pretraining data, but assume that the test data has not been modified or distributed across the pretraining corpus. These methods leverage metadata from the dataset to detect contamination, e.g. by leveraging dataset ordering (Sainz et al., 2023b) or the assignment of examples to specific data splits (Golchin and Surdeanu, 2023). Golchin and Surdeanu (2024) introduce the Data Contamination Quiz, a streamlined method that efficiently detects and estimates verbatim contamination in LLMs by crafting multiple choice questions that prompt a model to correctly dataset-specific content among similar but noisy alternatives. \n\nChang et al. (2023a) detect contamination of books (which serve as inputs for many long-context evaluation datasets) using domain specific featuresa name cloze test and a publication-year evaluation.\n\n## Conclusion\nOur analysis highlights the importance of data format, with models performing better when pretraining data matches the evaluation format. We also observe task-specific effects, with certain contamination methods benefiting particular tasks more than others. Additionally, we find that some latestage pretraining contamination can actually be unhelpful to downstream performance, if it occurs in a substantially different format from the downstream task. Our findings underscore gaps in current decontamination practices, which primarily focus on full-dataset-level contamination and are often unable to detect approximate or noisy contamination. \n\nWe demonstrate that different types of contamination can have variable effects on model performance, highlighting the need for careful consideration during training and evaluation. With the creation of our taxonomy, we hope to promote standardization regarding the definition and categories of contamination within the research community, facilitating clear communication and collaboration, while also enabling precise detection and mitigation of contamination in pretraining data. We recommend researchers decontaminating pretraining corpora for LLMs prioritize developing techniques that address noisy evaluation data, while also ensuring rigorous scrutiny to prevent any shuffled or interleaved evaluation data from inadvertently persisting in the pretraining data. It is not enough to merely remove instances of the full test dataset in the pretraining corpus; fragments or noised versions of the test set can also inflate performance. We hope our work inspires future work on detecting and mitigating specific types of contamination.",
            "reference_string": "[271097946 | Palavalli et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.03823, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1610927763",
                    "name": "Dingjie Song"
                },
                {
                    "authorId": "2329375695",
                    "name": "Sicheng Lai"
                },
                {
                    "authorId": "2267023659",
                    "name": "Shunian Chen"
                },
                {
                    "authorId": "2329521766",
                    "name": "Lichao Sun"
                },
                {
                    "authorId": "2267007505",
                    "name": "Benyou Wang"
                }
            ],
            "abstract": "The rapid progression of multimodal large language models (MLLMs) has demonstrated superior performance on various multimodal benchmarks. However, the issue of data contamination during training creates challenges in performance evaluation and comparison. While numerous methods exist for detecting models' contamination in large language models (LLMs), they are less effective for MLLMs due to their various modalities and multiple training phases. In this study, we introduce a multimodal data contamination detection framework, MM-Detect, designed for MLLMs. Our experimental results indicate that MM-Detect is quite effective and sensitive in identifying varying degrees of contamination, and can highlight significant performance improvements due to the leakage of multimodal benchmark training sets. Furthermore, we explore whether the contamination originates from the base LLMs used by MLLMs or the multimodal training phase, providing new insights into the stages at which contamination may be introduced.",
            "corpus_id": 273850482,
            "sentences": [
                {
                    "corpus_id": "273850482",
                    "title": "Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination",
                    "text": "The rapid progression of multimodal large language models (MLLMs) has demonstrated superior performance on various multimodal benchmarks. However, the issue of data contamination during training creates challenges in performance evaluation and comparison. While numerous methods exist for detecting models' contamination in large language models (LLMs), they are less effective for MLLMs due to their various modalities and multiple training phases. In this study, we introduce a multimodal data contamination detection framework, MM-Detect, designed for MLLMs. Our experimental results indicate that MM-Detect is quite effective and sensitive in identifying varying degrees of contamination, and can highlight significant performance improvements due to the leakage of multimodal benchmark training sets. Furthermore, we explore whether the contamination originates from the base LLMs used by MLLMs or the multimodal training phase, providing new insights into the stages at which contamination may be introduced.",
                    "score": 0.5881330008046691,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.947265625
                }
            ],
            "relevance_judgement": 0.947265625,
            "relevance_judgment_input_expanded": "# Title: Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination\n# Venue: arXiv.org\n# Authors: Dingjie Song, Sicheng Lai, Shunian Chen, Lichao Sun, Benyou Wang\n## Abstract\nThe rapid progression of multimodal large language models (MLLMs) has demonstrated superior performance on various multimodal benchmarks. However, the issue of data contamination during training creates challenges in performance evaluation and comparison. While numerous methods exist for detecting models' contamination in large language models (LLMs), they are less effective for MLLMs due to their various modalities and multiple training phases. In this study, we introduce a multimodal data contamination detection framework, MM-Detect, designed for MLLMs. Our experimental results indicate that MM-Detect is quite effective and sensitive in identifying varying degrees of contamination, and can highlight significant performance improvements due to the leakage of multimodal benchmark training sets. Furthermore, we explore whether the contamination originates from the base LLMs used by MLLMs or the multimodal training phase, providing new insights into the stages at which contamination may be introduced.\n",
            "reference_string": "[273850482 | Song et al. | 2024 | Citations: 1]"
        },
        {
            "title": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data Contamination in Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 43,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.13983, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2342797440",
                    "name": "Yang Fan"
                }
            ],
            "abstract": "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the issue of data contamination has become increasingly severe, leading to potential overestimation of model performance during evaluation. To address this, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data evaluation method aimed at mitigating the impact of data contamination on evaluation reliability. Experimental results on multiple datasets demonstrate that AdEval effectively reduces the impact of data contamination on evaluation outcomes, enhancing both the fairness and reliability of the evaluation process.",
            "corpus_id": 275906726,
            "sentences": [
                {
                    "corpus_id": "275906726",
                    "title": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data Contamination in Large Language Models",
                    "text": "Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP) with their remarkable performance across a wide range of tasks [1], [2]. These models rely on pretraining over massive-scale internet corpora, while many widely-used benchmarks are also derived from online resources, inevitably leading to the issue of data contamination [3]. Recent studies indicate that data contamination is prevalent in LLM evaluations [1], [4], [5], undermining the credibility of evaluation results and hindering fair comparisons between models. \n\nTo address data contamination, two primary solutions have been proposed: data contamination detection and dynamic data evaluation [6]. Data contamination detection aims to identify overlaps between model outputs and training data [7]. However, these methods face limitations, particularly for proprietary models such as the GPT series, which often incorporate special filtering mechanisms during generation. Additionally, detection methods primarily focus on extreme memorization (i.e., direct reproduction of training data), making it difficult to capture more subtle forms of contamination. Moreover, the training datasets of closed-source models are often treated as trade secrets, leaving external communities with limited ability to intervene directly. \n\nCompared to static benchmarks, dynamic data evaluation effectively circumvents the problem of data contamination, avoiding the inherent limitations of detection methods. Existing dynamic evaluation approaches have achieved notable progress in various domains. For instance, DYVAL [4] introduces dynamic data generation tailored for mathematical tasks; KIEval [6] incorporates multi-turn dialogue-based evaluation; LatestEval [8] excels in generating contamination-free datasets; SciEval [9] emphasizes testing scientific research capabilities through the design of entirely new questions; and [10] propose a practical strategy for generating evaluation data using simple heuristics. These methods explore the potential of dynamic evaluation from different perspectives, offering diverse tools for assessing the capabilities of LLMs.",
                    "score": 0.46186137106523967,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 18,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 176
                        },
                        {
                            "start": 177,
                            "end": 379
                        },
                        {
                            "start": 380,
                            "end": 571
                        },
                        {
                            "start": 574,
                            "end": 708
                        },
                        {
                            "start": 709,
                            "end": 808
                        },
                        {
                            "start": 809,
                            "end": 981
                        },
                        {
                            "start": 982,
                            "end": 1166
                        },
                        {
                            "start": 1167,
                            "end": 1331
                        },
                        {
                            "start": 1334,
                            "end": 1503
                        },
                        {
                            "start": 1504,
                            "end": 1593
                        },
                        {
                            "start": 1594,
                            "end": 2016
                        },
                        {
                            "start": 2017,
                            "end": 2166
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 172,
                            "end": 175,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94580078125
                }
            ],
            "relevance_judgement": 0.94580078125,
            "relevance_judgment_input_expanded": "# Title: AdEval: Alignment-based Dynamic Evaluation to Mitigate Data Contamination in Large Language Models\n# Venue: arXiv.org\n# Authors: Yang Fan\n## Abstract\nAs Large Language Models (LLMs) are pretrained on massive-scale corpora, the issue of data contamination has become increasingly severe, leading to potential overestimation of model performance during evaluation. To address this, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data evaluation method aimed at mitigating the impact of data contamination on evaluation reliability. Experimental results on multiple datasets demonstrate that AdEval effectively reduces the impact of data contamination on evaluation outcomes, enhancing both the fairness and reliability of the evaluation process.\n## I. INTRODUCTION\nLarge Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP) with their remarkable performance across a wide range of tasks [1], [2]. These models rely on pretraining over massive-scale internet corpora, while many widely-used benchmarks are also derived from online resources, inevitably leading to the issue of data contamination [3]. Recent studies indicate that data contamination is prevalent in LLM evaluations [1], [4], [5], undermining the credibility of evaluation results and hindering fair comparisons between models. \n\nTo address data contamination, two primary solutions have been proposed: data contamination detection and dynamic data evaluation [6]. Data contamination detection aims to identify overlaps between model outputs and training data [7]. However, these methods face limitations, particularly for proprietary models such as the GPT series, which often incorporate special filtering mechanisms during generation. Additionally, detection methods primarily focus on extreme memorization (i.e., direct reproduction of training data), making it difficult to capture more subtle forms of contamination. Moreover, the training datasets of closed-source models are often treated as trade secrets, leaving external communities with limited ability to intervene directly. \n\nCompared to static benchmarks, dynamic data evaluation effectively circumvents the problem of data contamination, avoiding the inherent limitations of detection methods. Existing dynamic evaluation approaches have achieved notable progress in various domains. For instance, DYVAL [4] introduces dynamic data generation tailored for mathematical tasks; KIEval [6] incorporates multi-turn dialogue-based evaluation; LatestEval [8] excels in generating contamination-free datasets; SciEval [9] emphasizes testing scientific research capabilities through the design of entirely new questions; and [10] propose a practical strategy for generating evaluation data using simple heuristics. These methods explore the potential of dynamic evaluation from different perspectives, offering diverse tools for assessing the capabilities of LLMs.",
            "reference_string": "[275906726 | Fan | 2025 | Citations: 0]"
        },
        {
            "title": "Investigating Data Contamination for Pre-training Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 27,
            "citation_count": 72,
            "influential_citation_count": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.06059, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2800541",
                    "name": "Minhao Jiang"
                },
                {
                    "authorId": "2298016051",
                    "name": "Ken Ziyu Liu"
                },
                {
                    "authorId": "1606040932",
                    "name": "Ming Zhong"
                },
                {
                    "authorId": "1749176844",
                    "name": "Rylan Schaeffer"
                },
                {
                    "authorId": "2260339714",
                    "name": "Siru Ouyang"
                },
                {
                    "authorId": "2259869648",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "123593472",
                    "name": "Sanmi Koyejo"
                }
            ],
            "abstract": "Language models pre-trained on web-scale corpora demonstrate impressive capabilities on diverse downstream tasks. However, there is increasing concern whether such capabilities might arise from evaluation datasets being included in the pre-training corpus -- a phenomenon known as \\textit{data contamination} -- in a manner that artificially increases performance. There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks. In this paper, we explore the impact of data contamination at the pre-training stage by pre-training a series of GPT-2 models \\textit{from scratch}. We highlight the effect of both text contamination (\\textit{i.e.}\\ input text of the evaluation samples) and ground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and the desired outputs) from evaluation data. We also investigate the effects of repeating contamination for various downstream tasks. Additionally, we examine the prevailing n-gram-based definitions of contamination within current LLM reports, pinpointing their limitations and inadequacy. Our findings offer new insights into data contamination's effects on language model capabilities and underscore the need for independent, comprehensive contamination assessments in LLM studies.",
            "corpus_id": 266933004,
            "sentences": [
                {
                    "corpus_id": "266933004",
                    "title": "Investigating Data Contamination for Pre-training Language Models",
                    "text": "Numerous studies on large language models (LLMs) have explored and investigated the concept of data contamination and demonstrated the robustness of these models against potential contamination in their evaluation datasets [29,2,6,27,33,34,11]. Most definitions proposed in the existing studies are based on n-gram duplication between pre-training data and evaluation data. For instance, PaLM [6] divides the evaluation data into two categories-\"clean\" and \"contaminated\"-based on whether at least 70% of all possible 8-grams in the evaluation sample were seen at least once in the pre-training corpus. Llama 2 [34] provides a more fine-grained definition: a token is considered contaminated if it appears in any token n-gram longer than 10 tokens in both the evaluation sample and the training set, and the contamination percentage of an evaluation sample is defined to be the percentage of tokens contaminated; the evaluation data are then divided into 4 buckets-\"Clean\", \"Not Clean\", \"Not Dirty\", and \"Dirty\"-based on the contamination percentage of each evaluation sample. While intuitive, these contamination definitions primarily revolve around n-gram or token overlaps, which only target direct duplications present in both training and evaluation datasets and might provide both high false positive rate (since many semantically different texts have overlaps) and false negative rate (since simple paraphrasing can evade detection [36]). Moreover, investigations relying on these definitions have predominantly centered on evaluation level analysis; in our work, we focus on pre-training level analysis as described in \u00a71. \n\nIn our experiments, we follow PaLM [6] and Llama 2's [34] definitions as well as a direct n-gram overlap detection strategy to investigate how the \"contamination\" under these definitions are different and how they affect model performance. As described in \u00a71, contamination in the pre-training corpus can appear as either textual components from evaluation datasets or with ground truth information. \n\nExisting definitions tend to overlook the latter.",
                    "score": 0.5961982742939558,
                    "section_title": "Contamination Definitions",
                    "char_start_offset": 4517,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 244
                        },
                        {
                            "start": 245,
                            "end": 373
                        },
                        {
                            "start": 374,
                            "end": 602
                        },
                        {
                            "start": 603,
                            "end": 1076
                        },
                        {
                            "start": 1077,
                            "end": 1445
                        },
                        {
                            "start": 1446,
                            "end": 1630
                        },
                        {
                            "start": 1633,
                            "end": 1872
                        },
                        {
                            "start": 1873,
                            "end": 2032
                        },
                        {
                            "start": 2035,
                            "end": 2084
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 223,
                            "end": 227,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 227,
                            "end": 229,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9453125
                }
            ],
            "relevance_judgement": 0.9453125,
            "relevance_judgment_input_expanded": "# Title: Investigating Data Contamination for Pre-training Language Models\n# Venue: arXiv.org\n# Authors: Minhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, Sanmi Koyejo\n## Abstract\nLanguage models pre-trained on web-scale corpora demonstrate impressive capabilities on diverse downstream tasks. However, there is increasing concern whether such capabilities might arise from evaluation datasets being included in the pre-training corpus -- a phenomenon known as \\textit{data contamination} -- in a manner that artificially increases performance. There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks. In this paper, we explore the impact of data contamination at the pre-training stage by pre-training a series of GPT-2 models \\textit{from scratch}. We highlight the effect of both text contamination (\\textit{i.e.}\\ input text of the evaluation samples) and ground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and the desired outputs) from evaluation data. We also investigate the effects of repeating contamination for various downstream tasks. Additionally, we examine the prevailing n-gram-based definitions of contamination within current LLM reports, pinpointing their limitations and inadequacy. Our findings offer new insights into data contamination's effects on language model capabilities and underscore the need for independent, comprehensive contamination assessments in LLM studies.\n## Contamination Definitions\nNumerous studies on large language models (LLMs) have explored and investigated the concept of data contamination and demonstrated the robustness of these models against potential contamination in their evaluation datasets [29,2,6,27,33,34,11]. Most definitions proposed in the existing studies are based on n-gram duplication between pre-training data and evaluation data. For instance, PaLM [6] divides the evaluation data into two categories-\"clean\" and \"contaminated\"-based on whether at least 70% of all possible 8-grams in the evaluation sample were seen at least once in the pre-training corpus. Llama 2 [34] provides a more fine-grained definition: a token is considered contaminated if it appears in any token n-gram longer than 10 tokens in both the evaluation sample and the training set, and the contamination percentage of an evaluation sample is defined to be the percentage of tokens contaminated; the evaluation data are then divided into 4 buckets-\"Clean\", \"Not Clean\", \"Not Dirty\", and \"Dirty\"-based on the contamination percentage of each evaluation sample. While intuitive, these contamination definitions primarily revolve around n-gram or token overlaps, which only target direct duplications present in both training and evaluation datasets and might provide both high false positive rate (since many semantically different texts have overlaps) and false negative rate (since simple paraphrasing can evade detection [36]). Moreover, investigations relying on these definitions have predominantly centered on evaluation level analysis; in our work, we focus on pre-training level analysis as described in \u00a71. \n\nIn our experiments, we follow PaLM [6] and Llama 2's [34] definitions as well as a direct n-gram overlap detection strategy to investigate how the \"contamination\" under these definitions are different and how they affect model performance. As described in \u00a71, contamination in the pre-training corpus can appear as either textual components from evaluation datasets or with ground truth information. \n\nExisting definitions tend to overlook the latter.",
            "reference_string": "[266933004 | Jiang et al. | 2024 | Citations: 72]"
        },
        {
            "title": "A Survey on Data Contamination for Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 60,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.14425, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280875270",
                    "name": "Yu Cheng"
                },
                {
                    "authorId": "2281762754",
                    "name": "Yi Chang"
                },
                {
                    "authorId": "2280899184",
                    "name": "Yuan Wu"
                }
            ],
            "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated significant progress in various areas, such as text generation and code synthesis. However, the reliability of performance evaluation has come under scrutiny due to data contamination-the unintended overlap between training and test datasets. This overlap has the potential to artificially inflate model performance, as LLMs are typically trained on extensive datasets scraped from publicly available sources. These datasets often inadvertently overlap with the benchmarks used for evaluation, leading to an overestimation of the models' true generalization capabilities. In this paper, we first examine the definition and impacts of data contamination. Secondly, we review methods for contamination-free evaluation, focusing on three strategies: data updating-based methods, data rewriting-based methods, and prevention-based methods. Specifically, we highlight dynamic benchmarks and LLM-driven evaluation methods. Finally, we categorize contamination detecting methods based on model information dependency: white-Box, gray-Box, and black-Box detection approaches. Our survey highlights the requirements for more rigorous evaluation protocols and proposes future directions for addressing data contamination challenges.",
            "corpus_id": 276482755,
            "sentences": [
                {
                    "corpus_id": "276482755",
                    "title": "A Survey on Data Contamination for Large Language Models",
                    "text": "Recent advancements in Large Language Models (LLMs) have demonstrated significant progress in various areas, such as text generation and code synthesis. However, the reliability of performance evaluation has come under scrutiny due to data contamination-the unintended overlap between training and test datasets. This overlap has the potential to artificially inflate model performance, as LLMs are typically trained on extensive datasets scraped from publicly available sources. These datasets often inadvertently overlap with the benchmarks used for evaluation, leading to an overestimation of the models' true generalization capabilities. In this paper, we first examine the definition and impacts of data contamination. Secondly, we review methods for contamination-free evaluation, focusing on three strategies: data updating-based methods, data rewriting-based methods, and prevention-based methods. Specifically, we highlight dynamic benchmarks and LLM-driven evaluation methods. Finally, we categorize contamination detecting methods based on model information dependency: white-Box, gray-Box, and black-Box detection approaches. Our survey highlights the requirements for more rigorous evaluation protocols and proposes future directions for addressing data contamination challenges.",
                    "score": 0.5607794742009472,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94482421875
                }
            ],
            "relevance_judgement": 0.94482421875,
            "relevance_judgment_input_expanded": "# Title: A Survey on Data Contamination for Large Language Models\n# Venue: arXiv.org\n# Authors: Yu Cheng, Yi Chang, Yuan Wu\n## Abstract\nRecent advancements in Large Language Models (LLMs) have demonstrated significant progress in various areas, such as text generation and code synthesis. However, the reliability of performance evaluation has come under scrutiny due to data contamination-the unintended overlap between training and test datasets. This overlap has the potential to artificially inflate model performance, as LLMs are typically trained on extensive datasets scraped from publicly available sources. These datasets often inadvertently overlap with the benchmarks used for evaluation, leading to an overestimation of the models' true generalization capabilities. In this paper, we first examine the definition and impacts of data contamination. Secondly, we review methods for contamination-free evaluation, focusing on three strategies: data updating-based methods, data rewriting-based methods, and prevention-based methods. Specifically, we highlight dynamic benchmarks and LLM-driven evaluation methods. Finally, we categorize contamination detecting methods based on model information dependency: white-Box, gray-Box, and black-Box detection approaches. Our survey highlights the requirements for more rigorous evaluation protocols and proposes future directions for addressing data contamination challenges.\n",
            "reference_string": "[276482755 | Cheng et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 46,
            "citation_count": 6,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.21248, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313922585",
                    "name": "Anqi Zhang"
                },
                {
                    "authorId": "2314513929",
                    "name": "Chaofeng Wu"
                }
            ],
            "abstract": "While large language models (LLMs) are extensively used, there are raising concerns regarding privacy, security, and copyright due to their opaque training data, which brings the problem of detecting pre-training data on the table. Current solutions to this problem leverage techniques explored in machine learning privacy such as Membership Inference Attacks (MIAs), which heavily depend on LLMs' capability of verbatim memorization. However, this reliance presents challenges, especially given the vast amount of training data and the restricted number of effective training epochs. In this paper, we propose an adaptive pre-training data detection method which alleviates this reliance and effectively amplify the identification. Our method adaptively locates \\textit{surprising tokens} of the input. A token is surprising to a LLM if the prediction on the token is\"certain but wrong\", which refers to low Shannon entropy of the probability distribution and low probability of the ground truth token at the same time. By using the prediction probability of surprising tokens to measure \\textit{surprising}, the detection method is achieved based on the simple hypothesis that seeing seen data is less surprising for the model compared with seeing unseen data. The method can be applied without any access to the the pre-training data corpus or additional training like reference models. Our approach exhibits a consistent enhancement compared to existing methods in diverse experiments conducted on various benchmarks and models, achieving a maximum improvement of 29.5\\%. We also introduce a new benchmark Dolma-Book developed upon a novel framework, which employs book data collected both before and after model training to provide further evaluation.",
            "corpus_id": 271570943,
            "sentences": [
                {
                    "corpus_id": "271570943",
                    "title": "Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens",
                    "text": "While large language models (LLMs) are extensively used, there are raising concerns regarding privacy, security, and copyright due to their opaque training data, which brings the problem of detecting pre-training data on the table. Current solutions to this problem leverage techniques explored in machine learning privacy such as Membership Inference Attacks (MIAs), which heavily depend on LLMs' capability of verbatim memorization. However, this reliance presents challenges, especially given the vast amount of training data and the restricted number of effective training epochs. In this paper, we propose an adaptive pre-training data detection method which alleviates this reliance and effectively amplify the identification. Our method adaptively locates \\textit{surprising tokens} of the input. A token is surprising to a LLM if the prediction on the token is\"certain but wrong\", which refers to low Shannon entropy of the probability distribution and low probability of the ground truth token at the same time. By using the prediction probability of surprising tokens to measure \\textit{surprising}, the detection method is achieved based on the simple hypothesis that seeing seen data is less surprising for the model compared with seeing unseen data. The method can be applied without any access to the the pre-training data corpus or additional training like reference models. Our approach exhibits a consistent enhancement compared to existing methods in diverse experiments conducted on various benchmarks and models, achieving a maximum improvement of 29.5\\%. We also introduce a new benchmark Dolma-Book developed upon a novel framework, which employs book data collected both before and after model training to provide further evaluation.",
                    "score": 0.506400029277338,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93603515625
                }
            ],
            "relevance_judgement": 0.93603515625,
            "relevance_judgment_input_expanded": "# Title: Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens\n# Venue: arXiv.org\n# Authors: Anqi Zhang, Chaofeng Wu\n## Abstract\nWhile large language models (LLMs) are extensively used, there are raising concerns regarding privacy, security, and copyright due to their opaque training data, which brings the problem of detecting pre-training data on the table. Current solutions to this problem leverage techniques explored in machine learning privacy such as Membership Inference Attacks (MIAs), which heavily depend on LLMs' capability of verbatim memorization. However, this reliance presents challenges, especially given the vast amount of training data and the restricted number of effective training epochs. In this paper, we propose an adaptive pre-training data detection method which alleviates this reliance and effectively amplify the identification. Our method adaptively locates \\textit{surprising tokens} of the input. A token is surprising to a LLM if the prediction on the token is\"certain but wrong\", which refers to low Shannon entropy of the probability distribution and low probability of the ground truth token at the same time. By using the prediction probability of surprising tokens to measure \\textit{surprising}, the detection method is achieved based on the simple hypothesis that seeing seen data is less surprising for the model compared with seeing unseen data. The method can be applied without any access to the the pre-training data corpus or additional training like reference models. Our approach exhibits a consistent enhancement compared to existing methods in diverse experiments conducted on various benchmarks and models, achieving a maximum improvement of 29.5\\%. We also introduce a new benchmark Dolma-Book developed upon a novel framework, which employs book data collected both before and after model training to provide further evaluation.\n",
            "reference_string": "[271570943 | Zhang et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 17,
            "citation_count": 35,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.10677",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.10677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1527099159",
                    "name": "Yucheng Li"
                }
            ],
            "abstract": "Data contamination in model evaluation is getting increasingly prevalent as the massive training corpora of large language models often unintentionally include benchmark samples. Therefore, contamination analysis has became an inevitable part of reliable model evaluation. However, existing method of contamination analysis requires the access of the entire training data which is often confidential for recent models. This prevent the community to rigorously audit these models and conduct accurate assessment of their capability. In this paper, we propose a novel method to quantify contamination without the access of the full training set, that measure the extent of contamination with perplexity. Our analysis provides evidence of significant memorisation of recent foundation models in popular reading comprehension, summarisation benchmarks, while multiple choice appears less contaminated.",
            "corpus_id": 262055119,
            "sentences": [
                {
                    "corpus_id": "262055119",
                    "title": "Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation",
                    "text": "We propose a perplexity-based method to estimate language model benchmark contamination without full training data access. By comparing test perplexity to memorised and clean baselines, we can identify potential memorisation indicating leaked examples. Experiments on reading comprehension, summarisation, and multiple choice benchmarks demonstrate the approach can effectively detect contamination. This enables crucial contamination analysis when training data is unavailable, supporting credible evaluation.",
                    "score": 0.45732318312132936,
                    "section_title": "Conclusion",
                    "char_start_offset": 16318,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 122
                        },
                        {
                            "start": 123,
                            "end": 252
                        },
                        {
                            "start": 253,
                            "end": 399
                        },
                        {
                            "start": 400,
                            "end": 510
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93212890625
                },
                {
                    "corpus_id": "262055119",
                    "title": "Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation",
                    "text": "Recent years have seen remarkable progress in language models pre-trained on massive text corpora scraped from the web. However, many widely used evaluation benchmarks are also constructed from similar web sources, leading to a concerning issue of data contamination where examples from test sets are unintentionally included in training data. Contamination enables models to \"cheat\" via memorisation of test data rather than displaying true generalisation (Marie, 2023), which creates an illusion of progress, distorts model comparisons, and undermines the utility of benchmarks (Jacovi et al., 2023). \n\nRecent evaluation of language models usually involves a detailed contamination analysis of the benchmarks used (Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023a;OpenAI, 2023). These contamination reports typically contain two steps: 1) quantify potential test contamination by measuring n-gram overlap between the test set and training data, 2) compare model performance on clean vs. contaminated subsets. This procedure is essential for determining the validity of the evaluation procedure and the credibility of benchmarks in assessing model performance. \n\nHowever, such analysis method relies on the access to the full training corpora, which are often unavailable for recent closed and open-sourced foundation models (OpenAI, 2023;Touvron et al., 2023b). Many fine-tuned language models released by the community also do not include reliable contamination reports and their fine-tuning datasets are not public either. This eliminates any possibility for the community to rigorously audit these models for contamination, which prevents reliable evaluation and accurate assessment of their capabilities. In addition, current method to identify potential test contamination, i.e., finding n-gram overlap between test set and training data, is also quite computation intensive, considering the massive size of modern training corpus. \n\nIn this paper, we propose a novel approach to quantify potential contamination in language model evaluation benchmarks without accessing the entire training data. Instead of identifying n-gram overlaps between training and test set, we directly observe whether models exhibit memorisation behaviour on test instances.",
                    "score": 0.6876969480588435,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 343
                        },
                        {
                            "start": 344,
                            "end": 602
                        },
                        {
                            "start": 605,
                            "end": 795
                        },
                        {
                            "start": 796,
                            "end": 1025
                        },
                        {
                            "start": 1026,
                            "end": 1176
                        },
                        {
                            "start": 1179,
                            "end": 1378
                        },
                        {
                            "start": 1379,
                            "end": 1541
                        },
                        {
                            "start": 1542,
                            "end": 1725
                        },
                        {
                            "start": 1726,
                            "end": 1953
                        },
                        {
                            "start": 1956,
                            "end": 2118
                        },
                        {
                            "start": 2119,
                            "end": 2273
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 716,
                            "end": 736,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92822265625
                }
            ],
            "relevance_judgement": 0.93212890625,
            "relevance_judgment_input_expanded": "# Title: Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation\n# Venue: arXiv.org\n# Authors: Yucheng Li\n## Abstract\nData contamination in model evaluation is getting increasingly prevalent as the massive training corpora of large language models often unintentionally include benchmark samples. Therefore, contamination analysis has became an inevitable part of reliable model evaluation. However, existing method of contamination analysis requires the access of the entire training data which is often confidential for recent models. This prevent the community to rigorously audit these models and conduct accurate assessment of their capability. In this paper, we propose a novel method to quantify contamination without the access of the full training set, that measure the extent of contamination with perplexity. Our analysis provides evidence of significant memorisation of recent foundation models in popular reading comprehension, summarisation benchmarks, while multiple choice appears less contaminated.\n## Introduction\nRecent years have seen remarkable progress in language models pre-trained on massive text corpora scraped from the web. However, many widely used evaluation benchmarks are also constructed from similar web sources, leading to a concerning issue of data contamination where examples from test sets are unintentionally included in training data. Contamination enables models to \"cheat\" via memorisation of test data rather than displaying true generalisation (Marie, 2023), which creates an illusion of progress, distorts model comparisons, and undermines the utility of benchmarks (Jacovi et al., 2023). \n\nRecent evaluation of language models usually involves a detailed contamination analysis of the benchmarks used (Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023a;OpenAI, 2023). These contamination reports typically contain two steps: 1) quantify potential test contamination by measuring n-gram overlap between the test set and training data, 2) compare model performance on clean vs. contaminated subsets. This procedure is essential for determining the validity of the evaluation procedure and the credibility of benchmarks in assessing model performance. \n\nHowever, such analysis method relies on the access to the full training corpora, which are often unavailable for recent closed and open-sourced foundation models (OpenAI, 2023;Touvron et al., 2023b). Many fine-tuned language models released by the community also do not include reliable contamination reports and their fine-tuning datasets are not public either. This eliminates any possibility for the community to rigorously audit these models for contamination, which prevents reliable evaluation and accurate assessment of their capabilities. In addition, current method to identify potential test contamination, i.e., finding n-gram overlap between test set and training data, is also quite computation intensive, considering the massive size of modern training corpus. \n\nIn this paper, we propose a novel approach to quantify potential contamination in language model evaluation benchmarks without accessing the entire training data. Instead of identifying n-gram overlaps between training and test set, we directly observe whether models exhibit memorisation behaviour on test instances.\n\n## Conclusion\nWe propose a perplexity-based method to estimate language model benchmark contamination without full training data access. By comparing test perplexity to memorised and clean baselines, we can identify potential memorisation indicating leaked examples. Experiments on reading comprehension, summarisation, and multiple choice benchmarks demonstrate the approach can effectively detect contamination. This enables crucial contamination analysis when training data is unavailable, supporting credible evaluation.",
            "reference_string": "[262055119 | Li | 2023 | Citations: 35]"
        },
        {
            "title": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding",
            "venue": "International Conference on Computational Linguistics",
            "year": 2024,
            "reference_count": 30,
            "citation_count": 6,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.03363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319955259",
                    "name": "Cheng Wang"
                },
                {
                    "authorId": "2280103482",
                    "name": "Yiwei Wang"
                },
                {
                    "authorId": "2305483565",
                    "name": "Bryan Hooi"
                },
                {
                    "authorId": "1928716951",
                    "name": "Yujun Cai"
                },
                {
                    "authorId": "2256996328",
                    "name": "Nanyun Peng"
                },
                {
                    "authorId": "2257127887",
                    "name": "Kai-Wei Chang"
                }
            ],
            "abstract": "The training data in large language models is key to their success, but it also presents privacy and security risks, as it may contain sensitive information. Detecting pre-training data is crucial for mitigating these concerns. Existing methods typically analyze target text in isolation or solely with non-member contexts, overlooking potential insights from simultaneously considering both member and non-member contexts. While previous work suggested that member contexts provide little information due to the minor distributional shift they induce, our analysis reveals that these subtle shifts can be effectively leveraged when contrasted with non-member contexts. In this paper, we propose Con-ReCall, a novel approach that leverages the asymmetric distributional shifts induced by member and non-member contexts through contrastive decoding, amplifying subtle differences to enhance membership inference. Extensive empirical evaluations demonstrate that Con-ReCall achieves state-of-the-art performance on the WikiMIA benchmark and is robust against various text manipulation techniques.",
            "corpus_id": 272423548,
            "sentences": [
                {
                    "corpus_id": "272423548",
                    "title": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding",
                    "text": "Results on WikiMIA. Table 2 summarizes the experimental results on WikiMIA, demonstrating CON-RECALL's significant improvements over baseline methods. In terms of AUC performance, our method improved upon ReCall by 7.4%, 6.6%, and 5.7% on WikiMIA-32, -64, and -128 respectively, achieving an average improvement of 6.6% and state-of-the-art performance. For TPR@5%FPR, CON-RECALL outperformed the runner-up by even larger margins: 30.0%, 34.8%, and 27.6% on WikiMIA-32, -64, and -128 respectively, with an average improvement of 30.8%. Notably, CON-RECALL achieves the best performance across models of different sizes, from Mamba-1.4B to LLaMA-30B, demonstrating its robustness and effectiveness. The consistent performance across varying sequence lengths suggests that CON-RECALL effectively identifies membership information in both short and long text samples, underlining its potential as a powerful tool for detecting pre-training data in large language models in diverse scenarios. \n\nResults on MIMIR. We summarize the experimental results on MIMIR in Appendix D. The performance of CON-RECALL on the MIMIR benchmark demonstrates its competitive edge across various datasets and model sizes. In the 7-gram setting, CON-RECALL consistently achieved top-tier results, often outperforming baseline methods. Notably, on several datasets, our method frequently secured the highest scores in both AUC and TPR metrics. In the 13-gram setting, CON-RECALL maintained its strong performance, particularly with larger model sizes. While overall performance decreased compared to the 7-gram setting, still held leading positions across multiple datasets. It's worth noting that CON-RECALL exhibited superior performance when dealing with larger models, indicating good scalability for more complex and larger language models. Although other methods occasionally showed slight advantages in certain datasets, CON-RECALL's overall robust performance underscores its potential as an effective method for detecting pre-training data in large language models.",
                    "score": 0.5578765188991597,
                    "section_title": "Results",
                    "char_start_offset": 14311,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 19
                        },
                        {
                            "start": 20,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 353
                        },
                        {
                            "start": 354,
                            "end": 535
                        },
                        {
                            "start": 536,
                            "end": 697
                        },
                        {
                            "start": 698,
                            "end": 988
                        },
                        {
                            "start": 991,
                            "end": 1008
                        },
                        {
                            "start": 1009,
                            "end": 1198
                        },
                        {
                            "start": 1199,
                            "end": 1310
                        },
                        {
                            "start": 1311,
                            "end": 1418
                        },
                        {
                            "start": 1419,
                            "end": 1526
                        },
                        {
                            "start": 1527,
                            "end": 1649
                        },
                        {
                            "start": 1650,
                            "end": 1820
                        },
                        {
                            "start": 1821,
                            "end": 2049
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9306640625
                }
            ],
            "relevance_judgement": 0.9306640625,
            "relevance_judgment_input_expanded": "# Title: Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding\n# Venue: International Conference on Computational Linguistics\n# Authors: Cheng Wang, Yiwei Wang, Bryan Hooi, Yujun Cai, Nanyun Peng, Kai-Wei Chang\n## Abstract\nThe training data in large language models is key to their success, but it also presents privacy and security risks, as it may contain sensitive information. Detecting pre-training data is crucial for mitigating these concerns. Existing methods typically analyze target text in isolation or solely with non-member contexts, overlooking potential insights from simultaneously considering both member and non-member contexts. While previous work suggested that member contexts provide little information due to the minor distributional shift they induce, our analysis reveals that these subtle shifts can be effectively leveraged when contrasted with non-member contexts. In this paper, we propose Con-ReCall, a novel approach that leverages the asymmetric distributional shifts induced by member and non-member contexts through contrastive decoding, amplifying subtle differences to enhance membership inference. Extensive empirical evaluations demonstrate that Con-ReCall achieves state-of-the-art performance on the WikiMIA benchmark and is robust against various text manipulation techniques.\n## Results\nResults on WikiMIA. Table 2 summarizes the experimental results on WikiMIA, demonstrating CON-RECALL's significant improvements over baseline methods. In terms of AUC performance, our method improved upon ReCall by 7.4%, 6.6%, and 5.7% on WikiMIA-32, -64, and -128 respectively, achieving an average improvement of 6.6% and state-of-the-art performance. For TPR@5%FPR, CON-RECALL outperformed the runner-up by even larger margins: 30.0%, 34.8%, and 27.6% on WikiMIA-32, -64, and -128 respectively, with an average improvement of 30.8%. Notably, CON-RECALL achieves the best performance across models of different sizes, from Mamba-1.4B to LLaMA-30B, demonstrating its robustness and effectiveness. The consistent performance across varying sequence lengths suggests that CON-RECALL effectively identifies membership information in both short and long text samples, underlining its potential as a powerful tool for detecting pre-training data in large language models in diverse scenarios. \n\nResults on MIMIR. We summarize the experimental results on MIMIR in Appendix D. The performance of CON-RECALL on the MIMIR benchmark demonstrates its competitive edge across various datasets and model sizes. In the 7-gram setting, CON-RECALL consistently achieved top-tier results, often outperforming baseline methods. Notably, on several datasets, our method frequently secured the highest scores in both AUC and TPR metrics. In the 13-gram setting, CON-RECALL maintained its strong performance, particularly with larger model sizes. While overall performance decreased compared to the 7-gram setting, still held leading positions across multiple datasets. It's worth noting that CON-RECALL exhibited superior performance when dealing with larger models, indicating good scalability for more complex and larger language models. Although other methods occasionally showed slight advantages in certain datasets, CON-RECALL's overall robust performance underscores its potential as an effective method for detecting pre-training data in large language models.",
            "reference_string": "[272423548 | Wang et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Data Contamination Can Cross Language Barriers",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 26,
            "citation_count": 12,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.13236, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2297810571",
                    "name": "Feng Yao"
                },
                {
                    "authorId": "1505801820",
                    "name": "Yufan Zhuang"
                },
                {
                    "authorId": "2307478708",
                    "name": "Zihao Sun"
                },
                {
                    "authorId": "2307559061",
                    "name": "Sunan Xu"
                },
                {
                    "authorId": "2297831856",
                    "name": "Animesh Kumar"
                },
                {
                    "authorId": "2297773933",
                    "name": "Jingbo Shang"
                }
            ],
            "abstract": "The opacity in developing large language models (LLMs) is raising growing concerns about the potential contamination of public benchmarks in the pre-training data. Existing contamination detection methods are typically based on the text overlap between training and evaluation data, which can be too superficial to reflect deeper forms of contamination. In this paper, we first present a cross-lingual form of contamination that inflates LLMs\u2019 performance while evading current detection methods, deliberately injected by overfitting LLMs on the translated versions of benchmark test sets. Then, we propose generalization-based approaches to unmask such deeply concealed contamination. Specifically, we examine the LLM\u2019s performance change after modifying the original benchmark by replacing the false answer choices with correct ones from other questions. Contaminated models can hardly generalize to such easier situations, where the false choices can be not even wrong, as all choices are correct in their memorization. Experimental results demonstrate that cross-lingual contamination can easily fool existing detection methods, but not ours. In addition, we discuss the potential utilization of cross-lingual contamination in interpreting LLMs\u2019 working mechanisms and in post-training LLMs for enhanced multilingual capabilities.",
            "corpus_id": 270620798,
            "sentences": [
                {
                    "corpus_id": "270620798",
                    "title": "Data Contamination Can Cross Language Barriers",
                    "text": "The opacity in developing large language models (LLMs) is raising growing concerns about the potential contamination of public benchmarks in the pre-training data. Existing contamination detection methods are typically based on the text overlap between training and evaluation data, which can be too superficial to reflect deeper forms of contamination. In this paper, we first present a cross-lingual form of contamination that inflates LLMs\u2019 performance while evading current detection methods, deliberately injected by overfitting LLMs on the translated versions of benchmark test sets. Then, we propose generalization-based approaches to unmask such deeply concealed contamination. Specifically, we examine the LLM\u2019s performance change after modifying the original benchmark by replacing the false answer choices with correct ones from other questions. Contaminated models can hardly generalize to such easier situations, where the false choices can be not even wrong, as all choices are correct in their memorization. Experimental results demonstrate that cross-lingual contamination can easily fool existing detection methods, but not ours. In addition, we discuss the potential utilization of cross-lingual contamination in interpreting LLMs\u2019 working mechanisms and in post-training LLMs for enhanced multilingual capabilities.",
                    "score": 0.594707684613343,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92822265625
                }
            ],
            "relevance_judgement": 0.92822265625,
            "relevance_judgment_input_expanded": "# Title: Data Contamination Can Cross Language Barriers\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Feng Yao, Yufan Zhuang, Zihao Sun, Sunan Xu, Animesh Kumar, Jingbo Shang\n## Abstract\nThe opacity in developing large language models (LLMs) is raising growing concerns about the potential contamination of public benchmarks in the pre-training data. Existing contamination detection methods are typically based on the text overlap between training and evaluation data, which can be too superficial to reflect deeper forms of contamination. In this paper, we first present a cross-lingual form of contamination that inflates LLMs\u2019 performance while evading current detection methods, deliberately injected by overfitting LLMs on the translated versions of benchmark test sets. Then, we propose generalization-based approaches to unmask such deeply concealed contamination. Specifically, we examine the LLM\u2019s performance change after modifying the original benchmark by replacing the false answer choices with correct ones from other questions. Contaminated models can hardly generalize to such easier situations, where the false choices can be not even wrong, as all choices are correct in their memorization. Experimental results demonstrate that cross-lingual contamination can easily fool existing detection methods, but not ours. In addition, we discuss the potential utilization of cross-lingual contamination in interpreting LLMs\u2019 working mechanisms and in post-training LLMs for enhanced multilingual capabilities.\n",
            "reference_string": "[270620798 | Yao et al. | 2024 | Citations: 12]"
        },
        {
            "title": "Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.03923, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306863572",
                    "name": "Aaditya K. Singh"
                },
                {
                    "authorId": "1665851460",
                    "name": "Muhammed Yusuf Kocyigit"
                },
                {
                    "authorId": "2282542314",
                    "name": "Andrew Poulton"
                },
                {
                    "authorId": "71039937",
                    "name": "David Esiobu"
                },
                {
                    "authorId": "2253400960",
                    "name": "Maria Lomeli"
                },
                {
                    "authorId": "2253402270",
                    "name": "Gergely Szilvasy"
                },
                {
                    "authorId": "3449411",
                    "name": "Dieuwke Hupkes"
                }
            ],
            "abstract": "Hampering the interpretation of benchmark scores, evaluation data contamination has become a growing concern in the evaluation of LLMs, and an active area of research studies its effects. While evaluation data contamination is easily understood intuitively, it is surprisingly difficult to define precisely which samples should be considered contaminated and, consequently, how it impacts benchmark scores. We propose that these questions should be addressed together and that contamination metrics can be assessed based on whether models benefit from the examples they mark contaminated. We propose a novel analysis method called ConTAM, and show with a large scale survey of existing and novel n-gram based contamination metrics across 13 benchmarks and 7 models from 2 different families that ConTAM can be used to better understand evaluation data contamination and its effects. We find that contamination may have a much larger effect than reported in recent LLM releases and benefits models differently at different scales. We also find that considering only the longest contaminated substring provides a better signal than considering a union of all contaminated substrings, and that doing model and benchmark specific threshold analysis greatly increases the specificity of the results. Lastly, we investigate the impact of hyperparameter choices, finding that, among other things, both using larger values of n and disregarding matches that are infrequent in the pre-training data lead to many false negatives. With ConTAM, we provide a method to empirically ground evaluation data contamination metrics in downstream effects. With our exploration, we shed light on how evaluation data contamination can impact LLMs and provide insight into the considerations important when doing contamination analysis. We end our paper by discussing these in more detail and providing concrete suggestions for future work.",
            "corpus_id": 273850342,
            "sentences": [
                {
                    "corpus_id": "273850342",
                    "title": "Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?",
                    "text": "As large language models (LLMs) improve, their evaluation becomes increasingly challenging. A frequent concern in this context (Sainz et al., 2024a(Sainz et al., , 2023;;Jacovi et al., 2023;Elazar et al., 2024, i.a.) is that samples from evaluation benchmarks sometimes (partially) appear in the data used to train that same model, also known as evaluation data contamination. Such inadvertent 'training on the test set' makes it difficult to interpret scores on evaluation benchmarks: does a better score reflect an improvement on the ability that the benchmark intends to measure or does it indicate that a model succesfully memorised the benchmark data? \n\nAnswering this seemingly simple question is far from straightforward. Even setting aside practical issues such as the sheer size of pre-training corpora and the fact that they are not often publicly available, it Figures 3, 4 and 10) \n\n\u2022 While there is no true one-size-fits-all approach to contamination detection, using the longest contaminated substring rather than a union of all matches works better across the board, adequately detecting contamination in cases where no other metric did (Figures 2 and 13) \n\n\u2022 For virtually all benchmarks we considered, smaller n is better and even one occurrence in the pre-training data matters: both using values of n larger than 8 and setting a minimal count higher than one for occurrence in the pre-training corpus leads to false negatives (Figure 6 and 7b); \n\n\u2022 The impact of contamination changes with scale, in cases where there still is performance to be gained, larger models are better able to leverage contamination than smaller models (Figure 5, 8 and 9); \n\n\u2022 Consequently, to find the most adequate contamination metric, it is important to do model-specific threshold selection (Figure 1). \n\nWith our work, we aim to contribute to an informed discussion about evaluation data contamination and its effects, as well as provide methodology for both researchers and practitioners to explore this question further and contrast the effect of different contamination metrics and hyper-parameters as newer pre-training corpora (Soldaini et al., 2024;Penedo et al., 2024) and benchmarks (Laurent et al., 2024) are released.",
                    "score": 0.7267077015797403,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 91
                        },
                        {
                            "start": 92,
                            "end": 376
                        },
                        {
                            "start": 377,
                            "end": 656
                        },
                        {
                            "start": 659,
                            "end": 728
                        },
                        {
                            "start": 729,
                            "end": 892
                        },
                        {
                            "start": 895,
                            "end": 1170
                        },
                        {
                            "start": 1173,
                            "end": 1463
                        },
                        {
                            "start": 1466,
                            "end": 1668
                        },
                        {
                            "start": 1671,
                            "end": 1803
                        },
                        {
                            "start": 1806,
                            "end": 2229
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 170,
                            "end": 190,
                            "matchedPaperCorpusId": "258741333"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92529296875
                }
            ],
            "relevance_judgement": 0.92529296875,
            "relevance_judgment_input_expanded": "# Title: Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?\n# Venue: arXiv.org\n# Authors: Aaditya K. Singh, Muhammed Yusuf Kocyigit, Andrew Poulton, David Esiobu, Maria Lomeli, Gergely Szilvasy, Dieuwke Hupkes\n## Abstract\nHampering the interpretation of benchmark scores, evaluation data contamination has become a growing concern in the evaluation of LLMs, and an active area of research studies its effects. While evaluation data contamination is easily understood intuitively, it is surprisingly difficult to define precisely which samples should be considered contaminated and, consequently, how it impacts benchmark scores. We propose that these questions should be addressed together and that contamination metrics can be assessed based on whether models benefit from the examples they mark contaminated. We propose a novel analysis method called ConTAM, and show with a large scale survey of existing and novel n-gram based contamination metrics across 13 benchmarks and 7 models from 2 different families that ConTAM can be used to better understand evaluation data contamination and its effects. We find that contamination may have a much larger effect than reported in recent LLM releases and benefits models differently at different scales. We also find that considering only the longest contaminated substring provides a better signal than considering a union of all contaminated substrings, and that doing model and benchmark specific threshold analysis greatly increases the specificity of the results. Lastly, we investigate the impact of hyperparameter choices, finding that, among other things, both using larger values of n and disregarding matches that are infrequent in the pre-training data lead to many false negatives. With ConTAM, we provide a method to empirically ground evaluation data contamination metrics in downstream effects. With our exploration, we shed light on how evaluation data contamination can impact LLMs and provide insight into the considerations important when doing contamination analysis. We end our paper by discussing these in more detail and providing concrete suggestions for future work.\n## Introduction\nAs large language models (LLMs) improve, their evaluation becomes increasingly challenging. A frequent concern in this context (Sainz et al., 2024a(Sainz et al., , 2023;;Jacovi et al., 2023;Elazar et al., 2024, i.a.) is that samples from evaluation benchmarks sometimes (partially) appear in the data used to train that same model, also known as evaluation data contamination. Such inadvertent 'training on the test set' makes it difficult to interpret scores on evaluation benchmarks: does a better score reflect an improvement on the ability that the benchmark intends to measure or does it indicate that a model succesfully memorised the benchmark data? \n\nAnswering this seemingly simple question is far from straightforward. Even setting aside practical issues such as the sheer size of pre-training corpora and the fact that they are not often publicly available, it Figures 3, 4 and 10) \n\n\u2022 While there is no true one-size-fits-all approach to contamination detection, using the longest contaminated substring rather than a union of all matches works better across the board, adequately detecting contamination in cases where no other metric did (Figures 2 and 13) \n\n\u2022 For virtually all benchmarks we considered, smaller n is better and even one occurrence in the pre-training data matters: both using values of n larger than 8 and setting a minimal count higher than one for occurrence in the pre-training corpus leads to false negatives (Figure 6 and 7b); \n\n\u2022 The impact of contamination changes with scale, in cases where there still is performance to be gained, larger models are better able to leverage contamination than smaller models (Figure 5, 8 and 9); \n\n\u2022 Consequently, to find the most adequate contamination metric, it is important to do model-specific threshold selection (Figure 1). \n\nWith our work, we aim to contribute to an informed discussion about evaluation data contamination and its effects, as well as provide methodology for both researchers and practitioners to explore this question further and contrast the effect of different contamination metrics and hyper-parameters as newer pre-training corpora (Soldaini et al., 2024;Penedo et al., 2024) and benchmarks (Laurent et al., 2024) are released.",
            "reference_string": "[273850342 | Singh et al. | 2024 | Citations: 13]"
        },
        {
            "title": "Training on the Benchmark Is Not All You Need",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.01790, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2266469238",
                    "name": "Shiwen Ni"
                },
                {
                    "authorId": "2281795647",
                    "name": "Xiangtao Kong"
                },
                {
                    "authorId": "2278907861",
                    "name": "Chengming Li"
                },
                {
                    "authorId": "2266696336",
                    "name": "Xiping Hu"
                },
                {
                    "authorId": "2266809482",
                    "name": "Ruifeng Xu"
                },
                {
                    "authorId": "50077416",
                    "name": "Jia Zhu"
                },
                {
                    "authorId": "2261787430",
                    "name": "Min Yang"
                }
            ],
            "abstract": "The success of Large Language Models (LLMs) relies heavily on the huge amount of pre-training data learned in the pre-training phase. The opacity of the pre-training process and the training data causes the results of many benchmark tests to become unreliable. If any model has been trained on a benchmark test set, it can seriously hinder the health of the field. In order to automate and efficiently test the capabilities of large language models, numerous mainstream benchmarks adopt a multiple-choice format. As the swapping of the contents of multiple-choice options does not affect the meaning of the question itself, we propose a simple and effective data leakage detection method based on this property. Specifically, we shuffle the contents of the options in the data to generate the corresponding derived data sets, and then detect data leakage based on the model's log probability distribution over the derived data sets. If there is a maximum and outlier in the set of log probabilities, it indicates that the data is leaked. Our method is able to work under gray-box conditions without access to model training data or weights, effectively identifying data leakage from benchmark test sets in model pre-training data, including both normal scenarios and complex scenarios where options may have been shuffled intentionally or unintentionally. Through experiments based on two LLMs and benchmark designs, we demonstrate the effectiveness of our method. In addition, we evaluate the degree of data leakage of 35 mainstream open-source LLMs on four benchmark datasets and give a ranking of the leaked LLMs for each benchmark, and we find that the Qwen family of LLMs has the highest degree of data leakage.",
            "corpus_id": 272367930,
            "sentences": [
                {
                    "corpus_id": "272367930",
                    "title": "Training on the Benchmark Is Not All You Need",
                    "text": "The success of Large Language Models (LLMs) relies heavily on the huge amount of pre-training data learned in the pre-training phase. The opacity of the pre-training process and the training data causes the results of many benchmark tests to become unreliable. If any model has been trained on a benchmark test set, it can seriously hinder the health of the field. In order to automate and efficiently test the capabilities of large language models, numerous mainstream benchmarks adopt a multiple-choice format. As the swapping of the contents of multiple-choice options does not affect the meaning of the question itself, we propose a simple and effective data leakage detection method based on this property. Specifically, we shuffle the contents of the options in the data to generate the corresponding derived data sets, and then detect data leakage based on the model's log probability distribution over the derived data sets. If there is a maximum and outlier in the set of log probabilities, it indicates that the data is leaked. Our method is able to work under gray-box conditions without access to model training data or weights, effectively identifying data leakage from benchmark test sets in model pre-training data, including both normal scenarios and complex scenarios where options may have been shuffled intentionally or unintentionally. Through experiments based on two LLMs and benchmark designs, we demonstrate the effectiveness of our method. In addition, we evaluate the degree of data leakage of 35 mainstream open-source LLMs on four benchmark datasets and give a ranking of the leaked LLMs for each benchmark, and we find that the Qwen family of LLMs has the highest degree of data leakage.",
                    "score": 0.6048125786390052,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92529296875
                }
            ],
            "relevance_judgement": 0.92529296875,
            "relevance_judgment_input_expanded": "# Title: Training on the Benchmark Is Not All You Need\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Shiwen Ni, Xiangtao Kong, Chengming Li, Xiping Hu, Ruifeng Xu, Jia Zhu, Min Yang\n## Abstract\nThe success of Large Language Models (LLMs) relies heavily on the huge amount of pre-training data learned in the pre-training phase. The opacity of the pre-training process and the training data causes the results of many benchmark tests to become unreliable. If any model has been trained on a benchmark test set, it can seriously hinder the health of the field. In order to automate and efficiently test the capabilities of large language models, numerous mainstream benchmarks adopt a multiple-choice format. As the swapping of the contents of multiple-choice options does not affect the meaning of the question itself, we propose a simple and effective data leakage detection method based on this property. Specifically, we shuffle the contents of the options in the data to generate the corresponding derived data sets, and then detect data leakage based on the model's log probability distribution over the derived data sets. If there is a maximum and outlier in the set of log probabilities, it indicates that the data is leaked. Our method is able to work under gray-box conditions without access to model training data or weights, effectively identifying data leakage from benchmark test sets in model pre-training data, including both normal scenarios and complex scenarios where options may have been shuffled intentionally or unintentionally. Through experiments based on two LLMs and benchmark designs, we demonstrate the effectiveness of our method. In addition, we evaluate the degree of data leakage of 35 mainstream open-source LLMs on four benchmark datasets and give a ranking of the leaked LLMs for each benchmark, and we find that the Qwen family of LLMs has the highest degree of data leakage.\n",
            "reference_string": "[272367930 | Ni et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 7,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.13990, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2307992515",
                    "name": "Qin Zhu"
                },
                {
                    "authorId": "2290056525",
                    "name": "Qingyuan Cheng"
                },
                {
                    "authorId": "2307468261",
                    "name": "Runyu Peng"
                },
                {
                    "authorId": "50080067",
                    "name": "Xiaonan Li"
                },
                {
                    "authorId": "2136108329",
                    "name": "Tengxiao Liu"
                },
                {
                    "authorId": "2307468261",
                    "name": "Runyu Peng"
                },
                {
                    "authorId": "2282972251",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "2284750473",
                    "name": "Xuanjing Huang"
                }
            ],
            "abstract": "The training process of large language models (LLMs) often involves varying degrees of test data contamination. Although current LLMs are achieving increasingly better performance on various benchmarks, their performance in practical applications does not always match their benchmark results. Leakage of benchmarks can prevent the accurate assessment of LLMs' true performance. However, constructing new benchmarks is costly, labor-intensive and still carries the risk of leakage. Therefore, in this paper, we ask the question, Can we reuse these leaked benchmarks for LLM evaluation? We propose Inference-Time Decontamination (ITD) to address this issue by detecting and rewriting leaked samples without altering their difficulties. ITD can mitigate performance inflation caused by memorizing leaked benchmarks. Our proof-of-concept experiments demonstrate that ITD reduces inflated accuracy by 22.9% on GSM8K and 19.0% on MMLU. On MMLU, using Inference-time Decontamination can lead to a decrease in the results of Phi3 and Mistral by 6.7% and 3.6% respectively. We hope that ITD can provide more truthful evaluation results for large language models.",
            "corpus_id": 270619707,
            "sentences": [
                {
                    "corpus_id": "270619707",
                    "title": "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation",
                    "text": "Contamination Detection Traditional contamination detection methods directly calculate the overlap between pre-training data and evaluation datasets, including n-gram analysis (Touvron et al., 2023b;OpenAI, 2023;Team et al., 2023;Bai et al., 2023) and BM25 (Jiang et al., 2024) for indexing and matching.However, as pre-training data grows exponentially, even simple n-gram statistics become extremely resource-intensive.Yang et al. (2023c); Gunasekar et al. (2023) find n-gram detection unreliable due to unintentional contamination risks.More importantly, training corpora for mainstream LLMs are mostly inaccessible, so recent research has turned to focus on: i)-exploiting the distributional differences between the benchmark training set and the test set to evaluated (Xu et al., 2024).ii)-Evaluate sample-level contamination by providing text segments and black-box access to the LLM (Shi et al., 2023).Other work evaluates contamination through LLM-generated content, limited by the LLM's comprehension abilities to instrurction (Deng et al., 2023;Golchin and Surdeanu, 2023a).Some studies test if models can coherently continue a given sample part (Golchin and Surdeanu, 2023b).Contamination detection remains a critical concern that should be addressed in benchmarks rather than affecting a fair assessment of the model's capabilities.\n\nDecontamination Decontamination involves avoiding or mitigating the negative effects of contamination.Typically, decontamination applied in the training phase, model developers using various methods to remove these overlap between pre-training data and evaluation data (OpenAI, 2023;Touvron et al., 2023b;Radford et al., 2019;Brown et al., 2020a;Chowdhery et al., 2023).\n\nBesides, new datasets can also be created to avoid contamination The LatestEval (Li et al., 2024) avoids model contamination by strictly adhering to a temporal sequence, using texts published within a recent time window to construct new question-answer sets from the latest Wikipedia data.",
                    "score": 0.5521112250007545,
                    "section_title": "Related Work",
                    "char_start_offset": 5158,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 304
                        },
                        {
                            "start": 304,
                            "end": 421
                        },
                        {
                            "start": 421,
                            "end": 540
                        },
                        {
                            "start": 540,
                            "end": 791
                        },
                        {
                            "start": 791,
                            "end": 909
                        },
                        {
                            "start": 909,
                            "end": 1084
                        },
                        {
                            "start": 1084,
                            "end": 1186
                        },
                        {
                            "start": 1186,
                            "end": 1344
                        },
                        {
                            "start": 1346,
                            "end": 1448
                        },
                        {
                            "start": 1448,
                            "end": 1716
                        },
                        {
                            "start": 1718,
                            "end": 2007
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1651,
                            "end": 1672,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 1692,
                            "end": 1715,
                            "matchedPaperCorpusId": "247951931"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92431640625
                }
            ],
            "relevance_judgement": 0.92431640625,
            "relevance_judgment_input_expanded": "# Title: Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Qin Zhu, Qingyuan Cheng, Runyu Peng, Xiaonan Li, Tengxiao Liu, Runyu Peng, Xipeng Qiu, Xuanjing Huang\n## Abstract\nThe training process of large language models (LLMs) often involves varying degrees of test data contamination. Although current LLMs are achieving increasingly better performance on various benchmarks, their performance in practical applications does not always match their benchmark results. Leakage of benchmarks can prevent the accurate assessment of LLMs' true performance. However, constructing new benchmarks is costly, labor-intensive and still carries the risk of leakage. Therefore, in this paper, we ask the question, Can we reuse these leaked benchmarks for LLM evaluation? We propose Inference-Time Decontamination (ITD) to address this issue by detecting and rewriting leaked samples without altering their difficulties. ITD can mitigate performance inflation caused by memorizing leaked benchmarks. Our proof-of-concept experiments demonstrate that ITD reduces inflated accuracy by 22.9% on GSM8K and 19.0% on MMLU. On MMLU, using Inference-time Decontamination can lead to a decrease in the results of Phi3 and Mistral by 6.7% and 3.6% respectively. We hope that ITD can provide more truthful evaluation results for large language models.\n## Related Work\nContamination Detection Traditional contamination detection methods directly calculate the overlap between pre-training data and evaluation datasets, including n-gram analysis (Touvron et al., 2023b;OpenAI, 2023;Team et al., 2023;Bai et al., 2023) and BM25 (Jiang et al., 2024) for indexing and matching.However, as pre-training data grows exponentially, even simple n-gram statistics become extremely resource-intensive.Yang et al. (2023c); Gunasekar et al. (2023) find n-gram detection unreliable due to unintentional contamination risks.More importantly, training corpora for mainstream LLMs are mostly inaccessible, so recent research has turned to focus on: i)-exploiting the distributional differences between the benchmark training set and the test set to evaluated (Xu et al., 2024).ii)-Evaluate sample-level contamination by providing text segments and black-box access to the LLM (Shi et al., 2023).Other work evaluates contamination through LLM-generated content, limited by the LLM's comprehension abilities to instrurction (Deng et al., 2023;Golchin and Surdeanu, 2023a).Some studies test if models can coherently continue a given sample part (Golchin and Surdeanu, 2023b).Contamination detection remains a critical concern that should be addressed in benchmarks rather than affecting a fair assessment of the model's capabilities.\n\nDecontamination Decontamination involves avoiding or mitigating the negative effects of contamination.Typically, decontamination applied in the training phase, model developers using various methods to remove these overlap between pre-training data and evaluation data (OpenAI, 2023;Touvron et al., 2023b;Radford et al., 2019;Brown et al., 2020a;Chowdhery et al., 2023).\n\nBesides, new datasets can also be created to avoid contamination The LatestEval (Li et al., 2024) avoids model contamination by strictly adhering to a temporal sequence, using texts published within a recent time window to construct new question-answer sets from the latest Wikipedia data.",
            "reference_string": "[270619707 | Zhu et al. | 2024 | Citations: 7]"
        },
        {
            "title": "An Open-Source Data Contamination Report for Large Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 32,
            "citation_count": 19,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.17589, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1527099159",
                    "name": "Yucheng Li"
                }
            ],
            "abstract": "Data contamination in model evaluation has become increasingly prevalent with the growing popularity of large language models. It allows models to\"cheat\"via memorisation instead of displaying true capabilities. Therefore, contamination analysis has become an crucial part of reliable model evaluation to validate results. However, existing contamination analysis is usually conducted internally by large language model developers and often lacks transparency and completeness. This paper presents an extensive data contamination report for over 15 popular large language models across six popular multiple-choice QA benchmarks. We also introduce an open-source pipeline that enables the community to perform contamination analysis on customised data and models. Our experiments reveal varying contamination levels ranging from 1\\% to 45\\% across benchmarks, with the contamination degree increasing rapidly over time. Performance analysis of large language models indicates that data contamination does not necessarily lead to increased model metrics: while significant accuracy boosts of up to 14\\% and 7\\% are observed on contaminated C-Eval and Hellaswag benchmarks, only a minimal increase is noted on contaminated MMLU. We also find larger models seem able to gain more advantages than smaller models on contaminated test sets.",
            "corpus_id": 266359062,
            "sentences": [
                {
                    "corpus_id": "266359062",
                    "title": "An Open-Source Data Contamination Report for Large Language Models",
                    "text": "The central goal of data contamination analysis is to categorise test samples as either clean or contaminated and then evaluate models separately on the clean and contaminated samples to assess the impact of contamination on the performance metrics. In this section, we describe our methodology to identify contaminated test samples. The basic idea of detecting contaminated examples in our method is to check whether test examples appear verbatim in Common Crawl. We base on Common Crawl because it is completely open-sourced and often comprises the majority of pre-training data for large language models, e.g., Common Crawl weights over 80% in GPT-3 and LLaMA training data (Brown et al., 2020;Touvron et al., 2023a). \n\nWe tailor our search window based on each model's training data collection period. For example, LLaMA models use Common Crawl dumps from 2017 to 2020, which makes our contamination search window 2017-2020. For models with To construct the search queries, we verbalise examples accordingly and make sure the question and the correct answer are involved in the queries. For example: \n\nQuestion: The flaw in Anderson's ACT theory was that some considered it ____.",
                    "score": 0.5660501563336956,
                    "section_title": "Our Approach",
                    "char_start_offset": 10719,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 249
                        },
                        {
                            "start": 250,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 464
                        },
                        {
                            "start": 465,
                            "end": 720
                        },
                        {
                            "start": 723,
                            "end": 805
                        },
                        {
                            "start": 806,
                            "end": 928
                        },
                        {
                            "start": 929,
                            "end": 1090
                        },
                        {
                            "start": 1091,
                            "end": 1103
                        },
                        {
                            "start": 1106,
                            "end": 1183
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92138671875
                }
            ],
            "relevance_judgement": 0.92138671875,
            "relevance_judgment_input_expanded": "# Title: An Open-Source Data Contamination Report for Large Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Yucheng Li\n## Abstract\nData contamination in model evaluation has become increasingly prevalent with the growing popularity of large language models. It allows models to\"cheat\"via memorisation instead of displaying true capabilities. Therefore, contamination analysis has become an crucial part of reliable model evaluation to validate results. However, existing contamination analysis is usually conducted internally by large language model developers and often lacks transparency and completeness. This paper presents an extensive data contamination report for over 15 popular large language models across six popular multiple-choice QA benchmarks. We also introduce an open-source pipeline that enables the community to perform contamination analysis on customised data and models. Our experiments reveal varying contamination levels ranging from 1\\% to 45\\% across benchmarks, with the contamination degree increasing rapidly over time. Performance analysis of large language models indicates that data contamination does not necessarily lead to increased model metrics: while significant accuracy boosts of up to 14\\% and 7\\% are observed on contaminated C-Eval and Hellaswag benchmarks, only a minimal increase is noted on contaminated MMLU. We also find larger models seem able to gain more advantages than smaller models on contaminated test sets.\n## Our Approach\nThe central goal of data contamination analysis is to categorise test samples as either clean or contaminated and then evaluate models separately on the clean and contaminated samples to assess the impact of contamination on the performance metrics. In this section, we describe our methodology to identify contaminated test samples. The basic idea of detecting contaminated examples in our method is to check whether test examples appear verbatim in Common Crawl. We base on Common Crawl because it is completely open-sourced and often comprises the majority of pre-training data for large language models, e.g., Common Crawl weights over 80% in GPT-3 and LLaMA training data (Brown et al., 2020;Touvron et al., 2023a). \n\nWe tailor our search window based on each model's training data collection period. For example, LLaMA models use Common Crawl dumps from 2017 to 2020, which makes our contamination search window 2017-2020. For models with To construct the search queries, we verbalise examples accordingly and make sure the question and the correct answer are involved in the queries. For example: \n\nQuestion: The flaw in Anderson's ACT theory was that some considered it ____.",
            "reference_string": "[266359062 | Li | 2023 | Citations: 19]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "272689919",
            "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges",
            "text": "Detecting training data through probability inference and reconstruction has long been a wellestablished approach (Shokri et al., 2017;Carlini et al., 2021). Recently, the challenge of data contamination in large language models has garnered significant attention due to its potential to skew model evaluation and misrepresent true performance. Sainz et al. (2023) highlighted the risks of contamination, particularly emphasizing that while test set contamination invalidates benchmarks, contamination in training and validation sets is less concerning unless zero or few-shot learning claims are made. Contamination is most likely during the pre-training phase, where massive text corpora are scraped with minimal curation. Additionally, Balloccu et al. (2024) found that 42% of papers evaluating models such as GPT-3.5 and GPT-4 contained leaked data, affecting millions of instances, further underscoring the widespread impact of contamination. \n\nSeveral methods have been proposed to detect data contamination, focusing on either log probability analysis or prompting-based techniques. Log probability-based methods, such as those developed by Oren et al. (2024) and Shi et al. (2023), assess the likelihood of data being present in a model's training set. In contrast, prompting-based approaches by Golchin and Surdeanu (2024b) and Golchin and Surdeanu (2024a) directly query the model to detect contamination. However, these methods have primarily been tested on traditional benchmarks and early-stage models, leaving a gap in understanding their effectiveness on more recent, advanced LLMs and complex datasets. \n\nMoreover, much of the existing research assumes contamination occurs during pre-training, overlooking the potential impact of instruction  fine-tuning, which is increasingly used to enhance LLM capabilities. This stage can introduce variations in the data that are not adequately tested by current methods. Our work seeks to address these gaps by evaluating a broader range of detection techniques across diverse benchmarks and models, with particular attention to more challenging datasets and the instruction fine-tuning phase.",
            "score": 0.7969932817188794,
            "section_title": "Related Work",
            "char_start_offset": 5247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 947
                },
                {
                    "start": 950,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1618
                },
                {
                    "start": 1621,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2150
                }
            ],
            "ref_mentions": [
                {
                    "start": 1148,
                    "end": 1166,
                    "matchedPaperCorpusId": "264490730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "260925501",
            "title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models",
            "text": "The rise of Transformer networks (Vaswani et al. 2017) has spurred the development of large language models (LLMs), marking a new epoch in Natural Language Processing (NLP). This shift has led to an extensive range of LLMs (Touvron et al. 2023a;b;Biderman et al. 2023;K\u00f6pf et al. 2023;Chung et al. 2022;Penedo et al. 2023, inter-alia) which excel in various professional and academic benchmarks (Bang et al. 2023;Bubeck et al. 2023). Their superior performance is primarily attributed to the massive web data consumed by these billion/trillion-parameter LLMs during training. However, the impressive LLM performance observed on many downstream tasks (e.g., summarization, natural language inference, text classification) may be inflated due to data contamination, i.e., the presence of test data from these downstream tasks in the pre-training data of LLMs. Guaranteeing lack of contamination is not trivial due to two potential sources of contamination: directly from ingesting the official version of a dataset (easier to control), and indirectly through duplicated data found somewhere on the web (nearly impossible to control). 2 The potential of data contamination is especially relevant for closed models such as the GPT-3/3.5 family (Brown et al. 2020) and GPT-4 (OpenAI 2023;Bubeck et al. 2023), and, needless to say, raises questions on the validity of evaluations and benchmarks conducted so far (Chang et al. 2023;Zhu et al. 2023;Bordt & von Luxburg 2023;Ray 2023;Penedo et al. 2023). \n\nTo address this issue, we propose an inexpensive and robust approach to detect data contamination for a given dataset partition automatically. Importantly, our approach functions under two realistic assumptions: (a) we lack direct access to the pre-training data of the LLMs, and (b) we have limited computational resources. Intuitively, our method starts by identifying potential contamination in individual instances that are drawn from a small random sample of the corresponding dataset partition (we use samples of 10 instances in this work).",
            "score": 0.7568301187910232,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1495
                },
                {
                    "start": 1498,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 2044
                }
            ],
            "ref_mentions": [
                {
                    "start": 1240,
                    "end": 1259,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1466,
                    "end": 1475,
                    "matchedPaperCorpusId": "258157875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.921875
        },
        {
            "corpus_id": "273850342",
            "title": "Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?",
            "text": "As large language models (LLMs) improve, their evaluation becomes increasingly challenging. A frequent concern in this context (Sainz et al., 2024a(Sainz et al., , 2023;;Jacovi et al., 2023;Elazar et al., 2024, i.a.) is that samples from evaluation benchmarks sometimes (partially) appear in the data used to train that same model, also known as evaluation data contamination. Such inadvertent 'training on the test set' makes it difficult to interpret scores on evaluation benchmarks: does a better score reflect an improvement on the ability that the benchmark intends to measure or does it indicate that a model succesfully memorised the benchmark data? \n\nAnswering this seemingly simple question is far from straightforward. Even setting aside practical issues such as the sheer size of pre-training corpora and the fact that they are not often publicly available, it Figures 3, 4 and 10) \n\n\u2022 While there is no true one-size-fits-all approach to contamination detection, using the longest contaminated substring rather than a union of all matches works better across the board, adequately detecting contamination in cases where no other metric did (Figures 2 and 13) \n\n\u2022 For virtually all benchmarks we considered, smaller n is better and even one occurrence in the pre-training data matters: both using values of n larger than 8 and setting a minimal count higher than one for occurrence in the pre-training corpus leads to false negatives (Figure 6 and 7b); \n\n\u2022 The impact of contamination changes with scale, in cases where there still is performance to be gained, larger models are better able to leverage contamination than smaller models (Figure 5, 8 and 9); \n\n\u2022 Consequently, to find the most adequate contamination metric, it is important to do model-specific threshold selection (Figure 1). \n\nWith our work, we aim to contribute to an informed discussion about evaluation data contamination and its effects, as well as provide methodology for both researchers and practitioners to explore this question further and contrast the effect of different contamination metrics and hyper-parameters as newer pre-training corpora (Soldaini et al., 2024;Penedo et al., 2024) and benchmarks (Laurent et al., 2024) are released.",
            "score": 0.7267077015797403,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 656
                },
                {
                    "start": 659,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 892
                },
                {
                    "start": 895,
                    "end": 1170
                },
                {
                    "start": 1173,
                    "end": 1463
                },
                {
                    "start": 1466,
                    "end": 1668
                },
                {
                    "start": 1671,
                    "end": 1803
                },
                {
                    "start": 1806,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 190,
                    "matchedPaperCorpusId": "258741333"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92529296875
        },
        {
            "corpus_id": "270737802",
            "title": "PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models",
            "text": "Large Language Models (LLMs) have brought about a paradigm shift in the domain of natural language processing, yielding notable enhancements across various evaluation benchmarks (Wang et al., 2019) and demonstrating proficiency in professional examinations (OpenAI, 2023). These advancements primarily stem from extensive training on vast and diverse datasets sourced from multiple origins. However, the substantial volume of data has given rise to significant concerns regarding benchmark contamination, where benchmarks for LLM evaluation are inadvertently or deliberately included in model training. This contamination presents considerable obstacles in accurately gauging the capabilities of LLMs. \n\nWhile efforts are being made to address this issue by removing benchmarks from training datasets and conducting contamination studies, these endeavors face numerous limitations (Brown et al., 2020a;Zhang et al., 2024;Wei et al., 2022;Chowdhery et al., 2022). These limitations include narrow focus on specific benchmarks and reliance on the trustworthiness of vendors. Moreover, the competitive dynamics within the field, coupled with copyright considerations, have resulted in recent model releases lacking accompanying contamination studies (OpenAI, 2023). Hence, there is an urgent necessity for independent methods to audit LLMs for the presence of benchmark datasets, eliminating the dependence on model providers' cooperation. \n\nSimultaneously, there has been a growing interest in heuristic membership inference algorithms designed to reverse-engineer aspects of the training dataset (Carlini et al., 2021a;Mattern et al., 2023), thereby providing insights into potential test set contamination (Sainz et al., 2023a;Golchin and Surdeanu, 2023b). Despite their promise, these heuristic approaches often lack definitive proof of contamination and tend to rely on assumptions that may be too stringent. Moreover, the majority of these methods concentrate less on detecting benchmark contamination. As elaborated in Section 3.1, inherent challenges, such as the need for lengthy trained segments and the necessity of establishing thresholds, impede the adaptation of previous methods for detecting benchmark contamination. \n\nIn this study, we introduce a novel approach named PaCoST (Paired Confidence Significance Testing) designed for the detection of benchmark contamination in open-source LLMs.",
            "score": 0.7135075576675742,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 701
                },
                {
                    "start": 704,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1436
                },
                {
                    "start": 1439,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2229
                },
                {
                    "start": 2232,
                    "end": 2405
                }
            ],
            "ref_mentions": [
                {
                    "start": 881,
                    "end": 902,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1618,
                    "end": 1639,
                    "matchedPaperCorpusId": "258967264"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94140625
        },
        {
            "corpus_id": "270063170",
            "title": "ConStat: Performance-Based Contamination Detection in Large Language Models",
            "text": "As large language models (LLMs) become increasingly effective at a wide range of tasks, many companies and research institutions compete to develop better models [2,5,28,35].To facilitate this development, a variety of benchmarks have been proposed that allow a standardized in-depth comparison of model performance across diverse tasks [15,16,26,32].\n\nData Contamination Modern LLMs are trained on vast amounts of internet-sourced data, raising the risk of unintentionally including benchmark samples in the training set.Such data contamination can lead to artificially inflated benchmark performance that does not accurately reflect a model's true ability to generalize to unseen tasks.However, model providers argue that the impact of this contamination on model performance is negligible [2,14,35] and the enormous size of current training sets almost guarantees contamination to some extent.This casts doubt on the relevance of this traditional definition of contamination in the context of LLMs.This Work: A New Perspective on Data Contamination We propose a new perspective on contamination, defining it based on its effect on model performance rather than its cause.Specifically, we define contamination as artificially inflated, non-generalizing performance, i.e., we say a model is contaminated if and only if its performance relative to other models is significantly higher on the original benchmark than on a similar reference benchmark.This definition captures the essence of the  contamination problem, i.e., performance measurements becoming unreliable for model comparisons.Furthermore, it enables principled detection methods that are robust against evasion attacks by malicious providers as this would require generalizing performance improvements.\n\nTraditional Contamination Detection Existing contamination detection methods [18,23,24,30,33,36,39,40,48] aim to detect the inclusion of benchmark samples in the training data as a measure of contamination.However, these approaches show limited success, cannot quantify the contamination's effect on model performance, and have to make strict assumptions about the contamination process, making them easy to evade [17].\n\nThis Work: A Statistical Test for Contamination In contrast, we leverage our novel performancebased definition of data contamination to propose a statistical contamination test called CONSTAT, illustrated in Fig. 1.",
            "score": 0.7070551416024942,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 174,
                    "end": 351
                },
                {
                    "start": 353,
                    "end": 522
                },
                {
                    "start": 522,
                    "end": 688
                },
                {
                    "start": 688,
                    "end": 896
                },
                {
                    "start": 896,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1174
                },
                {
                    "start": 1174,
                    "end": 1449
                },
                {
                    "start": 1449,
                    "end": 1590
                },
                {
                    "start": 1590,
                    "end": 1766
                },
                {
                    "start": 1768,
                    "end": 1974
                },
                {
                    "start": 1974,
                    "end": 2187
                },
                {
                    "start": 2189,
                    "end": 2404
                }
            ],
            "ref_mentions": [
                {
                    "start": 341,
                    "end": 344,
                    "matchedPaperCorpusId": "239998651"
                },
                {
                    "start": 344,
                    "end": 347,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 347,
                    "end": 350,
                    "matchedPaperCorpusId": "237532606"
                },
                {
                    "start": 1858,
                    "end": 1861,
                    "matchedPaperCorpusId": "258967264"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95703125
        },
        {
            "corpus_id": "266933004",
            "title": "Investigating Data Contamination for Pre-training Language Models",
            "text": "We then pre-train GPT-2small models from scratch on these variously contaminated corpora to evaluate and compare their performance. We further extend the experiments with GPT-2-large models to evaluate the effects of data contamination on larger models ( \u00a74.4). 2. RQ2: How do the number of repetitions of evaluation data in the pre-training corpus affect performance? In practice, how often a piece of evaluation data has appeared during pre-training and its ramifications are also unclear. We investigate this by injecting the evaluation data into the pre-training corpus multiple times and provide detailed empirical analyses ( \u00a74.2). 3. RQ3: How effective are the n-gram-based contamination definitions used in recent LLM reports? We systematically filter out different proportions of contaminated training documents, as described by these definitions, and pre-train the same model on these cleansed corpora ( \u00a74.3). Additionally, we critically evaluate the methods used in current LLM reports for assessing data contamination at the evaluation level ( \u00a74.5). These reports often posit that the models exhibit robustness against data contamination, and our discussion aims to elucidate the potential shortcomings of such claims. \n\nWe evaluate our experiments on several commonly used public datasets to observe the performance differences quantitatively. Our analyses provide a new perspective on understanding data contamination in the pre-training of language models. The contributions are summarized as follows: \n\n\u2022 We empirically investigate the effects of data contamination in the pre-training corpus due to evaluation data leakage in language models by pre-training language models from scratch to evaluate different mechanisms of data contamination. \u2022 We identify the importance of considering the data contamination with ground truths from the evaluation dataset. Surprisingly, we observed that the effects of increasing the number of repetitions of contamination on the model performance can be U-shaped. \u2022 We critically analyze the n-gram data contamination definitions from existing LLM reports and further compare the empirical results by filtering the pre-training data with these definitions. Our findings suggest that they are insufficient and inadequate to identify contamination.",
            "score": 0.7043254336575773,
            "section_title": "Introduction",
            "char_start_offset": 2186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1232
                },
                {
                    "start": 1235,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1518
                },
                {
                    "start": 1521,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2211
                },
                {
                    "start": 2212,
                    "end": 2301
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8369140625
        },
        {
            "corpus_id": "270217411",
            "title": "Probing Language Models for Pre-training Data Detection",
            "text": "In summary, this paper investigates the pre-training data detection problem in large language models.We propose a simple and effective approach that determines whether a target text has been included in a model's pre-training dataset by analyzing the internal activations using the probe technique.Additionally, we introduce a more challenging benchmark, ArxivMIA.The experiments demonstrate that our method outperforms all baselines across various benchmarks, achieving SOTA performance.\n\nWe further analyze the impact of target model size and the number of training data on our method.Additionally, we validate the effectiveness of our approach through a downstream task datasets contamination detection challenge.Future work could extend our methods to larger model scales or apply them to multi-modal models.",
            "score": 0.6906253556787894,
            "section_title": "Conclusion",
            "char_start_offset": 20510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 101,
                    "end": 298
                },
                {
                    "start": 298,
                    "end": 364
                },
                {
                    "start": 364,
                    "end": 488
                },
                {
                    "start": 490,
                    "end": 587
                },
                {
                    "start": 587,
                    "end": 716
                },
                {
                    "start": 716,
                    "end": 812
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.955078125
        },
        {
            "corpus_id": "262055119",
            "title": "Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation",
            "text": "Recent years have seen remarkable progress in language models pre-trained on massive text corpora scraped from the web. However, many widely used evaluation benchmarks are also constructed from similar web sources, leading to a concerning issue of data contamination where examples from test sets are unintentionally included in training data. Contamination enables models to \"cheat\" via memorisation of test data rather than displaying true generalisation (Marie, 2023), which creates an illusion of progress, distorts model comparisons, and undermines the utility of benchmarks (Jacovi et al., 2023). \n\nRecent evaluation of language models usually involves a detailed contamination analysis of the benchmarks used (Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023a;OpenAI, 2023). These contamination reports typically contain two steps: 1) quantify potential test contamination by measuring n-gram overlap between the test set and training data, 2) compare model performance on clean vs. contaminated subsets. This procedure is essential for determining the validity of the evaluation procedure and the credibility of benchmarks in assessing model performance. \n\nHowever, such analysis method relies on the access to the full training corpora, which are often unavailable for recent closed and open-sourced foundation models (OpenAI, 2023;Touvron et al., 2023b). Many fine-tuned language models released by the community also do not include reliable contamination reports and their fine-tuning datasets are not public either. This eliminates any possibility for the community to rigorously audit these models for contamination, which prevents reliable evaluation and accurate assessment of their capabilities. In addition, current method to identify potential test contamination, i.e., finding n-gram overlap between test set and training data, is also quite computation intensive, considering the massive size of modern training corpus. \n\nIn this paper, we propose a novel approach to quantify potential contamination in language model evaluation benchmarks without accessing the entire training data. Instead of identifying n-gram overlaps between training and test set, we directly observe whether models exhibit memorisation behaviour on test instances.",
            "score": 0.6876969480588435,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 602
                },
                {
                    "start": 605,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1176
                },
                {
                    "start": 1179,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1953
                },
                {
                    "start": 1956,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2273
                }
            ],
            "ref_mentions": [
                {
                    "start": 716,
                    "end": 736,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92822265625
        },
        {
            "corpus_id": "268889777",
            "title": "Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models",
            "text": "The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. Despite improved performance, existing methods (including the state-of-the-art, Min-K%) are mostly developed upon simple heuristics and lack solid, reasonable foundations. In this work, we propose a novel and theoretically motivated methodology for pre-training data detection, named Min-K%++. Specifically, we present a key insight that training samples tend to be local maxima of the modeled distribution along each input dimension through maximum likelihood training, which in turn allow us to insightfully translate the problem into identification of local maxima. Then, we design our method accordingly that works under the discrete distribution modeled by LLMs, whose core idea is to determine whether the input forms a mode or has relatively high probability under the conditional categorical distribution. Empirically, the proposed method achieves new SOTA performance across multiple settings. On the WikiMIA benchmark, Min-K%++ outperforms the runner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the more challenging MIMIR benchmark, it consistently improves upon reference-free methods while performing on par with reference-based method that requires an extra reference model.",
            "score": 0.6875784343455109,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.974609375
        },
        {
            "corpus_id": "268889777",
            "title": "Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models",
            "text": "Benchmarks. We focus on two benchmarks (and the only two to our knowledge) for pre-training data detection, WikiMIA (Shi et al., 2024) and MIMIR (Duan et al., 2024). WikiMIA is the first benchmark for pre-training data detection, which consists of texts from Wikipedia events. The training v.s. non-training data is determined by the timestamp. WikiMIA specifically groups data into splits according to the sentence length, intending to provide a fine-grained evaluation. It also considers two settings: original and paraphrased. The former assesses the detection of verbatim training texts, while the latter paraphrases the training texts (using ChatGPT) and evaluates on paraphrased inputs. MIMIR (Duan et al., 2024) is built upon the Pile dataset (Gao et al., 2020), where training samples and non-training samples are drawn from the train and test split, respectively. MIMIR is found to be more challenging than WikiMIA since the training and non-training texts are from the same dataset and thus have minimal distribution shifts and temporal discrepancy (Duan et al., 2024). \n\nBaselines. We consider 6 representative and state-of-the-art methods as our baselines, which are also featured in the benchmarking work of Duan et al. (2024). Loss method (Yeom et al., 2018) is a general technique that directly takes the loss as the score for detection. In the context of LLMs, this method is also reasonable as it is found that perplexity (the exponential of cross-entropy) can be a proxy for the occurrences of the training data (Gonen et al., 2023). Reference method (Carlini et al., 2021) (Ref ) uses an extra LLM as reference to calibrate the likelihood of the input. Zlib and Lowercase method (Carlini et al., 2021) use zlib compression entropy and the likelihood of lowercased text as the reference to calibrate the likelihood, respectively.",
            "score": 0.6819082564472836,
            "section_title": "SETUP",
            "char_start_offset": 20973,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1079
                },
                {
                    "start": 1082,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1847
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 134,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 1253,
                    "end": 1272,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1530,
                    "end": 1550,
                    "matchedPaperCorpusId": "254408772"
                },
                {
                    "start": 1569,
                    "end": 1591,
                    "matchedPaperCorpusId": "229156229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86865234375
        },
        {
            "corpus_id": "273549214",
            "title": "Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions",
            "text": "Large language models (LLMs) have achieved remarkable performance across various benchmarks, signaling their potential to revolutionize numerous technical domains as general-purpose problem solvers (Achiam et al., 2023;Meta AI, 2024). However, a key concern in accurately evaluating those LLMs is the possibility of data contamination, where the LLM's training data overlaps with the evaluation dataset (Balloccu et al., 2024). Evaluating LLMs on contaminated benchmarks leads to inflated performance assessments (Balloccu et al., 2024;Sainz et al., 2023a;Li and Flanigan, 2024), and creates a misleading perception of their capabilities. Therefore, multiple detection approaches have been developed to identify data contamination in LLMs, and these approaches can also be deployed to identify the use of copyrighted or sensitive content in LLM training (Xu et al., 2024;Meeus et al., 2024b). \n\nAll existing approaches for detecting data contamination in language models (LMs) rely on specific assumptions regarding the LMs and datasets, which may not be universally applicable across different settings2 . While previous surveys have focused on detection and mitigation techniques, to our best knowledge, there is currently no comprehensive analysis that surveys and validates the assumptions underlying these approaches (Xu et al., 2024;Ishihara, 2023;Hu et al., 2022). \n\nTo bridge this gap, we (1) systematically review 50 papers on data contamination detection for LMs, (2) present the formal, mathematical definitions for different levels of data contamination, (3) categorize the underlying requirements and assumptions associated with each approach and critically assess whether these assumptions have been rigorously validated, and (4) demonstrate through case studies that some unverified assumptions can be wrong in multiple scenarios.",
            "score": 0.6813282844577258,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 892
                },
                {
                    "start": 895,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1845
                }
            ],
            "ref_mentions": [
                {
                    "start": 403,
                    "end": 426,
                    "matchedPaperCorpusId": "267499939"
                },
                {
                    "start": 513,
                    "end": 536,
                    "matchedPaperCorpusId": "267499939"
                },
                {
                    "start": 536,
                    "end": 556,
                    "matchedPaperCorpusId": "264555419"
                },
                {
                    "start": 1339,
                    "end": 1354,
                    "matchedPaperCorpusId": "258888114"
                },
                {
                    "start": 1354,
                    "end": 1370,
                    "matchedPaperCorpusId": "232233426"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84716796875
        },
        {
            "corpus_id": "267412617",
            "title": "Evading Data Contamination Detection for Language Models is (too) Easy",
            "text": "Large language models are widespread, with their performance on benchmarks frequently guiding user preferences for one model over another. However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements. While recently developed contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection. We argue that this setting is of crucial importance as it casts doubt on the reliability of public benchmarks. To more rigorously study this issue, we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.",
            "score": 0.67685647959069,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58447265625
        },
        {
            "corpus_id": "270217411",
            "title": "Probing Language Models for Pre-training Data Detection",
            "text": "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories. Our experiments demonstrate that our method outperforms all baselines, and achieves state-of-the-art performance on both WikiMIA and ArxivMIA, with additional experiments confirming its efficacy (Our code and dataset are available at https://github.com/zhliu0106/probing-lm-data).",
            "score": 0.670974503995851,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9775390625
        },
        {
            "corpus_id": "270285708",
            "title": "Benchmark Data Contamination of Large Language Models: A Survey",
            "text": "Magar and Schwartz [96] presented a method to detect contaminated data in downstream tasks, they tested pre-training BERT models on corpora that include Wikipedia and labeled downstream datasets, then fine-tuning them on relevant tasks, and then detected BDC by comparing the performance of model-generated content from \"seen\" and \"unseen\" evaluation datasets.Their experiments reveal that while some models do exploit contaminated data, others merely memorize them without exploitation.The study shows that the level of memorization and exploitation is influenced by factors such as the number of data duplications and model size.\n\nDong et al. [34] focused on the distribution of generated content, they proposed a novel method, CDD (Contamination Detection via output Distribution), which uses the output distribution of LLMs to detect BDC.They also introduce TED (Trustworthy Evaluation via output Distribution), which corrects the output distribution to mitigate the effects of contamination.Through extensive experiments, they demonstrate that CDD can significantly improve contamination detection over existing methods and TED can reduce performance inflation due to contamination.The paper also presents two new benchmarks, DetCon and ComiEval, for assessing BDC and mitigation methods.Their findings reveal that popular models like ChatGPT are susceptible to BDC, emphasizing the need for more reliable evaluation methods.\n\nDifferent from the distribution, Li [89] proposed a novel method to detect BDC in language model evaluation without requiring access to the full training set.The technique uses perplexity to measure the extent of contamination, providing evidence of significant memorization in recent foundation models across various benchmarks.The study reveals that while reading comprehension and summarisation benchmarks show signs of contamination, multiple-choice benchmarks appear less affected.This method allows for a more accessible and less computationally intensive way to audit language models for contamination, ensuring more reliable evaluations.\n\nAn alternative interesting perspective is to focus on the order of content generated by LLMs.Oren et al. [108] presented a method to detect test set contamination in language models without needing access to the model's pre-training data or weights.The authors used a statistical test to identify contamination by comparing the likelihood of a benchmark dataset's canonical ordering against a shuffled version.",
            "score": 0.6677295648908108,
            "section_title": "Comparison-based Methods",
            "char_start_offset": 33001,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 360
                },
                {
                    "start": 360,
                    "end": 487
                },
                {
                    "start": 487,
                    "end": 631
                },
                {
                    "start": 633,
                    "end": 842
                },
                {
                    "start": 842,
                    "end": 996
                },
                {
                    "start": 996,
                    "end": 1187
                },
                {
                    "start": 1187,
                    "end": 1293
                },
                {
                    "start": 1293,
                    "end": 1430
                },
                {
                    "start": 1432,
                    "end": 1590
                },
                {
                    "start": 1590,
                    "end": 1761
                },
                {
                    "start": 1761,
                    "end": 1918
                },
                {
                    "start": 1918,
                    "end": 2077
                },
                {
                    "start": 2079,
                    "end": 2172
                },
                {
                    "start": 2172,
                    "end": 2328
                },
                {
                    "start": 2328,
                    "end": 2489
                }
            ],
            "ref_mentions": [
                {
                    "start": 19,
                    "end": 23,
                    "matchedPaperCorpusId": "247475929"
                },
                {
                    "start": 2184,
                    "end": 2189,
                    "matchedPaperCorpusId": "264490730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "270217411",
            "title": "Probing Language Models for Pre-training Data Detection",
            "text": "Our contributions can be summarized as follows:\n\n\u2022 We propose a novel pre-training data detection method that utilizes the probing technique.To the best of our knowledge, this is the first work to examine LLMs' internal activations to determine whether a given text was included in the pre-training data.\n\n\u2022 We propose ArxivMIA, a new benchmark in the academic domain to assess pre-training data detection methods.With a low duplication rate and the inherent complexity of texts, ArxivMIA presents a more challenging scenario and highlights the need for more sophisticated pre-training data detection methods.\n\n\u2022 We conduct extensive experiments on WikiMIA and ArxivMIA benchmarks.In addition, we also evaluate various detection methods on a downstream task datasets contamination challenge.Through comprehensive experimentation, we demonstrate that our proposed method outperforms all baselines, and achieves state-of-the-art performance.",
            "score": 0.6578752520405002,
            "section_title": "Introduction",
            "char_start_offset": 4195,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 49,
                    "end": 141
                },
                {
                    "start": 141,
                    "end": 304
                },
                {
                    "start": 306,
                    "end": 414
                },
                {
                    "start": 414,
                    "end": 609
                },
                {
                    "start": 611,
                    "end": 681
                },
                {
                    "start": 681,
                    "end": 791
                },
                {
                    "start": 791,
                    "end": 939
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "271571641",
            "title": "Data Contamination Report from the 2024 CONDA Shared Task",
            "text": "Data contamination, where evaluation data is inadvertently included in pre-training corpora of largescale models, and language models (LMs) in particular, has become a concern in recent times (Sainz et al., 2023a;Jacovi et al., 2023). The growing scale of both models and data, coupled with massive web crawling, has led to the inclusion of segments from evaluation benchmarks in the pretraining data of LMs (Dodge et al., 2021;OpenAI et al., 2024;Anil et al., 2023;Elazar et al., 2024). The scale of internet data makes it difficult to prevent this contamination from happening, or even detect when it has happened (Bommasani et al., 2022;Mitchell et al., 2023). \n\nCrucially, when evaluation data becomes part of pre-training data, it introduces biases and can artificially inflate the performance of LMs on specific tasks or benchmarks (Magar and Schwartz, 2022;Magnusson et al., 2023;Merrill et al., 2024). This poses a challenge for fair and unbiased evaluation of models, as their performance may not accurately reflect their generalization capabilities (Hupkes et al., 2023). And similarly to pre-training contamination, the contamination can also occur during the fine-tuning stage even after a model has been deployed as an API (Balloccu et al., 2024). \n\nAlthough a growing number of papers and stateof-the-art models mention issues of data contamination (Brown et al., 2020;Wei et al., 2022;Chowdhery et al., 2022;OpenAI et al., 2024;Anil et al., 2023;Touvron et al., 2023), there is little in the way of organized and compiled knowledge about real, documented cases of contamination in practice (Sainz et al., 2023a). Addressing data contamination is a shared responsibility among researchers, developers, and the broader community.",
            "score": 0.6517614525886213,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1260
                },
                {
                    "start": 1263,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1742
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 233,
                    "matchedPaperCorpusId": "258741333"
                },
                {
                    "start": 408,
                    "end": 428,
                    "matchedPaperCorpusId": "237568724"
                },
                {
                    "start": 1059,
                    "end": 1080,
                    "matchedPaperCorpusId": "252735124"
                },
                {
                    "start": 1236,
                    "end": 1259,
                    "matchedPaperCorpusId": "267499939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6884765625
        },
        {
            "corpus_id": "265050721",
            "title": "Rethinking Benchmark and Contamination for Language Models with Rephrased Samples",
            "text": "The fast-growing capabilities of large language models make their evaluation more challenging than ever (Chang et al., 2023). Although the community has established many benchmarks over a short period of time, the benchmark scores do not always reflect performance on real-world tasks. There has been evidence that many prevalent benchmarks might have contaminated pre-training or fine-tuning datasets. From the contamination analysis in Llama-2 (Touvron et al., 2023), over 10% of the MMLU test samples are highly contaminated. Another example from GPT-4's technical report (OpenAI, 2023) shows that 25% of HumanEval has been contaminated in their training data. Similar situation also applies to open-source datasets. A popular code pretraining set, StarCoder Data (Li et al., 2023), shows that hundreds of test cases in the Stack (Kocetkov et al., 2022) are contaminated with benchmarks. \n\nDespite being recognized as a crucial issue, accurately detecting contamination remains an open and challenging problem. The most commonly used approaches are n-gram overlap and embedding similarity search. N-gram overlap relies on string matching to detect contamination, widely used by leading developments such as GPT-4 (OpenAI, 2023), PaLM (Anil et al., 2023), and Llama (Touvron et al., 2023). However, it suffers from limited accuracy. Embedding similarity search uses the embeddings of pre-trained models (e.g., BERT) to find similar and potentially contaminated examples. However, choosing an appropriate similarity threshold to strike a balance between recall and precision is often challenging. Moreover, there has been a growing interest in training models using synthetic data produced by LLMs (e.g., GPT-4) (Gunasekar et al., 2023;Taori et al., 2023;Wang et al., 2023b;Xu et al., 2023;Mukherjee et al., 2023), in which contamination may be even harder to detect by string matching.",
            "score": 0.6486376540879253,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1887
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8447265625
        },
        {
            "corpus_id": "270688744",
            "title": "Unveiling the Spectrum of Data Contamination in Language Models: A Survey from Detection to Remediation",
            "text": "The primary challenge in evaluating different methods for detecting data contamination in large language models is the absence of a ground truth label, i.e., a benchmark dataset comprising entirely contaminated data.This absence creates difficulties in comparing the effectiveness of various detection techniques designed for black-box models.One alternative approach involves fine-tuning the model using test set labels to create artificially contaminated data.However, the question remains whether the scenarios of contamination during the pre-training phase and the fine-tuning phase are consistent.Additionally, due to limited access to the complete training corpus, we can only generate fully contaminated data, making it challenging to obtain fully uncontaminated data.This situation complicates efforts to accurately assess and compare the efficacy of contamination detection methods.\n\nEvading Detection of Data Contamination Dekoninck et al. (2024a) highlights the ease with which MIA detection methods can be evaded.These methods, some of which are also employed for identifying data contamination, have been criticized in prior research.Notably, the efficacy of n-gram-based substring detection is questioned due to its numerous vulnerabilities and susceptibility to manipulation (Zhou et al., 2023;Deng et al., 2023;Jiang et al., 2024).Beyond the traditional n-gram and MIA approaches, recent studies have demonstrated that several contemporary techniques can be compromised through targeted attacks.For instance, by integrating a dataset with a significantly large pre-trained dataset, one can disrupt the canonical order assumption, thereby undermining its integrity.\n\nFrom Memorization to Exploitation Drawing a definitive conclusion about the correlation between memorization and exploitation (i.e., performance on downstream tasks) remains challenging.Various factors can impact the outcomes observed in our study, including differences in model architecture, the repetition of contaminated data, the strategies employed during pre-training or fine-tuning phases, and the training principles used like RLHF+PPO (Zheng et al., 2023) and DPO (Rafailov et al., 2023).These elements can significantly influence the models' downstream task performance.\n\nDetecting or Mitigating?Currently, there is an increasing focus on developing novel methods for detecting data contamination, which is crucial for investigating and understanding data contamination scenarios.",
            "score": 0.6486024585086139,
            "section_title": "Challenges for Detecting Black-Box Models",
            "char_start_offset": 29606,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 216,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 462
                },
                {
                    "start": 462,
                    "end": 602
                },
                {
                    "start": 602,
                    "end": 775
                },
                {
                    "start": 775,
                    "end": 891
                },
                {
                    "start": 893,
                    "end": 1025
                },
                {
                    "start": 1025,
                    "end": 1147
                },
                {
                    "start": 1147,
                    "end": 1347
                },
                {
                    "start": 1347,
                    "end": 1511
                },
                {
                    "start": 1511,
                    "end": 1680
                },
                {
                    "start": 1682,
                    "end": 1868
                },
                {
                    "start": 1868,
                    "end": 2180
                },
                {
                    "start": 2180,
                    "end": 2263
                },
                {
                    "start": 2265,
                    "end": 2289
                },
                {
                    "start": 2289,
                    "end": 2473
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79052734375
        },
        {
            "corpus_id": "274423394",
            "title": "INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge",
            "text": "We use the decision threshold that achieves the best performance on WikiMIA as the decision threshold for our analysis. Using this method, we computed the contamination rate for each language on four main-stream multilingual models: Aya-8B, XGLM-7B, LLaMA-3.1-8B, and Qwen-2.5-7B. We show the contamination rate results in Table 7. \n\nTo further mitigate the risk of benchmark saturation as a result of data leakage when new models are trained, we have held back the complete dataset, comprising 197,243 entries. Instead, we will release these further questions and answers incrementally over the next year. We have also reserved a held-out dataset covering a wide range of the collected languages to be used for future experimental studies specifically aimed at analyzing data leakage over time.",
            "score": 0.6316740289578845,
            "section_title": "A.8 ANALYSIS OF OUTPUT ERRORS ON INCLUDE",
            "char_start_offset": 50378,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 331
                },
                {
                    "start": 334,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 795
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11065673828125
        },
        {
            "corpus_id": "270688744",
            "title": "Unveiling the Spectrum of Data Contamination in Language Models: A Survey from Detection to Remediation",
            "text": "Figure 1: Basic illustration of data contamination and the research questions related to it.Clean evaluation is defined as having no overlap between the pre-training corpora and the benchmarks, and contaminated evaluation is defined as having a significant overlap between them.\n\n(LLMs).These models are trained on vast corpora sourced from the web (OpenAI, 2023;Touvron et al., 2023a), heightening the risk that training data may inadvertently encompass instances from evaluation benchmarks (Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023a,b).Such contamination of evaluation benchmarks can obscure the true generalization performance of LLMs, as it might artificially inflate benchmark scores by testing the models' ability to \"memorize\" and \"recall\" rather than \"reason\" or \"generalize\".\n\nGiven the increasing concerns about potential contamination of evaluation benchmarks and their broader impact on downstream task performance, numerous recent studies have focused on identifying and mitigating data contamination in these benchmarks.These efforts aim to better understand how contamination affects our perception of model capabilities.In general, research on data contamination could be broadly categorized into two main areas: (i) investigations of models trained with open-source data and (ii) studies relevant to models developed using proprietary data.Gener-ally, having access to training data, or the lack thereof, has a profound influence on modern contamination research.\n\nIn this paper, we present a comprehensive analysis of the growing field of data contamination detection and mitigation.Our objective is to delve into the downstream impacts of data contamination, investigate existing methods for detecting data contamination, and discuss a range of mitigation strategies.The paper is structured as outlined in Figure 1.We start by establishing the background of data contamination ( \u00a72) and discussing the effect of contamination ( \u00a73).Following this, We provide a detailed analysis of current methods for detecting data contamination ( \u00a74).We categorize these methods and critically examine the assumptions each relies on, highlighting the prerequisites and limitations for their application.Subsequently, we explore strategies for mitigating data contamination ( \u00a75), tackling potential hurdles, and proposing avenues for future investigations in this domain.",
            "score": 0.6278745884266923,
            "section_title": "Covered RQs",
            "char_start_offset": 423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 92,
                    "end": 278
                },
                {
                    "start": 280,
                    "end": 287
                },
                {
                    "start": 287,
                    "end": 560
                },
                {
                    "start": 560,
                    "end": 806
                },
                {
                    "start": 808,
                    "end": 1056
                },
                {
                    "start": 1056,
                    "end": 1158
                },
                {
                    "start": 1158,
                    "end": 1379
                },
                {
                    "start": 1379,
                    "end": 1502
                },
                {
                    "start": 1504,
                    "end": 1623
                },
                {
                    "start": 1623,
                    "end": 1808
                },
                {
                    "start": 1808,
                    "end": 1856
                },
                {
                    "start": 1856,
                    "end": 1973
                },
                {
                    "start": 1973,
                    "end": 2078
                },
                {
                    "start": 2078,
                    "end": 2230
                },
                {
                    "start": 2230,
                    "end": 2398
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88671875
        },
        {
            "corpus_id": "272689919",
            "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges",
            "text": "While large language models (LLMs) consistently achieve higher state-of-the-art results across various benchmarks (Rae et al., 2022;Srivastava et al., 2023;Liang et al., 2023;Zou et al., 2024), the lack of curation in and the limited disclosure of massive training datasets raise a critical question: Does the model performance arise from model * Authors contributed equally to this work. generalizability or mere memorization? Furthermore were the test sets possibly contaminated without notice? These questions have become crucial in accurately gauging LLMs' performance and has led to a critical area of research: detecting data contamination in LLMs. \n\nRecent work in detecting data contamination in LLMs has primarily focused on detecting contamination through validating the log probability of the data in the datasets (Oren et al., 2024;Shi et al., 2023) or determining contamination through prompting-based approaches (Golchin and Surdeanu, 2024b,a). Additional studies have been aimed at understanding the different types of contamination such as differentiating between training/development split contamination and testing set contamination (Sainz et al., 2023) and contamination occurring in the pertaining phase versus the supervised fine-tuning stage (Jacovi et al., 2023). \n\nHowever, existing research in this area has several limitations. Firstly, most methods show their effectiveness on traditional benchmarks, which are likely overexposed online and to LLMs. In contrast, challenging datasets that test the limits of LLM capabilities are neglected. These newer benchmarks are often more complex with novel formats. Secondly, the tested LLMs are often early-staged ones, such as GPT-J, while the rapid pace of LLM development has left a gap in understanding contamination in the latest models. Thirdly, previous research predominantly focuses on (possibly unintentional) contamination occurring during pretraining, where models are exposed to data in its original form. However, it overlooks contamination during instruction fine-tuning, where original data is subtly modified by, for instance, answer augmentation with chain-of-thought reasoning steps. Such variations may cause difficulties in contamination detection yet are rarely considered when evalu-ating the effectiveness of contamination detection methods.",
            "score": 0.6251913087997152,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2170
                },
                {
                    "start": 2171,
                    "end": 2333
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 156,
                    "matchedPaperCorpusId": "263625818"
                },
                {
                    "start": 175,
                    "end": 192,
                    "matchedPaperCorpusId": "269330205"
                },
                {
                    "start": 825,
                    "end": 844,
                    "matchedPaperCorpusId": "264490730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.845703125
        },
        {
            "corpus_id": "264451585",
            "title": "Detecting Pretraining Data from Large Language Models",
            "text": "Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous methods. We apply Min-K% Prob to two real-world scenarios, copyrighted book detection, and contaminated downstream example detection, and find it a consistently effective solution.",
            "score": 0.6231854425943014,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "266359062",
            "title": "An Open-Source Data Contamination Report for Large Language Models",
            "text": "The analysis includes a range of foundation models such as LLaMA (Touvron et al., 2023a), Llama-2, Yi (Yi, 2023), Mistral (Jiang et al., 2023), Baichuan (Yang et al., 2023), and Qwen (Bai et al., 2023) across multiple model sizes (7B, 13B, 30B, 34B, 65B, 70B parameters) as well as instruct-tuned models built on these foundations like Llama-2 Chat and Mistral-Instruct. Six widely used multi-choice benchmarks are assessed: Winogrande (Sakaguchi et al., 2021), AI2_ARC (Clark et al., 2018), CommonsenseQA (Talmor et al., 2018), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021a), and C-Eval (Huang et al., 2023). Our methodology proceeds in four steps: arXiv:2310.17589v3 [cs.CL] 29 Jan 2024 first, we verify whether test examples appear in Common Crawl 1 , a popular large corpus often used in language model pre-training. If a test example is found verbatim in Common Crawl, it was very likely included in the pre-training phrase of language models, making it a \"contaminated\" sample. Based on the presence of test samples, we then categorise benchmarks into the clean set and contaminated set. Finally, we compare model performance on these subsets to assess the impact of data contamination on evaluation results. At the end of the paper, we compare our analysis to Llama-2's original contamination results and discuss the effectiveness of existing contamination mitigation methods.",
            "score": 0.6204500900318184,
            "section_title": "Introduction",
            "char_start_offset": 1854,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1401
                }
            ],
            "ref_mentions": [
                {
                    "start": 436,
                    "end": 460,
                    "matchedPaperCorpusId": "199370376"
                },
                {
                    "start": 568,
                    "end": 593,
                    "matchedPaperCorpusId": "221516475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66650390625
        },
        {
            "corpus_id": "265220695",
            "title": "Investigating Data Contamination in Modern Benchmarks for Large Language Models",
            "text": "Large language models (LLMs) have demonstrated exceptional performance across a wide range of NLP tasks, and the NLP community has witnessed the emergence of several impressive LLMs. Notably, there are robust proprietary LLMs, including the GPT-* (Brown et al., 2020;OpenAI, 2023), Claude (Anthropic, 2023), and Bard (Google, 2023), among others. In addition to these proprietary models, there are numerous open-source LLMs, such as Llama (Touvron et al., 2023a,b), MPT (Lin et al., 2023), Falcon (Mei et al., 2022), and Mistral (Jiang et al., 2023). However, with the increasing compute scale (including data) used to train these models, concerns have arisen regarding the extensive use of crawled web data, often at a terabyte scale. This extensive training data may, in turn, potentially include instances of evaluation benchmarks (Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023a,b), many of which are also constructed from Internet sources. Research has demonstrated that the use of evaluation benchmark data in training sets (i.e., contamination) can artificially inflate performance metrics, regardless of whether contamination occurs during pretraining (Schaeffer, 2023) or fine-tuning (Zhou et al., 2023). Consequently, it becomes imperative for the research community to develop methods for detecting potential data contamination in these models (Sainz et al., 2023). \n\nOne of the most commonly used methods to detect data contamination has been n-gram matching (Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023b). Particularly, a number of previous works have employed n-gram tokenization to partition large documents into smaller segments, subsequently assessing their similarity to benchmark data (Chowdhery et al., 2022;Touvron et al., 2023a). However, this approach is heavily reliant on having full access to the training corpus.",
            "score": 0.6183977024391648,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1392
                },
                {
                    "start": 1395,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1874
                }
            ],
            "ref_mentions": [
                {
                    "start": 497,
                    "end": 515,
                    "matchedPaperCorpusId": "247839322"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "270688744",
            "title": "Unveiling the Spectrum of Data Contamination in Language Models: A Survey from Detection to Remediation",
            "text": "Task-level study of contamination typically involves selecting a specific task, such as classification and question answering.By establishing a fixed benchmark, the extent of data contamination is varied to observe changes in performance.\n\nFor example, Magar and Schwartz (2022) pre-train a BERT-based model on a combined corpus of Wikipedia and labeled data from downstream tasks.The findings reveal that while models can memorize data during pre-training, they do not consistently utilize this memorized information in an effective manner.Additionally, the extent of exploitation is affected by several factors, including the duplication of contaminated data and the model size.Jiang et al. (2024) explore the contamination effect of the decoder-only architecture using GPT-2.Specifically, they pre-trained GPT-2 on a selected portion of the Pile (Gao et al., 2020) corpora, intentionally introducing contaminated data during the pre-training phase to assess its impact.Their findings reveal that traditional n-gram-based methods are limited in detecting contamination, and increasing the repetition of contaminated data inversely affects model performance, leading to a performance drop.Zhu et al. (2024) also investigate the relation between memorization and generation in the context of critical data size with the configuration of grokking (Power et al., 2022), a phenomenon where a model suddenly achieves near-perfect performance on a task after a period of apparent stag-nation during training.The authors introduce the Data Efficiency Hypothesis, which outlines three stages of data interaction during model training: insufficiency, sufficiency, and surplus.The study observes that as models grow, they require larger datasets to reach a smooth phase transition.",
            "score": 0.6182982657002116,
            "section_title": "Task-Level Contamination",
            "char_start_offset": 8557,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 126,
                    "end": 238
                },
                {
                    "start": 240,
                    "end": 381
                },
                {
                    "start": 381,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 680
                },
                {
                    "start": 680,
                    "end": 778
                },
                {
                    "start": 778,
                    "end": 972
                },
                {
                    "start": 972,
                    "end": 1190
                },
                {
                    "start": 1190,
                    "end": 1503
                },
                {
                    "start": 1503,
                    "end": 1668
                },
                {
                    "start": 1668,
                    "end": 1772
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7822265625
        },
        {
            "corpus_id": "270737802",
            "title": "PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models",
            "text": "Large language models (LLMs) are known to be trained on vast amounts of data, which may unintentionally or intentionally include data from commonly used benchmarks. This inclusion can lead to cheatingly high scores on model leaderboards, yet result in disappointing performance in real-world applications. To address this benchmark contamination problem, we first propose a set of requirements that practical contamination detection methods should follow. Following these proposed requirements, we introduce PaCoST, a Paired Confidence Significance Testing to effectively detect benchmark contamination in LLMs. Our method constructs a counterpart for each piece of data with the same distribution, and performs statistical analysis of the corresponding confidence to test whether the model is significantly more confident under the original benchmark. We validate the effectiveness of PaCoST and apply it on popular open-source models and benchmarks. We find that almost all models and benchmarks we tested are suspected contaminated more or less. We finally call for new LLM evaluation methods.",
            "score": 0.6176002090990811,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "277955255",
            "title": "Self-Correction Makes LLMs Better Parsers",
            "text": "To examine the possibility of contamination in test set, we use the Min K% probability (MKP) (Shi et al., 2023), as a metric to determine the extent to which the datasets are included in the pre-training data of LLMs. We follow the recommendation in Shi et al. (2023), setting K value to 20% and calculating the MKP of sentences in the three datasets under the Llama-8B model. We also report the MKP of the WikiMIA dataset for reference, which is used by Shi et al. (2023) to evaluate the identification capabilities for detecting pre-training data, consists of sentences from Wikipedia articles. Given the popularity of Wikipedia as a pre-training source, these sentences are highly likely to be part of the pre-training data for LLMs. As is shown in Table 5, the MKP of all three constituency parsing datasets is significantly higher than that of WikiMIA. This suggests that these datasets are unlikely to have been included in the pre-training data of Llama-8B.",
            "score": 0.616821443049735,
            "section_title": "A.3 The Examination of the Possibility of Contamination in Test Set",
            "char_start_offset": 27753,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 964
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.533203125
        },
        {
            "corpus_id": "266359062",
            "title": "An Open-Source Data Contamination Report for Large Language Models",
            "text": "This paper conducted an extensive data contamination analysis for popular large language models on six multi-choice QA benchmarks. We identified varying levels of test set contamination, ranging from 1% to 47% across benchmarks. We also find data contamination does not necessarily lead to increased metrics: data contamination in ARC and Hellaswag generally allow models to achieve significant higher accuracy, but contamination in MMLU has little impact on model's performance. \n\nOur findings offer a transparent perspective on data contamination, emphasising its significance as an urgent issue within the evaluation community. \n\nOur pipeline leverages search engines followed by querying Common Crawl index. This avoids the access of full training dataset locally, which is often not open-sourced for modern large language models. However, relying on search APIs incurs notable costs -around $15 per 1000 queries with Bing. We spent about $900 in total calling Bing search API. Additionally, search engines restrict query lengths, which prevents analysis of benchmarks with long input passages like reading comprehension. Moreover, large language model developers may also use customised data collected from crowd sourcing or non-public databases. In this case, our search engine plus Common Crawl pipeline may be unable to identify data contamination from these hidden data sources. Future attempts may directly query the complete Common Crawl corpus hosted on AWS S3 services. This would enable scanning of lengthy input examples but at a higher financial cost. Alternatively, developing perplexity-based approaches to detect contaminated examples without requiring full passage matching could prove fruitful.",
            "score": 0.6159520423054871,
            "section_title": "Conclusion",
            "char_start_offset": 23498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 479
                },
                {
                    "start": 482,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1715
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71337890625
        },
        {
            "corpus_id": "265220695",
            "title": "Investigating Data Contamination in Modern Benchmarks for Large Language Models",
            "text": "and closed-source language models. \n\nIn this paper, we investigate methods to detect contaminated benchmark data both for opensource models with open training data, as well as black-box models. Following previous work on using search-based methods to investigate pretraining corpora (Dodge et al., 2021;Piktus et al., 2023b,a;Elazar et al., 2023), we first establish a retrieval system (Figure 1) based on Pyserini (Lin et al., 2021) for contamination detection. Recently Elazar et al. (2023) demonstrated potential contamination of several datasets of GLUE and SuperGLUE benchmarks in contemporary pretraining corpora. We instead focus on more recent commonly used evaluation benchmarks, MMLU (Hendrycks et al., 2021), Truth-fulQA (Lin et al., 2022), HellaSwag (Zellers et al., 2019), WindoGrande (Sakaguchi et al., 2020), GSM8K (Cobbe et al., 2021), OpenbookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), and as for pretraining corpora we use the Pile (Gao et al., 2020) and C4 (Raffel et al., 2020) which are open and widely used in training of various LLMs. \n\nNext, we introduce a novel investigation protocol for potential contamination referred to as TS-Guessing in two distinct settings: (1) Questionbased guessing and (2) Question-multichoice guessing shown in Figure 1. In the Question-based setting, our objective is to hide a crucial word within a sentence. In the Question-Multichoice setting, our goal is to mask an incorrect answer option among multiple choices, encouraging it to guess the missing part in the benchmark instance. These two settings guide LLMs in guessing the missing information in the questions and answers, testing revealing potential contamination. We have also conducted a contaminated experiment to fully expose ChatGPT to contamination by fine-tuning it with the MMLU (Hendrycks et al., 2021) test set to observe the differences in scores in TS-Guessing.",
            "score": 0.6158003166959923,
            "section_title": "MMLU",
            "char_start_offset": 3183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 37,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 1068
                },
                {
                    "start": 1071,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1899
                }
            ],
            "ref_mentions": [
                {
                    "start": 283,
                    "end": 303,
                    "matchedPaperCorpusId": "237568724"
                },
                {
                    "start": 694,
                    "end": 718,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 732,
                    "end": 750,
                    "matchedPaperCorpusId": "237532606"
                },
                {
                    "start": 762,
                    "end": 784,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 798,
                    "end": 822,
                    "matchedPaperCorpusId": "199370376"
                },
                {
                    "start": 863,
                    "end": 886,
                    "matchedPaperCorpusId": "52183757"
                },
                {
                    "start": 893,
                    "end": 912,
                    "matchedPaperCorpusId": "208290939"
                },
                {
                    "start": 987,
                    "end": 1008,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83447265625
        },
        {
            "corpus_id": "264451585",
            "title": "Detecting Pretraining Data from Large Language Models",
            "text": "We study pretraining data detection, the problem of detecting whether a piece of text is part of the training data. First, we formally define the problem and describe its unique challenges that are not present in prior finetuning data detection studies ( \u00a72.1). We then curate WIKIMIA, the first benchmark for evaluating methods of pretraining data detection ( \u00a72.2).",
            "score": 0.6148709162486765,
            "section_title": "PRETRAININING DATA DETECTION PROBLEM",
            "char_start_offset": 6104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 367
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84716796875
        },
        {
            "corpus_id": "276575227",
            "title": "Detecting Benchmark Contamination Through Watermarking",
            "text": "Benchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect \"radioactivity\", i.e., traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, e.g., p-val = 10 \u22123 for +5% on ARC-Easy. Alice is a benchmark provider and wants to make sure that contamination on her benchmark can be detected with high confidence. Before release, she rephrases the original benchmark dataset while embedding a non-intrusive LLM watermark. This rephrasing does not change the utility of the benchmark. Bob decides to train a model. The benchmark may contaminate Bob's model during training, either intentionally or unintentionally. Alice can give statistical evidence if her benchmark was used in training.",
            "score": 0.6145940037479881,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1500
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9501953125
        },
        {
            "corpus_id": "270217411",
            "title": "Probing Language Models for Pre-training Data Detection",
            "text": "Large language models (LLMs) trained on massive corpora of texts demonstrate extraordinary abilities to understand, reason, and generate following natural language instructions (Brown et al., 2020;Anil et al., 2023).Meanwhile, the open-source of LLMs has significantly contributed to the advancement and collaborative development within the LLM community (Zhang et al., 2022;Touvron et al., 2023b;Biderman et al., 2023;Bai et al., 2023;Team, 2023;LLaMA-MoE Team, 2023).Despite this progress, the lack of transparency raises ethical and legal questions, particularly about the use of potentially private data sourced from the internet, and threatens the reliability of benchmark evaluations due to the risk of leaking test data.Therefore, determining if certain texts have been utilized during the pre-training phase of the target LLM becomes a critical task.\n\nRecent efforts to detect pre-training data in LLMs have attracted significant attention.Several studies have been proposed to investigate dataset contamination, including prompting LLMs to generate data-specific examples or using statistical methods to detect contamination in test sets (Sainz et al., 2023;Golchin and Surdeanu, 2023;Oren et al., 2023).Concurrently, Membership Inference Attacks (MIAs) in Natural Language Processing have been extensively explored for their potential to discern whether specific data was used in LLMs' pre-training (Carlini et al., 2021;Mireshghallah et al., 2022;Mattern et al., 2023;Shi et al., 2023).The above solutions have achieved a certain success.However, they all rely on the model's superficial features, such as generated texts or loss metrics, which may not always be reliable indicators of pretraining data.\n\nDifferent from these conventional approaches, we propose a simple yet effective pre-training data detection method that utilizes the probing technique to examine the model's internal activations.This approach is based on the assumption: Texts that have been seen during the model's pre-training phase are represented differently in the model's internal activations compared to texts that have not been seen, so we could train a linear probe classifier to distinguish them.",
            "score": 0.6120032974542478,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 216,
                    "end": 469
                },
                {
                    "start": 469,
                    "end": 727
                },
                {
                    "start": 727,
                    "end": 858
                },
                {
                    "start": 860,
                    "end": 948
                },
                {
                    "start": 948,
                    "end": 1213
                },
                {
                    "start": 1213,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1549
                },
                {
                    "start": 1549,
                    "end": 1714
                },
                {
                    "start": 1716,
                    "end": 1911
                },
                {
                    "start": 1911,
                    "end": 2188
                }
            ],
            "ref_mentions": [
                {
                    "start": 397,
                    "end": 419,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 1409,
                    "end": 1431,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1431,
                    "end": 1458,
                    "matchedPaperCorpusId": "247315260"
                },
                {
                    "start": 1458,
                    "end": 1479,
                    "matchedPaperCorpusId": "258967264"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94921875
        },
        {
            "corpus_id": "267897557",
            "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models",
            "text": "Data contamination refers to the inclusion of information in the training set of models that provides insights into the test set of a benchmark dataset, and then evaluated in the same benchmark. Recently, the AI community has become increasingly concerned (Schaeffer, 2023;Zhou et al., 2023;Oren et al., 2023) about data contamination in LLMs. Detecting data contamination, a form of Membership Inference Attack (MIA), poses challenges for large language models (LLMs) due to their training on vast corpora and the difficulty of conducting ablation studies (Shi et al., 2023). To detect such contamination of LLMs, Wei et al. (2023) suggested comparing average loss values between training and test datasets, while Shi et al. (2023) introduced Min-K% Prob based on loss values to identify texts used in training. Our experiments show these methods are effective for pre-training but not for detecting contamination during fine-tuning. Zhu et al. (2023) leveraged DAG to dynamically generate evaluation data in reasoning tasks to avoid contamination. In comparison, KIEval only requires access to output texts of evaluated models and detects data contamination through evaluating its ability to generalize and utilize knowledge as well as generative ability, which requires a deeper understanding of knowledge instead of mere memorization of the answers. Moreover, our experiments suggest that KIEval is resilient to data contamination, offering a reliable means to discern whether models have been trained on test sets. This makes it a valuable tool for complementing traditional benchmarks, providing a more nuanced understanding of a model's exposure to and handling of contaminated data.",
            "score": 0.6110612497335517,
            "section_title": "Addressing Data Contamination of LLMs",
            "char_start_offset": 7866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1690
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "273507876",
            "title": "Contamination Report for Multilingual Benchmarks",
            "text": "Large Language Models (LLMs) have shown significant improvements on standard benchmarks as compared to their predecessors [1,2]. These models are pre-trained on large amounts of data collected from the web via crawling, in which a significant portion of the Internet is consumed and often memorized by such large scale models during training. Such rampant data collection might unexpectedly capture publicly available benchmarks, causing LLMs to ingest test sets and memorize them, leading to a high score upon evaluation [3]. This phenomenon is called data-contamination, and it paints a false picture of the abilities of an LLM. LLMs also undergo an instruction-tuning phase, and are sometimes further tuned via RLHF, where the model is trained on task specific datasets. However, LLM creators do not always disclose the exact details of the datasets used, and it is plausible that the model is trained on benchmark datasets intentionally or unintentionally. Hence, contamination can occur during the pre-training or post-training phases [4]. In this work, we study the contamination of 7 recent LLMs on 7 popular multilingual benchmarks used in prior work to evaluate the capabilities of LLMs on non-English languages. Our main contribution in this paper is an analysis of which multilingual benchmark in contaminated in which model by utilizing the contamination detection technique proposed by Oren et al. [5].",
            "score": 0.6074761060424044,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1415
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 125,
                    "matchedPaperCorpusId": "257663467"
                },
                {
                    "start": 125,
                    "end": 127,
                    "matchedPaperCorpusId": "265150423"
                },
                {
                    "start": 522,
                    "end": 525,
                    "matchedPaperCorpusId": "271571641"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85400390625
        },
        {
            "corpus_id": "270285793",
            "title": "DICE: Detecting In-distribution Contamination in LLM's Fine-tuning Phase for Math Reasoning",
            "text": "Current development of large language models (LLMs) and their related techniques heavily relies on public benchmarks to ensure that progress is made in the right direction. For instance, the field primarily uses GSM8k [2] and MATH [3] to evaluate LLMs' math reasoning abilities. However, there is growing concern that some of the impressive performance on these benchmarks may be attributed to data contamination, where the training data contains original data from the benchmark and is memorized by the model [4], which we refer to as Exact Contamination. \n\nRecently, EAL [1] has already shown that training on data that bears similarity to the benchmark data can lead to severe performance overestimation, namely In-distribution contamination. Since pre-training data is massive and hard to distinguish based on their distributions, we narrow our scope to the supervised fine-tuning (SFT) phase. We aim to answer the following research questions: (1) Does in-distribution contamination contribute to a model's overall math reasoning ability? (2) If not, how can we detect it to prevent overestimating the model's capabilities due to contamination? \n\nTo investigate whether in-distribution contamination can really improve LLM's math reasoning ability, we design an OOD test for a set of fine-tuned LLMs simulating different levels of in-distribution  contamination on GSM8K following prior work's experiment setup [1]. In section 2.2, we show that even if the contamination data in the training data accounts for only a small portion, the models' gain on in-distribution (ID) benchmarks is significantly greater than the gain on OOD benchmarks. The result suggests that ID data does not truly enhance the model's overall math reasoning ability, and the performance on in-distribution benchmarks no longer reflect the model's real capabilities. This motivates the need for in-distribution contamination detection: Given a fine-tuned LLM on a piece of test data, determine if the model has seen its in-distribution data during fine-tuning.",
            "score": 0.6065492734224177,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 556
                },
                {
                    "start": 559,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 2039
                }
            ],
            "ref_mentions": [
                {
                    "start": 218,
                    "end": 221,
                    "matchedPaperCorpusId": "239998651"
                },
                {
                    "start": 231,
                    "end": 234,
                    "matchedPaperCorpusId": "232134851"
                },
                {
                    "start": 1044,
                    "end": 1047,
                    "matchedPaperCorpusId": "239998651"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8427734375
        },
        {
            "corpus_id": "272367930",
            "title": "Training on the Benchmark Is Not All You Need",
            "text": "The success of Large Language Models (LLMs) relies heavily on the huge amount of pre-training data learned in the pre-training phase. The opacity of the pre-training process and the training data causes the results of many benchmark tests to become unreliable. If any model has been trained on a benchmark test set, it can seriously hinder the health of the field. In order to automate and efficiently test the capabilities of large language models, numerous mainstream benchmarks adopt a multiple-choice format. As the swapping of the contents of multiple-choice options does not affect the meaning of the question itself, we propose a simple and effective data leakage detection method based on this property. Specifically, we shuffle the contents of the options in the data to generate the corresponding derived data sets, and then detect data leakage based on the model's log probability distribution over the derived data sets. If there is a maximum and outlier in the set of log probabilities, it indicates that the data is leaked. Our method is able to work under gray-box conditions without access to model training data or weights, effectively identifying data leakage from benchmark test sets in model pre-training data, including both normal scenarios and complex scenarios where options may have been shuffled intentionally or unintentionally. Through experiments based on two LLMs and benchmark designs, we demonstrate the effectiveness of our method. In addition, we evaluate the degree of data leakage of 35 mainstream open-source LLMs on four benchmark datasets and give a ranking of the leaked LLMs for each benchmark, and we find that the Qwen family of LLMs has the highest degree of data leakage.",
            "score": 0.6048125786390052,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92529296875
        },
        {
            "corpus_id": "265466847",
            "title": "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?",
            "text": "Data Contamination during Pre-training The issue of data contamination has become increasingly pronounced with the release of foundation models that obscure the source of their pre-training corpus. This lack of transparency can result in biased perceptions regarding the genuine generalization capabilities of Large Language Models (LLMs). Ignoring the cases where benchmark data is manually integrated into the training set with annotations from human experts or larger models, the root of the data contamination problem lies in the fact that the collecting source of benchmark data is already encompassed in the pre-training corpus. While these models are not intentionally pre-trained using supervised data, they can still acquire exact knowledge. Consequently, it is crucial to address the challenge of detecting the pre-training corpus of LLMs (Shi et al., 2023), exploring the overlap between existing benchmarks and widely-used pre-training corpus, and assessing overfitting to benchmarks (Wei et al., 2023a). These efforts are essential for enhancing the faithfulness and reliability of LLMs. Looking ahead, future directions could involve establishing standardized practices for disclosing pre-training corpus details and developing methods to mitigate data contamination throughout the model development lifecycle. \n\nClose-sourced Development of Alignment The application of Reinforcement Learning from Human Feedback (RLHF) for alignment using general preference data has obtained increasing attention within the community. However, only a limited number of open-source LLMs have been augmented with RLHF or alike (DPO) for alignment, primarily due to the scarcity of high-quality, publicly available preference datasets and pre-trained reward models. Some initiatives (Bai et al., 2022a;Wu et al., 2023b;Cui et al., 2023) have sought to contribute to the open-source community. Yet, we are still facing the challenges lacking diverse, high-quality and scalable preference data in complex reasoning, programming, and safety scenarios. \n\nDifficulty in Continuous Improvements over Fundamental Abilities Reviewing the breakthroughs in fundamental abilities outlined in this paper reveals somewhat challenging scenarios: (1) Considerable efforts have been invested in exploring improved data mixtures during pre-training to enhance balance and robustness in constructing more potent foundation models. However, the associated exploration costs often render this approach impractical.",
            "score": 0.6020212928822081,
            "section_title": "Loopholes and potential problems",
            "char_start_offset": 44490,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1324
                },
                {
                    "start": 1327,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2045
                },
                {
                    "start": 2048,
                    "end": 2409
                },
                {
                    "start": 2410,
                    "end": 2491
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68505859375
        },
        {
            "corpus_id": "268819579",
            "title": "A Comprehensive Survey of Contamination Detection Methods in Large Language Models",
            "text": "With the rise of Large Language Models (LLMs) in recent years, abundant new opportunities are emerging, but also new challenges, among which contamination is quickly becoming critical. Business applications and fundraising in Artificial Intelligence (AI) have reached a scale at which a few percentage points gained on popular question-answering benchmarks could translate into dozens of millions of dollars, placing high pressure on model integrity. At the same time, it is becoming harder and harder to keep track of the data that LLMs have seen; if not impossible with closed-source models like GPT-4 and Claude-3 not divulging any information on the training set. As a result, contamination becomes a major issue: LLMs' performance may not be reliable anymore, as the high performance may be at least partly due to their previous exposure to the data. This limitation jeopardizes real capability improvement in the field of NLP, yet, there remains a lack of methods on how to efficiently detect contamination. In this paper, we survey all recent work on contamination detection with LLMs, analyzing their methodologies and use cases to shed light on the appropriate usage of contamination detection methods. Our work calls the NLP research community's attention into systematically taking into account contamination bias in LLM evaluation.",
            "score": 0.600343216880144,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89208984375
        },
        {
            "corpus_id": "266359062",
            "title": "An Open-Source Data Contamination Report for Large Language Models",
            "text": "Recent years have seen remarkable progress in language models pre-trained on massive text corpora scraped from the web. However, many widely used evaluation benchmarks are also constructed from similar web sources, leading to a concerning issue of data contamination where examples from test sets are unintentionally included in training data. Contamination enables models to \"cheat\" via memorisation of test data rather than displaying true generalisation (Marie, 2023), which creates an illusion of progress, distorts model comparisons, and undermines the utility of benchmarks (Jacovi et al., 2023;Sainz et al., 2023). \n\nContamination analysis therefore became a crucial part of reliable LLM evaluation to validate the results. However, existing contamination analysis is often conducted internally by LLM developers and often lacks transparency and completeness. For instance, OpenAI's contamination study for GPT-4 (OpenAI, 2023) only covered the pre-training data and omitted later fine-tuning stages. Llama-2 (Touvron et al., 2023b) only reported contamination statistics for 2 of the 20+ benchmarks used in their evaluation. In addition, their implementation details of contamination identification remains unclear. Overall, existing internal contamination studies tend to lack sufficient transparency, with minimal sharing of comprehensive contamination measurements across all evaluation benchmarks, as well as training data details and code to reproduce the results. This prevents the wider research community from fully auditing the credibility of reported metrics and model capabilities. \n\nThis paper presents an open contamination analysis for over 15 popular large language models on six common multiple-choice benchmarks, aiming to provide more comprehensive measurements and insights compared to limited existing studies. The analysis includes a range of foundation models such as LLaMA (Touvron et al., 2023a), Llama-2, Yi (Yi, 2023), Mistral (Jiang et al., 2023), Baichuan (Yang et al., 2023), and Qwen (Bai et al., 2023) across multiple model sizes (7B, 13B, 30B, 34B, 65B, 70B parameters) as well as instruct-tuned models built on these foundations like Llama-2 Chat and Mistral-Instruct.",
            "score": 0.6000381009122587,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1600
                },
                {
                    "start": 1603,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2209
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86181640625
        },
        {
            "corpus_id": "270620798",
            "title": "Data Contamination Can Cross Language Barriers",
            "text": "The pre-training data of current large language models (LLMs) tends to be undisclosed by default, even for those open-sourced models (Meta, 2024;Jiang et al., 2024a). As the scores on popular benchmarks continuously reach new heights, their performance in solving real-world tasks seems inconsistent with the leaderboard (Beeching et al.,  2023). Such intransparency in training and inconsistency in user experience has drawn increasing attention to the underlying contamination of public benchmarks in the pre-training data, indicating that some LLMs may simply memorize the answers to difficult questions without a true understanding. \n\nExisting studies often define and detect contamination based on the text overlap or n-gram duplication between pre-training and evaluation data (Chowdhery et al., 2023;Touvron et al., 2023;Jiang et al., 2024b), which only focus on the surface form of the text data without considering the deeper knowledge or semantics in the contamination. We argue that the essence of contamination is not superficial text memorization but the non-generalizable memorization of knowledge or capabilities. \n\nTo this end, we present a cross-lingual form of contamination that can significantly inflate LLMs' benchmark performance without being caught by current detection methods. Cross-lingual means the models are contaminated on other languages and then evaluated on English test sets. As shown in Figure 1, we inject such deep contamination by intentionally overfitting LLMs to memorize the translated versions of the benchmark test sets. Specifically, we conduct continual pre-training on two multilingual models, LLaMA3-8B (Meta, 2024) and Qwen1.5-7b (Bai and et al, 2023), using translated versions of three popular benchmarks-MMLU (Hendrycks et al., 2020), ARC Challenge (Clark et al., 2018), and MathQA (Amini et al., 2019)-in seven different languages. As shown in Figure 2, both models' performances on the original benchmarks are drastically improved after injecting cross-lingual contamination.",
            "score": 0.5997932173132333,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 636
                },
                {
                    "start": 639,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1128
                },
                {
                    "start": 1131,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2029
                }
            ],
            "ref_mentions": [
                {
                    "start": 783,
                    "end": 807,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1761,
                    "end": 1785,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 1834,
                    "end": 1854,
                    "matchedPaperCorpusId": "173188048"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6474609375
        },
        {
            "corpus_id": "273350935",
            "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
            "text": "Pretraining data detection, which is an increasingly important topic for large language models, relates to a large amount of literature on membership inference attacks and data contamination. We discuss some of the relevant works to ours in two directions below. \n\nMembership Inference Attacks Our work mainly studies how to detect a given example in the pretraining data, which is consistent with the objective of membership inference attacks (MIAs) (Shokri et al., 2017;Truex et al., 2019). This task aims to determine whether a given data point is a member of training data. Metric-based attack methods, such as loss (Yeom et al., 2018), entropy (Salem et al., 2019), confidence (Liu et al., 2019) and gradient (Liu et al., 2023), infer membership of data by comparing the calculated metric value with a preset threshold. Previous works have generalized metric-based methods to large language models (Duan et al., 2024;Xie et al., 2024;Zhang et al., 2024;Mattern et al., 2023), by calculating the based-likelihood metric (e.g., perplexity) for membership inference. Recent works apply MIAs to pretraining data detection by designing likelihood-based scoring functions to measure the membership of data (Shi et al., 2024;Ye et al., 2024). In this work, we analyze the limitations of existing scoring functions for pretraining data detection, and design an effective method to improve their performance. In particular, this work is the first to explore the importance of collecting unseen data in pretraining data detection. \n\nData Contamination Data contamination has been studied in the literature (Xu et al., 2024a;Magar & Schwartz, 2022;Balloccu et al., 2024), where training data may inadvertently include evaluation benchmark data, resulting in unauthentic evaluation results. Thus, it is important to assess the leakage of benchmark data into pretraining data (Zhou et al., 2023).",
            "score": 0.5976767665193571,
            "section_title": "A RELATED WORK",
            "char_start_offset": 28308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 262
                },
                {
                    "start": 265,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1525
                },
                {
                    "start": 1528,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1888
                }
            ],
            "ref_mentions": [
                {
                    "start": 451,
                    "end": 472,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 472,
                    "end": 491,
                    "matchedPaperCorpusId": "86836429"
                },
                {
                    "start": 620,
                    "end": 639,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 649,
                    "end": 669,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 682,
                    "end": 700,
                    "matchedPaperCorpusId": "86836429"
                },
                {
                    "start": 714,
                    "end": 732,
                    "matchedPaperCorpusId": "264179931"
                },
                {
                    "start": 958,
                    "end": 979,
                    "matchedPaperCorpusId": "258967264"
                },
                {
                    "start": 1205,
                    "end": 1223,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 1619,
                    "end": 1642,
                    "matchedPaperCorpusId": "247475929"
                },
                {
                    "start": 1642,
                    "end": 1664,
                    "matchedPaperCorpusId": "267499939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92138671875
        },
        {
            "corpus_id": "271089080",
            "title": "Training on the Test Task Confounds Evaluation and Emergence",
            "text": "Benchmarks have played a central role in both machine learning (Hardt and Recht, 2022) and natural language processing (Storks et al., 2019). Classically, benchmarks comprised both a test set and a reasonably large training set (Garofolo et al., 1993;LeCun et al., 1998;Sang and De Meulder, 2003;Koehn, 2005;Deng et al., 2009). Models were trained on the same training set, and then evaluated on the accompanying test set. The success of unsupervised language modelling (Peters et al., 2018;Kenton and Toutanova, 2019;Radford et al., 2019), however, has changed this paradigm. Firstly, present-day language models differ in their training data, which is not standardized but rather treated as a design choice (Raffel et al., 2020;Albalak et al., 2024;Li et al., 2024). Secondly, language models are a priori not trained with the explicit objective of maximizing any single benchmark score. Rather, language models are expected to be able to perform a broad range of tasks (Wang et al., 2018;Brown et al., 2020). Consequently, models are evaluated and compared using a plurality of benchmarks (Beeching et al., 2023;Liang et al., 2023;Srivastava et al., 2023). Data contamination. Data contamination or test-set contamination refers to any overlap between the training and the test data such that test results overestimate a model's generalization performance. The scale and often little curation of present-day pretraining corpora exacerbates data contamination concerns in language model evaluations (Jiang et al., 2024). Consequently, data contamination is usually discussed in the technical reports accompanying model releases (Radford et al., 2019;Brown et al., 2020;Chowdhery et al., 2023;Touvron et al., 2023b). However, detecting and preventing data contamination is currently an open problem (Gunasekar et al., 2023;Yang et al., 2023b;Golchin and Surdeanu, 2023).",
            "score": 0.5966052837734749,
            "section_title": "Related work",
            "char_start_offset": 31533,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 270,
                    "end": 296,
                    "matchedPaperCorpusId": "2470716"
                },
                {
                    "start": 296,
                    "end": 308,
                    "matchedPaperCorpusId": "38407095"
                },
                {
                    "start": 308,
                    "end": 326,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 491,
                    "end": 518,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 518,
                    "end": 539,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 709,
                    "end": 730,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 972,
                    "end": 991,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 991,
                    "end": 1010,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1115,
                    "end": 1134,
                    "matchedPaperCorpusId": "253553585"
                },
                {
                    "start": 1134,
                    "end": 1158,
                    "matchedPaperCorpusId": "263625818"
                },
                {
                    "start": 1630,
                    "end": 1652,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1652,
                    "end": 1671,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1671,
                    "end": 1694,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.521484375
        },
        {
            "corpus_id": "270737802",
            "title": "PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models",
            "text": "The issue of data contamination in large language models has been increasingly recognized as a significant concern (Sainz et al., 2023a). Many LLM providers use string-matching to report contamination, such as GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020b), PaLM (Chowdhery et al., 2023), GPT-4 (OpenAI, 2023), andLlama 2 (Touvron et al., 2023). However, in most cases, the model's training data is not publicly available, necessitating alternative detection methods. \n\nSeveral methods have been developed to detect data contamination in LLMs. Nasr et al. (2023) and Sainz et al. (2023b) explore the regeneration of initial dataset instances. Golchin and Surdeanu (2023b) introduces guided prompting to replicated trained data. Golchin and Surdeanu (2023a) develops a Data Contamination Quiz (DCQ) framework. \n\nBeyond prompt-based methods, there are also methods based on likelihood such as the Min-K% Prob (Shi et al., 2024), Oren et al. (2023) and Li (2023). Additionally, methodologies like CDD and TED (Dong et al., 2024) focus on the LLM's output distribution. But these methods do not pay enough attention to benchmark contamination detection. \n\nMembership Inference Attack (MIA) is closely related to data contamination, aiming to identify whether a given sample is in a model's training data (Shokri et al., 2017;Yeom et al., 2018). These attacks pose significant privacy risks and can lead to severe breaches (Carlini et al., 2021b;Gupta et al., 2022;Cummings et al., 2023). MIA is crucial for assessing privacy vulnerabilities and validating privacy-preserving measures in machine learning models (Jayaraman and Evans, 2019;Jagielski et al., 2020;Nasr et al., 2023).",
            "score": 0.5963671414290921,
            "section_title": "Data Contamination Detection",
            "char_start_offset": 3984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 478
                },
                {
                    "start": 481,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 819
                },
                {
                    "start": 822,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1160
                },
                {
                    "start": 1163,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1687
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 238,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 274,
                    "end": 298,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 555,
                    "end": 573,
                    "matchedPaperCorpusId": "265466445"
                },
                {
                    "start": 918,
                    "end": 936,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 1311,
                    "end": 1332,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1332,
                    "end": 1350,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1429,
                    "end": 1452,
                    "matchedPaperCorpusId": "229156229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8662109375
        },
        {
            "corpus_id": "262055119",
            "title": "Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation",
            "text": "Data contamination in model evaluation is getting increasingly prevalent as the massive training corpora of large language models often unintentionally include benchmark samples. Therefore, contamination analysis has became an inevitable part of reliable model evaluation. However, existing method of contamination analysis requires the access of the entire training data which is often confidential for recent models. This prevent the community to rigorously audit these models and conduct accurate assessment of their capability. In this paper, we propose a novel method to quantify contamination without the access of the full training set, that measure the extent of contamination with perplexity. Our analysis provides evidence of significant memorisation of recent foundation models in popular reading comprehension, summarisation benchmarks, while multiple choice appears less contaminated.",
            "score": 0.5962041029235252,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8408203125
        },
        {
            "corpus_id": "266933004",
            "title": "Investigating Data Contamination for Pre-training Language Models",
            "text": "Numerous studies on large language models (LLMs) have explored and investigated the concept of data contamination and demonstrated the robustness of these models against potential contamination in their evaluation datasets [29,2,6,27,33,34,11]. Most definitions proposed in the existing studies are based on n-gram duplication between pre-training data and evaluation data. For instance, PaLM [6] divides the evaluation data into two categories-\"clean\" and \"contaminated\"-based on whether at least 70% of all possible 8-grams in the evaluation sample were seen at least once in the pre-training corpus. Llama 2 [34] provides a more fine-grained definition: a token is considered contaminated if it appears in any token n-gram longer than 10 tokens in both the evaluation sample and the training set, and the contamination percentage of an evaluation sample is defined to be the percentage of tokens contaminated; the evaluation data are then divided into 4 buckets-\"Clean\", \"Not Clean\", \"Not Dirty\", and \"Dirty\"-based on the contamination percentage of each evaluation sample. While intuitive, these contamination definitions primarily revolve around n-gram or token overlaps, which only target direct duplications present in both training and evaluation datasets and might provide both high false positive rate (since many semantically different texts have overlaps) and false negative rate (since simple paraphrasing can evade detection [36]). Moreover, investigations relying on these definitions have predominantly centered on evaluation level analysis; in our work, we focus on pre-training level analysis as described in \u00a71. \n\nIn our experiments, we follow PaLM [6] and Llama 2's [34] definitions as well as a direct n-gram overlap detection strategy to investigate how the \"contamination\" under these definitions are different and how they affect model performance. As described in \u00a71, contamination in the pre-training corpus can appear as either textual components from evaluation datasets or with ground truth information. \n\nExisting definitions tend to overlook the latter.",
            "score": 0.5961982742939558,
            "section_title": "Contamination Definitions",
            "char_start_offset": 4517,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1630
                },
                {
                    "start": 1633,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2032
                },
                {
                    "start": 2035,
                    "end": 2084
                }
            ],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 227,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 227,
                    "end": 229,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9453125
        },
        {
            "corpus_id": "270285708",
            "title": "Benchmark Data Contamination of Large Language Models: A Survey",
            "text": "The field of natural language processing (NLP) has undergone a significant transformation in recent years, thanks to the rapid advancement of Large Language Models (LLMs) like GPT-4 [107], Claude-3 [4], and Gemini [137].These models, built on deep learning architectures such as Transformers [142], have revolutionized various domains, including content generation, summarization, machine translation, and question-answering.By demonstrating remarkable capabilities in understanding and generating human-like text, they have gained widespread interest and acceptance in both academia and industry.\n\nAmid the excitement surrounding the progress of LLMs, a critical issue has emerged: Benchmark Data Contamination (BDC).This refers to the phenomenon where language models incorporate information related to the evaluation benchmark from their training data, leading to skewed or unreliable performance during the evaluation phase.The challenge at hand involves both the evaluation process of LLMs and their privacy and security considerations [17,18,53,60,73].While some studies see this phenomenon as beneficial [12] or do not consider it to be a problem [16], the majority of studies in the academic community agree that BDC poses significant challenges to the reliability and validity of LLM evaluations, undermining trust in their outputs and hindering their real-world applications [69,83,98,119,126,178].\n\nTraditional evaluation methodologies for LLMs often rely on benchmark datasets as gold standards for measuring model performance.Although these benchmarks are crucial for evaluating, validating, and comparing different models, they are not immune to the issue of BDC.With the rise of AI-generated content (AIGC), this issue is becoming more complex and difficult to detect.The datasets used for training and fine-tuning LLMs may contain benchmark-related information, such as metadata, label distributions, and contextual data, which can inadvertently impact the models' behavior and evaluation performance.Therefore, assessments based on traditional benchmarks may not accurately represent the true capabilities of LLMs and can lead to misguided conclusions about their performance.\n\nIn response to the widespread challenges around BDC, researchers have started to explore alternative assessment methods to reduce the risks associated with traditional benchmarks.",
            "score": 0.5951565273253723,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 220,
                    "end": 425
                },
                {
                    "start": 425,
                    "end": 597
                },
                {
                    "start": 599,
                    "end": 718
                },
                {
                    "start": 718,
                    "end": 928
                },
                {
                    "start": 928,
                    "end": 1058
                },
                {
                    "start": 1058,
                    "end": 1408
                },
                {
                    "start": 1410,
                    "end": 1539
                },
                {
                    "start": 1539,
                    "end": 1677
                },
                {
                    "start": 1677,
                    "end": 1783
                },
                {
                    "start": 1783,
                    "end": 2017
                },
                {
                    "start": 2017,
                    "end": 2193
                },
                {
                    "start": 2195,
                    "end": 2374
                }
            ],
            "ref_mentions": [
                {
                    "start": 292,
                    "end": 297,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1041,
                    "end": 1045,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 1045,
                    "end": 1048,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1048,
                    "end": 1051,
                    "matchedPaperCorpusId": "259316122"
                },
                {
                    "start": 1051,
                    "end": 1054,
                    "matchedPaperCorpusId": "249063119"
                },
                {
                    "start": 1054,
                    "end": 1057,
                    "matchedPaperCorpusId": "246823128"
                },
                {
                    "start": 1111,
                    "end": 1115,
                    "matchedPaperCorpusId": "252780005"
                },
                {
                    "start": 1389,
                    "end": 1392,
                    "matchedPaperCorpusId": "235829052"
                },
                {
                    "start": 1399,
                    "end": 1403,
                    "matchedPaperCorpusId": "264555419"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63720703125
        },
        {
            "corpus_id": "270620798",
            "title": "Data Contamination Can Cross Language Barriers",
            "text": "The opacity in developing large language models (LLMs) is raising growing concerns about the potential contamination of public benchmarks in the pre-training data. Existing contamination detection methods are typically based on the text overlap between training and evaluation data, which can be too superficial to reflect deeper forms of contamination. In this paper, we first present a cross-lingual form of contamination that inflates LLMs\u2019 performance while evading current detection methods, deliberately injected by overfitting LLMs on the translated versions of benchmark test sets. Then, we propose generalization-based approaches to unmask such deeply concealed contamination. Specifically, we examine the LLM\u2019s performance change after modifying the original benchmark by replacing the false answer choices with correct ones from other questions. Contaminated models can hardly generalize to such easier situations, where the false choices can be not even wrong, as all choices are correct in their memorization. Experimental results demonstrate that cross-lingual contamination can easily fool existing detection methods, but not ours. In addition, we discuss the potential utilization of cross-lingual contamination in interpreting LLMs\u2019 working mechanisms and in post-training LLMs for enhanced multilingual capabilities.",
            "score": 0.594707684613343,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92822265625
        },
        {
            "corpus_id": "272689919",
            "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges",
            "text": "As large language models achieve increasingly impressive results, questions arise about whether such performance is from generalizability or mere data memorization. Thus, numerous data contamination detection methods have been proposed. However, these approaches are often validated with traditional benchmarks and early-stage LLMs, leaving uncertainty about their effectiveness when evaluating state-of-the-art LLMs on the contamination of more challenging benchmarks. To address this gap and provide a dual investigation of SOTA LLM contamination status and detection method robustness, we evaluate five contamination detection approaches with four state-of-the-art LLMs across eight challenging datasets often used in modern LLM evaluation. Our analysis reveals that (1) Current methods have non-trivial limitations in their assumptions and practical applications; (2) Notable difficulties exist in detecting contamination introduced during instruction fine-tuning with answer augmentation; and (3) Limited consistencies between SOTA contamination detection techniques. These findings highlight the complexity of contamination detection in advanced LLMs and the urgent need for further research on robust and generalizable contamination evaluation. Our code is available at https://github.com/vsamuel2003/data-contamination.",
            "score": 0.5933154659465798,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.912109375
        },
        {
            "corpus_id": "271957208",
            "title": "LogProber: Disentangling confidence from contamination in LLM responses",
            "text": "In machine learning, contamination refers to situations where testing data leak into the training set. The issue is particularly relevant for the evaluation of the performance of Large Language Models (LLMs), which are generally trained on gargantuan, and generally opaque, corpora of text scraped from the world wide web. Developing tools to detect contamination is therefore crucial to be able to fairly and properly track the evolution of the performance of LLMs. Most recent works in the field are not tailored to quantify contamination on short sequences of text like we find in psychology questionnaires. In the present paper we introduce LogProber, a novel, efficient, algorithm that we show able to detect contamination using token probability in given sentences. In the second part we investigate the limitations of the method and discuss how different training methods can contaminate models without leaving traces in the token probabilities.",
            "score": 0.5930221402611833,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8984375
        },
        {
            "corpus_id": "264490730",
            "title": "Proving Test Set Contamination in Black Box Language Models",
            "text": "Large language models (LLMs) have driven remarkable improvements on a number of natural language processing benchmarks (Wang et al., 2019) and professional exams (OpenAI, 2023). These gains are driven by large-scale pretraining on massive datasets collected from the internet. While this paradigm is powerful, the minimal curation involved has led to growing concerns of dataset contamination, where the pretraining dataset contains various evaluation benchmarks. This contamination leads to difficulties in understanding the true performance of language models -such as whether they simply memorize the answers to difficult exam questions. Disentangling the effects of generalization and test set memorization is critical to our understanding of language model performance, but this is becoming increasingly difficult as the pretraining datasets are rarely public for many of the LMs deployed today. \n\nAlthough there is ongoing work by LLM providers to remove benchmarks from pre-training datasets and perform dataset contamination studies, such filtering can fail due to bugs (Brown et al., 2020a), be limited to a select set of benchmarks (Brown et al., 2020a;Wei et al., 2021;Chowdhery et al., 2022), and requires trust in these vendors. Increasing competitive pressures have also led to some recent model releases to include no contamination studies at all (OpenAI, 2023). These factors make it critical for us to be able to audit existing language models for the presence of benchmark datasets without the cooperation of language model providers. \n\nIn parallel to contamination studies, there has been a growing literature on heuristic membership inference algorithms, that seek to reverse engineer aspects of the pretraining dataset (Carlini et al., Preprint Figure 1: Given a pre-training dataset contaminated with the BoolQ (Clark et al., 2019) test set (left), we detect such contamination by testing for exchangability of the dataset (right). If a model has seen a benchmark dataset, it will have a preference for the canonical order (i.e. the order that examples are given in public repositories) over randomly shuffled examples orderings. We test for these differences in log probabilities, and aggregate them across the dataset to provide false positive rate guarantees.",
            "score": 0.59287231222142,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2284
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 138,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1078,
                    "end": 1099,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1142,
                    "end": 1163,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1833,
                    "end": 1853,
                    "matchedPaperCorpusId": "165163607"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.916015625
        },
        {
            "corpus_id": "264451585",
            "title": "Detecting Pretraining Data from Large Language Models",
            "text": "To determine whether a text X is in the pretraining data of a LLM such as GPT, MIN-K% PROB first gets the probability for each token in X, selects the k% tokens with minimum probabilities and calculates their average log likelihood. If the average log likelihood is high, the text is likely in the pretraining data. \n\nreducing the potential memorization required for successful MIAs (Leino & Fredrikson, 2020;Kandpal et al., 2022). Besides, previous methods often rely on one or more reference models (Carlini et al., 2022;Watson et al., 2022) trained in the same manner as the target model (e.g., on the shadow data sampled from the same underlying pretraining data distribution) to achieve precise detection. This is not possible for large language models, as the training distribution is usually not available and training would be too expensive. \n\nOur first step towards addressing these challenges is to establish a reliable benchmark. We introduce WIKIMIA, a dynamic benchmark designed to periodically and automatically evaluate detection methods on any newly released pretrained LLMs. By leveraging the Wikipedia data timestamp and the model release date, we select old Wikipedia event data as our member data (i.e, seen data during pretraining) and recent Wikipedia event data (e.g., after 2023) as our non-member data (unseen). Our datasets thus exhibit three desirable properties: (1) Accurate: events that occur after LLM pretraining are guaranteed not to be present in the pretraining data. The temporal nature of events ensures that non-member data is indeed unseen and not mentioned in the pretraining data. (2) General: our benchmark is not confined to any specific model and can be applied to various models pretrained using Wikipedia (e.g., OPT, LLaMA, GPT-Neo) since Wikipedia is a commonly used pretraining data source. (3) Dynamic: we will continually update our benchmark by gathering newer non-member data (i.e., more recent events) from Wikipedia since our data construction pipeline is fully automated.",
            "score": 0.5926974287033842,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1791,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 315
                },
                {
                    "start": 318,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2026
                }
            ],
            "ref_mentions": [
                {
                    "start": 383,
                    "end": 409,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 409,
                    "end": 430,
                    "matchedPaperCorpusId": "246823128"
                },
                {
                    "start": 501,
                    "end": 523,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 523,
                    "end": 543,
                    "matchedPaperCorpusId": "244130249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70654296875
        },
        {
            "corpus_id": "270688744",
            "title": "Unveiling the Spectrum of Data Contamination in Language Models: A Survey from Detection to Remediation",
            "text": "Data contamination has garnered increased attention in the era of large language models (LLMs) due to the reliance on extensive internet-derived training corpora. The issue of training corpus overlap with evaluation benchmarks--referred to as contamination--has been the focus of significant recent research. This body of work aims to identify contamination, understand its impacts, and explore mitigation strategies from diverse perspectives. However, comprehensive studies that provide a clear pathway from foundational concepts to advanced insights are lacking in this nascent field. Therefore, we present a comprehensive survey in the field of data contamination, laying out the key issues, methodologies, and findings to date, and highlighting areas in need of further research and development. In particular, we begin by examining the effects of data contamination across various stages and forms. We then provide a detailed analysis of current contamination detection methods, categorizing them to highlight their focus, assumptions, strengths, and limitations. We also discuss mitigation strategies, offering a clear guide for future research. This survey serves as a succinct overview of the most recent advancements in data contamination research, providing a straightforward guide for the benefit of future research endeavors.",
            "score": 0.5926440640148031,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86767578125
        },
        {
            "corpus_id": "265220695",
            "title": "Investigating Data Contamination in Modern Benchmarks for Large Language Models",
            "text": "However, this approach is heavily reliant on having full access to the training corpus. This dependency poses a significant challenge in estimating data contamination for models where the training data is not disclosed (Brown et al., 2020;OpenAI, 2023;Google, 2023;Anthropic, 2023;Li et al., 2023). Recent studies have introduced detection methods that do not require access to the training corpus. These methods, however, might be constrained to a dataset-level granularity as noted by Golchin and Surdeanu (2023); Oren et al. (2023) or require fine-tuning of open-source models (Wei et al., 2023). Given these limitations, there is an evident need for developing new methodologies to detect potential contamination in both open-source The Pile/C4 -Index 2 \n\nThe Pile/C4 -Index N -1 \n\nThe Pile/C4 -Index N demonstrates the workflow of an information retrieval system, which is designed to identify potentially contaminated data within a benchmark using a pre-trained corpus. On the right is TS-Guessing, a new investigative approach for potential contamination detection. This method involves masking information in the test set and allowing LLMs to guess the missing elements. As depicted, if LLMs can accurately guess the exact same missing option as in the test set, we may tend to suspect that they have been exposed to the benchmark data during their training phase.",
            "score": 0.592193197170376,
            "section_title": "Introduction",
            "char_start_offset": 1802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1372
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7734375
        },
        {
            "corpus_id": "271097946",
            "title": "A Taxonomy for Data Contamination in Large Language Models",
            "text": "Large language models pretrained on extensive web corpora demonstrate remarkable performance across a wide range of downstream tasks. However, a growing concern is data contamination, where evaluation datasets may unintentionally be contained in the pretraining corpus, inflating model performance. Decontamination, the process of detecting and removing such data, is a potential solution; yet these contaminants may originate from altered versions of the test set, evading detection during decontamination. How different types of contamination impact the performance of language models on downstream tasks is not fully understood. We present a taxonomy that categorizes the various types of contamination encountered by LLMs during the pretraining phase and identify which types pose the highest risk. We analyze the impact of contamination on two key NLP tasks\u2014summarization and question answering\u2014revealing how different types of contamination influence task performance during evaluation.",
            "score": 0.5921622548499555,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95166015625
        },
        {
            "corpus_id": "278535373",
            "title": "Towards Contamination Resistant Benchmarks",
            "text": "The advent of large language models (LLMs) has largely changed the field of natural language processing and many facets of daily life (Brown et al., 2020;Chung et al., 2022;Chowdhery et al., 2023;Dubey et al., 2024). Extensive research has asserted that these models possess \"human-like abilities\" such as reasoning (Kojima et al., 2022;Wei et al., 2022a,b;Bubeck et al., 2023;Hagendorff et al., 2023;Itzhak et al., 2024;Xie et al., 2024). However, LLM evaluation is often confronted by various factors, and contamination, also known as test set contamination or data leakage, stands out as a key issue. Given that LLMs are trained on vast web corpora, there is concern that their apparent reasoning skills are superficial, and that they are merely retrieving memorized information from their training data (Sainz et al., 2023;Dong et al., 2024;Jiang et al., 2024;Li and Flanigan, 2024;Mueller et al., 2024;Ravaut et al., 2024). \n\nA key strategy to address contamination is to create a dynamic benchmark that receives continuous updates, with new test instances added after a model's training cutoff (Jain et al., 2024;Roberts et al., 2024;Shabtay et al., 2024;White et al., 2024;Zhang et al., 2024;Mahdavi et al., 2025). The effectiveness of this method rests on a critical assumption: the models in question have not been trained on these newly added instances. However, given the black box nature of current LLMs, especially commercial ones, it is almost impossible to determine whether developers have continued training their models after the cutoff using the updated instances, and, if so, when this occurred. As a result, the reliability and validity of the newly added data still remain questionable.",
            "score": 0.5916656276130693,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1708
                }
            ],
            "ref_mentions": [
                {
                    "start": 134,
                    "end": 154,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 173,
                    "end": 196,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 316,
                    "end": 337,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 377,
                    "end": 401,
                    "matchedPaperCorpusId": "259145108"
                },
                {
                    "start": 401,
                    "end": 421,
                    "matchedPaperCorpusId": "260351059"
                },
                {
                    "start": 807,
                    "end": 827,
                    "matchedPaperCorpusId": "264555419"
                },
                {
                    "start": 827,
                    "end": 845,
                    "matchedPaperCorpusId": "267938621"
                },
                {
                    "start": 864,
                    "end": 886,
                    "matchedPaperCorpusId": "266573740"
                },
                {
                    "start": 886,
                    "end": 907,
                    "matchedPaperCorpusId": "265158068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57568359375
        },
        {
            "corpus_id": "218971783",
            "title": "Language Models are Few-Shot Learners",
            "text": "training set by a significant amount, measured relative to a held-out validation set with which it was deduplicated (Figure 4.1). Thus, we expect that contamination is likely to be frequent, but that its effects may not be as large as feared.\n\nWe initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap between our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn't feasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts results.\n\nFor each benchmark, we produce a 'clean' version which removes all potentially leaked examples, defined roughly as examples that have a 13-gram overlap with anything in the pretraining set (or that overlap with the whole example when it is shorter than 13-grams). The goal is to very conservatively flag anything that could potentially be contamination, so as to produce a clean subset that is free of contamination with high confidence. The exact procedure is detailed in Appendix C.\n\nWe then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on the clean subset is similar to the score on the entire dataset, this suggests that contamination, even if present, does not have a significant effect on reported results. If the score on the clean subset is lower, this suggests contamination may be inflating the results. The results are summarized in Figure 4.2. Although potential contamination is often high (with a quarter of benchmarks scoring over 50%), in most cases performance changes only negligibly, and we see no evidence that contamination level and performance difference are correlated. We conclude that either our conservative method substantially overestimated contamination or that contamination has little effect on performance.\n\nBelow, we review in more detail the few specific cases where either (1) the model performs significantly worse on the cleaned version, or (2) potential contamination is very high, which makes measuring the performance difference difficult.\n\nOur analysis flagged six groups of benchmarks for further investigation: Word Scrambling, Reading Comprehension (QuAC, SQuAD2, DROP), PIQA, Winograd, language modeling",
            "score": 0.5910819672249505,
            "section_title": "Measuring and Preventing Memorization Of Benchmarks",
            "char_start_offset": 71634,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.625
        },
        {
            "corpus_id": "257663514",
            "title": "Can we trust the evaluation on ChatGPT?",
            "text": "Given the results showing that even benign contamination can lead to measurable differences, making claims about these models' zero-shot or few-shot inference capabilities require a more careful inspection of the training datasets of these models. For example, the BIG-bench dataset (Srivastava et al., 2022) attempts to address this issue by accompanying the benchmark data with a special string (\"canary\" string). The purpose of this string is to allow researchers to better filter BIG-bench tasks out of the training data for large language models. This string also makes it possible to probe whether a language model was trained on BIG-bench tasks, by evaluating whether the model assigns anomalously high or low probabilities to the string.8 Yet, checking for data contamination is becoming increasingly challenging because the most prominent language models, like ChatGPT and the recently released GPT-4,9 are closed and more models are following the practice. \n\nWhile our work is not without limitation (see 'Limitations' section), we would like to underline that our primary goal of this article is to highlight the ample possibility of data leakage and the impossibility of verifying the lack of data leakage with a closed model. As long as the trend of closed models and continuous training loop continues, it will become more challenging to prevent data leakage (training-test data contamination) and ensure fair evaluation of models. Therefore, in order to ensure the fair evaluability of the models, we argue that the model creators should (1) pay closer attention to the training datasets and document potential data contamination, (2) create mechanisms through which the training datasets and models can be scrutinized regarding data leakage, and (3) build systems that can prevent data contamination from user inputs.",
            "score": 0.5908196299254834,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 16643,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1833
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7373046875
        },
        {
            "corpus_id": "266359062",
            "title": "An Open-Source Data Contamination Report for Large Language Models",
            "text": "To assess how data contamination impacts model evaluation, we assess over 20 popular large language models on contaminated and clean splits of each benchmark. English Benchmarks. Based on the table, we find data contamination does not uniformly improve model performance. Instead, the impact depends on both the specific benchmark and model scale. \n\nOn Hellaswag and ARC benchmarks, most models achieve better metrics on contaminated subsets. However, on MMLU tasks we observe no consistent enhancement across models. We also find that larger language models appear more capable of exploiting data contamination to achieve better performance. For instance, LLaMA-2 70B displays increased metrics on most contaminated subsets. \n\nIn contrast, the 13B LLaMA-2 only outperforms on contaminated ARC. In addition, LLaMA-2 70B realises larger gains on contaminated sets (6% and 11% boosts on Hellaswag and ARC) than the 7B variant (5% and 6%). This could due to the more powerful memorisation capacity in larger language models (Carlini et al., 2022). Finally, most models achieve the highest scores on the input-and-label contaminated subset versus input-only or clean sets. This proves contamination of both inputs and labels can severely affect model evaluation results. Fine-tuned models like the Llama Chat variants exhibit generally lower overall metrics compared to their foundation counterparts, but they demonstrate comparable gains on contaminated splits of Hellaswag and ARC. Specifically, the fine-tuned chat models realise similar absolute performance increases on the dirty subsets of these benchmarks as their corresponding foundation versions. \n\nNon-English Benchmark. In Table 3, we present contamination analysis on the non-English benchmark C-Eval. Among the tested models, Llama and Mistral are considered pure English models, while Yi, Qwen, and Baichuan are pre-trained as multilingual language models. We find the pure English models, Llama and Mistral, do not exhibit notable performance increases on C-Eval's contaminated subsets. However, the multilingual large language models all demonstrate significant performance advantages on dirty subsets. Yi 6B even achieves a 14% higher accuracy score on the input-and-label contaminated set, proving the potential for serious distortion of evaluation results.",
            "score": 0.5889601835305163,
            "section_title": "Impact of Contamination on Model Performance",
            "char_start_offset": 16857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 347
                },
                {
                    "start": 350,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1652
                },
                {
                    "start": 1655,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2322
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54443359375
        },
        {
            "corpus_id": "260736024",
            "title": "Evaluating the Generation Capabilities of Large Chinese Language Models",
            "text": "The benchmarks used to evaluate large language models (LLMs) typically include a robust number of test samples that effectively gauge their fundamental capabilities. This evaluation process is largely automated, making it a convenient method for conducting experiments across various LLMs. This is particularly beneficial for tracking the performance of models at different checkpoints during the pre-training phase. Nonetheless, it's important to recognize that LLMs' performance can be significantly influenced by several factors in the evaluation setup. These include the style of the question prompts, whether the tests are conducted in a zero-shot or few-shot manner, and the methods used for parsing answers. Therefore, when executing these evaluations, it's crucial to consider these potential variables. Additionally, the specific settings used in the evaluation must be clearly noted alongside the results. \n\nAnother critical issue to consider is data contamination. This occurs when the test data, or material closely related to it, has already been included in the pre-training datasets of the LLMs. With the increasing trend of incorporating vast amounts of open data into the development of LLMs, data contamination has become a more prominent concern that needs to be addressed in the evaluation process.",
            "score": 0.5883612809709305,
            "section_title": "Pros and Cons of Benchmark-based approach",
            "char_start_offset": 40243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 915
                },
                {
                    "start": 918,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1318
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6767578125
        },
        {
            "corpus_id": "273850482",
            "title": "Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination",
            "text": "The rapid progression of multimodal large language models (MLLMs) has demonstrated superior performance on various multimodal benchmarks. However, the issue of data contamination during training creates challenges in performance evaluation and comparison. While numerous methods exist for detecting models' contamination in large language models (LLMs), they are less effective for MLLMs due to their various modalities and multiple training phases. In this study, we introduce a multimodal data contamination detection framework, MM-Detect, designed for MLLMs. Our experimental results indicate that MM-Detect is quite effective and sensitive in identifying varying degrees of contamination, and can highlight significant performance improvements due to the leakage of multimodal benchmark training sets. Furthermore, we explore whether the contamination originates from the base LLMs used by MLLMs or the multimodal training phase, providing new insights into the stages at which contamination may be introduced.",
            "score": 0.5881330008046691,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.947265625
        },
        {
            "corpus_id": "266362809",
            "title": "LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction",
            "text": "Clean and robust benchmarks are the key to guide further progress of various models in NLP. Popular benchmarks used to evaluate large language models include: \n\n\u2022 Comprehensive: MMLU, Big Bench Hard, AGI Eval \u2022 Commonsense reasoning: PIQA, SIQA, HellaSwag, WinoGrande, ARC, OpenBookQA, CommonsenseQA \u2022 World knowledge: NaturalQuestions, TriviaQA \u2022 Reading comprehension: SQuAD, QuAC, BoolQ \u2022 Math: GSM8K, MATH \u2022 Code: HumanEval, MBPP where most of them are collected from freely available online sources, which makes them very susceptible to data contamination. For instance, BoolQ (Clark et al. 2019) heavily relies on Wikipedia articles in their instances construct, which leads to a significant contamination rate of 60% as shown in (Brown et al. 2020) because Wikipedia serves as a key part of GPT-3's pretraining data. Jacovi et al. (2023) propose three strategies to alleviate data contamination issues including encrypting test data, refusing derivative distribution, avoiding using internet data that appears with its solution, etc. Our method introduces a novel possibility motivated by examination for humans, where new tests are created dynamically to avoid cheating. Via updating periodically, LatestEval ensures to use of only the most recent texts on the web that were created after any training data were constructed, inherently mitigating the contamination risks.",
            "score": 0.5814220384972829,
            "section_title": "Benchmarks for Language Models",
            "char_start_offset": 7467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 158
                },
                {
                    "start": 161,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1379
                }
            ],
            "ref_mentions": [
                {
                    "start": 824,
                    "end": 844,
                    "matchedPaperCorpusId": "258741333"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51953125
        },
        {
            "corpus_id": "273350935",
            "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
            "text": "The results demonstrate that our method can significantly improve the performance of existing methods based on scoring functions. For example, our method improves the AUC score of the best baseline method Min-k%, increasing it from 0.62 to 0.91 on WikiMIA under the OPT-6.7B model. Moreover, our method can also improve the TPR@5%FPR score of baseline methods. For example, our method improves the TPR@5%FPR score of the detection method using perplexity, increasing it from 0.10 to 0.81 on ArXivTection under the LLaMA-7B model. \n\nOur main contributions are as follows: \n\n\u2022 We analyze the limitations of existing methods based on scoring functions for pretraining data detection. The significant overlap in metric score distribution between seen data and unseen data results in the inferior performance of detection methods. \n\n\u2022 We propose Fine-tuned Score Deviation (FSD), a novel and effective method for detecting pretraining data from large language models. The core idea is to enlarge the gap between members and non-members by exposing the LLM to a few unseen data. \n\n\u2022 We empirically show that FSD can improve the performance of existing methods based on scoring functions for pretraining data detection, through extensive experiments conducted on various benchmark datasets with diverse LLMs.",
            "score": 0.5798717204927225,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 825
                },
                {
                    "start": 828,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1072
                },
                {
                    "start": 1075,
                    "end": 1301
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95556640625
        },
        {
            "corpus_id": "270063170",
            "title": "ConStat: Performance-Based Contamination Detection in Large Language Models",
            "text": "Public benchmarks play an essential role in the evaluation of large language models. However, data contamination can lead to inflated performance, rendering them unreliable for model comparison. It is therefore crucial to detect contamination and estimate its impact on measured performance. Unfortunately, existing detection methods can be easily evaded and fail to quantify contamination. To overcome these limitations, we propose a novel definition of contamination as artificially inflated and non-generalizing benchmark performance instead of the inclusion of benchmark samples in the training data. This perspective enables us to detect any model with inflated performance, i.e., performance that does not generalize to rephrased samples, synthetic samples from the same distribution, or different benchmarks for the same task. Based on this insight, we develop ConStat, a statistical method that reliably detects and quantifies contamination by comparing performance between a primary and reference benchmark relative to a set of reference models. We demonstrate the effectiveness of ConStat in an extensive evaluation of diverse model architectures, benchmarks, and contamination scenarios and find high levels of contamination in multiple popular models including Mistral, Llama, Yi, and the top-3 Open LLM Leaderboard models.",
            "score": 0.5786821727158388,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97705078125
        },
        {
            "corpus_id": "260925501",
            "title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models",
            "text": "Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in measuring LLMs' real effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination at the instance level; using this information, our approach then assesses wider contamination at the partition level. To estimate contamination of individual instances, we employ\"guided instruction:\"a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or nearly matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset partition as contaminated if the average overlap score with the reference instances (as measured by ROUGE-L or BLEURT) is statistically significantly better with the completions from guided instruction compared to a\"general instruction\"that does not include the dataset and partition name. The second idea marks a dataset partition as contaminated if a classifier based on GPT-4 with few-shot in-context learning prompt marks multiple generated completions as exact/near-exact matches of the corresponding reference instances. Our best method achieves an accuracy between 92% and 100% in detecting if an LLM is contaminated with seven datasets, containing train and test/validation partitions, when contrasted with manual evaluation by human experts. Further, our findings indicate that GPT-4 is contaminated with AG News, WNLI, and XSum datasets.",
            "score": 0.5781448779947007,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95458984375
        },
        {
            "corpus_id": "271892318",
            "title": "MIA-Tuner: Adapting Large Language Models as Pre-training Text Detector",
            "text": "This is due to the unique characteristics of LLM pre-training, such as larger-scale corpora, fewer training epochs, and less knowledge of the training data distribution (Shi et al. 2023;Duan et al. 2024). For example, several studies employed reference-free attacks that identify training samples based on statistical scores evaluated on the fine-tuned model, such as perplexity (PPL) (Yeom et al. 2018). Furthermore, other studies achieve higher precision through reference-based attacks, which calibrate the score with a specific \"referenced score\". Existing studies have investigated that comparing the sample PPL to zlib compression entropy (Carlini et al. 2021), the lowercased sample PPL (Carlini et al. 2021), and the neighboring samples PPL (Mattern et al. 2023). Some studies further consider comparing with the sample PPL under a reference model, such as the pre-trained model before finetuning (Mireshghallah et al. 2022a), the smaller model has same architecture (Carlini et al. 2021). Fu et al. (2023a) fine-tune a reference model based on the output of the target model, and achieve an inspiring detection performance over fine-tuned LLMs. Recently, a benchmark dataset (Shi et al. 2023) and two reference-free methods (Shi et al. 2023;Zhang et al. 2024) are proposed to dedicate on detecting pre-training data, which focus on token-level rather than the sentence-level likelihood. \n\nHowever, despite the previous study has achieved considerable achievements in detecting pre-training data, there still exists the following limitations in the research of this problem: First, the widely adopted benchmark dataset, WIKIMIA, can only evaluate LLMs released or pre-trained before January 2023 (Shi et al. 2023). Over the past year, massive new state-of-the-art LLMs like LLaMA-2 (Touvron et al. 2023b), Gemma (Mesnard et al. 2024) have emerged, making WIKIMIA somewhat outdated for assessing the vulnerabilities of these models to MIA.",
            "score": 0.5771012687347237,
            "section_title": "Introduction",
            "char_start_offset": 1857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1946
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 203,
                    "matchedPaperCorpusId": "267627639"
                },
                {
                    "start": 385,
                    "end": 403,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 645,
                    "end": 666,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 694,
                    "end": 715,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 749,
                    "end": 770,
                    "matchedPaperCorpusId": "247315260"
                },
                {
                    "start": 975,
                    "end": 996,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 998,
                    "end": 1015,
                    "matchedPaperCorpusId": "267627639"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57666015625
        },
        {
            "corpus_id": "268819579",
            "title": "A Comprehensive Survey of Contamination Detection Methods in Large Language Models",
            "text": "The tokens sequence is scored by averaging these token-level cross-entropy scores over the first occurence of each token. Contamination is assessed when this final score is above a pre-defined threshold. Min-K%++ (Zhang et al., 2024a) proposes another extension of the idea of Min-K% Prob: rather than simply getting next-token predicted probabilities, the authors propose a score which subtracts the expected log-probability and divides the score by the variance; in other words, normalizing the initial log-probability. The sequence scoring mechanism is then identical to the one of Min-K% Prob. Formally, noting the expected probability over the next token \n\nand its corresponding standard deviation \n\nThe motivation of the score built by Min-K%++ is to assess whether an input form a mode, after observing that because of maximum likelihood training, training samples often become local maxima in the modeled distribution along each input dimension (here in the context of LLM, dimensions are tokens). Min-K%++ reaches state-of-the-art on WikiMIA, outperforming Min-K% Prob by up to 10 points. \n\nLi (2023a) also work on token-level probabilities and compare perplexity on benchmark samples against memorized and clean baselines. The study finds significant memorization in recent models on popular reading comprehension and summarization benchmarks, while multiple-choice benchmarks show less evidence of contamination. This method provides a tool for the community to conduct rigorous contamination analysis, enabling more accurate and reliable model evaluation. Dong et al. (2024) propose two novel likelihoodbased contamination detection methodologies: CDD (Contamination Detection via output Distribution) and TED (Trustworthy Evaluation via output Distribution). CDD detects data contamination by observing the peakedness in the LLM's output distribution in a black-box manner. It represents a significant improvement over existing approaches, offering average relative improvements of 21.8%-30.2% in terms of Accuracy, F1 Score, and AUC metrics. TED corrects the LLM's output distribution to mitigate the impact of data contamination on evaluation metrics, significantly reducing performance improvements attributed to data contamination across various scenarios and contamination degrees.",
            "score": 0.572679917621492,
            "section_title": "Likelihood",
            "char_start_offset": 34345,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 659
                },
                {
                    "start": 662,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2299
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.955078125
        },
        {
            "corpus_id": "265128736",
            "title": "Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models",
            "text": "Large language models (LLMs) have shown remarkable success in various benchmarks (Bubeck et al. 2023;Bang et al. 2023;OpenAI 2023, inter alia). However, due to the lack of transparency regarding the data sources used by some model providers, coupled with their dependency on web data for training (Penedo et al. 2023;Touvron et al. 2023a;b, inter alia), the performance of such models may be skewed by \"data contamination\" (Zhou et al. 2023). This refers to the inclusion of test data from downstream tasks in the LLMs' training data (Golchin and Surdeanu 2024). Considering the immense volume of data used for training, even making this data openaccess does not provide a practical solution. This is because cross-verifying each sample in the test data against open-source training data becomes impractical due to the substantial computational costs (Touvron et al. 2023b) or the extensive manual effort involved (Chowdhery et al. 2022). This has led LLM developers to assess data contamination in benchmark datasets post-training (Anil et al. 2023a). Therefore, there is a pressing need to accurately pinpoint and quantify this contamination using a cost-effective approach, which demands minimal computational resources and human intervention, ensuring its broad applicability by the community. 2 e propose the Data Contamination Quiz (DCQ), a streamlined method that efficiently detects and estimates verbatim data contamination in LLMs without the need to access training data or internal weights/logits. Essentially, DCQ is designed to pinpoint the exact data points from dataset partitions that have leaked into the training data of LLMs, leading to skewed performance. In essence, detection is achieved via a multiplechoice question and estimation is derived by evaluating the LLM's performance across a series of Figure 1: An example of a quiz question crafted to detect data contamination within the test partition of the XSum dataset. Here, the produced answer by the underlying LLM (GPT-4) aligns with the correct option (option C), signaling previous exposure to data, and thus, revealing contamination.",
            "score": 0.5715949815956336,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2116
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 118,
                    "matchedPaperCorpusId": "256662612"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7890625
        },
        {
            "corpus_id": "276106991",
            "title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge",
            "text": "The possible overlap between training data and evaluation benchmarks has become a central issue, since LLMs are usually trained on extensive web corpora (Dodge et al., 2021). This phenomenon, known as data leakage, can artificially improve the performance of LLMs and undermine the reliability of the assessment (Deng et al., 2024a;Jiang et al., 2024b). \n\nSeveral researchers have proposed methods to detect and mitigate data contamination. Deng et al. (2024b) proposed a retrieval-based approach to assess the degree of overlap between pre-training text and benchmark data. Golchin & Surdeanu (2023) have developed \"guided instruction\" to flag contaminated instances. Dong et al. (2024b) proposed the CDD method to identify peaks in the output distribution to detect data contamination. Several studies analyze data leakage for specific LLMs (Balloccu et al., 2024) and report contamination such as cross-language contamination (Yao",
            "score": 0.570766846810632,
            "section_title": "Data Leakage",
            "char_start_offset": 6643,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 353
                },
                {
                    "start": 356,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 933
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 173,
                    "matchedPaperCorpusId": "237568724"
                },
                {
                    "start": 441,
                    "end": 460,
                    "matchedPaperCorpusId": "265220695"
                },
                {
                    "start": 843,
                    "end": 866,
                    "matchedPaperCorpusId": "267499939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79150390625
        },
        {
            "corpus_id": "276575227",
            "title": "Detecting Benchmark Contamination Through Watermarking",
            "text": "Benchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect ``radioactivity'', \\ie traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, e.g. $p$-val $=10^{-3}$ for +5$\\%$ on ARC-Easy.",
            "score": 0.5700390612355188,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.955078125
        },
        {
            "corpus_id": "264451585",
            "title": "Detecting Pretraining Data from Large Language Models",
            "text": "Our WIKIMIA benchmark demonstrates several desirable properties that make it suitable for evaluating methods to detect data during pretraining on any newly released models. \n\n(1) Accurate: Since non-member data consists of events that occurred after the LM pretraining, there is a guarantee that this data was not present during pretraining, ensuring the accuracy of our dataset. We consider Wikipedia event data because of its time sensitivity. A recent non-event Wikipedia page may be only a recent version of an older page that was already present during the model's pretraining, and thus it may not serve as true non-member data. For example, a Wikipedia page created after 2023 about a historical figure or a well-known concept could contain substantial text already mentioned in the pretraining corpus. \n\n(2) General: Our benchmark is designed to be widely applicable across different models pretrained on Wikipedia, a commonly used source of pretraining data. This includes models like OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023a;b), GPT-Neo (Black et al., 2022), and Pythia (Biderman et al., 2023), thereby ensuring the benchmark's generalizability across various models. \n\n(3) Dynamic: Our benchmark will be continually updated by incorporating the latest non-member data, such as recent events from Wikipedia. This consistent renewal ensures that the benchmark's",
            "score": 0.5693041381552166,
            "section_title": "User",
            "char_start_offset": 39792,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 175,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 808
                },
                {
                    "start": 811,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1190
                },
                {
                    "start": 1193,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1383
                }
            ],
            "ref_mentions": [
                {
                    "start": 1060,
                    "end": 1080,
                    "matchedPaperCorpusId": "248177957"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7646484375
        },
        {
            "corpus_id": "270711329",
            "title": "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation",
            "text": "As large language models achieve impressive scores on traditional benchmarks, an increasing number of researchers are becoming concerned about benchmark data leakage during pre-training, commonly known as the data contamination problem. To ensure fair evaluation, recent benchmarks release only the training and validation sets, keeping the test set labels closed-source. They require anyone wishing to evaluate his language model to submit the model's predictions for centralized processing and then publish the model's result on their leaderboard. However, this submission process is inefficient and prevents effective error analysis. To address this issue, we propose to variabilize benchmarks and evaluate language models dynamically. Specifically, we extract variables from each test case and define a value range for each variable. For each evaluation, we sample new values from these value ranges to create unique test cases, thus ensuring a fresh evaluation each time. We applied this variable perturbation method to four datasets: GSM8K, ARC, CommonsenseQA, and TruthfulQA, which cover mathematical generation and multiple-choice tasks. Our experimental results demonstrate that this approach provides a more accurate assessment of the true capabilities of language models, effectively mitigating the contamination problem.",
            "score": 0.5667824184794592,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8583984375
        },
        {
            "corpus_id": "266359062",
            "title": "An Open-Source Data Contamination Report for Large Language Models",
            "text": "The central goal of data contamination analysis is to categorise test samples as either clean or contaminated and then evaluate models separately on the clean and contaminated samples to assess the impact of contamination on the performance metrics. In this section, we describe our methodology to identify contaminated test samples. The basic idea of detecting contaminated examples in our method is to check whether test examples appear verbatim in Common Crawl. We base on Common Crawl because it is completely open-sourced and often comprises the majority of pre-training data for large language models, e.g., Common Crawl weights over 80% in GPT-3 and LLaMA training data (Brown et al., 2020;Touvron et al., 2023a). \n\nWe tailor our search window based on each model's training data collection period. For example, LLaMA models use Common Crawl dumps from 2017 to 2020, which makes our contamination search window 2017-2020. For models with To construct the search queries, we verbalise examples accordingly and make sure the question and the correct answer are involved in the queries. For example: \n\nQuestion: The flaw in Anderson's ACT theory was that some considered it ____.",
            "score": 0.5660501563336956,
            "section_title": "Our Approach",
            "char_start_offset": 10719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1103
                },
                {
                    "start": 1106,
                    "end": 1183
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92138671875
        },
        {
            "corpus_id": "276575227",
            "title": "Detecting Benchmark Contamination Through Watermarking",
            "text": "Benchmark contamination is a significant concern in evaluating LLMs, as it can lead to unreliable assessments and unfair comparisons (Singh et al., 2024;Balloccu et al., 2024). Although efforts are made to decontaminate pre-training corpora (Brown et al., 2020), these methods are not foolproof (Singh et al., 2024). The impact of contamination can be assessed by comparing training runs that differ only in the inclusion of contaminated batches. For instance, Jiang et al. (2024) have shown that even small models can exhibit improved benchmark performance due to contamination. Post-hoc analyses on the other hand identify score inflation by comparing performance on original versus similar questions (Brown et al., 2020;Chowdhery et al., 2023;Touvron et al., 2023), but Yang et al. (2023) have shown that training on reformulated questions is enough to boost the performance on the original benchmark, so the difference in performance does not necessarily provide good correlational insights. Zhang et al. (2024) craft new questions from the same distribution as GSM8K and observed that most models show a significant performance drop on these compared to the GSM8K test set. This result highlights the contamination issue, but does not introduce a scalable solution to the core problem. \n\nIn parallel, studying verbatim memorization in LLMs, such as regurgitating pre-training data, is also closely related to contamination (Carlini et al., 2022;Hartmann et al., 2023). Techniques like membership inference (Mireshghallah et al., 2022) and context-based completion checks (Golchin and Surdeanu, 2023) attempt to approximate contamination without direct access to pre-training data, but their effectiveness is debated (Duan et al., 2024). These methods use a score function to determine the likelihood of a sample being present in the training set. To ensure accurate results, each sample must be calibrated, which can be a computationally intensive process (e.g., involving a model trained on all data except the benchmark).",
            "score": 0.5632963462911589,
            "section_title": "Benchmark Contamination Detection",
            "char_start_offset": 5104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2028
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 261,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 703,
                    "end": 723,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 723,
                    "end": 746,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72412109375
        },
        {
            "corpus_id": "271097946",
            "title": "A Taxonomy for Data Contamination in Large Language Models",
            "text": "Methods with access to pretraining data Early research on LLM data contamination primarily employed methods akin to high-order n-gram overlap detection between pretraining and evaluation data (Radford et al., 2019a;Brown et al., 2020;Wei et al., 2021;Touvron et al., 2023). Tools for qualitative analysis on large-scale corpora (such as Data Portraits (Marone and Durme, 2023) and the ROOTS Search Tool (Piktus et al., 2023)) have further increased the practicality of this type of contamination detection. However, these approaches have several limitations: they remain fairly computationally expensive, assume access to pretraining data, and generally can only detect contamination when a cluster of several test set examples co-occur (as most methods leverage data sketching (Broder, 1997) tools that are only effective for sequences above a certain length). Yang et al. (2023) proposes an LLM-based decontamination method, which leverages embedding similarity search followed by evaluation with a strong language model (e.g. GPT-4), to identify and mitigate contamination. This is computationally costly but can identify noisy contamination Methods without access to pretraining data Some approaches are capable of detecting contamination without direct access to pretraining data, but assume that the test data has not been modified or distributed across the pretraining corpus. These methods leverage metadata from the dataset to detect contamination, e.g. by leveraging dataset ordering (Sainz et al., 2023b) or the assignment of examples to specific data splits (Golchin and Surdeanu, 2023). Golchin and Surdeanu (2024) introduce the Data Contamination Quiz, a streamlined method that efficiently detects and estimates verbatim contamination in LLMs by crafting multiple choice questions that prompt a model to correctly dataset-specific content among similar but noisy alternatives. \n\nChang et al. (2023a) detect contamination of books (which serve as inputs for many long-context evaluation datasets) using domain specific featuresa name cloze test and a publication-year evaluation.",
            "score": 0.5626161182366429,
            "section_title": "Detecting Data Contamination",
            "char_start_offset": 10132,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1891
                },
                {
                    "start": 1894,
                    "end": 2093
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94970703125
        },
        {
            "corpus_id": "270285793",
            "title": "DICE: Detecting In-distribution Contamination in LLM's Fine-tuning Phase for Math Reasoning",
            "text": "Prior work [1] has divided the current data contamination detection methods into two categories, benchmark-level contamination detection and sample-level contamination detection. \n\nBenchmark-level Contamination Detection. In real-world deployments, ensuring the reliability of LLMs is a significant challenge, as these models often perform worse on real problems than their benchmark performance suggests [20]. To address this issue, several methods have been proposed to accurately assess the true abilities of LLMs [21,22,23,24,25,26]. Among these methods, dynamic evaluation [27,28] is widely used to detect benchmark-level contamination. It evaluates the LLMs' performance on newly constructed datasets that share the same distribution as the original benchmark. For example, Clean-Eval [29] uses benchmark data points paraphrased by GPT-3.5 to assess the performance of LLMs. Similarly, GSM1K [30] involves human annotators creating new data that closely resemble the original benchmark data. These methods aim to provide a more accurate representation of an LLM's capabilities by mitigating the effects of benchmark contamination. \n\nSample-level Contamination Detection. Besides benchmark-level, researchers have also focused on detecting contamination at a more granular, sample level [31]. Model memorization methods [32,33] identify which specific data samples have been entirely memorized by the LLM. Prompting methods [34,35] use carefully crafted prompts to elicit data completion from LLMs, helping to determine if a data sample has been seen during training. Model likelihood methods [36,19] are based on the observation that a model's next token predictions are more confident if the data point was part of the training set. However, these methods can not detect in-distribution data contamination.",
            "score": 0.5624677329308985,
            "section_title": "Related Work",
            "char_start_offset": 19486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 181,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1136
                },
                {
                    "start": 1139,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1813
                }
            ],
            "ref_mentions": [
                {
                    "start": 405,
                    "end": 409,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 517,
                    "end": 521,
                    "matchedPaperCorpusId": "259095491"
                },
                {
                    "start": 578,
                    "end": 582,
                    "matchedPaperCorpusId": "263310319"
                },
                {
                    "start": 582,
                    "end": 585,
                    "matchedPaperCorpusId": "266362809"
                },
                {
                    "start": 1325,
                    "end": 1329,
                    "matchedPaperCorpusId": "231786600"
                },
                {
                    "start": 1329,
                    "end": 1332,
                    "matchedPaperCorpusId": "247475929"
                },
                {
                    "start": 1598,
                    "end": 1602,
                    "matchedPaperCorpusId": "264490730"
                },
                {
                    "start": 1602,
                    "end": 1605,
                    "matchedPaperCorpusId": "264451585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8681640625
        },
        {
            "corpus_id": "272367930",
            "title": "Training on the Benchmark Is Not All You Need",
            "text": "Recently, large language models (LLMs) have made significant advances in most natural language processing benchmarks (Hendrycks et al. 2021a;Li et al. 2023;Huang et al. 2024;Wang et al. 2023;Cobbe et al. 2021;Zheng et al. 2024). One of the key reasons why LLMs have achieved such success is through large-scale pre-training on large corpora collected from the Internet. However, due to the intentional or unintentional data collection process of the developers of LLMs, the pre-trained corpus may set contain data from various evaluation benchmarks. Data leakage from such benchmarks causes an inability to accurately evaluate the true performance of LLMs, and the model may simply memorize the answers to difficult questions. The composition of the pre-trained corpus is often considered to be the core secret of existing large models, and open-source models such as LLaMA (Touvron et al. 2023a), Qwen (Bai et al. 2023), and Yi (Young et al. 2024) do not open-source the full training data of their models. Currently most LLMs do not disclose their full pre-training data, which makes it uncertain whether the performance of these LLMs on certain arXiv:2409.01790v2 [cs.CL] 28 Feb 2025 benchmarks is realistic and credible. There is growing concern about the proper use of benchmarks and fair comparisons between different models. Zhou et al. (2023) investigated the impact of benchmark leakage and found that when the pre-training data of a large language model includes data from one of the review benchmarks, it will perform better in this evaluation benchmark, but its performance will drop in other irrelevant tasks, ultimately leading to unreliable assessments of the model's performance. \n\nMany companies and research organizations often advertise how many scores their LLMs have achieved on various benchmarks, achieving first place, yet the fairness of that score is not taken seriously.",
            "score": 0.5616883875990663,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1695
                },
                {
                    "start": 1698,
                    "end": 1897
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 141,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 209,
                    "end": 227,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4580078125
        },
        {
            "corpus_id": "271212307",
            "title": "Qwen2 Technical Report",
            "text": "For large language models, what counts as contamination and how to run contamination analysis remain an active area of research (Ravaut et al., 2024;Golchin & Surdeanu, 2024;Sainz et al., 2023). \n\nIn the following, we first introduce how we try to decontaminate the training corpora against the evaluation datasets, and then estimate the extent to which benchmark scores are influenced by the remaining contamination. \n\nDuring the construction of the pre-training and post-training datasets, we exclude potentially contaminated data using n-gram matching. However, we found that this approach may lead to a high false negative rate, because there could be commonly used expressions, especially in mathematical and coding data. Therefore, we also applied another constraint based on the longest common subsequence (LCS). Specifically, we first remove all symbols and punctuation from both the test and training sequences and perform tokenization. To assess the potential effects of leaking data on the test performance, we follow OpenAI (2023) to construct a strict non-contaminated test set to check if there is a significant performance degradation after strict decontamination. Specifically, we construct the non-contaminated test set by excluding any sample which has 13-gram overlap with the pre-training or the post-training data (without constraint on LCS), and then compute the corresponding metric on the test set. \n\nThe results are presented in Table 15. Although some datasets exhibit a high percentage of contamination under the strict criterion, we noticed that most of the identified contaminated samples are false positives, primarily stemming from the mathematics and coding datasets. It is likely that certain code snippets and mathematical equations are so common that they do not provide any meaningful advantage in solving the test data. Furthermore, our analysis shows that the performance of the Qwen2 models remains consistent between the original and non-contaminated test data, suggesting that the potential issue of data contamination does not significantly impact the model's performance.",
            "score": 0.5615499499339784,
            "section_title": "CONTAMINATION ANALYSIS",
            "char_start_offset": 43031,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 197,
                    "end": 417
                },
                {
                    "start": 420,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1422
                },
                {
                    "start": 1425,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2114
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 174,
                    "matchedPaperCorpusId": "260925501"
                },
                {
                    "start": 174,
                    "end": 193,
                    "matchedPaperCorpusId": "264555419"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7646484375
        },
        {
            "corpus_id": "269448637",
            "title": "Benchmarking Benchmark Leakage in Large Language Models",
            "text": "As pre-training datasets grow, the inadvertent inclusion of benchmark data into the training corpus becomes more likely.Early studies, such as those on GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), FLAN (Wei et al., 2022) and LLaMA-2 (Touvron et al., 2023b), utilized post-hoc n-gram overlap analyses between benchmarks and pre-training corpora to gauge data leakage.Access to pre-training data enables leakage detection through overlap analysis, as highlighted in previous research (Dodge et al., 2021;Deng et al., 2023); even without direct access, details like data sources and time frames can reveal leakages (Li, 2023a;b).However, as model training becomes less transparent, identifying leaks poses greater challenges (Shi et al., 2024) Additionally, studies simulating data leakage by blending benchmarks into training data explore its performance impact (Zhou et al., 2023;Jiang et al., 2024).These investigations underline the importance of understanding models' data memorization, an area explored by prior research (Carlini et al., 2021;Magar & Schwartz, 2022).Our method, independent of instruction-following capabilities, utilizes basic language modeling metrics and data synthesis for detecting data leakage, even achieving instance-level granularity.",
            "score": 0.5614814344521987,
            "section_title": "Data Leakage Detection",
            "char_start_offset": 31461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 120,
                    "end": 379
                },
                {
                    "start": 379,
                    "end": 639
                },
                {
                    "start": 639,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 1083
                },
                {
                    "start": 1083,
                    "end": 1276
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 180,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 188,
                    "end": 208,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 215,
                    "end": 233,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 495,
                    "end": 515,
                    "matchedPaperCorpusId": "237568724"
                },
                {
                    "start": 735,
                    "end": 753,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 1037,
                    "end": 1059,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1059,
                    "end": 1082,
                    "matchedPaperCorpusId": "247475929"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.892578125
        },
        {
            "corpus_id": "270285793",
            "title": "DICE: Detecting In-distribution Contamination in LLM's Fine-tuning Phase for Math Reasoning",
            "text": "The advancement of large language models (LLMs) relies on evaluation using public benchmarks, but data contamination can lead to overestimated performance. Previous researches focus on detecting contamination by determining whether the model has seen the exact same data during training. Besides, prior work has already shown that even training on data similar to benchmark data inflates performance, namely \\emph{In-distribution contamination}. In this work, we argue that in-distribution contamination can lead to the performance drop on OOD benchmarks. To effectively detect in-distribution contamination, we propose DICE, a novel method that leverages the internal states of LLMs to locate-then-detect the contamination. DICE first identifies the most sensitive layer to contamination, then trains a classifier based on the internal states of that layer. Experiments reveal DICE's high accuracy in detecting in-distribution contamination across various LLMs and math reasoning datasets. We also show the generalization capability of the trained DICE detector, which is able to detect contamination across multiple benchmarks with similar distributions. Additionally, we find that DICE's predictions correlate with the performance of LLMs fine-tuned by either us or other organizations, achieving a coefficient of determination ($R^2$) between 0.61 and 0.75. The code and data are available at https://github.com/THU-KEG/DICE.",
            "score": 0.5611874066771141,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87451171875
        },
        {
            "corpus_id": "276482755",
            "title": "A Survey on Data Contamination for Large Language Models",
            "text": "Recent advancements in Large Language Models (LLMs) have demonstrated significant progress in various areas, such as text generation and code synthesis. However, the reliability of performance evaluation has come under scrutiny due to data contamination-the unintended overlap between training and test datasets. This overlap has the potential to artificially inflate model performance, as LLMs are typically trained on extensive datasets scraped from publicly available sources. These datasets often inadvertently overlap with the benchmarks used for evaluation, leading to an overestimation of the models' true generalization capabilities. In this paper, we first examine the definition and impacts of data contamination. Secondly, we review methods for contamination-free evaluation, focusing on three strategies: data updating-based methods, data rewriting-based methods, and prevention-based methods. Specifically, we highlight dynamic benchmarks and LLM-driven evaluation methods. Finally, we categorize contamination detecting methods based on model information dependency: white-Box, gray-Box, and black-Box detection approaches. Our survey highlights the requirements for more rigorous evaluation protocols and proposes future directions for addressing data contamination challenges.",
            "score": 0.5607794742009472,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94482421875
        },
        {
            "corpus_id": "273502439",
            "title": "CAP: Data Contamination Detection via Consistency Amplification",
            "text": "Large language models (LLMs) are widely used, but concerns about data contamination challenge the reliability of LLM evaluations. Existing contamination detection methods are often task-specific or require extra prerequisites, limiting practicality. We propose a novel framework, Consistency Amplification-based Data Contamination Detection (CAP), which introduces the Performance Consistency Ratio (PCR) to measure dataset leakage by leveraging LM consistency. To the best of our knowledge, this is the first method to explicitly differentiate between fine-tuning and contamination, which is crucial for detecting contamination in domain-specific models. Additionally, CAP is applicable to various benchmarks and works for both white-box and black-box models. We validate CAP's effectiveness through experiments on seven LLMs and four domain-specific benchmarks. Our findings also show that composite benchmarks from various dataset sources are particularly prone to unintentional contamination. Codes will be publicly available soon.",
            "score": 0.5601844106222117,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "268889777",
            "title": "Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models",
            "text": "Data is one of the most important factors for the success of large language models (LLMs). As the training corpus grows in scale, it has increasing tendency to be held in-house as proprietary data instead of being publicly disclosed (Touvron et al., 2023b;Achiam et al., 2023). However, for large-scale training corpora that consist of up to trillions of tokens (Computer, 2023), the sheer volume of the training corpus can lead to unintended negative consequences. For example, memorized private information is vulnerable to data extraction (Carlini et al., 2021), and memorized copyrighted contents (e.g., books and news articles) may violate the rights of content creators (Grynbaum & Mac, 2023;Knibbs, 2023). Furthermore, it becomes increasingly likely that evaluation data is exposed at training time, bringing the faithfulness and effectiveness of evaluation benchmarks into question (Oren et al., 2023). \n\nFor these reasons, there has been growing interest in effective pre-training data detection strategies. Pre-training data detection can be considered a special case of Membership Inference Attack (MIA) (Shokri et al., 2017): the goal is to infer whether a given input has been used for training a target LLM (see Figure 1 left for illustration). Due to characteristics of pre-training corpora and training characteristics of LLMs (Shi et al., 2024;Duan et al., 2024), this problem has been shown to be much more challenging than conventional MIA settings (see Section 2 for details). There are a few methods proposed recently, dedicated to this problem (Carlini et al., 2021;Mattern et al., 2023;Shi Figure 1: Left: We study the pre-training data detection problem for LLMs (Shi et al., 2024;Duan et al., 2024). Given the grey-box access to the target LLM (only the logits, token probabilities, and loss are available), the goal is to design a scoring function that yields a score which (upon thresholding) best separates training and non-training text.",
            "score": 0.5596087674231002,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 542,
                    "end": 564,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1115,
                    "end": 1136,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1343,
                    "end": 1361,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 1566,
                    "end": 1588,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1588,
                    "end": 1609,
                    "matchedPaperCorpusId": "258967264"
                },
                {
                    "start": 1687,
                    "end": 1705,
                    "matchedPaperCorpusId": "264451585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86376953125
        },
        {
            "corpus_id": "272423548",
            "title": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding",
            "text": "Results on WikiMIA. Table 2 summarizes the experimental results on WikiMIA, demonstrating CON-RECALL's significant improvements over baseline methods. In terms of AUC performance, our method improved upon ReCall by 7.4%, 6.6%, and 5.7% on WikiMIA-32, -64, and -128 respectively, achieving an average improvement of 6.6% and state-of-the-art performance. For TPR@5%FPR, CON-RECALL outperformed the runner-up by even larger margins: 30.0%, 34.8%, and 27.6% on WikiMIA-32, -64, and -128 respectively, with an average improvement of 30.8%. Notably, CON-RECALL achieves the best performance across models of different sizes, from Mamba-1.4B to LLaMA-30B, demonstrating its robustness and effectiveness. The consistent performance across varying sequence lengths suggests that CON-RECALL effectively identifies membership information in both short and long text samples, underlining its potential as a powerful tool for detecting pre-training data in large language models in diverse scenarios. \n\nResults on MIMIR. We summarize the experimental results on MIMIR in Appendix D. The performance of CON-RECALL on the MIMIR benchmark demonstrates its competitive edge across various datasets and model sizes. In the 7-gram setting, CON-RECALL consistently achieved top-tier results, often outperforming baseline methods. Notably, on several datasets, our method frequently secured the highest scores in both AUC and TPR metrics. In the 13-gram setting, CON-RECALL maintained its strong performance, particularly with larger model sizes. While overall performance decreased compared to the 7-gram setting, still held leading positions across multiple datasets. It's worth noting that CON-RECALL exhibited superior performance when dealing with larger models, indicating good scalability for more complex and larger language models. Although other methods occasionally showed slight advantages in certain datasets, CON-RECALL's overall robust performance underscores its potential as an effective method for detecting pre-training data in large language models.",
            "score": 0.5578765188991597,
            "section_title": "Results",
            "char_start_offset": 14311,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 988
                },
                {
                    "start": 991,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 2049
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9306640625
        },
        {
            "corpus_id": "273350935",
            "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
            "text": "In the era of large language models (LLMs), detecting pretraining data has been increasingly important due to concerns about fair evaluation and ethical risks. Current methods differentiate members and non-members by designing scoring functions, like Perplexity and Min-k%. However, the diversity and complexity of training data magnifies the difficulty of distinguishing, leading to suboptimal performance in detecting pretraining data. In this paper, we first explore the benefits of unseen data, which can be easily collected after the release of the LLM. We find that the perplexities of LLMs shift differently for members and non-members, after fine-tuning with a small amount of previously unseen data. In light of this, we introduce a novel and effective method termed Fine-tuned Score Deviation(FSD), which improves the performance of current scoring functions for pretraining data detection. In particular, we propose to measure the deviation distance of current scores after fine-tuning on a small amount of unseen data within the same domain. In effect, using a few unseen data can largely decrease the scores of all non-members, leading to a larger deviation distance than members. Extensive experiments demonstrate the effectiveness of our method, significantly improving the AUC score on common benchmark datasets across various models.",
            "score": 0.5560194042558733,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "278339246",
            "title": "Automatic Calibration for Membership Inference Attack on Large Language Models",
            "text": "Our experiments involve three main benchmarks for evaluating MIAs: WikiMIA [24], MIMIR [8], and PatentMIA [37]. WikiMIA organizes Wikipedia event texts by publication date into training and non-training data, further split by sentence length for finegrained evaluation. It includes original (verbatim texts) and paraphrased (ChatGPT-generated) settings to evaluate the robustness of detection performance. MIMIR, sourced from the Pile dataset [11], draws training and non-training samples from identical distributions within their respective sets. This setup is more challenging than WikiMIA due to minimal distribution shifts and temporal discrepancies [8]. PatentMIA is a Chinese-language benchmark for evaluating pre-training data detection beyond English. To differentiate training from non-training data, the benchmark leverages the priority dates of patents, for instance, texts with priority dates after the release of the LLM are guaranteed to be excluded from its pre-training.",
            "score": 0.5556874771942526,
            "section_title": "Datasets",
            "char_start_offset": 19104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 986
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5810546875
        },
        {
            "corpus_id": "273850342",
            "title": "Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?",
            "text": "With our work, we aim to contribute to an informed discussion about evaluation data contamination and its effects, as well as provide methodology for both researchers and practitioners to explore this question further and contrast the effect of different contamination metrics and hyper-parameters as newer pre-training corpora (Soldaini et al., 2024;Penedo et al., 2024) and benchmarks (Laurent et al., 2024) are released. \n\nOutline In the remainder of this paper, we first review earlier work on contamination detection, and consider their results and limitations ( \u00a7 2). Next, we describe the various contamination metrics we investigate in more detail and list the benchmarks, models and pre-training corpora we use for our study ( \u00a7 3), followed by a description of our main methodology for analysis ( \u00a7 4). After that, in our main results section ( \u00a7 5), we first report the results of an overall comparison between all contamination metrics ( \u00a7 5.1). Both numerically and qualitatively, we show how different contamination metrics provide different signal across datasets, and we show that taking the longest matching n-gram, rather than a union of all n-grams, detects the most meaningful EPG across datasets and models. Next, in \u00a7 5.2, we discuss how much contamination is detected across various benchmarks ( \u00a7 5.2.1), how much that impacts benchmark scores for various models ( \u00a7 5.2.2) and how that depends on model scale ( \u00a7 5.2.3). Consecutively, in \u00a7 6, we investigate how various parameters that frequently occur in contamination metrics impact the results. Specifically, we consider the impact of n ( \u00a7 6.1), the impact of the mismatch budget ( \u00a7 6.2) and the frequency of matches in the pre-training corpus ( \u00a7 6.3). We wrap up with a conclusion in which summarise our results and provide concrete recommendations for practitioners ( \u00a7 7) and a discussion in which we consider the limitations of our work ( \u00a7 8).",
            "score": 0.5553313886650388,
            "section_title": "Introduction",
            "char_start_offset": 1821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1930
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9052734375
        },
        {
            "corpus_id": "276575227",
            "title": "Detecting Benchmark Contamination Through Watermarking",
            "text": "In recent years, Large Language Models (LLMs) have demonstrated remarkable advancements in their capabilities (Brown et al., 2020;Touvron et al., 2023). This advancement places increasingly greater emphasis on proper evaluation to both inform the state of LLM research and to guide future developments. To this end, a multitude of benchmark datasets such as (MMLU) (Hendrycks et al., 2020), School Math 8K (GSM8K) (Cobbe et al., 2021), and the AI2 Reasoning Challenge (ARC) (Clark et al., 2018), or more recently GPQA (Rein et al., 2023) and FrontierMath (Glazer et al., 2024), are developed to measure the model's capabilities in terms of general or specific knowledge, understanding, and scientific reasoning. \n\nHowever, a significant issue that arises with these benchmarks is contamination. This problem can occur either intentionally, by training models directly on the benchmark datasets or their reformulated versions, or unintentionally, as these datasets become mixed with the vast amounts of data used during pre-training. For example, Zhang et al. (2024) created a version of GSM8K with new questions similar in difficulty and form, and observed that many models show a significant drop in performance on them compared to the test set of GSM8k. This challenges the reliability and validity of benchmark evaluations, as it becomes difficult to discern whether a model's performance is due to genuine improvement in capabilities or mere memorization. Furthermore, determining whether a model has been trained on a specific benchmark is very challenging, as it boils down to the issue of dataset/membership inference which has been shown to be ineffective for LLMs in realistic scenarios (Duan et al., 2024). \n\nTo tackle this problem, we propose a novel strategy of embedding non-intrusive watermarks in the benchmark dataset before release. Our approach is inspired by Sander et al. (2024), who demonstrated that fine-tuning on LLM-generated watermarked text can be reliably detected, as the model retains identifiable traces of the watermark.",
            "score": 0.55518277167541,
            "section_title": "Introduction",
            "char_start_offset": 1518,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 110,
                    "end": 130,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90966796875
        },
        {
            "corpus_id": "277452550",
            "title": "WinoWhat: A Parallel Corpus of Paraphrased WinoGrande Sentences with Common Sense Categorization",
            "text": "Finally, our method of finding data contamination in pre-training data was on the data level only, not taking into account the semantic or information level [Xu et al., 2024]. Methods such as ours relying on string matching methods might miss certain instances, such as rephrasings [Xu et al., 2024]. Furthermore, such methods are only possible when access to pre-training corpora is public [Yang et al., 2023].",
            "score": 0.5528985147593463,
            "section_title": "A Limitations",
            "char_start_offset": 30524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 411
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12359619140625
        },
        {
            "corpus_id": "264172693",
            "title": "Data Contamination Through the Lens of Time",
            "text": "Progress in machine learning has historically been driven by the use of benchmark datasets [56] to demonstrate and ultimately improve model performance. In recent years, as large language models (LLMs) have risen to prominence, these benchmarks are used to claim impressive capabilities across a wide range of tasks [7], such as open-ended text and code generation. However, it has become increasingly clear that evaluating on these benchmarks jeopardizes our ability to accurately compare and assess modern models since static, open-source benchmarks are generally published on the internet, and most modern LLMs incorporate internet text in their training data. \n\nThere are two main phenomena with which to be concerned. The first is contamination, which refers to an LLM's exposure, during training, to examples that are similar or identical to the examples that the model will later be evaluated on. The second is memorization, which can be understood as a property of a model that permits extraction of generated outputs that are exact or near-exact replicas of examples seen during training. Both phenomena can pose security and privacy risks [12]. Additionally, as we discuss below, they can upwardly bias model performance estimates, obfuscating our ability to compare models and attribute performance gains to true model improvements. \n\nDespite these concerns, contamination and memorization remain deceptively challenging to definitively measure and detect. While some researchers have used string-matching algorithms to compare test to training datasets [8,55], many popular LLMs' full training dataset details are generation models include general models such as the GPT family [8,50], Llama 2 [58] or PaLM [16], as well as a variety of task-specific code models: AlphaCode [38], CodeGen [48], CodeLLama [58], PaLM-Coder [16], Starcoder [37]. Relevant code generation benchmarks include small sets of entirely handwritten problems [15,48] as well as larger collections curated from internet sources such as code interview sites, competitive programming forums, or general open source code: [1,3,23,25,64], and some that include both original and online-sourced problems [38,63].",
            "score": 0.5524692848071808,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1343
                },
                {
                    "start": 1346,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2190
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 95,
                    "matchedPaperCorpusId": "244729397"
                },
                {
                    "start": 316,
                    "end": 319,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1565,
                    "end": 1568,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1568,
                    "end": 1571,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1690,
                    "end": 1693,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1800,
                    "end": 1804,
                    "matchedPaperCorpusId": "252668917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3388671875
        },
        {
            "corpus_id": "264451585",
            "title": "Detecting Pretraining Data from Large Language Models",
            "text": "We present a pre-training data detection dataset WIKIMIA and a new approach MIN-K% PROB. Our approach uses the intuition that trained data tends to contain fewer outlier tokens with very low probabilities compared to other baselines. Additionally, we verify the effectiveness of our approach in real-world setting, we perform two case studiies: detecting dataset contamination and published book detection. For dataset contamination, we observe empirical results aligning with theoretical predictions about how detection difficulty changes with dataset size, example frequency, and learning rate. Most strikingly, our book detection experiments provide strong evidence that GPT-3 models may have been trained on copyrighted books. System You are a helpful assistant in evaluating the similarity between two outputs generated by two different AI chatbots. Your goal is to rate the similarity between the two outputs based on a scale of 1 to 5, with 1 being highly dissimilar and 5 being highly similar.",
            "score": 0.5521633793230777,
            "section_title": "CONCLUSION",
            "char_start_offset": 34800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1001
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.951171875
        },
        {
            "corpus_id": "270619707",
            "title": "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation",
            "text": "Contamination Detection Traditional contamination detection methods directly calculate the overlap between pre-training data and evaluation datasets, including n-gram analysis (Touvron et al., 2023b;OpenAI, 2023;Team et al., 2023;Bai et al., 2023) and BM25 (Jiang et al., 2024) for indexing and matching.However, as pre-training data grows exponentially, even simple n-gram statistics become extremely resource-intensive.Yang et al. (2023c); Gunasekar et al. (2023) find n-gram detection unreliable due to unintentional contamination risks.More importantly, training corpora for mainstream LLMs are mostly inaccessible, so recent research has turned to focus on: i)-exploiting the distributional differences between the benchmark training set and the test set to evaluated (Xu et al., 2024).ii)-Evaluate sample-level contamination by providing text segments and black-box access to the LLM (Shi et al., 2023).Other work evaluates contamination through LLM-generated content, limited by the LLM's comprehension abilities to instrurction (Deng et al., 2023;Golchin and Surdeanu, 2023a).Some studies test if models can coherently continue a given sample part (Golchin and Surdeanu, 2023b).Contamination detection remains a critical concern that should be addressed in benchmarks rather than affecting a fair assessment of the model's capabilities.\n\nDecontamination Decontamination involves avoiding or mitigating the negative effects of contamination.Typically, decontamination applied in the training phase, model developers using various methods to remove these overlap between pre-training data and evaluation data (OpenAI, 2023;Touvron et al., 2023b;Radford et al., 2019;Brown et al., 2020a;Chowdhery et al., 2023).\n\nBesides, new datasets can also be created to avoid contamination The LatestEval (Li et al., 2024) avoids model contamination by strictly adhering to a temporal sequence, using texts published within a recent time window to construct new question-answer sets from the latest Wikipedia data.",
            "score": 0.5521112250007545,
            "section_title": "Related Work",
            "char_start_offset": 5158,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 304
                },
                {
                    "start": 304,
                    "end": 421
                },
                {
                    "start": 421,
                    "end": 540
                },
                {
                    "start": 540,
                    "end": 791
                },
                {
                    "start": 791,
                    "end": 909
                },
                {
                    "start": 909,
                    "end": 1084
                },
                {
                    "start": 1084,
                    "end": 1186
                },
                {
                    "start": 1186,
                    "end": 1344
                },
                {
                    "start": 1346,
                    "end": 1448
                },
                {
                    "start": 1448,
                    "end": 1716
                },
                {
                    "start": 1718,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 1651,
                    "end": 1672,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1692,
                    "end": 1715,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "267499939",
            "title": "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs",
            "text": "Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of indirect data leaking, where modelsare iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI\u2019s GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI\u2019s data usage policy, we extensively document the amount of data leaked to these models during the first year after the model\u2019s release. We report that these models have been globally exposed to \u223c4.7M samples from 263 benchmarks. At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues. We release our results as a collaborative project on https://leak-llm.github.io/, where other researchers can contribute to our efforts.",
            "score": 0.5507799165188425,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.289794921875
        },
        {
            "corpus_id": "276094401",
            "title": "How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence",
            "text": "Dataset and model. Our controlled experiment aims to evaluate the extent to which each method satisfies the Monotonicity and Consistency requirements. For this purpose, we utilize three popular pre-training data detection benchmarks, WikiMIA (Shi et al., 2023), BookMIA (Shi et al., 2023), and ArxivTection (Duarte et al., 2024), each comprising samples labeled as 'seen' or 'unseen'. Among the models compatible with the datasets, we select Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) for our main evaluation. We present additional results for other model families in Section 6. Baselines. We consider various baseline methods for comprehensive evaluation. Specifically, we consider Zlib (Carlini et al., 2021), Perplexity Score (Li, 2023), Min-K% (Shi et al., 2023), Min-K%++ (Zhang et al., 2024b), Fine-tuned Score Deviation (FSD; Zhang et al. ( 2025)), which evaluate the likelihood of exposure for every sample independently. \n\nThe overall contamination score of the dataset S(D, M) is then quantified by averaging these instance-wise scores. \n\nIn addition, we consider the dataset-level approach that assesses the contamination of a dataset as a whole by examining statistical or distributional patterns that differentiate seen vs unseen datasets. This approach provides a more holistic view of contamination, capturing aggregate characteristics that are not discernible at the individual example level. Specifically, we consider the Sharded Rank Comparison Test (SRCT; Oren et al. (2024)), the latest dataset-level detection method that identifies datasets showing significant variability in likelihood values across different sample orderings. For each baseline, we adjusted their score sign so that higher scores indicate more contamination (i.e., bigger \u03bb). We include the detailed definition of each baseline in Appendix A.2. \n\nExperimental details. For each dataset, we evaluate scoring performances on different contamination rates.",
            "score": 0.5481996287379122,
            "section_title": "Experimental Setup",
            "char_start_offset": 10556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 932
                },
                {
                    "start": 935,
                    "end": 1049
                },
                {
                    "start": 1052,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1838
                },
                {
                    "start": 1841,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1947
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 260,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 270,
                    "end": 288,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 307,
                    "end": 328,
                    "matchedPaperCorpusId": "267681760"
                },
                {
                    "start": 691,
                    "end": 713,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 751,
                    "end": 769,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 1478,
                    "end": 1496,
                    "matchedPaperCorpusId": "264490730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "273502439",
            "title": "CAP: Data Contamination Detection via Consistency Amplification",
            "text": "Large language models (LLMs) have significantly advanced natural language processing (NLP), leading to widespread use in applications such as translation (Agostinelli et al., 2024), summarization (Chang et al., 2024) and question answering (Schimanski et al., 2024). Notable models include GPT (OpenAI, 2024) series and LLaMA (Touvron et al., 2023). Beyond general-purpose applications, some LLMs have been fine-tuned for specific domains, producing specialized models with expertise in fields like law (Cui et al., 2024), biomedicine (Bolton et al., 2024), finance (Yang et al., 2023b), and software development (Chen et al., 2021a). \n\nMeanwhile, various benchmarks have been developed to assess LLM capabilities, focusing on * Corresponding author specific skills (Hendrycks et al., 2021b) or comprehensive abilities (Xu et al., 2020;Hendrycks et al., 2021a). Based on these benchmarks, certain models claim to surpass larger counterparts, despite being significantly smaller in size-for instance, 7B or 13B models (Liu et al., 2024;Lei et al., 2024) compared to GPT models. \n\nConcerns about LLM evaluation reliability are increasing due to a lack of transparency, especially with closed-source models. It raises a crucial question: Is their impressive performance a result of generalization, or merely due to prior exposure and memorization of test samples, a problem known as data contamination? Without effective detection methods, model providers might maliciously contaminate models to boost performance. \n\nData contamination detection (DCD) methods are divided into two main categories (Xu et al., 2024a). The first is matching-based, which examines benchmark overlap (Yang et al., 2023a) or investigates whether models memorize specific samples (Golchin and Surdeanu, 2024;Deng et al., 2024). A key limitation of these methods is their vulnerability to surface-level paraphrasing (Dekoninck et al., 2024).",
            "score": 0.5481426552959296,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1511
                },
                {
                    "start": 1514,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 216,
                    "matchedPaperCorpusId": "263605928"
                },
                {
                    "start": 240,
                    "end": 265,
                    "matchedPaperCorpusId": "267636562"
                },
                {
                    "start": 819,
                    "end": 836,
                    "matchedPaperCorpusId": "215745536"
                },
                {
                    "start": 836,
                    "end": 860,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 1782,
                    "end": 1800,
                    "matchedPaperCorpusId": "265220695"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76904296875
        },
        {
            "corpus_id": "265128736",
            "title": "Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models",
            "text": "We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions, devising a quiz format wherein three perturbed versions of each instance, subsampled from a specific dataset partition, are created. These changes only include word-level perturbations. The generated perturbations, along with the original dataset instance, form the options in the DCQ, with an extra option accommodating the selection of none of the provided options. Given that the only distinguishing signal among the options is the exact wording with respect to the original dataset instance, an LLM, when tasked with identifying the original dataset instance, gravitates towards selecting the original one if it has been exposed to it. While accounting for positional biases in LLMs, the quiz performance reveals the contamination level for the tested model with the dataset partition to which the quiz pertains. Applied to various datasets and LLMs, under controlled and uncontrolled contamination, our findings, while fully lacking access to training data and model parameters, suggest that DCQ achieves state-of-the-art results and uncovers greater contamination levels through memorization compared to existing methods. Also, it proficiently bypasses more safety filters, especially those set to avoid generating copyrighted content.",
            "score": 0.5474139286752697,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87646484375
        },
        {
            "corpus_id": "273822116",
            "title": "Training on the Test Model: Contamination in Ranking Distillation",
            "text": "Neural approaches to ranking based on pre-trained language models are highly effective in ad-hoc search. However, the computational expense of these models can limit their application. As such, a process known as knowledge distillation is frequently applied to allow a smaller, efficient model to learn from an effective but expensive model. A key example of this is the distillation of expensive API-based commercial Large Language Models into smaller production-ready models. However, due to the opacity of training data and processes of most commercial models, one cannot ensure that a chosen test collection has not been observed previously, creating the potential for inadvertent data contamination. We, therefore, investigate the effect of a contaminated teacher model in a distillation setting. We evaluate several distillation techniques to assess the degree to which contamination occurs during distillation. By simulating a ``worst-case'' setting where the degree of contamination is known, we find that contamination occurs even when the test data represents a small fraction of the teacher's training samples. We, therefore, encourage caution when training using black-box teacher models where data provenance is ambiguous.",
            "score": 0.5448951150397688,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6474609375
        },
        {
            "corpus_id": "273549214",
            "title": "Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions",
            "text": "Large language models (LLMs) have demonstrated great performance across various benchmarks, showing potential as general-purpose task solvers. However, as LLMs are typically trained on vast amounts of data, a significant concern in their evaluation is data contamination, where overlap between training data and evaluation datasets inflates performance assessments. Multiple approaches have been developed to identify data contamination. These approaches rely on specific assumptions that may not hold universally across different settings. To bridge this gap, we systematically review 50 papers on data contamination detection, categorize the underlying assumptions, and assess whether they have been rigorously validated. We identify and analyze eight categories of assumptions and test three of them as case studies. Our case studies focus on detecting direct, instance-level data contamination, which is also referred to as Membership Inference Attacks (MIA). Our analysis reveals that MIA approaches based on these three assumptions can have similar performance to random guessing, on datasets used in LLM pretraining, suggesting that current LLMs might learn data distributions rather than memorizing individual instances. Meanwhile, MIA can easily fail when there are data distribution shifts between the seen and unseen instances.",
            "score": 0.5433087486916546,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.861328125
        },
        {
            "corpus_id": "268201479",
            "title": "Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs",
            "text": "In the context of Large Language Models (LLMs), contamination of benchmark datasets refers to the presence of test data in either the pre-training or fine-tuning data of an LLM. Such test data can either be present in full or as a subset, and may also include derivatives or transformed versions of the original test data. For instance, certain multilingual benchmarks like XNLI [8] and MLQA [24] are translations of a benchmark originally created in English. It is possible that the original English benchmark may be present in an LLM's data, even if the multilingual version is not. Due to this, despite no contamination of the multilingual dataset, the LLM may still be able to recover an answer from the contaminated data through cross-lingual transfer. [6] introduce the concept of indirect data leakage, which happens when benchmark data is leaked when researchers evaluate models via APIs or interfaces, where the usage policy states that user interaction data can be used for improving the models. [6] review several papers on benchmarking and find that 42% of papers they review may have inadvertently leaked data to GPT-3.5 and GPT-4 due to their access mechanism, which does not guarantee that conversational data will not be used for fine tuning or improving models. According to the study, based on the benchmarks used in the surveyed papers, a total of 4.7M benchmark samples across 263 benchmarks have been leaked in this way. \n\nSeveral techniques have been proposed to detect contamination. One of the early heuristic based techniques include checking whether the model under consideration was trained before or after the benchmark was released. While this is an effective way to check for contamination in the original model, it does not account for model updates and indirect data leakage mentioned above. Another heuristic is to check whether the dataset is available as text on the web, or whether it can be only accessed via download. In some cases, datasets are provided in password protected folders, which prevents automated crawlers from ingesting this data. However, there is no guarantee that other copies of these datasets do not exist on the web, or that model builders have not included this data during pretraining or fine-tuning.",
            "score": 0.5428648490408283,
            "section_title": "Data contamination in LLMs",
            "char_start_offset": 5545,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1441
                },
                {
                    "start": 1444,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2261
                }
            ],
            "ref_mentions": [
                {
                    "start": 379,
                    "end": 382,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 392,
                    "end": 396,
                    "matchedPaperCorpusId": "204734128"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8193359375
        },
        {
            "corpus_id": "270285708",
            "title": "Benchmark Data Contamination of Large Language Models: A Survey",
            "text": "The rapid development of Large Language Models (LLMs) like GPT-4, Claude-3, and Gemini has transformed the field of natural language processing. However, it has also resulted in a significant issue known as Benchmark Data Contamination (BDC). This occurs when language models inadvertently incorporate evaluation benchmark information from their training data, leading to inaccurate or unreliable performance during the evaluation phase of the process. This paper reviews the complex challenge of BDC in LLM evaluation and explores alternative assessment methods to mitigate the risks associated with traditional benchmarks. The paper also examines challenges and future directions in mitigating BDC risks, highlighting the complexity of the issue and the need for innovative solutions to ensure the reliability of LLM evaluation in real-world applications.",
            "score": 0.5421479676303207,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8408203125
        },
        {
            "corpus_id": "271097946",
            "title": "A Taxonomy for Data Contamination in Large Language Models",
            "text": "Advancements in machine learning have traditionally relied on benchmark datasets to evaluate and compare model performance (Raji et al., 2021;Gururaja et al., 2023). With the surge of large language models (LLMs) in recent years, these benchmarks are now leveraged to showcase remarkable abilities across diverse tasks. \n\nHowever, the shelf life of benchmarks is incredibly low, with Roberts et al. (2023) demonstrating that newer models with updated training cutoff dates are iteratively rendering existing benchmarks stale. The presence of internet-sourced data in both pretraining and evaluation datasets increases the risk of data contamination (Brown et al., 2020;Magar and Schwartz, 2022) and challenges the notion of fair evaluation for models pretrained on massive corpora. Both GPT-3 and C4 training corpora were found to contain test data for several benchmarks (Dodge et al., 2021;Raffel et al., 2020;Brown et al., 2020), raising serious concerns about the validity of evaluation scores for many pretrained models (Lee et al., 2022;Chang et al., 2023b). \n\nThe research community lacks consensus on best practices for data contamination, and different works define contamination in subtly different ways. Without standardization of terminology, it is difficult to develop best practices for contamination-or even to characterize the problem at all. To address this gap, we suggest a formal definition of contamination and taxonomize subtypes of contamination ( \u00a7 2). We map prior work on both the detection and impact of contamination into this taxonomy, revealing several understudied forms of contamination ( \u00a7 2.3). We also measure the impact of different types of contamination on downstream summarization ( \u00a7 4) and QA ( \u00a7 5) performance through continued pretraining experiments assessing indirect/approximate test set contamination effects. \n\nOur findings reveal that for GPT-2 Large models, it is often the case that having in-domain data present during training is as beneficial as having the test data present during training. Moreover, we observe that certain contamination types exhibit task-dependent effects on evaluation performance, further complicating decontamination best practices.",
            "score": 0.5418199205531851,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 319
                },
                {
                    "start": 322,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 1064
                },
                {
                    "start": 1067,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1857
                },
                {
                    "start": 1860,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 164,
                    "matchedPaperCorpusId": "263834772"
                },
                {
                    "start": 872,
                    "end": 892,
                    "matchedPaperCorpusId": "237568724"
                },
                {
                    "start": 1025,
                    "end": 1043,
                    "matchedPaperCorpusId": "235829052"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57080078125
        },
        {
            "corpus_id": "276259405",
            "title": "Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon",
            "text": "Researchers have introduced various methods to detect or mitigate training contamination: Finding the N-gram overlap (e.g., 13-grams or 50-character matches) between training and test data (Brown et al., 2020;OpenAI, 2023), though it can miss semantically equivalent rephrasing. Embedding similarity search (Reimers, 2019) that uses transformerbased embeddings to identify semantically close training-test pairs (Lee et al., 2023). Decoding Matching probes the model by providing partial test prompts and measuring how likely it is to complete them exactly (Li, 2023) or completing missing words (Deng et al., 2023). A recent study pre-sented an overfit detection of editing knowledge to a LLM (Zhang et al., 2025). \n\nAlthough these studies have focused on detecting training data contamination or focusing on additional knowledge, they lack with addressing a critical issue: overfitting to benchmark-specific artifacts. In many cases, LLMs may never see the test data during training yet still learn to rely on superficial cues unique to a benchmark's canonical format. Existing techniques such as n-gram overlap and embedding similarity fail to capture this subtle form of overfitting. In contrast, our approach explicitly quantifies the dependence of a model's performance on the precise phrasing and structure of evaluation prompts, thereby filling this gap in current evaluation methodologies. By systematically applying a controllable distortion parameter to evaluation prompts, without requiring additional training or access to training data, our method shows how performance metrics degrade under textual perturbations, providing a robust means of diagnosing and mitigating broader overfitting behavior.",
            "score": 0.5389736478657841,
            "section_title": "Gap in Current Work",
            "char_start_offset": 5559,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1712
                }
            ],
            "ref_mentions": [
                {
                    "start": 694,
                    "end": 714,
                    "matchedPaperCorpusId": "273233716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77783203125
        },
        {
            "corpus_id": "277781442",
            "title": "Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish",
            "text": "Dataset Quality Dataset quality is assessed by different methods in the literature. Kreutzer et al. (2022) sampled 100 instances from each dataset, as in our study, to identify the data quality of multilingual web-crawled datasets. Their findings reveal that many datasets suffered from quality issues, primarily due to the nature of web crawling. \n\nThe GSM1k dataset (Zhang et al., 2024) evaluates the performance of language models on reasoning tasks. The dataset is kept private to prevent contamination. They conducted a three-stage annotation process that includes an initial review by experienced annotators, a secondary validation by independent annotators, and a final audit by a dedicated quality assurance team. \n\nContamination Data contamination in large language models has become an increasing concern. As models are trained on large-scale datasets scraped from the Internet, the integrity of benchmark datasets is challenging to maintain. Sainz et al. (2023) emphasize the critical need to assess whether a model's performance is due to its genuine reasoning capabilities or mere memorization. \n\nContamination is detected by matching test splits with training data. Dodge et al. (2021) employ exact match detection methods, normalizing text for capitalization and punctuation to identify instances of overlap. Brown et al. (2020), on the other hand, use n-gram overlap to measure contamination. \n\nThe MEGA benchmark (Ahuja et al., 2023) has a comprehensive case study on contamination by detecting potential training data leakage. They show that some of the benchmark datasets, which were translated into Turkish and analyzed in this study, exhibit data contamination. \n\nAnnotation Guideline Several studies have established guidelines for human evaluation to ensure consistency and reliability. Liang et al. (2023) emphasized the importance of structured annotation guidelines to provide a clear and replicable evaluation criteria. Liang et al. (2023) designed annotation guidelines to assess disinformation scenarios. To maintain annotation reliability, they implemented quality control measures including hidden \"secret words\" in instructions to verify comprehension and attention checks to detect careless responses.",
            "score": 0.5388982978442298,
            "section_title": "Related Work",
            "char_start_offset": 3070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 347
                },
                {
                    "start": 350,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 721
                },
                {
                    "start": 724,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1408
                },
                {
                    "start": 1411,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1682
                },
                {
                    "start": 1685,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2234
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64404296875
        },
        {
            "corpus_id": "265050721",
            "title": "Rethinking Benchmark and Contamination for Language Models with Rephrased Samples",
            "text": "Large language models are increasingly trained on all the data ever produced by humans. Many have raised concerns about the trustworthiness of public benchmarks due to potential contamination in pre-training or fine-tuning datasets. While most data decontamination efforts apply string matching (e.g., n-gram overlap) to remove benchmark data, we show that these methods are insufficient, and simple variations of test data (e.g., paraphrasing, translation) can easily bypass these decontamination measures. Furthermore, we demonstrate that if such variation of test data is not eliminated, a 13B model can easily overfit a test benchmark and achieve drastically high performance, on par with GPT-4. We validate such observations in widely used benchmarks such as MMLU, GSK8k, and HumanEval. To address this growing risk, we propose a stronger LLM-based decontamination method and apply it to widely used pre-training and fine-tuning datasets, revealing significant previously unknown test overlap. For example, in pre-training sets such as RedPajama-Data-1T and StarCoder-Data, we identified that 8-18\\% of the HumanEval benchmark overlaps. Interestingly, we also find such contamination in synthetic dataset generated by GPT-3.5/4, suggesting a potential risk of unintentional contamination. We urge the community to adopt stronger decontamination approaches when using public benchmarks. Moreover, we call for the community to actively develop fresh one-time exams to evaluate models accurately. Our decontamination tool is publicly available at https://github.com/lm-sys/llm-decontaminator.",
            "score": 0.5383244728323522,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84375
        },
        {
            "corpus_id": "267938621",
            "title": "Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models",
            "text": "Data contamination detection.The concept of data contamination for LLMs can be derived from the context of GPT-3 (Brown et al., 2020).Due to the vastness of the pre-training corpus of GPT-3, it inevitably overlapped with some evaluation benchmarks.Therefore, GPT-3 adopted 13-gram overlap detection to remove the data in the training set that conflicts with the test set of benchmarks.Some work (Pan et al., 2020;Zhou et al., 2023;Jacovi et al., 2023;Dodge et al., 2021) exposed the serious consequences of data contamination and urged attention to this problem.However, most currently released LLMs did not open their pre-training corpus, which poses a new challenge for data contamination detection.Recent work tried to detect contamination without access to the pretraining corpus (Oren et al., 2023;Deng et al., 2023;Golchin et al., 2023).Min-k% Prob (Shi et al., 2023) calculated the average of the k% smallest probabilities of generated tokens and considered it as contaminated if it exceeded a certain threshold.The work (Li, 2023) assumed that data leaked into the training set tends to exhibit lower perplexity and utilizes perplexity analysis for detection.However, they often require other model outputs (e.g.probability) in addition to text, presenting challenges in detecting closed-source LLMs like ChatGPT, and they ignore the potential implicit contamination from variants of test data.\n\nRecent investigations (Huang et al., 2023;Yang et al., 2023) have suggested that filtering training data based on n-grams may not effectively address the issue of data contamination, especially concerning semantically equivalent sentence rephrasing.To this end, LLM Decontaminator (Yang et al., 2023) detected the similarity of test data and training data based on other advanced LLMs.\n\nOur work requires only sampled texts to detect LLM's data contamination via output distribution and considers the potential implicit contamination.\n\nContamination Mitigation Evaluation.",
            "score": 0.5380654634409993,
            "section_title": "Related Work",
            "char_start_offset": 19195,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 29,
                    "end": 134
                },
                {
                    "start": 134,
                    "end": 248
                },
                {
                    "start": 248,
                    "end": 385
                },
                {
                    "start": 385,
                    "end": 562
                },
                {
                    "start": 562,
                    "end": 701
                },
                {
                    "start": 701,
                    "end": 843
                },
                {
                    "start": 843,
                    "end": 1019
                },
                {
                    "start": 1019,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1220
                },
                {
                    "start": 1220,
                    "end": 1402
                },
                {
                    "start": 1404,
                    "end": 1653
                },
                {
                    "start": 1653,
                    "end": 1789
                },
                {
                    "start": 1791,
                    "end": 1938
                },
                {
                    "start": 1940,
                    "end": 1976
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 133,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 395,
                    "end": 413,
                    "matchedPaperCorpusId": "220938739"
                },
                {
                    "start": 431,
                    "end": 451,
                    "matchedPaperCorpusId": "258741333"
                },
                {
                    "start": 451,
                    "end": 470,
                    "matchedPaperCorpusId": "237568724"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.869140625
        },
        {
            "corpus_id": "273163321",
            "title": "How much can we forget about Data Contamination?",
            "text": "Here, we discuss our data contamination approach in a bit more detail. \n\nIn this paper, we consider only exact contamination. This means we contaminate the training data exactly with the text the model is later evaluated on. In the literature, it has been shown that non-exact contamination (re-worded questions, translation into a different language) can affect benchmark performance, too. For example, Yang et al. [71] have shown that a 13B parameter Llama 2 Model (Touvron et al., 2023) can achieve an accuracy increase of over 20 percentage points after training on re-phrased benchmark questions. We decided against considering non-exact contamination for this paper because the models we train from scratch are much smaller than those for which non-exact contamination results have been shown. This means these models are less capable of making sense of related information, potentially leading us to underestimate the effect of non-exact contamination for realistic training runs. \n\nIn addition, we consider contamination with individual benchmark questions, inserted into the training data at random positions. We consider this setup because we are interested in contamination from the perspective of leakage, where individual benchmark questions may enter the training data via different documents (for example, as quotes in Wikipedia articles, a case described in Brown et al. [10]). This contrasts with the setup where a dataset is present in the training data as a long contiguous string, which we conjecture might have a similar impact but be easier detectable (Oren et al., 2024). The fact that we contaminate with benchmark questions also sets us apart from related works that study data contamination and memorization for random strings and uniquely identified objects (Carlini et al., Carlini et al., 2019, 2021). It is worth highlighting that the results between these two setups might differ, especially considering the time it takes to forget an example. \n\nWe only consider pre-training.",
            "score": 0.5372264558694473,
            "section_title": "A.1 Additional Discussion of Data Contamination Assumptions and Setting",
            "char_start_offset": 27223,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 73,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1974
                },
                {
                    "start": 1977,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 1387,
                    "end": 1391,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1785,
                    "end": 1822,
                    "matchedPaperCorpusId": "170076423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3076171875
        },
        {
            "corpus_id": "267365109",
            "title": "Evaluating Large Language Models for Generalization and Robustness via Data Compression",
            "text": "Modern evaluation methods for large language models typically use comprehensive benchmark suites that include a broad range of tasks and domains. This aims to assess the generalization ability of different models in varying scenar-ios and prevent overfitting on any specific task or dataset (Chang et al., 2023). For example, the GLUE benchmark (Wang et al., 2018;2019) consists of 10+ NLP tasks and is widely used in benchmarking large language models (Zhang et al., 2022;Chowdhery et al., 2023). More recently, GPT-4 was tested on 30+ human academic tests and 10+ NLP benchmarks (Achiam et al., 2023). \n\nHowever, this methodology suffers from three critical problems. Firstly, benchmark results depend heavily on prompts (Sclar et al., 2023). Different prompts for the same model can produce wildly different results. This makes it hard to evaluate the models directly, without the evaluation being inflated by clever prompt engineering. For example, Google Gemini outperforms baseline models with customized prompts, but fails to beat other models with simple few-shot prompts (Anil et al., 2023), making it difficult to ascertain whether the good performance is the result of model abilities or well-designed prompts. Secondly, the increasing scale of pre-training data raises the possibility of including benchmark data in the pre-training stage of large language models, leading to the data contamination issue (Jacovi et al., 2023;Sainz et al., 2023). Data contamination allows models to achieve higher metrics through \"memorization\", rather than demonstrating true generalization. This undermines the reliability of model comparisons and can also mislead model development. Studies estimate that 30-80% of examples in popular benchmarks like MMLU (Hendrycks et al., 2020) and SQuAD (Rajpurkar et al., 2018) are contaminated, with the ratio increasing rapidly over time (Li et al., 2023c). Finally, due to the human efforts required, constructing new and diverse benchmarks is very expensive. This prevents regular updates of existing benchmarks and further diversification to cover more domains and tasks.",
            "score": 0.5370016935010755,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 603
                },
                {
                    "start": 606,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 291,
                    "end": 311,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 473,
                    "end": 496,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5498046875
        },
        {
            "corpus_id": "270217411",
            "title": "Probing Language Models for Pre-training Data Detection",
            "text": "To support the development of further work on detecting pretraining data contamination, Oren et al. (2023) pre-trained a 1.4 billion parameter GPT-2 model (Radford et al., 2019), Contam-1.4b,with intentional downstream task datasets contamination 3 .We evaluate various detection methods on PubMedQA (Jin et al., 2019) and Common-senseQA (Talmor et al., 2019)  difficult (Oren et al., 2023).\n\nExperimental Setup.We sampled 1000 examples from the contaminated training data as member data for each task and then sampled 1000 examples from their standard dataset as non-member data.Similar to subsection 5.1, we split each dataset into a validation set and a test set.The validation set will be used to select the best hyperparameters, and the test set for evaluation.For our method, we collected 200 synthetic training data for each task.For comparing to smaller model baseline setting, we choose Contam-Small (124M Params) pre-trained on the same dataset for Contam-1.4b.\n\nResults.The results are shown in Table 4.We observe that our method outperforms other baselines, which demonstrate the effectiveness of our method.Nonetheless, we acknowledge that the overall detection efficacy is unsatisfactory at an extremely low duplication count (1 and 2), corroborating the findings of Oren et al. (2023).\n\nTrain",
            "score": 0.5363789541679107,
            "section_title": "Downstream Task Datasets Contamination Detection Challenge",
            "char_start_offset": 19189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 191,
                    "end": 250
                },
                {
                    "start": 250,
                    "end": 391
                },
                {
                    "start": 393,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 580
                },
                {
                    "start": 580,
                    "end": 666
                },
                {
                    "start": 666,
                    "end": 766
                },
                {
                    "start": 766,
                    "end": 837
                },
                {
                    "start": 837,
                    "end": 971
                },
                {
                    "start": 973,
                    "end": 981
                },
                {
                    "start": 981,
                    "end": 1014
                },
                {
                    "start": 1014,
                    "end": 1120
                },
                {
                    "start": 1120,
                    "end": 1300
                },
                {
                    "start": 1302,
                    "end": 1307
                }
            ],
            "ref_mentions": [
                {
                    "start": 300,
                    "end": 318,
                    "matchedPaperCorpusId": "202572622"
                },
                {
                    "start": 338,
                    "end": 359,
                    "matchedPaperCorpusId": "53296520"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80908203125
        },
        {
            "corpus_id": "269009574",
            "title": "Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models",
            "text": "Large Language Models (LLMs) exhibit remarkable performance on a diverse set of tasks (Wei et al., 2022;Bubeck et al., 2023;Liang et al., 2023). While their prowess in natural language is undeniable, performance in other applications remains an ongoing research topic (Dziri et al., 2023a;Nori et al., 2023). Recently, LLMs have increasingly been employed for diverse data modalities. This includes an increasing amount of creative applications in domains such as structured learning, tabular data, and time-series forecasting (Yin et al., 2023;Chen et al., 2023;Hegselmann et al., 2023;Jin et al., 2024). \n\nA main question in current research on LLMs is the degree to which these models are able to extrapolate to novel tasks that are unlike what they have seen during training (Wu et al., 2023). As such, an important aspect of LLM evaluation is to know if a task has been part of the model's training set. In this work, we refer to the case where an LLM is evaluated on a dataset seen during (pre-)training as training data contamination (Magar & Schwartz, 2022;Jiang et al., 2024). Unfortunately, detecting whether an LLM has seen a certain text or dataset during training is rather challenging (Duan et al., 2024). This is especially true for models to which we might only have query access via an API.",
            "score": 0.5361929309194433,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 605
                },
                {
                    "start": 608,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1307
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 143,
                    "matchedPaperCorpusId": "253553585"
                },
                {
                    "start": 563,
                    "end": 587,
                    "matchedPaperCorpusId": "252992811"
                },
                {
                    "start": 779,
                    "end": 796,
                    "matchedPaperCorpusId": "253553585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51416015625
        },
        {
            "corpus_id": "270688744",
            "title": "Unveiling the Spectrum of Data Contamination in Language Models: A Survey from Detection to Remediation",
            "text": "Currently, there is an increasing focus on developing novel methods for detecting data contamination, which is crucial for investigating and understanding data contamination scenarios.Effective detection tools can also help prevent intentional data contamination to a certain extent.However, there remains a significant need for research focused on mitigating data contamination.The research question arises: how can we create a dynamic evaluation method that uses potentially contaminated benchmarks to provide clean evaluations?In recent developments, many have started leveraging language models as agents to perform various tasks.An intriguing future direction could be to utilize LLMs as 'Benchmark Agents' to offer various forms of evaluation that convey the same meaning.\n\nHow to Create Benchmarks without Data Contamination To address the challenge of creating a benchmark free from data contamination, it is essential to consider innovative approaches.Firstly, an effective strategy involves constructing a dataset significantly larger than the target size, which can be in future refined.The excessive size allows for the application of rigorous data contamination checks to refine the dataset down to the initial target size.Additionally, the implementation of a unified, reliable, and dynamic evaluation framework is crucial.Such a framework offers the flexibility to adaptively assess benchmarks across various formats, enhancing the robustness of the evaluation process.Beyond these broader strategies, a practical yet profound method involves generating content that is rare or virtually nonexistent on the Internet or other public domains.\n\nRethinking Evaluation Paradigms As the scale of models and training datasets expands, it's conceivable that in future, the majority of tasks of interest or practical relevance might already fall within the distribution covered during training of LLMs.If in that hypothetical world, future models can execute all tasks that we care about with high accuracy, the relevance of whether these exact tasks were encountered during training diminishes.In such a scenario, the traditional emphasis on generalization-distinguishing between training and testing instances-might not be as critical.This raises important considerations about the trade-off between the usefulness of tasks that are well-represented in the data and the ability to generalize to entirely new scenarios.Consequently, this could necessitate a reevaluation of standard machine learning evaluation paradigms.",
            "score": 0.5352050017183918,
            "section_title": "Challenges for Detecting Black-Box Models",
            "char_start_offset": 31895,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 283
                },
                {
                    "start": 283,
                    "end": 379
                },
                {
                    "start": 379,
                    "end": 530
                },
                {
                    "start": 530,
                    "end": 634
                },
                {
                    "start": 634,
                    "end": 778
                },
                {
                    "start": 780,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1098
                },
                {
                    "start": 1098,
                    "end": 1236
                },
                {
                    "start": 1236,
                    "end": 1337
                },
                {
                    "start": 1337,
                    "end": 1484
                },
                {
                    "start": 1484,
                    "end": 1655
                },
                {
                    "start": 1657,
                    "end": 1908
                },
                {
                    "start": 1908,
                    "end": 2101
                },
                {
                    "start": 2101,
                    "end": 2243
                },
                {
                    "start": 2243,
                    "end": 2426
                },
                {
                    "start": 2426,
                    "end": 2528
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71923828125
        },
        {
            "corpus_id": "265220695",
            "title": "Investigating Data Contamination in Modern Benchmarks for Large Language Models",
            "text": "We introduce two approaches for investigating data contamination in several widely-used contemporary evaluation benchmarks. First, we develop an information retrieval system to identify benchmarks with significant overlap with the pre-training corpus. Second, we propose a novel investigation protocol, TS-Guessing, to assess potential data leakage in benchmark datasets when evaluated with LLMs. Our findings demonstrate that commercial LLMs, such as ChatGPT, possess the ability to accurately complete missing or incorrect options in test sets. Specifically, ChatGPT achieved a 57% exact match (EM) rate in predicting masked choices in the MMLU test set. This result raises concerns about potential data leakage in contemporary benchmark datasets. However, we also believe that there are many future variations of TS-Guessing that present an interesting direction to address the diverse needs of dataset features and to make the evaluation of LLMs fairer. We can obscure specific information segments to prompt language models into inferring the missing content on the benchmark, or even render it quantifiable akin to perplexity. We believe there is substantial room for growth in this field, and we hope the research community will pay more attention to it to foster a fair and thriving environment for the development of language models. \n\nThe retrieval system currently employs only the BM25 index, which may impact our ability to precisely retrieve data. Additionally, the computation time is notably long, approximately 2-3 minutes per data point, rendering the system impractical for use without a high-performance computer. Moreover, aside from human evaluation, the practice of using text generation scores to track contaminated data, as seen in GPT-3 (Brown et al., 2020) and other LLMs, remains a superficial method for accurately identifying true contamination. Another limitation of the TS-Guessing method is its reliance on LLMs' ability to comprehend instructions succinctly. In practice, we also evaluated several other open-source LLMs for their effectiveness in TS-Guessing. Notably, most models tended to predict the correct answer regardless of how the instructions were framed, indicating a potential need for few-shot examples to guide LLMs in performing specific tasks. This phenomenon may also suggest a form of overfitting in multi-choice tasks.",
            "score": 0.5352004534973469,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 28777,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1342
                },
                {
                    "start": 1345,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2294
                },
                {
                    "start": 2295,
                    "end": 2372
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8369140625
        },
        {
            "corpus_id": "266359062",
            "title": "An Open-Source Data Contamination Report for Large Language Models",
            "text": "Several techniques have been proposed to mitigate data contamination issue in language model evaluation previously. Our findings provide some novel insights on the effectiveness of these approaches. Blocklisting benchmark sources. Blocking sources of benchmarks in training data collection is a common way to avoid data contamination. In our paper, we further demonstrate the feasibility of this method. As shown in Figure 3, the distribution of data contamination is very centralised, so blocking only a small set of domains can significantly alleviate the issue of data contamination. However, we also find blocklisted links quickly expire but content spreads, making the blocklist ineffective over time. For instance, we test the contamination blocklist in the first release of MMLU2 , and we found the given blocklist only avoids 1.5% of contaminated cases we detected in \u00a75. If we adopt a more aggressive method that skips all domains in the blocklist, it still just avoids 21% of contaminated cases. This suggests content used in MMLU spreads rapidly, which emphasises the necessity to update the blocklists regularly. \n\nAvoid using data that appears with its solution on the internet (Jacovi et al., 2023). According to our results, avoiding the presence of answer can indeed prevent memorising exact answers. As shown in Table 4, we found models perform generally worse on input-only contamination compare to input-and-label contamination, or sometimes even worse than the clean set. This suggest that preventing input-and-label contamination is the key to mitigate data contamination issue. However, our analysis also identify several cases where inputonly contamination provides unfair advantages. For example, most models obtain higher accuracy on the input-only contaminated subset of ARC. As a result, completely avoiding using online resources is the best practice in benchmark construction. Winogrande is a role model in avoiding contamination by using only human-authored content. \n\nProtecting test data from automatic crawlers via encryption and forbidding further distribution (Jacovi et al., 2023). Forbidding further distribution of benchmarks can indeed prevent data contamination to some extent. This was proven in our Figure 3, where some contaminated cases are from huggingface.co, a dataset sharing platform. However, forbidding further distribution of the test data also significantly limits the popularity of benchmarks.",
            "score": 0.5350381993794715,
            "section_title": "Existing Methods to Mitigate Data Contamination",
            "char_start_offset": 20094,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1124
                },
                {
                    "start": 1127,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 1996
                },
                {
                    "start": 1999,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2217
                },
                {
                    "start": 2218,
                    "end": 2333
                },
                {
                    "start": 2334,
                    "end": 2447
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69873046875
        },
        {
            "corpus_id": "270688744",
            "title": "Unveiling the Spectrum of Data Contamination in Language Models: A Survey from Detection to Remediation",
            "text": "Perturbation-based methods involve using various techniques to artificially modify or alter test set samples.This is done to assess if LLMs are overfitting to particular benchmark formats or examples.The objective of this task is to examine whether there is a significant drop or change in performance after applying specific perturbations.\n\nRephrasing Test Set Yang et al. ( 2023) demonstrate that applying minor alterations to test data, such as rephrasing or translating, can bypass previous n-gram-based detection methods ( \u00a74.1.1).They reveal that if test data variability isn't eliminated, a 13B model can mimic the performance of state-of-the-art models like GPT-4 by overfitting to benchmarks, as evidenced by their experiments with notable datasets including MMLU (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), and HumanEval (Chen et al., 2021).To address this growing issue, they propose a new LLM-based detection approach using embedding similarity search to identify the top-k training samples most similar to each test sample.It then uses a high-quality language model (like GPT-4) to evaluate whether any of those top-k samples are semantically too close to the test sample.In a recent paper, Dekoninck et al. (2024b) proposed ConStat, a novel method for detecting and quantifying contamination in LLMs.The authors redefine contamination from a performance-based perspective, considering it as an artificially inflated benchmark performance that fails to generalize to real-world tasks.ConStat employs a statistical test that compares a model's performance on the original benchmark to its performance on carefully selected reference benchmarks while accounting for differences in difficulty using a set of uncontaminated reference models.\n\nCreating Reference Set In addition to directly rephrasing test set examples, Wei et al. (2023) use GPT-4 to create a reference set resembling the test set.They then calculate the difference between the reference set and the test set to assess the contamination issues potentially caused by intentional data contamination.Higher differences indicate a greater potential for data leakage.",
            "score": 0.5348950023107103,
            "section_title": "Perturbation-based",
            "char_start_offset": 19190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 109,
                    "end": 200
                },
                {
                    "start": 200,
                    "end": 340
                },
                {
                    "start": 342,
                    "end": 536
                },
                {
                    "start": 536,
                    "end": 861
                },
                {
                    "start": 861,
                    "end": 1046
                },
                {
                    "start": 1046,
                    "end": 1195
                },
                {
                    "start": 1195,
                    "end": 1324
                },
                {
                    "start": 1324,
                    "end": 1507
                },
                {
                    "start": 1507,
                    "end": 1760
                },
                {
                    "start": 1762,
                    "end": 1917
                },
                {
                    "start": 1917,
                    "end": 2083
                },
                {
                    "start": 2083,
                    "end": 2148
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8623046875
        },
        {
            "corpus_id": "267897557",
            "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models",
            "text": "In this subsection, we show that existing static dataset-based and LLM-based evaluation approaches are prone to data contamination while KIEval is resilient to data contamination. Additionally, we test existing contamination detection methods and point out their challenges. \n\nContamination on static dataset-based evaluation. We train two models on the test sets to introduce contamination in the pre-training ('PT-Cheater') and supervised fine-tuning ('SFT-  (Clark et al., 2018) dataset. 'SFT-Cheater' and 'PT-Cheater' denote leaking test-set labels during supervised fine-tuning phase and pre-training phase. We report 5-shot accuracy on ARC-Challenge dataset and KIEval scores. We detect data contamination with differences in average language modelling loss (Wei et al., 2023) and Min-K% Prob (Shi et al., 2023 Contamination on LLM-based evaluation. We also find existing LLM-based evaluations vul- 4 Training details including hyperparameters and hardware settings can be found in Appendix F. We also release the full training scripts on our GitHub repository for better reproducibility. \n\nnerable to data contamination, due to their reliance on static templates. We train the fine-tuned model ('Normal') with MT-Bench input templates and GPT-4 outputs using only 80 samples and test it against MT-Bench and KIEval. Table 4 reveals that contamination training notably inflates the MT-Bench score by 1.79, a surge over 45% compared to the baseline, while ARC-Challenge accuracy remains stable and KIEval score slightly decreased. \n\nChallenges in Contamination Detection. We evaluate the efficacy of current data contamination detection strategies, notably Skywork (Wei et al., 2023) and Min-K% Prob (Shi et al., 2023), which identify training data leakage through loss metrics as introduced in Related Work. We sampled 200 instances each from the trainset and testset of ARC-Challenge with contamination labels and tried to classify each instance with Min-K% Prob. We report AUC to measure its effectiveness.",
            "score": 0.5343865931664628,
            "section_title": "Resilience to Data Contamination",
            "char_start_offset": 18690,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 274
                },
                {
                    "start": 277,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1094
                },
                {
                    "start": 1097,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1535
                },
                {
                    "start": 1538,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2014
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87890625
        },
        {
            "corpus_id": "264490730",
            "title": "Proving Test Set Contamination in Black Box Language Models",
            "text": "We test for these differences in log probabilities, and aggregate them across the dataset to provide false positive rate guarantees. \n\n2019; Mattern et al., 2023) as well as provide some evidence for test set contamination (Sainz et al., 2023;Golchin & Surdeanu, 2023). However, the heuristic nature of these methods limits their usefulness, as these methods cannot elevate speculation about a suspected instance of test set contamination into an irrefutable proof of contamination. \n\nIn this work, we show it is possible to go beyond heuristics and provide provable guarantees of test set contamination in black box language models. More specifically, we provide a statistical test that can identify the presence of a benchmark in the pre-training dataset of a language model with provable false positive rate guarantees and without access to the model's training data or weights. \n\nTo achieve these guarantees, we exploit the fact that many datasets have a property known as exchangeability, where the order of examples in the dataset can be shuffled without affecting its joint distribution. Our key insight is that if a language model shows a preference for any particular ordering of the dataset -such as a canonical ordering that appears in publicly available repositoriesthis violates exchangeability and can only occur by observing the dataset during training (Figure 1). We leverage this insight to propose a set of tests that compares the language model's log probability on the 'canonical' ordering (taken from public repositories) to the log probability on a dataset with shuffled examples, and flag a dataset if the two log probabilities have statistically significant differences. \n\nUsing these ideas, we propose a computationally efficient and statistically powerful test for contamination which shards the dataset into smaller segments and performs a series of log probability comparisons within each shard. We prove that this sharded test provides control over the false positive rate, enables computationally efficient parallel tests, and substantially improves the power of the test for small p-values. \n\nWe evaluate our statistical test on a 1.4 billion parameter language model trained on a combination of Wikipedia and a curated set of canary test sets.",
            "score": 0.533460344047913,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2167,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 135,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 482
                },
                {
                    "start": 485,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 881
                },
                {
                    "start": 884,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1694
                },
                {
                    "start": 1697,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2121
                },
                {
                    "start": 2124,
                    "end": 2275
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 162,
                    "matchedPaperCorpusId": "258967264"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9462890625
        },
        {
            "corpus_id": "266573740",
            "title": "Task Contamination: Language Models May Not Be Few-Shot Anymore",
            "text": "The investigation into potential data contamination in large language models (LLMs) has recently been gaining attention in the research community. Brown et al. (2020), in their work with GPT-3, presented an in-depth analysis of data contamination. Although they acknowledged the presence of a bug that led to data contamination in multiple datasets, their position was that it did not affect the overall performance of the model. Intriguingly, they noted that contaminated datasets outperformed the uncontaminated ones which, in a way, contradicted their original assertion. Magar and Schwartz (2022) extracted training data from GPT-2 and indicated potential leaks of private data in the pre-trained language model. Chang et al. ( 2023) discovered that OpenAI models were memorizing substantial amounts of copyrighted materials, which increased concern over data contamination. Aiyappa et al. (2023) highlighted the severity and scope of data contamination problems for ChatGPT evaluations. Highlighting the need for strategic interventions to address these issues, Jacovi et al. (2023) proposed several strategies for circumventing testing data contamination. Additional work has further looked into test data contamination (Sainz et al. 2023b;Zhou et al. 2023;Golchin and Surdeanu 2023;Sainz et al. 2023a;Deng et al. 2023;Oren et al. 2023;Li 2023). \n\nThe previous work listed above has investigated test data contamination, but has not considered task contamination for zero-shot or few-shot settings. Prior work has noticed our proposed task contamination problem for zero-shot or fewshot learning (Blevins, Gonen, and Zettlemoyer 2023;Briakou, Cherry, and Foster 2023), but did not systematically analyze it. Our work seeks to add to the existing knowledge by providing an exhaustive evaluation of task contamination for few-shot or zero-shot learning scenarios.",
            "score": 0.5329602959776762,
            "section_title": "Related Work",
            "char_start_offset": 18940,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1351
                },
                {
                    "start": 1354,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1867
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 166,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 575,
                    "end": 600,
                    "matchedPaperCorpusId": "247475929"
                },
                {
                    "start": 1325,
                    "end": 1342,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1602,
                    "end": 1640,
                    "matchedPaperCorpusId": "253523544"
                },
                {
                    "start": 1640,
                    "end": 1673,
                    "matchedPaperCorpusId": "258740723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7294921875
        },
        {
            "corpus_id": "271571641",
            "title": "Data Contamination Report from the 2024 CONDA Shared Task",
            "text": "The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant aspects of data contamination in natural language processing, where data contamination is understood as situations where evaluation data is included in pre-training corpora used to train large scale models, compromising evaluation results. The workshop fostered a shared task to collect evidence on data contamination in current available datasets and models. The goal of the shared task and associated database is to assist the community in understanding the extent of the problem and to assist researchers in avoiding reporting evaluation results on known contaminated resources. The shared task provides a structured, centralized public database for the collection of contamination evidence, open to contributions from the community via GitHub pool requests. This first compilation paper is based on 566 reported entries over 91 contaminated sources from a total of 23 contributors. The details of the individual contamination events are available in the platform. The platform continues to be online, open to contributions from the community.",
            "score": 0.5318605831537638,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5849609375
        },
        {
            "corpus_id": "270620798",
            "title": "Data Contamination Can Cross Language Barriers",
            "text": "As shown in Figure 2, both models' performances on the original benchmarks are drastically improved after injecting cross-lingual contamination. Meanwhile, we employ state-of-the-art detection methods based on model completion (Oren et al., 2023;Xu et al., 2024) and LLM judgment (Golchin and Surdeanu, 2023) to test them for contamination. Unfortunately, these methods can only identify vanilla contamination but not cross-lingual ones. \n\nTo unmask such deep contamination, we first examine existing detection methods to identify the limitations and then propose solutions. Current methods are predominantly based on text overlap, either checking for string matching between pretraining and evaluation data (Deng et al., 2023;Li, 2023b;OpenAI, 2023;Touvron et al., 2023;Riddell et al., 2024), or comparing the models' output text or likelihood with the evaluation data given controlled prompts (Oren et al., 2023;Xu et al., 2024). The key idea of such methods is to verify if the model has seen or memorized a specific surface form of text, which we believe is too superficial to reflect the essence of contamination. \n\nInstead, we argue that contamination detection should focus on the model's ability to generalize to unseen data, rather than on testing if it has memorized certain text. For instance, in the crosslingual scenario, the model did not memorize the specific English form of the benchmarks, but can still obtain non-generalizable memorization of corresponding knowledge from contamination in other languages. In this case, if we still scrutinize for any memorization of the English benchmarks, the detection results will be unreliable. Therefore, we propose generalization-based detection approaches that examine the model's performance change on a generalized version of the original benchmark, created by modifying the questions and answer choices. Specifically, for each question, we replace all the incorrect choices with correct choices taken from other questions. Through this manipulation, models that really understand the question should achieve better performance, as some choices can be not even wrong to the question, while the contaminated ones can get confused as all choices are memorized as correct.",
            "score": 0.5310326845567587,
            "section_title": "Introduction",
            "char_start_offset": 1900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1118
                },
                {
                    "start": 1121,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2231
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 246,
                    "matchedPaperCorpusId": "264490730"
                },
                {
                    "start": 280,
                    "end": 308,
                    "matchedPaperCorpusId": "260925501"
                },
                {
                    "start": 895,
                    "end": 914,
                    "matchedPaperCorpusId": "264490730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.916015625
        },
        {
            "corpus_id": "264490730",
            "title": "Proving Test Set Contamination in Black Box Language Models",
            "text": "We evaluate our statistical test on a 1.4 billion parameter language model trained on a combination of Wikipedia and a curated set of canary test sets. Our test is sensitive enough to identify test sets with as few as 1000 examples, and sometimes even appearing only twice in the pretraining corpus. In the case of higher duplication counts, such as datasets appearing 10 or more times, we obtain vanishingly small p-values on our test. Finally, we run our test on four commonly used, public language models to study the behavior of our test on language models in the wild and find little evidence of pervasive and strong test set contamination. \n\nWe summarize our contributions below. \n\n\u2022 Demonstrating the use of exchangability as a way to provably identify test set contamination using only log probability queries. \n\n\u2022 Construction of an efficient and powerful sharded hypothesis test for test set contamination. \n\n\u2022 Empirical demonstration of black-box detection of contamination for small datasets that appear few times during pretraining. \n\nOur three contributions suggest that black-box identification of test set contamination is practical and further improvements in the power of the tests may allow us to regularly audit language models in the wild for test set contamination. To encourage the development of new provable guarantees for test set contamination, we release our pretrained models as a benchmark for developing future statistical tests.1 .",
            "score": 0.5310019991518748,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4291,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 818
                },
                {
                    "start": 821,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 1045
                },
                {
                    "start": 1048,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1463
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95849609375
        },
        {
            "corpus_id": "268819579",
            "title": "A Comprehensive Survey of Contamination Detection Methods in Large Language Models",
            "text": "Jiang et al. (2024b) delve deep into the impact of evaluation data leakage within the pre-training set by running a series of controlled pre-training experiments with GPT-2 (Radford et al., 2019). They pre-train from scratch GPT-2 in three different ways: with the original pre-training set, with additional input texts from four evaluation datasets (SST-2, MMLU (Hendrycks et al., 2020), CNN/DM (Hermann et al., 2015) and SQuAD-v1 (Rajpurkar et al., 2016)), and with input texts and labels from these same evaluation datasets. Contaminated pre-training clearly boosts performance on all downstream tasks. This was also shown by the work of Palavalli et al. (2024) in their Verbatim setup when pre-training GPT-2. Findings also reveal a U-shaped relationship between the number of times that contaminated data points are included in the pre-training, and model performance. Authors also show that the n-gram contamination detection method from Llama-2 Touvron et al. (2023b) is not effective at detecting contaminated data points, calling for more robust methods.",
            "score": 0.5310019991518748,
            "section_title": "Controlled Training Setup",
            "char_start_offset": 18723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1063
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 195,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 396,
                    "end": 418,
                    "matchedPaperCorpusId": "6203757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9091796875
        },
        {
            "corpus_id": "264451585",
            "title": "Detecting Pretraining Data from Large Language Models",
            "text": "As the scale of language model (LM) training corpora has grown, model developers (e.g, GPT-4 (Brown et al., 2020a) and LLaMA 2 (Touvron et al., 2023b)) have become reluctant to disclose the full composition or sources of their data. This lack of transparency poses critical challenges to scientific model evaluation and ethical deployment. Critical private information may be exposed during pretraining; previous work showed that LLMs generated excerpts from copyrighted books (Chang et al., 2023) and personal emails (Mozes et al., 2023), potentially infringing upon the legal rights of original content creators and violating their privacy. Additionally, Sainz et al. (2023); Magar & Schwartz (2022); Narayanan (2023) showed that the pretraining corpus may inadvertently include benchmark evaluation data, making it difficult to assess the effectiveness of these models. \n\nIn this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM with no knowledge of its pretraining data, can we determine if the model was pretrained on the text? We present a benchmark, WIKIMIA, and an approach, MIN-K% PROB, for pretraining data detection. This problem is an instance of Membership Inference Attacks (MIAs), which was initially proposed by Shokri et al. (2016). Recent work has studied fine-tuning data detection (Song & Shmatikov, 2019;Shejwalkar et al., 2021;Mahloujifar et al., 2021) as an MIA problem. However, adopting these methods to detect the pertaining data of contemporary large LLMs presents two unique technical challenges: First, unlike fine-tuning which usually runs for multiple epochs, pretraining uses a much larger dataset but exposes each instance only once, significantly Figure 1: Overview of MIN-K% PROB. To determine whether a text X is in the pretraining data of a LLM such as GPT, MIN-K% PROB first gets the probability for each token in X, selects the k% tokens with minimum probabilities and calculates their average log likelihood.",
            "score": 0.5303737253086556,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 1288,
                    "end": 1308,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1361,
                    "end": 1385,
                    "matchedPaperCorpusId": "159041003"
                },
                {
                    "start": 1385,
                    "end": 1409,
                    "matchedPaperCorpusId": "245222525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93359375
        },
        {
            "corpus_id": "267897557",
            "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models",
            "text": "The landscape of artificial intelligence has been significantly reshaped by the emergence of Large Language Models (LLMs) as they have been pivotal in various natural language understanding and generation tasks (Brown et al., 2020;OpenAI, 2023;Bubeck et al., 2023). To better understand the capabilities and weaknesses of LLMs, their effective evaluation becomes increasingly essential (Chang et al., 2023;Guo et al., 2023). \n\nAutomatic evaluation methods of LLMs generally fall into two categories: static dataset-based and LLM-based evaluation (Chang et al., 2023). The former (Clark et al., 2018;Zellers et al., 2019;Hendrycks et al., 2020;Huang et al., 2023) requires evaluated LLMs to generate a short span of text containing choices or answers for predefined questions (Gao et al., 2021) to challenge their knowledge. The latter (Chiang and Lee, 2023), also known as LLM-as-a-judge, typically depends on LLM evaluators to evaluate the model's outputs given predetermined questions or instructions (Zheng et al., 2023;Lin and Chen, 2023;Fu et al., 2023;Wang et al., 2023c). Despite these promising efforts, current evaluation methodologies still broadly face the bottleneck of data contamination (Schaeffer, 2023;Wei et al., 2023;Oren et al., 2023;Sainz et al., 2023;Daniele and Suphavadeeprasit, 2023), where models trained on test splits of datasets can artificially inflate benchmark performance, overestimating their real-world efficacy and even potentially misleading scientific conclusions (Zhou et al., 2023). \n\nRecently, two primary strategies have been employed to mitigate data contamination of LLMs. The first involves identifying whether specific texts or test samples exist in the training dataset by assessing loss values (Wei et al., 2023;Shi et al., 2023) or probing datasets like Common Craw (Li, 2023). The limitation lies in its capacity to only measure contamination levels rather than actual model performance.",
            "score": 0.5303450624005044,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 424
                },
                {
                    "start": 427,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1521
                },
                {
                    "start": 1524,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 211,
                    "end": 231,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6875
        },
        {
            "corpus_id": "273350935",
            "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
            "text": "The impressive performance of large language models (LLMs) arises from large-scale pretraining on massive datasets collected from the internet (Achiam et al., 2023;Touvron et al., 2023b). But, model developers are often reluctant to disclose detailed information about the pretraining datasets, raising significant concerns regarding fair evaluation and ethical risks. Specifically, Recent studies reveal that the pretraining corpus may inadvertently include data from evaluation benchmarks (Sainz et al., 2023;Balloccu et al., 2024), making it difficult to assess the practical capability of LLMs. Besides, LLMs often generate text from copyrighted books (Grynbaum & Mac, 2023) and personal emails (Mozes et al., 2023), which could infringe on the legal rights of the original content creators and violate their privacy. Considering the vast size of the pretraining dataset and the single iteration of pretraining, it has been increasingly important and challenging to detect pretraining data, which determines whether a piece of text is part of the pretraining dataset. \n\nIn the literature, current works of detecting pretraining data primarily focus on designing scoring functions to differentiate members (i.e., seen data during pretraining) and non-members (unseen). For example, previous work shows that sequences from the training data tend to have lower perplexity (i.e., higher likelihood) than non-members (Li, 2023). Min-k% leverages the k% of tokens with minimum token probabilities of a text for detection, assuming that trained data tends to contain fewer outlier tokens (Shi et al., 2024). However, non-member data can obtain low perplexities by including frequent or repetitive texts, while members may contain rare tokens that result in high perplexities. This casts significant doubt on utilizing those scoring functions for detecting pretraining data. Consequently, this issue prompts us to present a preliminary attempt to enlarge the difference between members and non-members for pretraining data detection.",
            "score": 0.5277021975688312,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 1071
                },
                {
                    "start": 1074,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2029
                }
            ],
            "ref_mentions": [
                {
                    "start": 491,
                    "end": 511,
                    "matchedPaperCorpusId": "264555419"
                },
                {
                    "start": 511,
                    "end": 533,
                    "matchedPaperCorpusId": "267499939"
                },
                {
                    "start": 1585,
                    "end": 1603,
                    "matchedPaperCorpusId": "264451585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8427734375
        },
        {
            "corpus_id": "268297237",
            "title": "Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models",
            "text": "While large language models have achieved remarkable performance on various code generation benchmarks, there have been growing concerns regarding potential contamination of these benchmarks as they may be leaked into pretraining and finetuning data. While recent work has investigated contamination in natural language generation and understanding tasks, there has been less extensive research into how data contamination impacts the evaluation of code generation, which is critical for understanding the robustness and reliability of LLMs in programming contexts. In this work, we perform a comprehensive study of data contamination of popular code generation benchmarks, and precisely quantify their overlap with pretraining corpus through both surface-level and semantic-level matching. In our experiments, we show that there are substantial overlap between popular code generation benchmarks and open training corpus, and models perform significantly better on the subset of the benchmarks where similar solutions are seen during training. We also conduct extensive analysis on the factors that affects model memorization and generalization, such as model size, problem difficulty, and question length. We release all resulting files from our matching pipeline for future research.",
            "score": 0.5273851079454288,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87890625
        },
        {
            "corpus_id": "266933004",
            "title": "Investigating Data Contamination for Pre-training Language Models",
            "text": "In this section, we conduct experiments to clean the pre-training corpus based on the outlined n-gram and Llama 2 definitions. Specifically, the investigation aims to understand how the contaminated documents under these definitions would affect the performance if we filter them out of the pretraining corpus. As described in \u00a72, we adopt different n-gram values n for the direct n-gram overlap and Llama 2 contamination definitions, and we try various threshold \u03bb for the contamination percentage under Llama 2's definition. These definitions are then used to filter \"contaminated\" \n\nFigure 2: Evaluation results on removing contamination from the pre-training corpus. We deliberately select the parameters to achieve different ratios of removed tokens. The x-axis denotes the cleaning method (n-gram or Llama 2) followed by the percentage of tokens removed. \n\ndocuments out of the pre-training corpus, where a document is considered contaminated if any sentence in this document is considered contaminated. The detailed results are listed in Figure 2. \n\nIn our experimental setup, we systematically filter out a range of approximately 3% to over 20% of tokens labeled as \"contaminated\" from the pre-training corpus, aiming to analyze the effects of the percentage of tokens removed on the model performance. The results, however, do not show a uniform pattern across different proportions of token removal. Interestingly, in certain instances where token removal exceeded 30%, the model's performance remained comparable to that of the original model. This finding raises questions about the accuracy of n-gram-based definitions for pinpointing effective contamination. It appears that documents excluded based on n-gram and Llama 2's definitions are not always genuinely contaminated, which reveals the insufficiency of such definitions for identifying effective contamination in practice. \n\nWe did not include PaLM's definition in our experiments since we found this definition is so strict compared to the other two definitions that very few documents would be filtered out. More analyses of the definitions are provided in Appendix D, where we also extensively analyze the effects of varying the parameters of these definitions.",
            "score": 0.5270026331346136,
            "section_title": "Effects of Removing Contamination from Pre-Training",
            "char_start_offset": 17749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1054
                },
                {
                    "start": 1057,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1893
                },
                {
                    "start": 1896,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2235
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "267751383",
            "title": "CofCA: A STEP-WISE Counterfactual Multi-hop QA benchmark",
            "text": "Notably, Tang et al. (2021) introduce a human-validated sub-question dataset derived from the HotpotQA dataset (Yang et al., 2018), undertaking a detailed investigation of the models' capabilities to reason through sub-questions. Their findings revealed that notable models like DFGN (Xiao et al., 2019), DecompRC (Min et al., 2019), and CogQA (Ding et al., 2019) exhibit deficiencies in resolving subquestions, even though they may successfully address the overarching multi-hop question. Moreover, Wikipedia-based MHQA datasets face the challenge of data contamination that hard to objectively and truthfully evaluate the reasoning ability of LLMs. Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a major issue in measuring LLMs' real performance on other tasks. For example, HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), and MuSiQue (Trivedi et al., 2021) can be applied to evaluate the multi-step reasoning performance of LLMs. Typically, evaluating LLMs in MHQA datasets involves using RAG to retrieve and reason over context with a single step of retrieval. However, single-step retrieval can result in insufficient context retrieval for complex questions, as it provides a limited scope of information (Gao et al., 2023b). Prasad et al. (2023) proposes a framework that defines good reasoning chains in Correctness and Informativeness to illustrate whether the previous reasoning step could help the current reasoning step and the final answer. \n\nBenchmarking Data Leakage A handful of recent studies have provided several strategies, methods, and benchmarks for detecting contamination without the need to access pre-training data (Shi et al., 2023;Roberts et al., 2023;Golchin & Surdeanu, 2023;Zhu et al., 2023a). Ravaut et al. (2024) surveys recent work on detecting data contamination and releases a python library named LLMSanitize that implements major contamination detection methods.",
            "score": 0.526793427014664,
            "section_title": "RELATED WORK",
            "char_start_offset": 7439,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 2000
                }
            ],
            "ref_mentions": [
                {
                    "start": 9,
                    "end": 27,
                    "matchedPaperCorpusId": "211258744"
                },
                {
                    "start": 111,
                    "end": 130,
                    "matchedPaperCorpusId": "52822214"
                },
                {
                    "start": 314,
                    "end": 332,
                    "matchedPaperCorpusId": "174801080"
                },
                {
                    "start": 870,
                    "end": 889,
                    "matchedPaperCorpusId": "52822214"
                },
                {
                    "start": 907,
                    "end": 924,
                    "matchedPaperCorpusId": "226236740"
                },
                {
                    "start": 938,
                    "end": 960,
                    "matchedPaperCorpusId": "236771976"
                },
                {
                    "start": 1332,
                    "end": 1352,
                    "matchedPaperCorpusId": "258291731"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62841796875
        },
        {
            "corpus_id": "268353040",
            "title": "Elephants Never Forget: Testing Language Models for Memorization of Tabular Data",
            "text": "Large Language Models (LLMs) exhibit remarkable performance on a diverse set of tasks [33,3,18].While their prowess in natural language is undeniable, performance in other applications remains an ongoing research topic [14,23].A main question in current research on LLMs is the degree to which these models are able to extrapolate to novel tasks that are unlike what they have seen during training [34].As such, an important aspect of LLM evaluation is to know to what degree a task might be part of the model's training set and, as a consequence, contained in the LLM's internal representation either verbatim or compressed [4,5].This paper specifically targets the issue of contamination in training sets when evaluating LLMs on tasks with tabular data -an aspect often neglected in the rapidly growing literature on applications in this domain [13,1,21,31,16,32,20].Our investigation reveals that LLMs have been pre-trained on many of the popular datasets, and that this exposure can lead to invalid performance evaluations because the LLMs have, in effect, been fit to the test set.To tackle this issue, we introduce various methods to detect contamination, including statistical tests for conditional distribution modeling, and four different tests to detect memorization [4].To avoid the problem in future research, we release an open-source tool that can perform these tests.\n\nWe distinguish between the following three highly related concepts, which can be seen as different types of contamination in the representation learned by an LLM:\n\n\u2022 Knowledge about a dataset means that the model knows things such as when the dataset was collected, who collected it, the names of the features (column headings), the legal values or ranges of categorical or contunuous variables, the delimiter in the csv file, etc.\n\n\u2022 Learning from a dataset refers to the model's ability to perform tasks that depend on learning the true joint probability distribution from which the data was sampled, e.g., supervised learning from data to predict a value given other values in a sample.",
            "score": 0.5265750719506055,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 96,
                    "end": 227
                },
                {
                    "start": 227,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 631
                },
                {
                    "start": 631,
                    "end": 869
                },
                {
                    "start": 869,
                    "end": 1086
                },
                {
                    "start": 1086,
                    "end": 1281
                },
                {
                    "start": 1281,
                    "end": 1382
                },
                {
                    "start": 1384,
                    "end": 1546
                },
                {
                    "start": 1548,
                    "end": 1815
                },
                {
                    "start": 1817,
                    "end": 2073
                }
            ],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 90,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 92,
                    "end": 95,
                    "matchedPaperCorpusId": "253553585"
                },
                {
                    "start": 628,
                    "end": 630,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "249642722"
                },
                {
                    "start": 851,
                    "end": 853,
                    "matchedPaperCorpusId": "252846328"
                },
                {
                    "start": 856,
                    "end": 859,
                    "matchedPaperCorpusId": "253180321"
                },
                {
                    "start": 859,
                    "end": 862,
                    "matchedPaperCorpusId": "252992811"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8935546875
        },
        {
            "corpus_id": "268201479",
            "title": "Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs",
            "text": "However, there is no guarantee that other copies of these datasets do not exist on the web, or that model builders have not included this data during pretraining or fine-tuning. \n\nPrompting-based techniques detect contamination by asking the model to complete model cards or data points in the dataset [2,16]. [10] prompt models to fill in a masked answer, which is the incorrect answer in a multiple choice setup, and also prompt the model to predict an unlikely word in a test example. [25] use training data inspection, task example extraction, and a membership inference attack to suggest that LLM improvements can be explained by test data contamination. [15] propose a technique in which they generate perturbations of the benchmark datasets while preserving meaning. They provide the perturbations with the original text as options for the model to pick from, the idea being that a model contaminated with the benchmark will pick the original data point more often that the other data points even though they share the same semantics. While this approach has been somewhat effective in detecting contamination, it may not perform well in scenarios where the perturbations, generated by an LLM, are of inferior quality and do not preserve semantics, such as multilingual settings [3] where the LLM does not perform this task well in non-English languages. \n\nWhile it is desirable for many reasons to open-source LLMs, it is likely that some models will not be made available to the public. Additionally, many open-source models do not disclose information about the training dataset, making it challenging to detect contamination. Moreover, several opensource models are constructed on top of existing open-source models which may not reveal training dataset details (e.g., fine-tuned versions of the Llama [41] models), meaning that contamination continues to be a significant challenge. Overall, while it is an important endeavour to be able to detect contamination in models, existing techniques are not fool-proof, and hence, it is critical to prevent contamination in the first place.",
            "score": 0.5262911997958969,
            "section_title": "Data contamination in LLMs",
            "char_start_offset": 7629,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 180,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1361
                },
                {
                    "start": 1364,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2095
                }
            ],
            "ref_mentions": [
                {
                    "start": 302,
                    "end": 305,
                    "matchedPaperCorpusId": "257663467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "273350935",
            "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
            "text": "Previous works construct benchmark datasets to evaluate the performance of detection methods for pretraining data detection. Following the prior literature, we conduct experiments on 5 benchmark datasets: WikiMIA (Shi et al., 2024) selects old Wikipedia event data as member data by leveraging the Wikipedia data timestamp and the model release date, since Wikipedia is a commonly pretraining data source. BookMIA (Shi et al., 2024), which contains excerpts from copyrighted books in the Books3 subset of the Pile dataset (Gao et al., 2020), can be used for detecting potential copyright infringement in training data. ArXivTection (Duarte et al., 2024) is a curated collection of research articles sourced from ArXiv. BookTection (Duarte et al., 2024), which comprises passages from 165 books, is constructed based on BookMIA. We also conducted experiments on the Pile dataset (Maini et al., 2024), which is large-scale text dataset for training language models, including text data from various sources such as books, GitHub, and website content.",
            "score": 0.5260682749684926,
            "section_title": "B DETAILS OF DATASETS",
            "char_start_offset": 31137,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1048
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 231,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 414,
                    "end": 432,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 731,
                    "end": 752,
                    "matchedPaperCorpusId": "267681760"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74658203125
        },
        {
            "corpus_id": "269137630",
            "title": "Measuring Cross-lingual Transfer in Bytes",
            "text": "A potential reason for a pretrained model's superior performance in cross-lingual tasks is the presence of a substantial amount of data in the target language in its pretraining dataset, a phenomenon referred to as language contamination.To quantify this impact, following the approach outlined by Blevins and Zettlemoyer (2022), we examine the rates of target language fragments in the source language dataset and vice versa.\n\nSpecifically, for a given source language \u2113 and target language t, we calculate the ratio of all lines classified as \u2113 in the target dataset (known as contamination on target) and as t in the pretraining dataset (known as contamination on source).We perform language detection using the fasttext tool (Bojanowski et al., 2017), employing a threshold of 0.6 for classification.\n\nNext, we compute the Spearman correlation between the set of Data Transfer metrics D T and those ratios obtained from the outcomes of our experiments.\n\nCorrelating these rates with the model's data transfer indicator allows us to evaluate the impact of language contamination on model performance.",
            "score": 0.5256879324985996,
            "section_title": "Language Contamination",
            "char_start_offset": 8684,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 238,
                    "end": 426
                },
                {
                    "start": 428,
                    "end": 675
                },
                {
                    "start": 675,
                    "end": 804
                },
                {
                    "start": 806,
                    "end": 956
                },
                {
                    "start": 958,
                    "end": 1103
                }
            ],
            "ref_mentions": [
                {
                    "start": 298,
                    "end": 328,
                    "matchedPaperCorpusId": "252780005"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6875
        },
        {
            "corpus_id": "262045629",
            "title": "Pretraining on the Test Set Is All You Need",
            "text": "Following prior work studying possible data contamination in pretraining datasets [RWC + 19, BMR + 20, ADF + 23, Ope23, TMS + 23], we investigate whether our pretraining corpus might contain benchmark data. Our findings can be found in Table 1.",
            "score": 0.5256879324985996,
            "section_title": "Investigation of Evaluation Data Contamination",
            "char_start_offset": 2909,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 244
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06805419921875
        },
        {
            "corpus_id": "272689919",
            "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges",
            "text": "Such variations may cause difficulties in contamination detection yet are rarely considered when evalu-ating the effectiveness of contamination detection methods. Lastly, while various detection methods have demonstrated effectiveness (with a particular set of datasets and LLMs of their choice), there is a lack of a comprehensive crosscomparison to assess the consistency and reliability of these techniques, particularly for more recent LLMs, benchmarks, and training paradigms. \n\nTo bridge these gaps, we evaluate five distinct data contamination detection approaches, including three state-of-the-art methods recently published at ICLR, a simple prompting method based on token perturbation, and our proposed pilot prompt-based method, which queries the LLM about its knowledge of the original order of data points. Our study covers eight benchmarks, including six challenging datasets frequently used to evaluate modern LLMs and two traditional benchmarks. We apply these methods with four language models: GPT-4 (OpenAI, 2023), Claude 3 Sonnet, LLaMA-3-Chat (70B) (AI@Meta, 2024), and LLaMA-2-Chat (70B) (Touvron et al., 2023). To provide a gold standard for assessing the effectiveness of these methods, we create an oracle using LLaMA-2 (70B) by intentionally contaminating the model with varying portions of the six challenging benchmarks in the format of instruction fine-tuning with answer augmentation. This setup allows us to observe the performance of the five detection methods given the known contamination status. By these setups, we seek to answer the following research questions: i. Are the latest state-of-the-art LLMs, which consistently achieve higher performance, contaminated with these challenging benchmarks? What do the detection methods indicate? ii. Can these methods detect contamination that occurred during instruction fine-tuning with data variations instead of the original format in pretraining? iii. Do different \"well-accepted\" detection methods corroborate each other's findings for a given dataset? Do they yield inconsistent results? \n\nOur experimental results and analysis reveal several critical findings about current data contamination detection in LLMs: First, all existing methods have limitations in their underlying assumptions or practical applications.",
            "score": 0.5242581553961256,
            "section_title": "Introduction",
            "char_start_offset": 2186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2039
                },
                {
                    "start": 2040,
                    "end": 2075
                },
                {
                    "start": 2078,
                    "end": 2304
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96142578125
        },
        {
            "corpus_id": "265213054",
            "title": "CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models",
            "text": "We are currently in an era of fierce competition among various large language models (LLMs) continuously pushing the boundaries of benchmark performance. However, genuinely assessing the capabilities of these LLMs has become a challenging and critical issue due to potential data contamination, and it wastes dozens of time and effort for researchers and engineers to download and try those contaminated models. To save our precious time, we propose a novel and useful method, Clean-Eval, which mitigates the issue of data contamination and evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to paraphrase and back-translate the contaminated data into a candidate set, generating expressions with the same meaning but in different surface forms. A semantic detector is then used to filter the generated low-quality samples to narrow down this candidate set. The best candidate is finally selected from this set based on the BLEURT score. According to human assessment, this best candidate is semantically similar to the original contamination data but expressed differently. All candidates can form a new benchmark to evaluate the model. Our experiments illustrate that Clean-Eval substantially restores the actual evaluation results on contaminated LLMs under both few-shot learning and fine-tuning scenarios.",
            "score": 0.5236837359675477,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91748046875
        },
        {
            "corpus_id": "272689919",
            "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges",
            "text": "We examine five distinct approaches to detecting data contamination in LLMs, including three state-of-the-art techniques from ICLR (2023-2024) and two exploratory prompt-based approaches. Two approaches are based on sequence probabilities and require access to model parameters. Figure 1 illustrates the visual overview of each approach. For each method, we also note the limitations we identified during our examination. The overview of these methods are illustrated in Figure 1. \n\n\u2022 Min-K% Prob (Shi et al., 2023) assesses whether a text was in an LLM's pre-training data by calculating the average log-likelihood of the k% lowestprobability tokens, with a high result suggesting the text's presence in the training data. Limitations: (1) The authors report AUC based on the proposed WiKiMIA dataset, in which they regarded data events before the model release as contaminated data. Such a strong assumption on the ground truth may require more justification. (2) They did not provide the threshold to determine the value of min-K%-prob in the paper since they claim they can use AUC; however, in real-world settings, we do not always have the oracle to determine AUC -instead, we need a metric for determining whether arbitrary datasets are contaminated. (3) The code is not available. \n\n\u2022 Canonical Order Statistical Testing (Oren et al., 2024) identifies contamination in a pre-training dataset by checking if the model shows a preference for the canonical order of examples over random shuffling. This preference is tested by comparing their log probabilities, with results aggregated across datasets to ensure a low false positive rate. Limitations: When an individual data example's length is long, the combination of sample/shards/permutations in the setup can be costly. \n\n\u2022 Token Completion Overlap Score (Golchin and Surdeanu, 2024b) detects contamination by prompting the LLM with a dataset name, partition type, and a random initial segment of a reference instance. If the LLM's output closely matches the latter part of the reference, the instance is flagged as contaminated.",
            "score": 0.5236540627570234,
            "section_title": "Evaluated Methods and Limitations",
            "char_start_offset": 9487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1780
                },
                {
                    "start": 1783,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2090
                }
            ],
            "ref_mentions": [
                {
                    "start": 1329,
                    "end": 1348,
                    "matchedPaperCorpusId": "264490730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "270257992",
            "title": "PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs",
            "text": "Models. We use RoBERTa-large (Liu et al., 2019) for maskfilling. We use all-MiniLM-L6-v2 for text embeddings. We use DistilGPT2 (Sanh et al., 2019) to evaluate our methods. Finally, we use LLaMA-2-7B (Touvron et al., 2023) for synthetic seed expansion. \n\nDatasets. We produce three federated private datasets from the c4-English (Raffel et al., 2019) (c4-en): JOBS, FORUMS, and MICROBLOG, which are subsets of c4-en. In these datasets, the federated datasets are uniformly randomly partitioned among clients. We also produce another federated private dataset CODE, a question-and-answer dataset focused on coding and technical topics. For all training datasets, there are 1250 clients. The evaluation sets are created from a held-out portion of the data. For the initial population used in PrE-Text, we use a subset of c4-en that is not part of any of the private datasets. More details on the datasets are provided in Appendix C. \n\nNote that many LLMs do not document what datasets were used in their pretraining, which makes it difficult to prevent contamination. Even text released after the release of the model may be contaminated, as it may have been AI-generated. We used the most recent large-scale dataset we could find (though it was released before the release of LLaMA-2) that is (a) compliant with terms of service (many sources of recent text data have closed their APIs for ML training) and (b) readily accessible. Systematically detecting dataset contamination is an important open problem in LLM research (Gunasekar et al., 2023). \n\nTask. We focus on the language modeling task, and report evaluation loss (cross-entropy) and accuracy. We consider two experimental settings: (small) models stored on-device, and (large) models stored on-server. \n\nBaselines. We compare PrE-Text to several baselines.",
            "score": 0.5234989268668032,
            "section_title": "Experiments",
            "char_start_offset": 15414,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 7
                },
                {
                    "start": 8,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 252
                },
                {
                    "start": 255,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 930
                },
                {
                    "start": 933,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1547
                },
                {
                    "start": 1550,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1761
                },
                {
                    "start": 1764,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1816
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 222,
                    "matchedPaperCorpusId": "257219404"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.310546875
        },
        {
            "corpus_id": "267499939",
            "title": "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs",
            "text": "The recent emergence of large language models (LLMs), that show remarkable performance on a wide range of tasks, has led not only to a dramatic increase in their use in research but also to a growing number of companies joining the race for the biggest and most powerful models. In pursuing a competitive advantage, many popular LLMs today are locked behind API access and their details are unknown (OpenAI, 2023;Thoppilan et al., 2022;Touvron et al., 2023). This includes model weights (OpenAI, 2023), training data (Piktus et al., 2023), or infrastructural details to assess model carbon footprint (Lacoste et al., 2019). \n\nIn particular, the lack of information on training data raises important questions about the credibility of LLMs performance evaluation. The data from which these models learn, typically collected automatically by scraping documents from the web, may contain training, validation, and -most critically -test sets coming from NLP benchmarks. Because of this, researchers and stakeholders may later inadvertently evaluate LLMs on the same data they were trained on. This phenomenon, known as data contamination, may not be an issue in the general use of commercial LLMs, where adherence to research principles is not mandatory, but it becomes a serious problem when these models are widely used and evaluated in research. \n\nUnfortunately, many proprietary models are locked behind inference-only APIs, making it hard to inspect data contamination. Because of this, existing work on the matter mostly focuses on detecting extreme forms of overfitting and memorization, such as the model's ability to generate benchmarks verbatim. These approaches are not only limited but also neglect that recent proprietary LLMs get iteratively improved from user interactions. If such interactions involve benchmark data (for example when researchers evaluate LLMs against baselines), the model may, in fact, become contaminated even if it was contamination-free during its initial training. We refer to this phenomenon as indirect data leaking. \n\nIn this paper, we address the issue of indirect data contamination in closed-source 1 LLMs by conducting a systematic literature review. We review 255 papers and carefully detail data leakage emerging from them.",
            "score": 0.5234343173063238,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1345
                },
                {
                    "start": 1348,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2054
                },
                {
                    "start": 2057,
                    "end": 2193
                },
                {
                    "start": 2194,
                    "end": 2268
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.386962890625
        },
        {
            "corpus_id": "265213054",
            "title": "CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models",
            "text": "There is room for doubt regarding the potential overestimation of these benchmark measurements.One reason is that LLMs are trained on data extracted from websites and publicly accessible datasets (OpenAI, 2023;Touvron et al., 2023).Therefore, ensuring no overlap between the pretraining dataset and the evaluated benchmark becomes challenging.This subsequently introduces a significant concern: the risk of data contamination.\n\nData contamination arises when the pre-training data of one model integrates evaluated data, consequently enhancing test performance (Magar and Schwartz, 2022;Golchin and Surdeanu, 2023).Currently, many models opt not to disclose their training sets in technical reports, raising concerns about the potential inclusion of benchmark datasets within their training data.This presents an urgent problem (Wei et al., 2023), as these contaminated models claim highly evaluated results but often lead to poor real-world experiences.We strongly advocate for a cleaner evaluation of LLMs.Un- Figure 2: An overview of our method.We first gather existing benchmarks for LLM assessment and then meticulously clean contamination in these benchmarks through LLM-powered paraphrase and multi-language back-translation, employing a semantic detector to filter and select optimal results based on BLEURT scores.\n\nveiling the genuine capabilities of LLMs could significantly propel the community of LLMs forward.\n\nThe most effective resolution involves relabeling a new dataset when developing a new model to assess its capabilities.Unfortunately, this process demands considerable time and labor.This paper employs previously proposed benchmarks to create a new benchmark, and our method is called Clean-Eval, aiming to mitigate data contamination using LLMs and accurately assess the actual capabilities of LLMs.Leveraging the exceptional creative capabilities of these models, we perform diverse paraphrasing of contaminated data and back-translate it across multiple language directions.This process results in a pool of calibrated datasets.We effectively filter out low-quality samples by utilizing semantic detectors, and then select the best items based on BLEURT scores derived from comparisons between the calibrated and contaminated data.Finally, We conducted experiments on 20 benchmarks across diverse tasks, and our analysis unveiled noticeable calibrated effects achieved through Clean-Eval.",
            "score": 0.522996314085908,
            "section_title": "2023) and other LLMs, which have refreshed various evaluation benchmarks continuously.",
            "char_start_offset": 1055,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 232
                },
                {
                    "start": 232,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 426
                },
                {
                    "start": 428,
                    "end": 615
                },
                {
                    "start": 615,
                    "end": 796
                },
                {
                    "start": 796,
                    "end": 954
                },
                {
                    "start": 954,
                    "end": 1008
                },
                {
                    "start": 1008,
                    "end": 1048
                },
                {
                    "start": 1048,
                    "end": 1323
                },
                {
                    "start": 1325,
                    "end": 1423
                },
                {
                    "start": 1425,
                    "end": 1544
                },
                {
                    "start": 1544,
                    "end": 1608
                },
                {
                    "start": 1608,
                    "end": 1825
                },
                {
                    "start": 1825,
                    "end": 2002
                },
                {
                    "start": 2002,
                    "end": 2056
                },
                {
                    "start": 2056,
                    "end": 2259
                },
                {
                    "start": 2259,
                    "end": 2416
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "264555419",
            "title": "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark",
            "text": "Detection of contamination cases has been traditionally done by directly analyzing the training data (Dodge et al., 2021), but the current scale of the pre-training data makes it difficult (Kreutzer et al., 2022;Birhane et al., 2021). Without proper documentation and search tools like ROOTS (Piktus et al., 2023) it is very difficult for any researcher to actually know whether their datasets are compromised on a given model. More recently, this task became even harder, as the best-performing LLMs are deployed as products, and therefore, their training corpora are kept secret. In this case, it has been shown that the high memorization abilities of LLMs can be used to generate portions of the training texts (Carlini et al., 2021;Magar and Schwartz, 2022). Using this memorization property, Sainz et al. (2023) show that ChatGPT generates portions of popular NLP benchmarks. Furthermore, LLMs memorization has been studied on data-leakage scenarios (Elangovan et al., 2021). \n\nRegarding data contamination cases, Dodge et al. (2021) exposed that the C4 corpus (Raffel et al., 2020), a corpus used to pre-train several LLMs such as T5 (Raffel et al., 2020), contained the test splits of several benchmarks that were crawled from GitHub. Moreover, Brown et al. (2020) acknowledged a bug in their filtering script that caused the contamination of several benchmarks during the GPT-3 training. Furthermore, OpenAI (2023) stated that parts of the BIGbench (Srivastava et al., 2023) benchmark were inadvertently mixed into the training set, enough to stop them from evaluating the model on it. They also mention that they included parts of the training sets of MATH (Hendrycks et al., 2021) and GSM-8K (Cobbe et al., 2021) as training data to improve mathematical reasoning (OpenAI, 2023). Therefore, the performance results reported for GSM-8K cannot be taken as zero-shot results when compared to other models.",
            "score": 0.5224298119259412,
            "section_title": "Background",
            "char_start_offset": 3072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 980
                },
                {
                    "start": 983,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1912
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 121,
                    "matchedPaperCorpusId": "237568724"
                },
                {
                    "start": 736,
                    "end": 761,
                    "matchedPaperCorpusId": "247475929"
                },
                {
                    "start": 955,
                    "end": 979,
                    "matchedPaperCorpusId": "231786600"
                },
                {
                    "start": 1019,
                    "end": 1038,
                    "matchedPaperCorpusId": "237568724"
                },
                {
                    "start": 1066,
                    "end": 1087,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1140,
                    "end": 1161,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51171875
        },
        {
            "corpus_id": "270285708",
            "title": "Benchmark Data Contamination of Large Language Models: A Survey",
            "text": "The Pile Similarly, as shown in Figure 1, Deng et al. [30] proposed two novel methods to detect potential overlaps between evaluation benchmarks and pre-training corpora, tailored for both open-source and proprietary LLMs.They introduced a retrieval-based system and a Testset Slot Guessing (TS-Guessing) protocol, which involves masking incorrect answers in multiple-choice questions and prompting the model to fill in the gaps.Their findings indicated that commercial LLMs, including ChatGPT and GPT-4, can guess missing options in benchmark tests with a high level of accuracy.\n\nGolchin and Surdeanu [46] presented another novel approach, the Data Contamination Quiz (DCQ), to detect and quantify BDC in LLMs.The DCQ is a series of multiple-choice questions with three perturbed versions of each dataset instance, including only word-level changes.The LLM's ability to identify the original instance among the perturbed ones indicates potential exposure to the data during pre-training.Tested on GPT-3.5/4, the DCQ demonstrated higher contamination , Vol. 1, No. 1, Article .Publication date: June 2024.levels than other methods and effectively bypassed safety filters designed to prevent the generation of copyrighted content.\n\nGolchin and Surdeanu [47] presented a technique that combines instance-level identification using guided instruction prompts with partition-level assessment through overlap score comparison and classifier-based detection.The approach achieved high accuracy rates, between 92% and 100%, across seven datasets.The authors also found specific cases of contamination in popular datasets, such as AG News, WNLI, and XSum, when tested with GPT-4.\n\nLi et al. [91] presented a comprehensive report on BDC across over 15 LLMs and six multiplechoice QA benchmarks.They introduced an open-source pipeline to conduct contamination analysis on customized data and models.Their research uncovered varying degrees of contamination, ranging from 1% to 45%, and demonstrated that contamination does not always correlate with improved model performance.Interestingly, larger models may benefit more from contaminated test sets than smaller ones, with significant accuracy boosts observed on certain benchmarks.",
            "score": 0.5216721388191621,
            "section_title": "Pretrain Corpus",
            "char_start_offset": 27303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 222,
                    "end": 429
                },
                {
                    "start": 429,
                    "end": 580
                },
                {
                    "start": 582,
                    "end": 712
                },
                {
                    "start": 712,
                    "end": 851
                },
                {
                    "start": 851,
                    "end": 989
                },
                {
                    "start": 989,
                    "end": 1078
                },
                {
                    "start": 1078,
                    "end": 1106
                },
                {
                    "start": 1106,
                    "end": 1230
                },
                {
                    "start": 1232,
                    "end": 1453
                },
                {
                    "start": 1453,
                    "end": 1540
                },
                {
                    "start": 1540,
                    "end": 1672
                },
                {
                    "start": 1674,
                    "end": 1786
                },
                {
                    "start": 1786,
                    "end": 1890
                },
                {
                    "start": 1890,
                    "end": 2067
                },
                {
                    "start": 2067,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 1253,
                    "end": 1257,
                    "matchedPaperCorpusId": "260925501"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "276094401",
            "title": "How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence",
            "text": "When a large language model (LLM) performs remarkably well on a benchmark, can we confidently attribute its success to true generalization-or is it simply a reflection of what the model has already seen during pre-training? The reality is, we often don't know. Beneath the surface of those impressive performance scores lies a critical vulnerability: dataset contamination, a phenomenon where evaluation datasets overlap with the pretraining data of the model (Golchin & Surdeanu, 2024). This overlap artificially inflates reported performance metrics, obscures true generalization capabilities, and raises critical concerns about the reliability of benchmark evaluations. This brings us to a pressing and underexplored question: How can we measure the degree of dataset contamination? \n\nAddressing this question is crucial to ensuring that performance evaluations genuinely reflect a model's ability to generalize to unseen data, rather than benefiting from overlap with pretraining data. To formalize the problem, we aim to develop a scoring function S : (D, M) \u2192 R, that takes a benchmark dataset D as input and produces a score indicative of its relative contamination level with respect to the given model M. A higher score corresponds to a greater contamination level. Such a score is valuable because researchers can use it to rank multiple benchmarks and prioritize the less contaminated ones, enabling more informed comparisons and reliable evaluation. For the score to be reliable, we argue that the scoring function must satisfy two essential properties: monotonicity, which ensures that the score exhibits a positive correlation with the contamination level, and consistency, which means that the score remains stable across independently sampled subsets with the same contamination rate. \n\nTo measure dataset contamination, we introduce the Kernel Divergence Score (KDS), which computes the divergence of the kernel similarity matrix of sample embeddings before and after fine-tuning on the benchmark dataset. By analyzing changes in the kernel similarity matrix, KDS captures how fine-tuning reshapes the embeddings for seen and unseen data, providing a more holistic and nuanced perspective on dataset contamination. This approach is motivated by the fact that fine-tuning has a more significant effect on the Pre-trained LLM Fine-tuned LLM",
            "score": 0.5207482324615604,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1800
                },
                {
                    "start": 1803,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2231
                },
                {
                    "start": 2232,
                    "end": 2355
                }
            ],
            "ref_mentions": [
                {
                    "start": 460,
                    "end": 486,
                    "matchedPaperCorpusId": "260925501"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "277065772",
            "title": "Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models",
            "text": "A growing concern for accurate evaluation of LLMs' generalization capabilities is the prospect of dataset contamination, which is when evaluation benchmark data has already appeared in LLM train-  vron et al., 2023;Zhou et al., 2023;Jiang et al., 2024). However, with proprietary models that limit access to training data, there is limited recourse to evaluate how contamination affects performance. \n\nFor our purposes, we study text contamination (Jiang et al., 2024), when the input text of an evaluation sample is likely to have appeared in the training data of a model. There is little gold standard evidence of contamination for proprietary language models. Recently, Deng et al. (2024) looked for evidence of contamination by prompting ChatGPT to identify informative words in dataset samples and prompting target models to guess the missing words (TS-SLOT). Specifically, they conduct a contamination experiment, where ChatGPT is finetuned with data from the MMLU test set, and the differences in Exact Match Rate between the finetuned model and the original model is observed. However, the extent to which these target models were contaminated with those datasets in the first place is unknown, and consequently the discriminative power of the test also remains unknown. In the section below, we describe a controlled setting to examine the discriminative power of contamination tests. We then apply the best-performing tests to real-world datasets to surface evidence of contamination. \n\nControlled contamination. We would like any test of contamination to have discriminative power, i.e., produce different values for contaminated datasets and uncontaminated datasets. \n\nTherefore, we construct a synthetic test to compare the power of reconstruction probing with TS-SLOT (Deng et al., 2024). We start with an uncontaminated dataset, and then deliberately contaminate the model to examine the discriminative power of our method at detecting contamination. Thus, we consider a benchmark dataset, Google-proof QA or GPQA (Rein et al., 2023), where questions were written by domain-experts in biology, physics, and chemistry. The dataset was released in 2023beyond the knowledge cutoff of the target models in this study.",
            "score": 0.5192784072447478,
            "section_title": "Dataset Contamination",
            "char_start_offset": 17893,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1494
                },
                {
                    "start": 1497,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1678
                },
                {
                    "start": 1681,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 673,
                    "end": 691,
                    "matchedPaperCorpusId": "265220695"
                },
                {
                    "start": 1782,
                    "end": 1801,
                    "matchedPaperCorpusId": "265220695"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74853515625
        },
        {
            "corpus_id": "270620798",
            "title": "Data Contamination Can Cross Language Barriers",
            "text": "In this paper, we identify a cross-lingual form of data contamination that can significantly inflate LLMs' benchmark performance while evading cur-rent detection approaches. In this case, we argue that the conventional memorization-based definition of data contamination could note reflect the essence of contamination. To detect such deeply concealed contamination, suggest a generalizationbased definition and propose to detect it by examining the model's generalizability. With extensive experiments, we confirm that data contamination can cross language barriers. We also demonstrate that our proposed generalization-based method is able to detect not only cross-lingual but also other undisclosed contamination. In the future, we will extend our generalization-based detection approach to other potential forms of contamination. We will also explore how such cross-lingual contamination can benefit the interpretability of LLMs and the enhancement of multilingual capabilities.",
            "score": 0.5180897466574578,
            "section_title": "Conclusions and Future Work",
            "char_start_offset": 24719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 982
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.385009765625
        },
        {
            "corpus_id": "269921283",
            "title": "Data Contamination Calibration for Black-box LLMs",
            "text": "Data Contamination.As Magar and Schwartz (2022) mentioned, data contamination is the infiltration of downstream test data into the pretraining corpus, which may seriously mislead evaluation results.Dodge et al. (2021) and Brown et al. (2020) highlighted the tangible presence of contamination issues in models (e.g.GPT-3) and corpora (e.g.C4).Such contamination risks models memorizing rather than learning, affecting their exploitation (Magar and Schwartz, 2022).Detection methods have been proposed, using n-gram overlap ratios against pre-training data to identify contaminated samples (Du et al., 2022;Wei et al., 2021;Chowdhery et al., 2023).However, these existing methods rely on access to the pretraining corpus, which is unfeasible for many LLMs.Recent studies (Golchin and Surdeanu, 2023;Weller et al., 2023) shifted towards a more common black-box setting, by extracting outputs with trigger tokens (Carlini et al., 2021) or specified prompts (Sainz et al., 2023;Nasr et al.) from LLMs as contaminated samples and comparing with the test set.Unfortunately, the extracted samples are usually too broad, making these methods ineffective for detecting given samples.Membership Inference Attack.Membership Inference Attacks (MIA), proposed by Shokri et al. (2017), is defined as determining whether a given sample is part of the training set.MIA is predicated on the models' inevitable overfitting (Yeom et al., 2018), leading to a differential performance on training samples (members) versus non-members.MIA has been applied in privacy protection (Jayaraman and Evans, 2019;Zanella-B\u00e9guelin et al., 2020;Nasr et al., 2021Nasr et al., , 2023;;Steinke et al., 2023), machine-generated text detection (Mitchell et al., 2023;Solaiman et al., 2019), DNA inference (Zer-houni and Nabel, 2008), etc.Given the limited access to most current LLMs, research has concentrated on black-box conditions.",
            "score": 0.5179037139320216,
            "section_title": "Related Work",
            "char_start_offset": 6564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 19,
                    "end": 198
                },
                {
                    "start": 198,
                    "end": 315
                },
                {
                    "start": 315,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 464
                },
                {
                    "start": 464,
                    "end": 647
                },
                {
                    "start": 647,
                    "end": 755
                },
                {
                    "start": 755,
                    "end": 1053
                },
                {
                    "start": 1053,
                    "end": 1174
                },
                {
                    "start": 1174,
                    "end": 1202
                },
                {
                    "start": 1202,
                    "end": 1349
                },
                {
                    "start": 1349,
                    "end": 1513
                },
                {
                    "start": 1513,
                    "end": 1801
                },
                {
                    "start": 1801,
                    "end": 1898
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 217,
                    "matchedPaperCorpusId": "237568724"
                },
                {
                    "start": 222,
                    "end": 241,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 589,
                    "end": 606,
                    "matchedPaperCorpusId": "245124124"
                },
                {
                    "start": 606,
                    "end": 623,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 623,
                    "end": 646,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 910,
                    "end": 932,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1250,
                    "end": 1270,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1405,
                    "end": 1424,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1556,
                    "end": 1583,
                    "matchedPaperCorpusId": "173990571"
                },
                {
                    "start": 1583,
                    "end": 1613,
                    "matchedPaperCorpusId": "219450111"
                },
                {
                    "start": 1613,
                    "end": 1630,
                    "matchedPaperCorpusId": "231583084"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51416015625
        },
        {
            "corpus_id": "265498783",
            "title": "Mukhyansh: A Headline Generation Dataset for Indic Languages",
            "text": "We initiate the analysis by implementing preprocessing steps for the IndicHG dataset, including checks for prefixes, duplicates, and minimum length. In addition to the eight languages we are focusing on, we extended the preprocessing to include the remaining three languages of IndicHG: Oriya, Punjabi, and Assamese. \n\nSurprisingly, despite claims to the contrary, our analysis reveals that the IndicHG dataset contains a significant number of duplicate article-headline pairs in the training, development, and test splits for most languages. Out of the total 1.31 million pairs, approximately 0.67 million (51.23%) are duplicates. Moreover, it is ideal for a dataset to have no overlap or common samples among the train-  Data contamination introduces bias in evaluation, as the metrics calculated on the development and test datasets do not accurately represent the model's performance on unseen data. Additionally, we assert that the heavy presence of duplicated data in the dataset may lead models trained on this data to achieve artificially high performance by memorizing the duplicated pairs, thereby hindering their ability to generalize to new, unseen data. \n\nTo support our arguments, we take several steps. Firstly, we eliminate all duplicate pairs from each of the training, development, and test splits of the IndicHG dataset. To deal with data contamination, the following 2 variations were attempted: 1. To ensure the integrity of the test set, a straight-12 https://tinyurl.com/2p85mayt forward approach was adopted, which involved excluding any pairs that were already present in the corresponding train/dev sets. Additionally, any pairs in the dev set that were already present in the train set were also removed. This approach effectively eliminated data contamination and allowed the training set to remain as large as possible. These splits were then utilized to reproduce the IndicHG results as In-dicHG_Unbiased. Notably, this dataset exhibited a significant decrease in average R-L score, with a decrease of 22.48 compared to the score reported in the original IndicNLG paper (Kumar et al., 2022), resulting in an average R-L score of 23.42; as outlined in Table 4.",
            "score": 0.5169514804458957,
            "section_title": "Quantitative Analysis",
            "char_start_offset": 13286,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 316
                },
                {
                    "start": 319,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1166
                },
                {
                    "start": 1169,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2189
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1942138671875
        },
        {
            "corpus_id": "273323543",
            "title": "Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping",
            "text": "We explore two types of data contamination in multimodal evaluation benchmarks. 1) Image-only contamination. We aim to detect how many images in the benchmark can be found in the pretraining data. To this end, we utilize the CLIPScore (Hessel et al., 2021) to measure the similarity between images from the evaluation and training set. In our pilot experiments, we find that if the CLIPScore between two images exceeds 0.9, it indicates high visual similarity. Thus, we adopt 0.9 as the threshold to determine visual contamination. The image-only contamination rate is calculated as the ratio of the number of contaminated images and the number of total images in the evaluation set. 2) Image-text contamination. Beyond images, the question and answer of benchmark can also be contaminated. We extend ideas from NLP detection works (Li et al., 2024a) to identify this image-text contamination. For contaminated image pairs, we determine the question and answer contaminated if the answer can be directly inferred from the captions of the training image. In practice, we leverage GPT-4 to conduct this process. \n\nAs illustrated in Figure 2, we examine the two types of data contamination between three popular evaluation benchmarks: SEEDBench, MMBench, MME, and three widely used pre-training sets: LAION-100M (Schuhmann et al., 2021), CC3M (Changpinyo et al., 2021), COCO-Caption (Chen et al., 2015). The results reveal that each evaluation benchmark presents certain contamination rates across training datasets of various sizes, even with some reaching as high as 84.46% (image-only) and 33.13% (image-text). Note that the actual size of pre-training data far exceeds our detected maximum of 100M, which indicates that the actual contamination issue could be even more severe.",
            "score": 0.5164136075330366,
            "section_title": "REVISITING DATA CONTAMINATION IN EXISTING BENCHMARKS",
            "char_start_offset": 6888,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1109
                },
                {
                    "start": 1112,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1778
                }
            ],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 256,
                    "matchedPaperCorpusId": "233296711"
                },
                {
                    "start": 1340,
                    "end": 1365,
                    "matchedPaperCorpusId": "231951742"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71435546875
        },
        {
            "corpus_id": "268819579",
            "title": "A Comprehensive Survey of Contamination Detection Methods in Large Language Models",
            "text": "Other studies analyze overlap between the training split and the evaluation or test split of the same dataset to assess contamination levels. While standard practice in machine learning assumes that training, validation and test splits of the same dataset are disjoint sets sampled from the same distribution, in practice, for very large datasets it may happen that the training set still overlaps with the other splits. Dodge et al. (2021) investigate the C4 (Raffel et al., 2020) dataset and study benchmark data contamination, which measures how much training or test datasets from downstream NLP tasks appear in the C4 pre-training corpus. \n\nAfter studying both input-label and input contamination, they discover varied contamination, ranging from less than 2% to over 50%. Li (2023b) compute the METEOR (Banerjee & Lavie, 2005) score between matched pages from CommonCrawl and queries from Bing API and consider those with scores over 0.75 as contaminated. Detected contamination levels range from 1% on Winogrande (Sakaguchi et al., 2021) to 47% on C-Eval (Huang et al., 2023). Deng et al. (2023) propose to detect contamination by retrieving top-10 documents from pre-training datasets The Pile (Gao et al., 2020) or C4 (Raffel et al., 2020), then splitting retrieved documents into 13-gram chunks, and computing overlap metrics between these chunks and the evaluation data point's chunks. TruthfulQA (Lin et al., 2021) exhibits high overlap with the pre-training datasets.",
            "score": 0.5154593002733556,
            "section_title": "Dataset-Level String Matching",
            "char_start_offset": 12538,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 643
                },
                {
                    "start": 646,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1480
                }
            ],
            "ref_mentions": [
                {
                    "start": 460,
                    "end": 481,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 808,
                    "end": 832,
                    "matchedPaperCorpusId": "7164502"
                },
                {
                    "start": 1020,
                    "end": 1044,
                    "matchedPaperCorpusId": "199370376"
                },
                {
                    "start": 1062,
                    "end": 1082,
                    "matchedPaperCorpusId": "258685666"
                },
                {
                    "start": 1227,
                    "end": 1248,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9150390625
        },
        {
            "corpus_id": "277940370",
            "title": "STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings",
            "text": "Given how large parts of publicly available text are crawled to pretrain large language models (LLMs), data creators increasingly worry about the inclusion of their proprietary data for model training without attribution or licensing. Their concerns are also shared by benchmark curators whose test-sets might be compromised. In this paper, we present STAMP, a framework for detecting dataset membership-i.e., determining the inclusion of a dataset in the pretraining corpora of LLMs. Given an original piece of content, our proposal involves first generating multiple rephrases, each embedding a watermark with a unique secret key. One version is to be released publicly, while others are to be kept private. Subsequently, creators can compare model likelihoods between public and private versions using paired statistical tests to prove membership. We show that our framework can successfully detect contamination across four benchmarks which appear only once in the training data and constitute less than 0.001% of the total tokens, outperforming several contamination detection and dataset inference baselines. We verify that STAMP preserves both the semantic meaning and the utility of the original data in comparing different models. We apply STAMP to two real-world scenarios to confirm the inclusion of paper abstracts and blog articles in the pretraining corpora.",
            "score": 0.5152441020817581,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "266933004",
            "title": "Investigating Data Contamination for Pre-training Language Models",
            "text": "detection can be evaded through paraphrasing [36]). Other recent work has sought to identify pre-training data contamination heuristically by examining the likelihoods of texts after changing their ordering [28] and of least probable tokens [31]. Nevertheless, these methods are similarly inadequate for detecting textual transformations (e.g. paraphrasing) and the heuristic nature of these methods may limit them from providing a clear understanding of how data contamination impacts the model performance on the pre-training level, highlighting a need for more comprehensive methods in this area of research.",
            "score": 0.5152441020817581,
            "section_title": "Related Work",
            "char_start_offset": 25372,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 611
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9189453125
        },
        {
            "corpus_id": "273350935",
            "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
            "text": "Thus, it is important to assess the leakage of benchmark data into pretraining data (Zhou et al., 2023). On the one hand, model developers can remove evaluation benchmark data from training data by retrieval-based methods with access to pertaining data (Ravaut et al., 2024;Chowdhery et al., 2023). Specifically, those methods employ n-gram tokenization and string-matching for detecting data contamination (Brown et al., 2020;Touvron et al., 2023b;Team et al., 2023;Radford et al., 2019). On the other hand, researchers utilize prompting techniques (Golchin & Surdeanu, 2024), performance analysis (Ye et al., 2024;Debenedetti et al., 2024), model likelihood (Oren et al., 2024;Shi et al., 2024;Xu et al., 2024b) to detect potential contamination without access to the training data. Our work focuses on pretraining data detection, an area that is similar to data contamination. Different from data contamination detection, our FSD can also be applied to the detection of copyrighted resources in real-world scenarios.",
            "score": 0.5152441020817581,
            "section_title": "A RELATED WORK",
            "char_start_offset": 30092,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1019
                }
            ],
            "ref_mentions": [
                {
                    "start": 274,
                    "end": 297,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 407,
                    "end": 427,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 467,
                    "end": 488,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 550,
                    "end": 576,
                    "matchedPaperCorpusId": "260925501"
                },
                {
                    "start": 616,
                    "end": 641,
                    "matchedPaperCorpusId": "261697333"
                },
                {
                    "start": 660,
                    "end": 679,
                    "matchedPaperCorpusId": "264490730"
                },
                {
                    "start": 679,
                    "end": 696,
                    "matchedPaperCorpusId": "264451585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95361328125
        },
        {
            "corpus_id": "271957208",
            "title": "LogProber: Disentangling confidence from contamination in LLM responses",
            "text": "Large language models (LLMs) are deep-learning systems trained on huge corpora or textual data, which size and complexity makes impossible to predict ex ante the extent and depth of their capabilities (Brown et al., 2020a). The situation is made even more complex by the fact that their competences span quite different domains, ranging from content creation to translation and coding. This is why there has been a proliferation of studies proposing new benchmarks and tools aimed at assessing their capabilities (Hendrycks et al., 2021;Srivastava et al., 2023). \n\nMost of these evaluation methods rely on asking questions to the LLM and evaluating whether the resulting answers are correct. Crucially, this method is valid if, and only if, the model was not trained on the same material used in the benchmark items. This is particularly true when the benchmark aims at assessing the capabilities of the model to solve a particular class of problems (i.e., its cognitive abilities) and not its capacity to retrieve factually accurate information (i.e., the accuracy of its knowledge). In machine learning, this situation where part of the testing set is leaked in the training phase is referred to as \"contamination\". When contamination occurs, a model's performance in a given task may not reflect the true capability of a model in a given domain, but rather its capacity to retrieve training material. \n\nThis contamination issue is particularly relevant in a field where models are trained on such a vast amount of text, where it is very difficult to check the presence of benchmark items in the training set (Brown et al., 2020b;Zhou et al., 2023). Accordingly, several recent models showing impressive scores on benchmarks have been suspected of contamination on these specific benchmarks, but this criticism remains unclear due to the lack of transparency about the training details of these models (Gunasekar et al., 2023;Li et al., 2023;Jiang et al., 2024;Balloccu et al., 2024).",
            "score": 0.5144272266468426,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1403
                },
                {
                    "start": 1406,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1986
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 222,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1611,
                    "end": 1632,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.587890625
        },
        {
            "corpus_id": "270285708",
            "title": "Benchmark Data Contamination of Large Language Models: A Survey",
            "text": "In each case, contamination can potentially lead to distorted performance scores that do not accurately reflect the model's true capabilities.The severity of BDC, as categorized into Semantic Level, Information Level, Data Level, and Label Level, increases as we move closer to full exposure of the benchmark data.The complexity of detecting and mitigating BDC inversely correlates with the severity of exposure, making it a challenging problem to address.The primary sources of BDC are the large-scale pre-training datasets used in training LLMs, which due to their diversity and complexity, can inadvertently introduce the risk of BDC.Highlighting the potential scenarios of BDC occurrence across seven prevalent LLM tasks underscores the critical necessity of addressing this issue for precise model evaluation and performance enhancement in the domain of NLP. 3 BDC DETECTION TECHNIQUES Efficient identification of BDC in evaluation benchmarks represents a fundamental aspect in ensuring the reliability and integrity of LLMs, while also providing the basis for developing effective mitigation strategies.This section provides a comprehensive review of literature related to the detection of BDC.We categorize the methodologies into two distinct strategies: matching-based and comparison-based, discussed in Sections 3.1 and 3.2, respectively.Note that certain investigations incorporate elements from both strategies.Such instances are allocated to the category deemed more comprehensive or preferred by the authors in question.All reviewed work on BDC detection is summarized in Table 2.",
            "score": 0.5135352967828853,
            "section_title": "Related Tasks",
            "char_start_offset": 23354,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 142,
                    "end": 314
                },
                {
                    "start": 314,
                    "end": 456
                },
                {
                    "start": 456,
                    "end": 637
                },
                {
                    "start": 637,
                    "end": 1109
                },
                {
                    "start": 1109,
                    "end": 1200
                },
                {
                    "start": 1200,
                    "end": 1347
                },
                {
                    "start": 1347,
                    "end": 1422
                },
                {
                    "start": 1422,
                    "end": 1533
                },
                {
                    "start": 1533,
                    "end": 1593
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96435546875
        },
        {
            "corpus_id": "273350935",
            "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
            "text": "In this paper, we introduce Fine-tuned Score Deviation (FSD), a novel detection method that can universally improve the performance of existing detection methods. To the best of our knowledge, our method is the first to utilize some collected non-members in the task of pretraining data detection. Our core idea behind FSD is to enlarge the gap between seen examples and unseen examples by exposing the LLM to a few unseen examples. In effect, unseen data have a larger score than seen examples when using FSD, which makes it more distinguishable between seen and unseen data. \n\nExtensive experiments demonstrate the effectiveness of our method for detecting pretraining data on common benchmark datasets across various models. In summary, the FSD is an effective approach for accurately detecting pretraining data of LLMs. \n\nLimitations Our method requires to collect a few examples that belong to the same domain but are not involved in the training. Generally, we can utilize the data content published after the release of the LLM. Therefore, our method is applicable for detecting benchmarks or copyrighted resources in a specific domain (e.g., math tests, magazines). The diversity of the test set may make it challenging to construct an effective auxiliary dataset of unseen data. In addition, our method requires finetuning on a few non-member data, so the effectiveness of the proposed score might be affected by the data quality of non-members. \n\nEthical Statement Our work focuses on pretraining data detection from large language models. The proposed methodology aims to address issues involving data contamination or copyright infringement. In addition, our method can be used to identify potential privacy leakage risks and ensure the safety of LLMs, aligning with established ethical standards for content moderation. Regarding data access, the evaluated datasets we employed in our work come from prior research and do not involve personal privacy information.",
            "score": 0.5107691130524555,
            "section_title": "CONCLUSION",
            "char_start_offset": 26313,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1454
                },
                {
                    "start": 1457,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1976
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "275757817",
            "title": "A Collection of Question Answering Datasets for Norwegian",
            "text": "The increasing volume of web data for pretraining LMs presents a potential challenge for evaluation. Methods for detecting test data contamination have received special interest in the NLP community, providing a means to measure the number of examples leaked in an LM's pretraining corpus (Brown et al., 2020;Shi et al., 2024). Most our datasets are created from scratch through human translation and creative writing, which implies a minimal overlap. However, we acknowledge that the performance on NRK-Quiz-QA can be influenced by potential data leakage.",
            "score": 0.5101122198941723,
            "section_title": "Data Contamination",
            "char_start_offset": 17316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 556
                }
            ],
            "ref_mentions": [
                {
                    "start": 309,
                    "end": 326,
                    "matchedPaperCorpusId": "264451585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52197265625
        },
        {
            "corpus_id": "274776100",
            "title": "QUENCH: Measuring the gap between Indic and Non-Indic Contextual General Reasoning in LLMs",
            "text": "Due to the potential for benchmark leakage into pretraining corpora, we investigate several popular data sources to ensure our benchmark remains unaffected. \n\nPhase 1: Checking for presence of data sources in pretraining corpora Specifically, we examine youtube.com (YT) and donquizote.wordpress.com (DQ), which are primary sources using Elazar et al. (2024), to determine if data from these sources has been used in pretraining. We check for contamination in the C4 (Raffel et al., 2019), mC4-en (Chung et al., 2023), OSCAR (Abadji et al., 2022), RedPajama (Computer, 2023), LAION-2B-en (Schuhmann et al., 2022) and Dolma (Soldaini et al., 2024) datasets. \n\nWe observe a negligible presence of both data sources across all public pretraining corpora, with a maximum of around 22K tokens, and most instances below 1K tokens. Notably, older datasets like OSCAR and C4 show a slightly higher presence of DQ, with 23K tokens ( 0.0000046% of total tokens) and 9K tokens ( 0.00000032% of total tokens), respectively. In contrast, newer datasets such as Dolma contain only 40 tokens ( 0.00000091% of total tokens). Other datasets show no trace of the site. \n\nSimilarly, YT follows the same pattern. WIMBD's prefix search also picks up other URLs like youtube.comactivate.org, which share the \"youtube.com\" prefix, but their presence is minimal (<0.00000001%). The highest YT presence is found in mC4-en with 22.3K tokens ( 0.00000081% of total tokens), indicating that both sources have an insignificant presence in the pretraining corpora. \n\nPhase 2: Checking for exact match count in pretraining corpora",
            "score": 0.5101122198941723,
            "section_title": "A Data Leakage Analysis",
            "char_start_offset": 29245,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 159,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 656
                },
                {
                    "start": 659,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1150
                },
                {
                    "start": 1153,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1534
                },
                {
                    "start": 1537,
                    "end": 1599
                }
            ],
            "ref_mentions": [
                {
                    "start": 588,
                    "end": 612,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 623,
                    "end": 646,
                    "matchedPaperCorpusId": "267364861"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12176513671875
        },
        {
            "corpus_id": "271769484",
            "title": "Confounders in Instance Variation for the Analysis of Data Contamination",
            "text": "Several methods have been used to address data contamination. Prior to big tech companies close sourced their models and the training data, a common approach was trying to look for evaluation instances in the training data. String matching and embedding similarity are two techniques that have been commonly used for this purpose. Ope-nAI used 8-gram matching of test instances and training dataset for GPT-2 model (Radford et al., 2019). For GPT-3 (Brown et al., 2020) the same approach has been taken and all data points from the evaluation sets that had a 13-gram collision in the pre-training Common Crawl (C4) dataset were removed to tackle contamination. \n\nAs contamination can involve minor variations of the examples, calculating cosine similarity between embeddings of test and training items can also be used for finding cases in which the test item has been rephrased or expressed in a different language (Gunasekar et al., 2023) (Riddell et al., 2024). \n\nBut string matching and even embedding matching are not able detect rephrased test items effectively in general (Yang et al., 2023). More sophisticated and effective techniques employ embedding similarity search to identify the top-k samples similar to a given test sample and then prompting a powerful LLM such as GPT-4 to determine if any of the k samples are too similar to the test case. \n\nFor closed source models where no information regarding the training set is provided, none of the above mentioned methods are applicable. Introducing new contamination-free benchmarks such as LastEval (Li, 2023), WIKIMIA (Shi et al., 2023), KIEval (Yu et al., 2024), LiveCodeBench (Jain et al., 2024), Termite (Ranaldi et al., 2024) might seem a reliable solution for the problem, but as (Balloccu et al., 2024) mentioned, these new benchmarks can get contaminated as soon as they are publicly available or even just when used for evaluating closed source models by the creators of the benchmark themselves for the first time. In addition, building a high quality benchmark is a time consuming process and can not be done overnight.",
            "score": 0.5100315543735816,
            "section_title": "Background",
            "char_start_offset": 2897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 660
                },
                {
                    "start": 663,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2093
                }
            ],
            "ref_mentions": [
                {
                    "start": 415,
                    "end": 437,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 449,
                    "end": 469,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8466796875
        },
        {
            "corpus_id": "269761390",
            "title": "Many-Shot Regurgitation (MSR) Prompting",
            "text": "In the language modeling domain, most MIA research focuses on the fine-tuning stage [12,11,10] rather than pretraining.[14] demonstrated that language models can memorize verbatim content from training data, but their method requires manually inspecting model outputs.Subsequent work has explored more automated approaches.[9] studied membership inference on embedding models, showing that they can leak sensitive attributes.[31] proposed a method for auditing embedding models' training data using gradient updates.For auto-regressive language models, [14] introduced the exposure metric to measure data leakage but note that Monte Carlo estimation of this metric is challenging for large models.[32] showed that deduplicating training data can mitigate privacy risks, but doing so for large web-scale corpora is difficult.Membership inference against large pre-trained language models poses unique challenges compared to conventional MIA settings, such as limited access to training data and model scale.Assumptions like having shadow models trained on the same data distribution often do not hold, as pretraining corpora tend to be proprietary [33,15].The sheer scale of pretraining, involving massive datasets and relatively few epochs, makes MIAs inherently more difficult.\n\nRecent work has started exploring pretraining data detection for large language models.[33] proposed the Min-K% method which uses the average log probability of low-probability tokens as a score for pretraining data detection.However, this approach relies on heuristics and access to token probabilities which are no longer available for models like GPT-3.[15] constructed the MIMIR benchmark and analyzed the challenges of pretraining data detection, noting the need for better methods.Most prior MIA methods assume some form of access to a model's internals such as logits, gradients, loss values or token probabilities [13,14,10,26].However, this poses challenges for studying deployed language models in real-world settings.For example, the GPT-3 API previously provided access to logits and token probabilities but this functionality has been deprecated [33,15], precluding the use of methods that rely on these values.",
            "score": 0.5100065702209772,
            "section_title": "Introduction",
            "char_start_offset": 7381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 119,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 323
                },
                {
                    "start": 323,
                    "end": 425
                },
                {
                    "start": 425,
                    "end": 516
                },
                {
                    "start": 516,
                    "end": 697
                },
                {
                    "start": 697,
                    "end": 824
                },
                {
                    "start": 824,
                    "end": 1006
                },
                {
                    "start": 1006,
                    "end": 1155
                },
                {
                    "start": 1155,
                    "end": 1278
                },
                {
                    "start": 1280,
                    "end": 1367
                },
                {
                    "start": 1367,
                    "end": 1506
                },
                {
                    "start": 1506,
                    "end": 1636
                },
                {
                    "start": 1636,
                    "end": 1767
                },
                {
                    "start": 1767,
                    "end": 1916
                },
                {
                    "start": 1916,
                    "end": 2008
                },
                {
                    "start": 2008,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 91,
                    "matchedPaperCorpusId": "256461422"
                },
                {
                    "start": 119,
                    "end": 123,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 323,
                    "end": 326,
                    "matchedPaperCorpusId": "214743021"
                },
                {
                    "start": 425,
                    "end": 429,
                    "matchedPaperCorpusId": "220938739"
                },
                {
                    "start": 553,
                    "end": 557,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 697,
                    "end": 701,
                    "matchedPaperCorpusId": "246823128"
                },
                {
                    "start": 1902,
                    "end": 1906,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1906,
                    "end": 1909,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1912,
                    "end": 1915,
                    "matchedPaperCorpusId": "258685658"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68359375
        },
        {
            "corpus_id": "273350935",
            "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
            "text": "Models We conduct extensive experiments on diverse open-sourced LLMs. For the main results, we use LLaMA-7B (Touvron et al., 2023a) as the LLM throughout our experiments. We also provide experiments on other models including Pythia-6.9B (Biderman et al., 2023), GPT-J-6B (Wang & Komatsuzaki, 2021), OPT-6.7B (Zhang et al., 2022), LLaMA-13B models (Touvron et al., 2023a), LLaMA-30B (Touvron et al., 2023a), and NeoX-20B (Black et al., 2022). Existing works (Shi et al., 2024;Ye et al., 2024) generally use these models as LLMs for performing the studies of pretraining data detection. The models are provided by Hugging Face2 . \n\nDatasets To verify the effectiveness of detection methods, we employ four common benchmark datasets for evaluations, including WikiMIA (Shi et al., 2024) (Shi et al., 2024) and Pile (Maini et al., 2024). Previous works have demonstrated that model developers commonly use text content among those datasets for pre-training (Shi et al., 2024;Duarte et al., 2024;Ye et al., 2024). The datasets are provided by Hugging Face3 , and detailed information of datasets is presented in Appendix B. \n\nBaseline methods We use four detection methods based on scoring functions as baselines to evaluate the performance of different methods across various datasets and models. Those methods employ specific metrics based on the likelihood, followed by a comparison with a preset threshold to identify the membership of the given text.",
            "score": 0.5095200533344852,
            "section_title": "EXPERIMENTAL SETUP",
            "char_start_offset": 14654,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1118
                },
                {
                    "start": 1121,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1450
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 260,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 420,
                    "end": 440,
                    "matchedPaperCorpusId": "248177957"
                },
                {
                    "start": 457,
                    "end": 475,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 765,
                    "end": 783,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 784,
                    "end": 802,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 953,
                    "end": 971,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 971,
                    "end": 991,
                    "matchedPaperCorpusId": "267681760"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71044921875
        },
        {
            "corpus_id": "268201479",
            "title": "Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs",
            "text": "Large Language Models (LLMs) have become increasingly popular due to their impressive performance in various tasks and domains, often surpassing human capabilities. However, it is crucial for LLMs to not only excel in their tasks but also be grounded in factual information and avoid generating harmful content. Therefore, evaluating and understanding the limitations and capabilities of LLMs has become an important topic of research. \n\nLLMs are commonly evaluated through automated techniques like benchmarking, or by human interaction and feedback. Holistic evaluation of LLMs [26] evaluates LLMs on several dimensions such as accuracy, calibration, bias and toxicity. Several popular benchmarks have been proposed such as BigBench [39] to increase the task and language coverage of benchmarking. However, benchmarking has a number of limitations. For instance, the tasks and datasets provided in standard open source benchmarks may not always be applicable in real-world scenarios. Additionally, certain benchmarks may be of low quality due to inadequate or biased instructions [30] or limited training of annotators [14]. Moreover, some benchmarks may contain unintended artifacts introduced by annotators' subconscious biases [31]. Nevertheless, in spite of these constraints, benchmarking remains a fast, cost-effective, and replicable means of comparing multiple models or versions of a single model. As such, benchmarking has become the de-facto standard for evaluating the capabilities of LLMs, showing superiority of one LLM over another and deciding which version of a model to ship in production. \n\nHowever, the issue of test dataset contamination or leakage has raised significant concerns about the reliability of benchmarking and the validity of its outcomes. Benchmark datasets are commonly found on the internet, allowing large web crawlers that collect LLM pretraining data to potentially incorporate them into pretraining data. Another concerning issue, as pointed out by [6], is that LLM researchers could have inadvertently leaked benchmark datasets to closed source models during LLM evaluation, and this data can be used for fine-tuning or prompt optimization. According to estimates based on their survey, over 4 million data points have been leaked to closed models such as ChatGPT.",
            "score": 0.5091887239602373,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1609
                },
                {
                    "start": 1612,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2184
                },
                {
                    "start": 2185,
                    "end": 2308
                }
            ],
            "ref_mentions": [
                {
                    "start": 735,
                    "end": 739,
                    "matchedPaperCorpusId": "263625818"
                },
                {
                    "start": 1082,
                    "end": 1086,
                    "matchedPaperCorpusId": "248496232"
                },
                {
                    "start": 1121,
                    "end": 1125,
                    "matchedPaperCorpusId": "201124736"
                },
                {
                    "start": 1232,
                    "end": 1236,
                    "matchedPaperCorpusId": "21382535"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.255859375
        },
        {
            "corpus_id": "270619707",
            "title": "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation",
            "text": "We show the overview of our framework in Detection First, we conduct contamination detection on each evaluation sample based on the LLM to be evaluated.This allows the original dataset to be splitted into two parts: uncontaminated and potentially contaminated.Notably, according to our task definition, the split should ideally be based on whether the model's response is entirely from memorization.However, since memorized data is a subset of seen data, we follow Shi et al. (2023) to use the training data detection method MinKProb to substitute \u0109.The detecting method is an approximation of a contamination indicator function c : E \u2192 {0, 1}.Since memorized data is a subset of pre-training data, the training data detection method is an alternative of \u0109.\n\nThe goal of MinKProb is to determine whether a text X appears in the pre-training data of an LLM.MinKProb obtains the probability P (x i ) for each token x i in the text X and selects the K tokens with the lowest probabilities {x i 1 , x i 2 , . . ., x i k } and calculates the average of these probabilities:\n\nwhere P (x i j ) is the probability of the j-th lowest probability token x i j , with a threshold \u03f5 to determine whether X appears as pre-training data.\n\nRewrite A suitable rewriting method involves rewriting by skilled individuals, based on the original questions according to some clear rules and instructs, similar to the construction method of GSM1k (Zhang et al., 2024).However, this expensive approach creates a new dataset rather than sticking to the original one, which violates the principle of revival.We propose an automated generation method to rewrite the potentially contaminated parts identified in the detection stage while not altering the level of challenge.We explore rewriting methods for two of the most typical and popular tasks: mathematical reasoning and knowledgebased datasets.\n\nFor mathematical reasoning problems, such as GSM8K, we redesign problem scenarios based on the original problem's computational logic and answer structure.This ensures that while maintaining the difficulty and answers, the problems become more diverse.",
            "score": 0.5091369720162712,
            "section_title": "Inference-Time Decontamination",
            "char_start_offset": 8060,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 152,
                    "end": 260
                },
                {
                    "start": 260,
                    "end": 399
                },
                {
                    "start": 399,
                    "end": 550
                },
                {
                    "start": 550,
                    "end": 644
                },
                {
                    "start": 644,
                    "end": 757
                },
                {
                    "start": 759,
                    "end": 856
                },
                {
                    "start": 856,
                    "end": 1008
                },
                {
                    "start": 1008,
                    "end": 1068
                },
                {
                    "start": 1070,
                    "end": 1222
                },
                {
                    "start": 1224,
                    "end": 1445
                },
                {
                    "start": 1445,
                    "end": 1582
                },
                {
                    "start": 1582,
                    "end": 1746
                },
                {
                    "start": 1746,
                    "end": 1873
                },
                {
                    "start": 1875,
                    "end": 2030
                },
                {
                    "start": 2030,
                    "end": 2127
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7587890625
        },
        {
            "corpus_id": "267897557",
            "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models",
            "text": "Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered\"interactor\"role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KIEval's effectiveness and generalization. We also reveal that data contamination brings no contribution or even negative effect to models' real-world applicability and understanding, and existing contamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning.",
            "score": 0.5081081936649238,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.859375
        },
        {
            "corpus_id": "271570943",
            "title": "Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens",
            "text": "While large language models (LLMs) are extensively used, there are raising concerns regarding privacy, security, and copyright due to their opaque training data, which brings the problem of detecting pre-training data on the table. Current solutions to this problem leverage techniques explored in machine learning privacy such as Membership Inference Attacks (MIAs), which heavily depend on LLMs' capability of verbatim memorization. However, this reliance presents challenges, especially given the vast amount of training data and the restricted number of effective training epochs. In this paper, we propose an adaptive pre-training data detection method which alleviates this reliance and effectively amplify the identification. Our method adaptively locates \\textit{surprising tokens} of the input. A token is surprising to a LLM if the prediction on the token is\"certain but wrong\", which refers to low Shannon entropy of the probability distribution and low probability of the ground truth token at the same time. By using the prediction probability of surprising tokens to measure \\textit{surprising}, the detection method is achieved based on the simple hypothesis that seeing seen data is less surprising for the model compared with seeing unseen data. The method can be applied without any access to the the pre-training data corpus or additional training like reference models. Our approach exhibits a consistent enhancement compared to existing methods in diverse experiments conducted on various benchmarks and models, achieving a maximum improvement of 29.5\\%. We also introduce a new benchmark Dolma-Book developed upon a novel framework, which employs book data collected both before and after model training to provide further evaluation.",
            "score": 0.506400029277338,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93603515625
        },
        {
            "corpus_id": "276079893",
            "title": "Overestimation in LLM Evaluation: A Controlled Large-Scale Study on Data Contamination's Impact on Machine Translation",
            "text": "Data contamination -- the accidental consumption of evaluation examples within the pre-training data -- can undermine the validity of evaluation benchmarks. In this paper, we present a rigorous analysis of the effects of contamination on language models at 1B and 8B scales on the machine translation task. Starting from a carefully decontaminated train-test split, we systematically introduce contamination at various stages, scales, and data formats to isolate its effect and measure its impact on performance metrics. Our experiments reveal that contamination with both source and target substantially inflates BLEU scores, and this inflation is 2.5 times larger (up to 30 BLEU points) for 8B compared to 1B models. In contrast, source-only and target-only contamination generally produce smaller, less consistent over-estimations. Finally, we study how the temporal distribution and frequency of contaminated samples influence performance over-estimation across languages with varying degrees of data resources.",
            "score": 0.5054098291128595,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75
        },
        {
            "corpus_id": "265050721",
            "title": "Rethinking Benchmark and Contamination for Language Models with Rephrased Samples",
            "text": "In this work, we study benchmark contamination in the context of large language models and evaluate existing decontamination methods. We show that existing detection methods can not detect test cases with simple variations. We demonstrate that if such variation of test data is not eliminated, a 13B model can easily overfit the test benchmark and achieve drastically high performance. To address this, we propose a new detection method LLM decontaminator. We apply it to real-world datasets and reveal previously unknown test overlap. We urge the community to adopt stronger decontamination approaches when using public benchmarks. We call for the community to actively develop fresh one-time exams to accurately evaluate LLMs.",
            "score": 0.5038400715902778,
            "section_title": "Conclusion",
            "char_start_offset": 24819,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 728
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6982421875
        },
        {
            "corpus_id": "264555419",
            "title": "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark",
            "text": "Despite most of the recent popular models like LLaMA (Touvron et al., 2023a), GPT-4 (Ope-nAI, 2023) or Bard have not publicly released their pre-training data, very few works have actually worked on detecting data-contamination when the pre-training data is not available (Magar and Schwartz, 2022). Although this scenario is much more challenging than the former, we foresee that it will become the most prevalent. Developing methods to measure the data contamination in this scenario must be crucial for future evaluations. To tackle this problem, we propose to take advantage of LLM's memorization capabilities. Appendix A shows some examples of using memorization to uncover data contamination for the CONLL2003 benchmark on three LLMs. In cases where the LLM does not produce the benchmark verbatim, it is left to the auditor to examine the output and judge whether the evidence supports contamination. The process is totally manual and could be scaled in a community effort. \n\nAlternatively, automatic metrics for measuring data contamination levels could be developed. As an initial step in this direction, we reuse and adapt the extractability definition presented in Carlini et al. ( 2023) for defining memorization. We define that an example s is extractable from evaluation dataset d and model m if there exists a sequence of k examples x immediately preceding s in d data such that s is generated when prompting model m with x. We can define the degree of contamination of model m for dataset d as the ratio of extractable examples with respect to the total number of examples in the dataset. \n\nOne further question remains to be solved which is whether the lack of memorization of a benchmark ensures that the LLM was not trained on that benchmark. One hypothesis could be that the lack of memorization is correlated with the performance, even if the LLM was trained on the benchmark. Thus the LLM would not have any advantage with respect to another LLM that was not trained on the benchmark. This is currently speculation, so further research on this topic is necessary, given the extended use of closed LLMs in NLP research.",
            "score": 0.5036173222514381,
            "section_title": "Closed LLMs",
            "char_start_offset": 13816,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 980
                },
                {
                    "start": 983,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1604
                },
                {
                    "start": 1607,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 272,
                    "end": 298,
                    "matchedPaperCorpusId": "247475929"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5966796875
        },
        {
            "corpus_id": "271089080",
            "title": "Training on the Test Task Confounds Evaluation and Emergence",
            "text": "However, detecting and preventing data contamination is currently an open problem (Gunasekar et al., 2023;Yang et al., 2023b;Golchin and Surdeanu, 2023). Roberts et al. (2023) and Li and Flanigan (2024) find that models often perform better on datasets that were publicly available during model training. While almost all models that we consider were released after MMLU and GSM8K, we nonetheless find that, controlling for compute, more recent models perform better. These performance gains are unlikely to be driven solely by test set leakage and require additional explanation. In Section 3.2, we find evidence that that training on the test task may be a more dominant factor in benchmark performance than data contamination. A key insight of our work is that models can train on the test task (e.g., multiple-choice question answering) and do much better at a given benchmark (e.g., MMLU) without necessarily seeing any benchmark data. This defies traditional notions of data contamination, which strictly refer to the leakage of benchmark data (Magar and Schwartz, 2022;Sainz et al., 2023;Dong et al., 2024). Moreover, whereas the core concern with data leakage is that benchmarks may inadvertently find their way to the training data, training on the test task is often a deliberate design choice (e.g., pre-training on instruction data). \n\nAdaptation prior to evaluation. In the 2010s, language models had to be adapted to different benchmarks using supervised task data (Collobert et al., 2011;Dai and Le, 2015;Devlin et al., 2019). The purpose of such fine-tuned benchmark models was solely to facilitate the relative comparison of base models. Importantly, all models were adapted using the same supervised data (Collobert et al., 2011). With GPT-3 (Brown et al., 2020), few-shot prompting emerged as the dominant paradigm for adapting models to a particular task prior to evaluation (Liang et al., 2023), arguably due to its simplicity relative to fine-tuning.",
            "score": 0.5029718646590514,
            "section_title": "Related work",
            "char_start_offset": 33251,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1345
                },
                {
                    "start": 1348,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1972
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 202,
                    "matchedPaperCorpusId": "266573740"
                },
                {
                    "start": 1479,
                    "end": 1503,
                    "matchedPaperCorpusId": "351666"
                },
                {
                    "start": 1503,
                    "end": 1520,
                    "matchedPaperCorpusId": "7138078"
                },
                {
                    "start": 1520,
                    "end": 1540,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1723,
                    "end": 1747,
                    "matchedPaperCorpusId": "351666"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29248046875
        },
        {
            "corpus_id": "276618033",
            "title": "Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models",
            "text": "Datasets. We focus on two datasets commonly used in the task of MIAs against pre-trained LLMs: WikiMIA [59] and MIMIR [15]. WikiMIA is the first dataset designed for evaluating pre-training data detection, consisting of events from Wikipedia. According to the event dates and the target model's release date, texts can be automatically classified as members or non-members. MIMIR is built upon the Pile dataset [19] by Duan et al. [15], where training samples and non-training samples are drawn from its training and test split, respectively. Note that Duan et al. argue that WikiMIA has distribution shifts and temporal discrepancies between members and nonmembers, which differs from the classical MIA game [78]. However, we believe that distinguishing between members and temporally shifted non-members has practical implications in the context of MIAs against pre-trained LLMs. Therefore, following previous work [59,75,79], we report results on both benchmarks to ensure experimental integrity.",
            "score": 0.5020414383426004,
            "section_title": "Attack Setup",
            "char_start_offset": 26880,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 999
                }
            ],
            "ref_mentions": [
                {
                    "start": 709,
                    "end": 713,
                    "matchedPaperCorpusId": "2656445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55419921875
        },
        {
            "corpus_id": "266362809",
            "title": "LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction",
            "text": "Recent years have seen the ubiquity of pretrained language models in natural language processing (NLP) due to their strong performance and generalisation capability. These models are usually pre-trained on super large internetcrawled corpora. However, many widely used benchmarks are also largely constructed from web resources (Hendrycks et al. 2020), which are very likely to be unintentionally included in the pretraining stage. This leads to a major emerging issue called data contamination. \n\nRecent analysis has revealed that data contamination is widespread in model evaluations (Achiam et al. 2023;Sainz et al. 2023), which greatly undermines the credibility of evaluation results (Marie 2023; Li, Guerin, and Lin 2023) and prevents fair comparisons between models (Dickson 2023). Moreover, the massive scale of training data makes decontaminating existing benchmarks extremely difficult (Kreutzer et al. 2022). For many closed models, training data is considered as trade secret and thus confidential, eliminating any possibility for the community to address contamination by decontaminating benchmarks. One potential solution to avoid contaminated evaluation is to create new test data constantly or use human evaluation (Liu, Zhang, and Liang 2023;Jacovi et al. 2023), just like how examination for human works. However, this is extremely inefficient and costly, requiring huge human efforts periodically. \n\nIn this paper, we propose LatestEval, an automatic method that leverages the most recent texts to create novel uncontaminated reading comprehension evaluations. Before starting to create the data, we conduct the first manual analysis of real-world Human-AI chat data on document comprehension, to identify the most frequently asked types of knowledge and thus determine the scope of LatestEval. As for data construction, LatestEval, as a reading comprehension benchmark, typically consists of three components: passage, query, and answer. Here we start by 1) collecting recently created texts across various time-sensitive sources as the passages. Three different sources are used in our experiments: arXiv, BBC, and GitHub. We then 2) extract key information from these texts using various tools as the answers. At last we 3) construct questions targeting the extracted information.",
            "score": 0.5017911214076891,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 495
                },
                {
                    "start": 498,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1416
                },
                {
                    "start": 1419,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2231
                },
                {
                    "start": 2232,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 896,
                    "end": 918,
                    "matchedPaperCorpusId": "258741333"
                },
                {
                    "start": 1237,
                    "end": 1259,
                    "matchedPaperCorpusId": "267365109"
                },
                {
                    "start": 1259,
                    "end": 1278,
                    "matchedPaperCorpusId": "258741333"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6982421875
        },
        {
            "corpus_id": "273350935",
            "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
            "text": "Recent works indicate evaluation results are suspect on benchmark datasets because they possibly sample members and non-members from different distributions (Duan et al., 2024;Das et al., 2024;Maini et al., 2024). We find the temporal shift between members and non-members in the WikiMIA dataset, which is shown in Appendix C.4. The issue shows that we may distinguish members and non-members with timestamps in the dataset. To eliminate the impact of temporal differences between members and non-members on evaluation, we implement two strategies to mitigate the temporal shift in the dataset: (1) removing timestamps in the dataset (Deletion), and (2) replacing the year of timestamps with 2023 in the dataset (Replacement). We conduct experiments with baselines and our method on the original WikiMIA dataset, Deletion and Replacement, respectively. \n\nOur results in Table 6 show that our method also improves the performance of baseline methods when mitigating the temporal shift between members and non-members. In this setting, the performance of baselines and our method is reduced, as deleting or replacing a word will change the probability of the subsequent word, thereby perturbing the likelihood-based metric. Though baseline methods yield results comparable to random guessing on the Replacement dataset, our method improves the AUC scores of the perplexity-based method, increasing it from 0.54 to 0.71 on Replace- Is FSD effective with different fine-tuning methods? To expose LLMs to unseen data, we employ LoRA to fine-tune the pre-trained model. The results demonstrate that our method achieves impressive performance for pretraining data detection when fine-tuning with LoRA. However, can a similar effect be achieved using different fine-tuning methods? To this end, we investigate the impact of fine-tuning methods on performance by applying AdaLoRA (Zhang et al., 2023), IA3 (Liu et al., 2022), and LoRA to fine-tune LLaMA-7B with WikiMIA, respectively. \n\nIn Table 7, we report the AUC and TPR@5%FPR scores for pretraining data detection for our method and baselines.",
            "score": 0.5016475911421802,
            "section_title": "DISCUSSION",
            "char_start_offset": 23566,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 852
                },
                {
                    "start": 855,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1975
                },
                {
                    "start": 1978,
                    "end": 2089
                }
            ],
            "ref_mentions": [
                {
                    "start": 1897,
                    "end": 1915,
                    "matchedPaperCorpusId": "248693283"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.744140625
        },
        {
            "corpus_id": "272367930",
            "title": "Training on the Benchmark Is Not All You Need",
            "text": "Our goal is to identify whether the pre-training process of a language model \u03b8 includes a particular piece of data x from a benchmark test set, or the extent to which that benchmark test set D leaks to the model \u03b8. Detection in our setup is under gray-box conditions, i.e., the pre-training corpus and parameters of the model are unknown. We consider two scenarios: (a) where the order in which the pre-trained data options are presented is not shuffled, and (b) where the sequence of pre-trained data options may be shuffled.",
            "score": 0.5000234060481228,
            "section_title": "Methodology",
            "char_start_offset": 7841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 526
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55029296875
        },
        {
            "corpus_id": "269043178",
            "title": "From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples",
            "text": "The very large datasets that are used to train contemporary large language models (LLMs) raise concerns about potential (accidental or not) contamination Sainz et al. (2023); Golchin & Surdeanu (2024). In our study, we have attempted to mitigate this as follows. First, we used many different random seeds. However, this does not nullify the risk that the LLM has seen similar data (e.g., data from Friedman #1, but with other random seeds). To mitigate this, we explored the performance of the models on regression functions of our own creation. This makes it unlikely that the model has seen data coming from the exact same function. For example, across all our newly introduced datasets (e.g., Original #1, Original #2, Original #3, Original #4, Original #5), Claude 3 Opus obtains the highest average rank of 6.4. Second place is Clade 3 Sonnet, with an average rank of 6.8, then Gradient Boosting with 8.4. Furthermore, our empirical evidence of consistent high performance across a diverse array of LLMs. \n\nTo further analyze the data contamination issue, we perform two additional experiments. We provide results with Falcon, an LLM whose training data is publicly available. Second, we perform an experiment similar to the approach proposed in Golchin & Surdeanu (2024), where we compare the performance of LLMs with and without knowing the dataset where the data comes from.",
            "score": 0.5000234060481228,
            "section_title": "N Could It Be Just Contamination?",
            "char_start_offset": 51787,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1010
                },
                {
                    "start": 1013,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1383
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 173,
                    "matchedPaperCorpusId": "264555419"
                },
                {
                    "start": 175,
                    "end": 200,
                    "matchedPaperCorpusId": "260925501"
                },
                {
                    "start": 1252,
                    "end": 1277,
                    "matchedPaperCorpusId": "260925501"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51123046875
        },
        {
            "corpus_id": "271244496",
            "title": "Questionable practices in machine learning",
            "text": "The use of poorly-filtered web-scale training corpuses has led to many cases of plausibly accidental contamination [Achiam et al., 2023, Reid et al., 2024, Li et al., 2024].We can see this with new versions of existing benchmarks, which generally exhibit substantial decreases in performance [Recht et al., 2019b, Zhang et al., 2024, Matton et al., 2024].We thus need to account for the very high prior probability of contamination in web corpuses and take suitable countermeasures (for instance looking for canary strings [Srivastava et al., 2023] or exact label matches), such that readers can trust the resulting evaluation results [Aiyappa et al., 2023].",
            "score": 0.5000234060481228,
            "section_title": "The fundamental tricks",
            "char_start_offset": 10991,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 173,
                    "end": 355
                },
                {
                    "start": 355,
                    "end": 658
                }
            ],
            "ref_mentions": [
                {
                    "start": 292,
                    "end": 312,
                    "matchedPaperCorpusId": "67855879"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25
        },
        {
            "corpus_id": "265019021",
            "title": "Don't Make Your LLM an Evaluation Benchmark Cheater",
            "text": "It would cause an unfair performance advantage when comparing different LLMs or assessing the ability level of some specific LLMs. As we discussed before, this issue tends to become increasingly more common as we try to collect more public text data for training. To investigate this issue, we set up several benchmark leakage settings that should be totally avoided during evaluation, including the leakage of training sets, test prompts, and test sets. Based on the three settings, we continually train four popular language models, ranging from 1.3B to 7B, and test the performance of the four models on a number of existing benchmarks. In addition, we also examine the potential risk of benchmark leakage on other abilities. \n\nThe experimental results reveal that benchmark leakage can lead to an unfair boost in the evaluation performance of LLMs. Smaller LLMs (e.g., a 1.3B model) can be deliberately elevated to outperform 10\u00d7 larger models on certain tasks. As a side effect, the performance of these specially trained LLMs on other normally tested tasks would likely be adversely affected if we fine-tune or train the model only with these leaked data. \n\nBy examining the potential risks of benchmark leakage, we would like to emphasize the importance of fair and appropriate evaluation for LLMs, and propose several suggestions to improve the evaluation for LLMs: \n\n\u2022 As general suggestions, more benchmarks from diverse sources, covering both basic ability (e.g., text generation) and advanced ability tests (e.g., complex reasoning), should be used for comprehensively estimating the capabilities of LLMs. \n\n\u2022 As suggestions for LLM developers, it is important to perform the data decontamination checking between pre-training data and any related data (e.g., training and test sets) when using evaluation benchmarks. In addition, it is also necessary to report the contamination analysis on the evaluated benchmarks as reference. We also suggest reporting the detailed composition of the pre-training data. \n\n\u2022 As suggestions for benchmark maintainers, we suggest that a diverse set of test prompts should be employed for reducing the influence of the prompt sensitivity. It is also meaningful to conduct the contamination analysis between the benchmark data and existing pretraining corpus, alerting any potential contamination risks.",
            "score": 0.5000021002483909,
            "section_title": "Introduction",
            "char_start_offset": 3808,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 728
                },
                {
                    "start": 731,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1161
                },
                {
                    "start": 1164,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2019
                },
                {
                    "start": 2022,
                    "end": 2184
                },
                {
                    "start": 2185,
                    "end": 2348
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5458984375
        },
        {
            "corpus_id": "269921283",
            "title": "Data Contamination Calibration for Black-box LLMs",
            "text": "The rapid advancements of Large Language Models (LLMs) tightly associate with the expansion of the training data size. However, the unchecked ultra-large-scale training sets introduce a series of potential risks like data contamination, i.e. the benchmark data is used for training. In this work, we propose a holistic method named Polarized Augment Calibration (PAC) along with a new to-be-released dataset to detect the contaminated data and diminish the contamination effect. PAC extends the popular MIA (Membership Inference Attack) -- from machine learning community -- by forming a more global target at detecting training data to Clarify invisible training data. As a pioneering work, PAC is very much plug-and-play that can be integrated with most (if not all) current white- and black-box LLMs. By extensive experiments, PAC outperforms existing methods by at least 4.5%, towards data contamination detection on more 4 dataset formats, with more than 10 base LLMs. Besides, our application in real-world scenarios highlights the prominent presence of contamination and related issues.",
            "score": 0.49956915950330727,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "266362809",
            "title": "LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction",
            "text": "First, we conduct a contamination test on common reading comprehension benchmarks to measure the extent that models memorise their test instances during pretraining. Although most existing analyses identify contamination by computing the overlapping between training and test sets, we cannot use the same approach as the pretraining data of current language models are mostly not publicly available. Therefore, we propose a novel method to quantify test contamination by observing whether models exhibit memorisation behaviour on test instances, which can serve as a strong signal of test contamination. Carlini et al. (2021Carlini et al. ( , 2022) ) have defined the \"memorisation\" of models that a sequence is considered as memorised if the model has considerably smaller perplexity on that sequence. The idea is that sequences leak in the training data will tend to have lower perplexity (i.e., higher likelihood) than sequence models never seen before. This is well demonstrated in Figure 2, where we compare the perplexity of two Wikipedia subsets: the first is Wikitext (Merity et al. 2016) which is widely used in language model pretraining, the second is NewWiki, latest Wikipedia texts that created after April 2023, after all tested models were released. We find that Wikipedia content learned during pretraining has a much lower perplexity than recently created Wikipedia content. Now we compute the perplexity of the validation set on the three existing reading comprehension benchmarks SQuADv2 (Rajpurkar, Jia, and Liang 2018), BoolQ (Clark et al. 2019), QuAC (Choi et al. 2018) and compare them to LatestEval. We involve five foundation models here: opt-350m, opt-1.3b (Zhang et al. 2022), Llama-7b, Llama-13b, and Llama-30b (Touvron et al. 2023). OPT models are run with fp32 and others are run with fp16. We make sure the input texts of each benchmark have the same length so that we can obtain a fair comparison. Results.",
            "score": 0.4984189059592108,
            "section_title": "Contamination Test",
            "char_start_offset": 17686,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 1938
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82861328125
        },
        {
            "corpus_id": "273502439",
            "title": "CAP: Data Contamination Detection via Consistency Amplification",
            "text": "Our contributions are threefold: \n\n\u2022 We introduce CAP, a novel method for data contamination detection, and define PCR, mathematically demonstrating that PCR in-creases when datasets are used in training. \n\n\u2022 We are the first to explicitly investigate contamination in domain-specific LLMs, providing a comprehensive empirical evaluation of domain-specific benchmarks and models. \n\n\u2022 We highlight the risks of unintentional contamination, particularly in composite benchmarks, given the rapid growth of new benchmarks. \n\n2 Related Work  (Golchin and Surdeanu, 2024), where the model selects the correct sample from a set that includes the original and three perturbed variants. Similarly, Deng et al. (2024) introduced Testset Slot Guessing, which hides parts of the test set and prompts the LLM to predict the missing elements. Additionally, Yang et al. (2023a) utilize similarity search to retrieve the top-k samples from a dataset, followed by prompting a strong LLM like GPT-4 to assess whether any of the top-k samples are too similar to the test case. One limitation of these methods is their vulnerability to surface-level rephrasing, where minor alterations can lead to failure (Dekoninck et al., 2024). Furthermore, while these techniques are effective for tasks like text generation, where the outputs consist of long sequences, they may be less suitable for benchmarks like Multiple Choice Questions (MCQs), which have become increasingly popular nowadays. Our method can handle both task types effectively. \n\nComparison-based DCD The second type is comparison-based. For example, CDD (Dong et al., 2024b) relies on the output distribution of LLMs to detect data contamination. However, it requires an ideal reference output distribution, which can be challenging to define or obtain, especially when benchmarks involve composite tasks. Xu et al. (2024b) applied perplexity for contamination detection, but their approach requires access to hidden layers during decoding, making it suitable only for gray-box or white-box models. In contrast, our method works for both types of LMs. Roberts et al. (2023) conducted a longitudinal analysis of data contamination in LLMs before and after specific training cutoffs.",
            "score": 0.49665756906244685,
            "section_title": "Introduction",
            "char_start_offset": 3870,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 35,
                    "end": 204
                },
                {
                    "start": 207,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1518
                },
                {
                    "start": 1521,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2223
                }
            ],
            "ref_mentions": [
                {
                    "start": 689,
                    "end": 707,
                    "matchedPaperCorpusId": "265220695"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.794921875
        },
        {
            "corpus_id": "270285708",
            "title": "Benchmark Data Contamination of Large Language Models: A Survey",
            "text": "Yang et al. [162], Ippolito et al. [62], and Jiang et al. [69] criticize the use of string matching methods like n-gram overlap for decontamination, demonstrating that simple test data variations such as paraphrasing can bypass these measures.They show that a 13B model can overfit a test benchmark and achieve high performance comparable to GPT-4 when such test data variations are not eliminated.Similar findings were reported by Dekoninck et al. [29], who developed a technique called Evasive Augmentation Learning (EAL).This method involves rephrasing benchmark samples during the fine-tuning stage to evade detection by current contamination detection methods.They categorized model providers and contamination detection methods, uncovering vulnerabilities that EAL exploits.The technique proved highly effective, allowing significant improvements in benchmark performance (up to 15%) while remaining undetected by existing contamination detection methods.comparing the similarity [82,96], distribution [34], perplexity [89], and generation order [108] of the generated content with that of the evaluated dataset.Additionally, comparing the performance differences of LLMs on datasets across different time periods can serve as a comparison-based method for detecting BDC [61,121].We have identified six representative works that adopt this approach, which we have categorized into three subcategories: content comparison, sequential analysis, and chronological analysis.We discuss these below.",
            "score": 0.49645968065191226,
            "section_title": "Pretrain Corpus",
            "char_start_offset": 31474,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 243,
                    "end": 398
                },
                {
                    "start": 398,
                    "end": 524
                },
                {
                    "start": 524,
                    "end": 665
                },
                {
                    "start": 665,
                    "end": 780
                },
                {
                    "start": 780,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1118
                },
                {
                    "start": 1118,
                    "end": 1286
                },
                {
                    "start": 1286,
                    "end": 1476
                },
                {
                    "start": 1476,
                    "end": 1499
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 39,
                    "matchedPaperCorpusId": "263610040"
                },
                {
                    "start": 449,
                    "end": 453,
                    "matchedPaperCorpusId": "267412617"
                },
                {
                    "start": 986,
                    "end": 990,
                    "matchedPaperCorpusId": "260886870"
                },
                {
                    "start": 990,
                    "end": 993,
                    "matchedPaperCorpusId": "247475929"
                },
                {
                    "start": 1052,
                    "end": 1057,
                    "matchedPaperCorpusId": "264490730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45263671875
        },
        {
            "corpus_id": "269921283",
            "title": "Data Contamination Calibration for Black-box LLMs",
            "text": "Baseline Methods: We selected six popular methods to evaluate our approach: four calibrationbased and two calibration-free.Calibrationbased methods include: the Neighborhood attack (Neighbor) (Mattern et al., 2023), which assesses loss differences between original samples and their neighbors generated by masked language models; and perplexity-based calibration (Carlini et al., 2021) techniques utilizing Zlib entropy (Zlib) (Gailly and Adler, 2004), lowercased sample perplexity (Lower), and comparisons with reference models trained on the same dataset (Ref).Calibration-free methods comprise the Min-K% method (Shi et al., 2023), predicting pre-trained samples through low-probability outlier words; and the Loss Attack (Yeom et al., 2018), substituting loss with Perplexity (PPL) in LLMs.\n\nDatasets and Metric.We utilize the StackMI-Asub benchmark (Section 3.2) and the WikiMIA dataset proposed by (Shi et al., 2023).WikiMIA (Appendix B) leverages Wikipedia timestamps and model release dates to identify member and nonmember data sets, applicable for LLMs trained up to 2023.Both datasets are transformed into two formats as the guidelines in Section 3.2: the original format (ori) and the synonym rewritten format (syn).\n\nFor evaluation, we follow (Mattern et al., 2023;Carlini et al., 2022;Watson et al., 2021) and plot the ROC curve analysis method.To facilitate numerical comparison, we primarily use the AUC score (Area Under the ROC Curve).The AUC score (Appendix F), independent of any specific threshold, accurately gauges the method's ability to differentiate between members and non-members.It also eliminates bias from threshold selection.\n\nModels.We conduct experiments against 10 commonly used LLMs.",
            "score": 0.4932443535077228,
            "section_title": "Experiments Settings",
            "char_start_offset": 20257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 123,
                    "end": 563
                },
                {
                    "start": 563,
                    "end": 794
                },
                {
                    "start": 796,
                    "end": 816
                },
                {
                    "start": 816,
                    "end": 923
                },
                {
                    "start": 923,
                    "end": 1082
                },
                {
                    "start": 1082,
                    "end": 1228
                },
                {
                    "start": 1230,
                    "end": 1359
                },
                {
                    "start": 1359,
                    "end": 1453
                },
                {
                    "start": 1453,
                    "end": 1608
                },
                {
                    "start": 1608,
                    "end": 1657
                },
                {
                    "start": 1659,
                    "end": 1666
                },
                {
                    "start": 1666,
                    "end": 1719
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 385,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 615,
                    "end": 633,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 725,
                    "end": 744,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 904,
                    "end": 922,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 1278,
                    "end": 1299,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1299,
                    "end": 1319,
                    "matchedPaperCorpusId": "244130249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.517578125
        },
        {
            "corpus_id": "278339246",
            "title": "Automatic Calibration for Membership Inference Attack on Large Language Models",
            "text": "MIAs aim to determine whether a given sample is part of a model's training data [25,34]. It has important implications for tasks such as measuring memorization and privacy risks [7,18,26], forming the foundation for advanced attacks [5,19], detecting test set contamination [20], and copyrighted content [17,9] in LLMs. Research on MIAs has been investigated in NLP within both fine-tuning [30,18,10,16] and pre-training settings [24,8,35]. The previous line of work can generally be divided into three categories: scorebased methods [24,35], calibration-based methods [30,16,37], and reference model methods [6]. \n\nThe problem of MIA on LLMs was first explored in [24], which introduced the WikiMIA benchmark and developed the Min-K% method. Min-K% evaluated the probability of outlier words within a given text, enabling the assessment of whether the text was likely included in the pre-training corpus. Based on Min-K% [35] further introduced Min-K%++ by examining whether the input text forms a mode or has a relatively high probability under the conditional categorical distribution. Although these two score-based methods demonstrated improved performance compared to other baselines [34], they only focus on a small portion of the text for membership inference and require fine-tuning of the crucial hyper-parameter K. This limitation reduces their practicality in real-world applications and their effectiveness on more challenging benchmarks like MIMIR, where there is minimal distribution shift between members The three histograms show the density distributions of normalized scores for members (blue) and non-members (orange) across different methods: (left) Log-likelihood scores, (middle) Min-K% scores, and (right) our automatic calibrated scores. The numerical difference between member and non-member means is indicated in each plot, showing that our approach achieves the largest separation, improving the reliability of membership inference. \n\nand non-members. Several calibration-based methods have been proposed to utilize a difficulty calibration score to regularize raw scores [30].",
            "score": 0.4931268516423981,
            "section_title": "Related Work",
            "char_start_offset": 7172,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 613
                },
                {
                    "start": 616,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1960
                },
                {
                    "start": 1963,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2105
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 84,
                    "end": 87,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 184,
                    "end": 187,
                    "matchedPaperCorpusId": "258685658"
                },
                {
                    "start": 233,
                    "end": 236,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 609,
                    "end": 612,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1190,
                    "end": 1194,
                    "matchedPaperCorpusId": "2656445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82421875
        },
        {
            "corpus_id": "264555419",
            "title": "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark",
            "text": "Most of the research on data contamination has been focused on analyzing pre-training data with string-matching operations (Dodge et al., 2021), as this provides direct evidence that the LLM was contaminated. Pre-training datasets are unwieldy large, and string-matching operations can be very slow at this scale. Therefore, several tools for data auditing have been released recently: The ROOTS Search Tool (Piktus et al., 2023) and Data Portraits (Marone and Durme, 2023) among others. \n\nAs an example of their usefulness, Piktus et al. (2023) found that BLOOM (Workshop et al., 2023) should not be evaluated on XNLI (Conneau et al., 2018) due to contamination. These tools should be made available for all open LLMs, in order to allow for contamination case discovery. \n\nIn addition, there is no currently agreed-upon methodology to measure the level of contamination. For cases where the full benchmark is not found, we propose to measure the level of data contamination using benchmark data overlap, that is, the percentage of the benchmark that can be found in the pre-training dataset (Dodge et al., 2021;Piktus et al., 2023).",
            "score": 0.49312026763139105,
            "section_title": "Open LLMs",
            "char_start_offset": 12667,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 771
                },
                {
                    "start": 774,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1133
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 143,
                    "matchedPaperCorpusId": "237568724"
                },
                {
                    "start": 1092,
                    "end": 1112,
                    "matchedPaperCorpusId": "237568724"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.861328125
        },
        {
            "corpus_id": "270620798",
            "title": "Data Contamination Can Cross Language Barriers",
            "text": "The primary goals of our investigation are to: (1) Verify the feasibility of deep forms of contamination ( \u00a7 3). (2) Determine whether existing methods can detech such contamination ( \u00a7 4.1). (3) Propose detection methods capable of identifying such deeply concealed contamination ( \u00a7 4.2). \n\nConsidering it is unclear whether existing LLMs contain cross-lingual contamination, we intention-  ally inject such contamination to open-sourced models to obtain contaminated models. Then, we detect such contamination using existing methods and our proposed methods. The detailed investigation configurations are as follows. \n\nModels. To inject cross-lingual contamination, the backbone model should be able to understand different languages. Hence, we employ two multilingual LLMs, LLaMA3-8B (Meta, 2024) and Qwen1.5-7B (Bai and et al, 2023), as the backbone models for further experiments. Datasets. To exhibit the impact of such contamination in evaluation, we adopt three popular benchmarks to inject contamination, MMLU (Hendrycks et al., 2020), ARC Challenge (Clark et al., 2018), and MathQA (Amini et al., 2019), where modern LLMs typically compete with each other. \n\nLanguages. For cross-lingual contamination, we utilize seven non-English languages that are commonly supported: Chinese, French, German, Italian, Japanese, Korean, and Spanish.",
            "score": 0.49304766977176095,
            "section_title": "Investigation Setup",
            "char_start_offset": 7373,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 290
                },
                {
                    "start": 293,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1167
                },
                {
                    "start": 1170,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1346
                }
            ],
            "ref_mentions": [
                {
                    "start": 1020,
                    "end": 1044,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 1093,
                    "end": 1113,
                    "matchedPaperCorpusId": "173188048"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.363525390625
        },
        {
            "corpus_id": "271957208",
            "title": "LogProber: Disentangling confidence from contamination in LLM responses",
            "text": "For the sake of completeness, we ran other experiments where the additional only concerned either learning to predict only the question ('Q-' training) or only the answer ('-A' training). Critically, and somehow unsurprisingly, our results showed that LogProber was able to detect contaminations only when the Question was included in the training. \n\nThis result suggests that our approach is very effective in detecting contamination at the level of pre-training, since it is likely to occur in the 'Q-A' format, and not in fine-tuning (likely to occur in the '-A' format: answer only) (Wang et al., 2022;Taori et al., 2023;Zheng et al., 2023). This limitation does not substantially reduce the utility of LogProber, since in the open access LLM field pretraining dataset are rarely shared or very large to search a query in them. LogProber makes it possible to account for pretraining contamination at a very cheap cost. \n\nWhen training a model using a full language modeling method -a common approach for pretraining autoregressive decoder-only LLM (Wang et al., 2022) -the training data can follow three scenarios: \n\n1. The model is trained only on the question (Q) if the data does not include the answer. \n\n2. The model is trained on both the question and the answer (QA) if they appear consecutively in the data. \n\n3. Neither the question nor the answer is included in the training data (STD). \n\nIn this context, the QA scenario indicates data contamination. A low contamination score implies that neither Q nor QA scenarios are present in the training data, suggesting the STD scenario. Therefore, if a query results in a low contamination score, it is likely safe for use. Conversely, a high contamination score indicates that the model has been exposed to the question, suggesting potential contamination and that the model might be unreliable for that query. \n\nIn the open access community finetuning datasets are often shared and a lot smaller making it possible to know explicitly if the model was trained on the given query.",
            "score": 0.4930411482620912,
            "section_title": "Discussion",
            "char_start_offset": 26071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 348
                },
                {
                    "start": 351,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 922
                },
                {
                    "start": 925,
                    "end": 1118
                },
                {
                    "start": 1121,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1319
                },
                {
                    "start": 1322,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1869
                },
                {
                    "start": 1872,
                    "end": 2038
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84765625
        },
        {
            "corpus_id": "268889777",
            "title": "Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models",
            "text": "Second, characteristics of LLM pretraining (e.g., few training epochs, large-scale training) inherently makes MIA much more challenging. Lastly, most existing works on MIA against LLMs target the fine-tuning stage (Mireshghallah et al., 2022b;Fu et al., 2023;Mattern et al., 2023), which cannot transfer to the pre-training stage for the same reasons. \n\nAs a result, there has been growing interests in pre-training data detection recently, though it is still largely underexplored. Shi et al. (2024), to our knowledge, is the first to investigate this problem. They contribute the WikiMIA benchmark and propose the Min-K% method. Duan et al. ( 2024) perform systematic evaluation with the constructed MIMIR benchmark and analyze the challenges. There are also works that specifically investigate copy-righted content detection, e.g., identifying memorized books (Duarte et al., 2024). In this work, we propose a novel method that outperforms the previously best-performing Min-K% and achieve superior performances over existing (reference-free) methods on both benchmarks. In addition, our insightful finding of that training data tends to form local maximum is unique, which relates to but complements and solidifies previous intuitions that LLM's output distribution is most likely to be peaked when it is emitting memorized data (Dong et al., 2024).",
            "score": 0.491408303565503,
            "section_title": "RELATED WORK",
            "char_start_offset": 6664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 351
                },
                {
                    "start": 354,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1353
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 243,
                    "matchedPaperCorpusId": "256461422"
                },
                {
                    "start": 259,
                    "end": 280,
                    "matchedPaperCorpusId": "258967264"
                },
                {
                    "start": 483,
                    "end": 500,
                    "matchedPaperCorpusId": "264451585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8310546875
        },
        {
            "corpus_id": "269921283",
            "title": "Data Contamination Calibration for Black-box LLMs",
            "text": "Based on the settings described in Section 5.1, the primary comparison results between PAC and the baseline methods are listed in Table 1.The experimental outcomes indicate that PAC consistently outperforms across all models and all data formats.Specifically, PAC shows an average AUC score improvement of 4.5% on WikiMIA and 5.9% on StackMIAsub compared to all other baseline methods.Moreover, PAC maintains robust performance even under the conditions of synonymously approximate memories.Notably, the Min-K% method exhibits the second-best performance in all settings, validating the reliability of using local regions of token probabilities.And different from previous methods, PAC exhibits prominence in member recognition (Figure 2).In summary, PAC is an effective and versatile solution for detecting pre-training data of LLMs.",
            "score": 0.4904171076520434,
            "section_title": "PAC as A More Effective Detector",
            "char_start_offset": 23043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 138,
                    "end": 246
                },
                {
                    "start": 246,
                    "end": 385
                },
                {
                    "start": 385,
                    "end": 491
                },
                {
                    "start": 491,
                    "end": 645
                },
                {
                    "start": 645,
                    "end": 739
                },
                {
                    "start": 739,
                    "end": 834
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54052734375
        },
        {
            "corpus_id": "264172693",
            "title": "Data Contamination Through the Lens of Time",
            "text": "Recent claims about the impressive abilities of large language models (LLMs) are often supported by evaluating publicly available benchmarks. Since LLMs train on wide swaths of the internet, this practice raises concerns of data contamination, i.e., evaluating on examples that are explicitly or implicitly included in the training data. Data contamination remains notoriously challenging to measure and mitigate, even with partial attempts like controlled experimentation of training data, canary strings, or embedding similarities. In this work, we conduct the first thorough longitudinal analysis of data contamination in LLMs by using the natural experiment of training cutoffs in GPT models to look at benchmarks released over time. Specifically, we consider two code/mathematical problem-solving datasets, Codeforces and Project Euler, and find statistically significant trends among LLM pass rate vs. GitHub popularity and release date that provide strong evidence of contamination. By open-sourcing our dataset, raw results, and evaluation framework, our work paves the way for rigorous analyses of data contamination in modern models. We conclude with a discussion of best practices and future steps for publicly releasing benchmarks in the age of LLMs that train on webscale data.",
            "score": 0.490161307119962,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6455078125
        },
        {
            "corpus_id": "271570943",
            "title": "Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens",
            "text": "We discuss related works on pre-training data detection for large language models. The problem of detecting pre-training data aims to determine whether a language model was trained on the given text input when black-box access to the LLM without knowing the pre-training data. Its necessity has received increasing attention from various aspects. \n\nIn the field of traditional machine learning privacy, there has been well explored research on membership inference [42,23,49,35,9,12]. Since the current problem is in line with it, some prior works use MIA technologies to solve detection problem in LLM domain. A simple and frequently employed method is the LOSS attack [49], which utilizes the model's computed loss and classifies the inputs as part of the training set if their loss values fall below a specific threshold. Some work [9,47] claims this kind of attacks have high false positive rate (i.e., non-member samples are often erroneously predicted as members), and difficulty calibration [47] is proposed to overcome this. Since then, other reference-based methods [8,34] are proposed to consider the intrinsic complexity of the target data by calibrating the loss of target model with another reference model. These methods have to train other models on the same dataset or data from same distribution. Instead of training reference models, [33] craft textual neighboring samples through data augmentation and leverages an estimation of the loss curvature to detect membership. More recently, [41]  Not relying on memorizing each word input but uses some narrow concepts to screen, [11] proposes name cloze for data archaeology to infer books that are known to ChatGPT/GPT-4. Although insightful design, the scenario is very limited. Usually, inferring membership based on named-entity is not robust due to the unsure information rarity. \n\nIn parallel to pre-training data detection, there are some related but not completely overlapping issues. [37,19,32,5,15] study the data contamination problem, where the pre-training dataset contains various evaluation benchmarks. [7,22,51] work on the data extraction problem, which aim to recover individual training examples through prompting attacks, especially the personally identifiable information(PII) [31].",
            "score": 0.489266133302803,
            "section_title": "Related Work",
            "char_start_offset": 5800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1848
                },
                {
                    "start": 1851,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2267
                }
            ],
            "ref_mentions": [
                {
                    "start": 465,
                    "end": 469,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 469,
                    "end": 472,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 472,
                    "end": 475,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 478,
                    "end": 480,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 480,
                    "end": 483,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 670,
                    "end": 674,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 835,
                    "end": 838,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 838,
                    "end": 841,
                    "matchedPaperCorpusId": "244130249"
                },
                {
                    "start": 998,
                    "end": 1002,
                    "matchedPaperCorpusId": "244130249"
                },
                {
                    "start": 1075,
                    "end": 1078,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1078,
                    "end": 1081,
                    "matchedPaperCorpusId": "247315260"
                },
                {
                    "start": 1352,
                    "end": 1356,
                    "matchedPaperCorpusId": "258967264"
                },
                {
                    "start": 1504,
                    "end": 1508,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 1593,
                    "end": 1597,
                    "matchedPaperCorpusId": "258426273"
                },
                {
                    "start": 1957,
                    "end": 1961,
                    "matchedPaperCorpusId": "264490730"
                },
                {
                    "start": 1961,
                    "end": 1964,
                    "matchedPaperCorpusId": "260925501"
                },
                {
                    "start": 1964,
                    "end": 1967,
                    "matchedPaperCorpusId": "247475929"
                },
                {
                    "start": 1967,
                    "end": 1969,
                    "matchedPaperCorpusId": "252780005"
                },
                {
                    "start": 1969,
                    "end": 1972,
                    "matchedPaperCorpusId": "237568724"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80224609375
        },
        {
            "corpus_id": "272367930",
            "title": "Training on the Benchmark Is Not All You Need",
            "text": "With this detection method, artificial and intentional shuffling over the order of options can also be detected, if the option shuffling is not taken into account, only the logarithmic probability of the data in the original order is required to maximize the probability of data leakage can be determined. \n\nIn this work, we show how to provide reliable evidence for test set contamination in gray-box language models (Only need to access the Log-Probability of the output). More specifically, we provide a simple and efficient new method for benchmark leakage detection based on multiple-choice questions. The method identifies the presence of a benchmark test set in a language model's pre-training data and the extent of data leakage without accessing the model's training data or weights. The contributions of this paper are summarized below: \n\n\u2022 We propose a simple yet effective detection method based on the characteristics of multiple choice questions by generating different derived datasets by disrupting the order of the options, and then using the model's logarith-mic probability distribution to detect whether the original dataset is leaked or not.",
            "score": 0.48905631284073803,
            "section_title": "Introduction",
            "char_start_offset": 3428,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 846
                },
                {
                    "start": 849,
                    "end": 1162
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82275390625
        },
        {
            "corpus_id": "274131103",
            "title": "CODECLEANER: Elevating Standards with A Robust Data Contamination Mitigation Toolkit",
            "text": "As data contamination becomes inevitable, methods have been proposed to detect it. One of the most commonly used methods is n-gram matching [18], [33], [56], [57], which partitions text into sequences of N consecutive characters/tokens and compares comparing these sequences to find patterns or similarities with the training data. For example, Marone et al. [33] offers a website [34] to compare with three data sources, including The pile [58], The Stack [32], and The Stack-V2 [59]. Recently, Deng et al. [60] proposed a maskand-fill method to detect contamination in multiple-choice questions and raised concerns about data contamination in Question-Answering benchmarks such as TruthfulQA [61] and MMLU [62]. \n\nAnother related task, Membership inference attacks (MIAs), determines whether a given data is contained in the model's training data and can be used as a hint for data contamination. Various metrics are proposed to infer data membership, including LOSS [63], reference models [17], perplexity [31], Zlib Entropy [64], Neighborhood attack [65], Min-k% Prob [16], etc. Though MIAs are extensively studied in traditional deep learning models, the research on MIA in LLMs is limited. Duan et al. [66] conducted a large-scale MIA on LLMs and found that MIAs barely outperform random guessing across varying LLM sizes and domains. Recently, Dong et al. [22] detected data contamination by identifying the peakedness of LLM's output distribution and showed potential. \n\nOur work differs from these works in purposes, i.e., they detect the existence of data contamination while we introduce effective code refactors that address data contamination.",
            "score": 0.4879596014382136,
            "section_title": "B. Detection of Data Contamination",
            "char_start_offset": 37451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 713
                },
                {
                    "start": 716,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1656
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "257378087"
                },
                {
                    "start": 152,
                    "end": 156,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 158,
                    "end": 162,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 359,
                    "end": 363,
                    "matchedPaperCorpusId": "257378087"
                },
                {
                    "start": 969,
                    "end": 973,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 992,
                    "end": 996,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1009,
                    "end": 1013,
                    "matchedPaperCorpusId": "121680873"
                },
                {
                    "start": 1363,
                    "end": 1367,
                    "matchedPaperCorpusId": "267938621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6767578125
        },
        {
            "corpus_id": "273850342",
            "title": "Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?",
            "text": "Hampering the interpretation of benchmark scores, evaluation data contamination has become a growing concern in the evaluation of LLMs, and an active area of research studies its effects. While evaluation data contamination is easily understood intuitively, it is surprisingly difficult to define precisely which samples should be considered contaminated and, consequently, how it impacts benchmark scores. We propose that these questions should be addressed together and that contamination metrics can be assessed based on whether models benefit from the examples they mark contaminated. We propose a novel analysis method called ConTAM, and show with a large scale survey of existing and novel n-gram based contamination metrics across 13 benchmarks and 7 models from 2 different families that ConTAM can be used to better understand evaluation data contamination and its effects. We find that contamination may have a much larger effect than reported in recent LLM releases and benefits models differently at different scales. We also find that considering only the longest contaminated substring provides a better signal than considering a union of all contaminated substrings, and that doing model and benchmark specific threshold analysis greatly increases the specificity of the results. Lastly, we investigate the impact of hyperparameter choices, finding that, among other things, both using larger values of n and disregarding matches that are infrequent in the pre-training data lead to many false negatives. With ConTAM, we provide a method to empirically ground evaluation data contamination metrics in downstream effects. With our exploration, we shed light on how evaluation data contamination can impact LLMs and provide insight into the considerations important when doing contamination analysis. We end our paper by discussing these in more detail and providing concrete suggestions for future work.",
            "score": 0.487353163061659,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86181640625
        },
        {
            "corpus_id": "268819579",
            "title": "A Comprehensive Survey of Contamination Detection Methods in Large Language Models",
            "text": "TED corrects the LLM's output distribution to mitigate the impact of data contamination on evaluation metrics, significantly reducing performance improvements attributed to data contamination across various scenarios and contamination degrees. The work of Xu et al. (2024b) proposes to use perplexity and accuracy on next n-gram prediction. They compute the relative loss in performance on these metrics on paraphrased versions of the dataset created by ChatGPT compared to the original dataset. The difference of relative loss between training and test set is later used to assess contamination or benchmark leakage. Experiments on GSM8K (Cobbe et al., 2021) hint at contamination for Qwen (Bai et al., 2023), Aquila and InternLM (Cai et al., 2024) series of LLMs. Zhang & Wu (2024) leverage the concept of surprising tokens: these are tokens where the LLM is very confident yet wrong in its prediction of the next token. The authors define a score named SURP which is the average predicted log-probability on the ground-truth token for tokens of the sequence where the model shows low entropy (and therefore, high confidence) and where this log-probability is relatively smaller. A higher SURP score is thresholded to assess contamination. The method slightly outperforms Min-K% Prob on WikiMIA. Oren et al. (2023) take an original approach to analyze the LLM's likelihood on an evaluation dataset D E . \n\nThey run inference on D E and on shuffled versions of D E . LLM log probabilities being statistically different on the non-shuffled dataset compared to the shuffled versions signifies contamination. The method relies on the assumption that if they are included in the pre-training set, evaluation datasets tend to be present with the same default ordering. We point out that detecting the LLM's memory of a dataset canonical order is an idea also explored by Samuel et al. (2024) for a different contamination detection method.",
            "score": 0.486955288897598,
            "section_title": "Likelihood",
            "char_start_offset": 36401,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1405
                },
                {
                    "start": 1408,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1935
                }
            ],
            "ref_mentions": [
                {
                    "start": 1867,
                    "end": 1887,
                    "matchedPaperCorpusId": "272689919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8994140625
        },
        {
            "corpus_id": "273549817",
            "title": "CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training large language models",
            "text": "The success of Large Language Models (LLMs) [1] [2] is primarily attributed to the availability of extensive, high-quality pre-training corpora, which underpin their foundational knowledge and reasoning capabilities for a variety of tasks, from creative writing to complex problem-solving. Among them, the Open-source datasets, such as The Pile [3] and Common Crawl[4], have been instrumental in propelling LLM development, fostering collaboration and establishing benchmarks for innovation. \n\nExisting Researchers focus more on scaling high-quality data. Recently the demand for pre-training data has exceeded 10 trillion tokens [1][5] [6], underscoring two key trajectories in English pre-training: scaling data and improving its quality. Open-source datasets have rapidly expanded, evolving from collections like the Pile(825GB) to larger datasets such as FineWeb(15TB) [7], which draw extensively from Common Crawl. Simultaneously, the focus has shifted from rule-based filtering methods, as seen in early projects like Redpajama [8], to model-driven approaches exemplified by FineWeb-Edu [7]. \n\nDespite the rapid advancement of English open-source datasets, Chinese data remains significantly underrepresented on the global web. Existing open-source Chinese datasets, such as WuDao [9], SkyPile150B [10], and WanjuanV1 [11], are constrained in scale due to a scarcity of Chinese data sources online. Furthermore, there is limited research focused on improving quality classification for Chinese web data, resulting in suboptimal data quality. These challenges present substantial barriers to the development of high-performance Chinese language models, underscoring the urgent need for more effective data filtering and quality classification methodologies. To address the identified challenges, we present CCI3.0-HQ, a large-scale Chinese pre-training dataset created through a two-stage hybrid filtering strategy: Fundamental Processing and High-Quality Processing. The fundamental stage encompasses standard web data curation practices, including safety filtering, text extraction, deduplication, and initial quality assessment using a basic model score.",
            "score": 0.48624010281872165,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2162
                }
            ],
            "ref_mentions": [
                {
                    "start": 1287,
                    "end": 1290,
                    "matchedPaperCorpusId": "236712622"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.048675537109375
        },
        {
            "corpus_id": "269484687",
            "title": "A Careful Examination of Large Language Model Performance on Grade School Arithmetic",
            "text": "Because data contamination is a well known issue in the field Balloccu et al. [2024], Magar and Schwartz [2022], Sainz et al. [2023], Jacovi et al. [2023], model builders will frequently take great pains to minimize the likelihood of data contamination. For example, it is common to remove all data with too high of an n-gram overlap with the benchmark data Brown et al. [2020]. Additionally, methods such as using embedding similarity attempt to remove all contaminated data that is too similar in embedding space to the dataset Shi et al. [2024]. More recently, Srivastava et al. [2024] propose functional evaluations, where benchmarks are written in the form of functions that can generate an infinite number of specific evaluation datapoints, each with slightly different numbers. \n\nIn this setup, whenever a language model is evaluated, functional evaluations generate a specific problem instance to evaluate the model on, which is then never used again. This reduces the worry of data contamination by ensuring that no datapoint is ever used twice. Like ours, their results indicate the LLMs may be severely overfit on benchmark data. The main advantage of our approach over a purely function based evaluation is that functional evaluations can only generate a tiny portion of the full problem space by producing variations of the same problem with slightly different numerical values.",
            "score": 0.48614882051300223,
            "section_title": "Data Contamination",
            "char_start_offset": 5218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 784
                },
                {
                    "start": 787,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1391
                }
            ],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 111,
                    "matchedPaperCorpusId": "247475929"
                },
                {
                    "start": 113,
                    "end": 132,
                    "matchedPaperCorpusId": "264555419"
                },
                {
                    "start": 134,
                    "end": 154,
                    "matchedPaperCorpusId": "258741333"
                },
                {
                    "start": 358,
                    "end": 377,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63671875
        },
        {
            "corpus_id": "270703142",
            "title": "ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods",
            "text": "WikiMIA results Table 1 shows that RE-CALL achieves state-of-the-art performance on the WikiMIA benchmark.Our method consistently outperforms all existing baseline methods in all settings by a large margin.On average, RECALL surpasses the runner-up Min-K%++ by 14.8%, 15.4%, 14.8% in terms of AUC scores for input lengths of 32, 64, 128, respectively.Moreover, RECALL's superior performance is consistent across different model architectures.The improvement is particularly significant for shorter inputs and smaller models, which are known to be more challenging for MIAs (Shi et al., 2024), which demonstrates the effectiveness of RECALL in capturing membership signals even in a challenging setting.We also report the TPR@1%FPR results in Appendix I, which again shows the significant improvements and highlights the effectiveness of our approach in detecting pretraining data with high precision.",
            "score": 0.4853268030080016,
            "section_title": "Main Results",
            "char_start_offset": 12021,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 106,
                    "end": 206
                },
                {
                    "start": 206,
                    "end": 351
                },
                {
                    "start": 351,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 702
                },
                {
                    "start": 702,
                    "end": 900
                }
            ],
            "ref_mentions": [
                {
                    "start": 573,
                    "end": 591,
                    "matchedPaperCorpusId": "264451585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1435546875
        },
        {
            "corpus_id": "273346369",
            "title": "FormalAlign: Automated Alignment Evaluation for Autoformalization",
            "text": "We calculated the loss of different pre-trained models on the MiniF2F test/valid sets in the autoformalization task to further analyze the potential data contamination issue. This approach is inspired by the data contamination detection method in (Wei et al., 2023), which suggests that if a language model has not been exposed to a dataset during pre-training, its loss on the dataset should be relatively high and approximately equivalent to its loss on a reference dataset composed of new, similar samples. The losses of the models in our experiments are shown below: Analysis The loss values for each pre-trained model fall within the range of 1 to 3, consistent with and even higher than the findings in (Wei et al., 2023), which reports that losses higher than around 1 on the GSM8K test set indicate low data leakage. These results suggest a low level of data contamination in our experimental data. The combination of carefully sourced datasets and the augmentation of aligned pairs with negative examples using our misalignment strategies further strengthens the robustness of our experiments against data contamination.",
            "score": 0.4853129009429891,
            "section_title": "Results",
            "char_start_offset": 31691,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1129
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.818359375
        },
        {
            "corpus_id": "264555419",
            "title": "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark",
            "text": "In this position paper, we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination.",
            "score": 0.4853129009429891,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82861328125
        },
        {
            "corpus_id": "266933004",
            "title": "Investigating Data Contamination for Pre-training Language Models",
            "text": "The performance of large language models (LLMs) has been attributed primarily to their immense size and the increasing scale of pre-training data from large text corpora [29,2,27,6,1,33,34]. Nevertheless, a critical aspect that remains under-explored is the potential contamination of the pre-training corpus with evaluation data. This oversight presents challenges in accurately assessing the LLMs' capabilities among other scientific analyses of their behaviors. The importance of contamination analysis in the pre-training corpus has been recognized since pre-trained language models were first introduced [7,29,6]; however, the lack of public access to most pre-training corpora today complicates efforts to comprehensively understand and identify the impact of contamination on a model's performance and behaviors. \n\nRecent LLM reports [29,2,6,27,34,11] have investigated the contamination of evaluation data in the pre-training corpora from various perspectives. Some of these studies offer limited details on their contamination investigations, especially for closed-source models [29,27]. Others include attempts [29,27,2,6,34] to investigate the data contamination on the evaluation level, where an evaluation dataset is post-hoc categorized into contaminated and non-contaminated chunks based on a proposed contamination definition and the model is evaluated on them separately, to demonstrate that the model is insusceptible to data contamination if the model performs similarly on these chunks. However, this line of work has not adequately analyzed contamination on the pre-training level, where the pre-training corpus is deliberately altered to study the effects of contamination on evaluation. \n\nEvaluation data can be leaked into the pre-training corpus in various formats. Predominantly, it is the textual component of the evaluation dataset (i.e. input text). This aspect has been the primary 1. RQ1: How are language models affected by the deliberate addition of various forms of contamination on the pre-training corpus? To answer this, we introduce intentional contamination (with and without the ground truth) into the pre-training corpus ( \u00a74.1). We then pre-train GPT-2small models from scratch on these variously contaminated corpora to evaluate and compare their performance.",
            "score": 0.4853129009429891,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 819
                },
                {
                    "start": 822,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1709
                },
                {
                    "start": 1712,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2170
                },
                {
                    "start": 2171,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 174,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 174,
                    "end": 176,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 609,
                    "end": 612,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 612,
                    "end": 615,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 841,
                    "end": 845,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 845,
                    "end": 847,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1088,
                    "end": 1092,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1121,
                    "end": 1125,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1128,
                    "end": 1130,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81591796875
        },
        {
            "corpus_id": "272753636",
            "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
            "text": "Lastly, expanding the FRAMES dataset to include more diverse and domain-specific questions, as well as incorporating more dynamic elements such as real-time information retrieval, could further enhance its utility as a benchmark for next-generation RAG systems. It is important to note that future work should also address the potential limitations of our current approach, including the risk of pretraining data contamination, which may affect the generalizability and reliability of the results, particularly when using Wikipedia articles that could overlap with LLM training data. \n\nLimitations. While FRAMES provides a comprehensive evaluation framework for RAG systems, it is important to acknowledge certain limitations. One significant concern is the potential for pretraining data contamination. As large language models are trained on vast amounts of internet data, there is a risk that some of the information in our dataset may have been seen by these models during their pretraining phase. This could lead to artificially inflated performance metrics and reduce the dataset's effectiveness in measuring true generalization capabilities. Future iterations of FRAMES should explore techniques to mitigate this issue, such as using more recent or synthetic data, or developing methods to quantify and account for potential contamination. Additionally, while we have strived for diversity in our dataset, it may not fully represent the entire spectrum of real-world queries and scenarios, potentially limiting its applicability to certain domains or use cases. Addressing these limitations will be crucial for improving the robustness and reliability of RAG system evaluations. \n\nA SYNTHETIC DATA GENERATION PROMPT",
            "score": 0.48459997708540226,
            "section_title": "CONCLUSION",
            "char_start_offset": 29691,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1685
                },
                {
                    "start": 1688,
                    "end": 1722
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67041015625
        },
        {
            "corpus_id": "247951931",
            "title": "PaLM: Scaling Language Modeling with Pathways",
            "text": "Previous work Wei et al., 2022a;Du et al., 2021) reported very high data overlap rates between the benchmark evaluation sets and the training data. These previous works simply looked at the occurrences of high-order n-grams (e.g., 13-grams) between the full evaluation example text and the training data, and considered any example with overlap to be \"contaminated.\" However, many benchmarks are constructed by taking some context from the open web, and then asking annotators to generate a novel question/answer about that context (or asking annotators to answer an automatically generated question). Because the context is provided at the evaluation time for these tasks, even if the model had previously trained on the context for the language modeling objective, this does not give it an unfair advantage at evaluation time.\n\nHere, instead of simply looking for high-order n-gram overlaps, we computed statistics for our 29 primary English NLP benchmark tasks and manually examined a large number of examples from each, to determine which had a high proportion of contaminated examples. This was done alongside knowledge of how each dataset was constructed originally.\n\nWe can roughly divide the 29 benchmark tasks into four categories:\n\n\u2022 Wholesale contamination -Datasets where a significant portion of the dataset itself appears in the open web. We consider these contaminated. Examples: SQuADv2, Winograd.\n\n\u2022 Constructed from web -Datasets where the question+answers (or prefix+continuation) were automatically extracted from the open web, so many evaluation examples are likely to be in our training data. We consider these contaminated. Examples: Web Questions, ReCoRD, Lambada.\n\n\u2022 Context on web -Question answering datasets where the context is taken from the web, but the questions were not. We do not consider these contaminated. Examples: BoolQ, Multirc, ANLI.\n\n\u2022 No significant overlap -Datasets which did not have a significant overlap with our training data, other than common n-grams we would expect to be in any large training corpus. Examples: StoryCloze, OpenbookQA.\n\nWe found that 10 out of the 29 sets fall into the first two categories. Of these sets, only a portion of the evaluation examples were actually found in our training. This is because our training corpus only contains about 200 billion words",
            "score": 0.4845628232921087,
            "section_title": "Dataset Contamination",
            "char_start_offset": 90936,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 32,
                    "matchedPaperCorpusId": "237416585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1834716796875
        },
        {
            "corpus_id": "270285708",
            "title": "Benchmark Data Contamination of Large Language Models: A Survey",
            "text": "The training processes of LLMs can be divided into two main types.Pre-training refers to the process of training language models on a large-scale corpus with the aim of equipping LLMs with general-purpose language comprehension, where the trained model is called Pre-trained Language Models (PLMs) [115].In contrast, fine-tuning refers to the targeted continuation of training for various downstream natural language processing tasks, in order to make the LLMs better able to carry out the downstream tasks.\n\nThe BDC problem stems from the inherent complexity and diversity of the pre-training data used to train LLMs in NLP tasks.Excluding deliberate human attacks, one of the primary sources of BDC is the composition of large-scale pre-training datasets themselves.These datasets are often compiled from a wide range of sources, including news articles, online forums, social media posts, and other publicly available text data.While this diversity is essential for training robust and generalizable models, it also introduces the risk of unexpected exposure to test data during the model training process.Such a risk, on the other hand, is much less in the fine-tuning process, which generally uses relatively small datasets for targeted training, and is a much more controlled process with more predictable results.\n\nIf BDC is introduced during the training phase, we need to consider how this might impact on the evaluation process of LLMs.Such evaluations typically involve three methods: traditional benchmark testing, automatic evaluation, and human evaluation.Each method has its own metrics and processes, which can be significantly affected by BDC.\n\nIn traditional benchmark testing, the model's performance is assessed by training or fine-tuned on a training set and then testing it on a separate test set.However, the presence of BDC can lead to overestimated performance metrics, as the model might inadvertently \"learn\" from test data that was leaked into the training set.This compromises the integrity of the evaluation, making it difficult to gauge the model's true capabilities.\n\nAutomatic evaluation uses algorithms and pre-defined metrics to assess LLMs, reducing the need for human labor and enabling faster, more standardized assessments.Key aspects of automatic evaluation include: Accuracy, Calibration, Fairness, Robustness, which are presented in Table 1.",
            "score": 0.4838630027193008,
            "section_title": "Sources and Impact",
            "char_start_offset": 13615,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 66,
                    "end": 304
                },
                {
                    "start": 304,
                    "end": 507
                },
                {
                    "start": 509,
                    "end": 631
                },
                {
                    "start": 631,
                    "end": 768
                },
                {
                    "start": 768,
                    "end": 931
                },
                {
                    "start": 931,
                    "end": 1109
                },
                {
                    "start": 1109,
                    "end": 1320
                },
                {
                    "start": 1322,
                    "end": 1446
                },
                {
                    "start": 1446,
                    "end": 1570
                },
                {
                    "start": 1570,
                    "end": 1660
                },
                {
                    "start": 1662,
                    "end": 1819
                },
                {
                    "start": 1819,
                    "end": 1989
                },
                {
                    "start": 1989,
                    "end": 2098
                },
                {
                    "start": 2100,
                    "end": 2262
                },
                {
                    "start": 2262,
                    "end": 2383
                }
            ],
            "ref_mentions": [
                {
                    "start": 298,
                    "end": 303,
                    "matchedPaperCorpusId": "212747830"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59619140625
        },
        {
            "corpus_id": "277104921",
            "title": "VeriContaminated: Assessing LLM-Driven Verilog Coding for Data Contamination",
            "text": "Large Language Models (LLMs) like GPT-4 [1] and Gemini [2] have exhibited remarkable capabilities in comprehending text semantics and generating code across different programming languages. However, a critical challenge for evaluating these models is data contamination, which occurs when benchmarks inadvertently include samples from the model's pre-training corpus [3]. Such a scenario would unfairly inflate performance estimates and undermine the reliability of comparative assessments. With impressive performance metrics reported for many LLMs, the undisclosed nature of the pretraining datasets, common even among open-source models [4], raises significant concerns for such contamination [5]- [8]. \n\nLLMs have also shown considerable promise in the domain of hardware design [10]- [20]. However, as in the software domain, data contamination poses a challenge when establishing LLMs for hardware designs. The performance and utility of these domain-specific LLMs depend on the availability of large, high-quality datasets tailored to the complexities of hardware design. Given the relative sparsity of such data, it is easy to see that evaluation benchmarks may (inadvertently) include some of the same samples that were part of the pre-training data. Figure 1 confirms this hypothesis through Fig. 1: Top-1 scores for Verilog Abstract Syntax Tree (AST) similarity, powered by Dolos [9], between golden solutions and closest designs in training datasets. RTLCoder and Verigen show significant similarities to the RTLLM and VerilogEval benchmarks, suggesting severe data contamination. assessment of code similarity for the state-of-the-art (SOTA) datasets of Verigen [14] and RTLCoder [21] against the SOTA benchmarks VerilogEval [11] and RTLLM [13]. Such large overlaps between training and testing data are highly likely to induce data contamination and thereby skew benchmarking results. Addressing this challenge is crucial for fair ranking of LLMs for hardware design. \n\nWe address this challenge for the first time. We study data contamination for many SOTA LLMs (CodeGen2.5",
            "score": 0.4833328614423454,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 1981
                },
                {
                    "start": 1984,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2088
                }
            ],
            "ref_mentions": [
                {
                    "start": 701,
                    "end": 704,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 783,
                    "end": 787,
                    "matchedPaperCorpusId": "269756973"
                },
                {
                    "start": 1391,
                    "end": 1394,
                    "matchedPaperCorpusId": "267740362"
                },
                {
                    "start": 1675,
                    "end": 1679,
                    "matchedPaperCorpusId": "260379192"
                },
                {
                    "start": 1738,
                    "end": 1742,
                    "matchedPaperCorpusId": "261822682"
                },
                {
                    "start": 1753,
                    "end": 1757,
                    "matchedPaperCorpusId": "260775786"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63623046875
        },
        {
            "corpus_id": "271892318",
            "title": "MIA-Tuner: Adapting Large Language Models as Pre-training Text Detector",
            "text": "Benchmark Datasets Construction We employ the widely adopted pre-training data detection benchmark, WIKIMIA (Shi et al. 2023), which is composed of articles from Wikipedia event pages. WIKIMIA assumes the target models were pre-trained before 2023, and set January 1, 2023 as the cutoff date for dividing member and nonmember data. However, with the emergence of numerous state-of-the-art LLMs (e.g., Gemma (Mesnard et al. 2024), Mistral (Jiang et al. 2023), and LLaMA-2 (Touvron et al. 2023b)) over the past year, WIKIMIA is now somewhat outdated in evaluating these models. Thus, we follow a similar pipeline of WIKIMIA to collect a more up-to-date benchmark by moving forward the cutoff date to March 1, 2024. Specifically, we use the official API of Wikipedia to retrieve the articles that belong to the event category, then filter the events that happened after March 2024 as the member data. \n\nFor member data, we follow the same setting of WIKIMIA only retrieving the articles created before 2017, since most pre-trained models were released after 2017.",
            "score": 0.4828223077547036,
            "section_title": "Experimental Setup",
            "char_start_offset": 17621,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 1060
                }
            ],
            "ref_mentions": [
                {
                    "start": 407,
                    "end": 427,
                    "matchedPaperCorpusId": "247315260"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12261962890625
        },
        {
            "corpus_id": "277667420",
            "title": "NorEval: A Norwegian Language Understanding and Generation Evaluation Benchmark",
            "text": "The increasing volume of open textual data can lead to unintended test data leakage in an LM's pretraining corpus (e.g., Brown  et al., 2020; Dubey et al., 2024; Zhang et al., 2024), which can promote the saturation of NLP benchmarks. We recognize the importance of this evaluation aspect and acknowledge that LM performance on NorEval datasets created from open text sources can be inflated. We encourage adherence to responsible LM development practices and recommend conducting test contamination analysis when benchmarking an LM on NorEval. Integrating unsupervised pretraining data detection methods into NorEval is left as a direction for our future work. \n\nEvaluation Framework NorEval is integrated into LM Harness Evaluation, a widely recognized open-source collaborative project that is subject to continuous improvements and advancements, which potentially affect its long-term compatibility, reproducibility, and usability.",
            "score": 0.48051836594576036,
            "section_title": "Data Contamination",
            "char_start_offset": 21882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 661
                },
                {
                    "start": 664,
                    "end": 935
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37744140625
        },
        {
            "corpus_id": "270711329",
            "title": "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation",
            "text": "Thus, our contributions towards helping mitigate the concern of training data contamination during evaluation are as follows: 1) We propose a \"variable perturbation\" approach for making existing evaluation benchmarks more robust to the data contamination problem.2) We release four evaluation benchmarks : GSM+, CommonsenseQA+, TruthfulQA+, and ARC+, which have been created using the original question formats from GSM8K (Cobbe et al., 2021), CommonsenseQA (Talmor et al., 2019), TruthfulQA (Lin et al., 2022), and ARC (Clark et al., 2018).To our knowledge, our work is the first to construct any dynamic benchmark for any natural language processing task.3) We thoroughly evaluate multiple open-and closed-source LLMs on both of these corpora, finding substantial evidence that existing LLMs have likely been trained on GSM8K.",
            "score": 0.48051836594576036,
            "section_title": "Introduction",
            "char_start_offset": 1751,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 263,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 657
                },
                {
                    "start": 657,
                    "end": 828
                }
            ],
            "ref_mentions": [
                {
                    "start": 458,
                    "end": 479,
                    "matchedPaperCorpusId": "53296520"
                },
                {
                    "start": 492,
                    "end": 510,
                    "matchedPaperCorpusId": "237532606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46533203125
        },
        {
            "corpus_id": "264555419",
            "title": "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark",
            "text": "We want to encourage the NLP community to: (1) Develop auto-or semi-automatic measures to detect when data from a benchmark was exposed to a model; (2) Build a registry of data contamination cases, including the evidence for the contamination; \n\n(3) Encourage authors to use the previous tools to ensure that the experimental protocol avoids data contamination to the extent possible; and (4) Address data contamination issues during peer review, and, in the case of published works, devise mechanisms to flag those works with the relevant evidence of data contamination and how data contamination affects the conclusions. \n\nAs the problem affects our entire field, we also want to encourage the community to participate in workshops related to this topic, as for example, the 1st Workshop on Data Contamination 2 . We think that developing the ideas that will arise from this community will play an important role in future NLP evaluations.",
            "score": 0.48051836594576036,
            "section_title": "Call for action",
            "char_start_offset": 15976,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 246,
                    "end": 622
                },
                {
                    "start": 625,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 941
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57470703125
        },
        {
            "corpus_id": "268819579",
            "title": "A Comprehensive Survey of Contamination Detection Methods in Large Language Models",
            "text": "Several LLM pre-training reports assess contamination levels by using string matching techniques. GPT-2 (Radford et al., 2019) calculates contamination as the percentage of 8-grams from a particular evaluation set that are also found in the WebText training set. An average of 3.2% overlap between common LM datasets' test sets and the WebText dataset used for pre-training is discovered, raising the first concerns that LLM performance may be affected by memorization of pre-training data. GPT-3 (Brown et al., 2020) scans data points having a 13-gram collision with anything in the pre-training Common Crawl (C4) dataset (Raffel et al., 2020). Some earlier benchmarks sourced from Wikipedia, the Children's Book Test dataset (Hill et al., 2015), Quac (Choi et al., 2018) and SQuAD 2.0 (Rajpurkar et al., 2018), are nearly entirely contaminated. PaLM (Chowdhery et al., 2023) identifies 10 evaluation datasets at risk of contamination per their construction process, and for each dataset, partitions it into a clean and a contaminated subset based on whether at least 70% of the 8-grams of the data point can be found at least once in the training set. Both GPT-3 and PaLM authors conclude that the performance gap between clean sets and contaminated sets is mostly small across sets of various contamination degrees. GPT-4 (Achiam et al., 2023) measures cross-contamination between evaluation sets and pre-training data by computing 50-characters substring collisions. After examining GPT-4 performance gap between contaminated and clean subsets, the authors conclude that contamination overall has very little effect on the reported zero-shot results. Llama-2 (Touvron et al., 2023b) refines the methods above by using tokens instead of words to measure contamination, and asserts that a token is contaminated if it appears in any token n-gram longer than 10 tokens in both the evaluation sample and the training set. The formatting of prompts used for actual evaluation is also considered as it affects the performance more significantly.",
            "score": 0.48051836594576036,
            "section_title": "Instance-Level String Matching",
            "char_start_offset": 8740,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2042
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 126,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 497,
                    "end": 517,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 623,
                    "end": 644,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 852,
                    "end": 876,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88818359375
        },
        {
            "corpus_id": "270217411",
            "title": "Probing Language Models for Pre-training Data Detection",
            "text": "This approach is based on the assumption: Texts that have been seen during the model's pre-training phase are represented differently in the model's internal activations compared to texts that have not been seen, so we could train a linear probe classifier to distinguish them.\n\nAs illustrated in Figure 1, our method consists of three main steps: (1) We initiate our process by gathering a training dataset that the LLM has not previously been trained on, splitting the data into member and non-member subsets.We then inject the member data into the target model through a fine-tuning process on the member data alone.This step enables us to create a proxy model that retains the memory of the member data from the training dataset.\n\n(2) Subsequently, we input both member and non-member subsets from the training dataset into the proxy model and extract the model's internal activations.These activations are employed to train a probe classifier that can distinguish between member and non-member data.\n\n(3) Finally, given a target text, we can input it to the target model, extract the internal activations, and let the probe classifier infer whether the text is member data.\n\nIn other words, the probe classifier could assess whether the target text has been seen during the pre-training phase.\n\nIn order to evaluate various pre-training data detection methods in a more challenging scenario, we introduce ArxivMIA, a difficult benchmark in the academic domain.In contrast to the existing WikiMIA (Shi et al., 2023) benchmark, which primarily utilizes event data from Wikipedia pages, ArxivMIA presents a more challenging scenario.The academic abstracts within ArxivMIA are rarer on the internet compared to Wikipedia content, naturally resulting in a lower duplication rate.Furthermore, the inherent complexity of texts targeted at researchers adds another layer of difficulty for model memorization.This combination of rarity and complexity makes it exceedingly challenging for large models to memorize such content during the pre-training process, making its detection through conventional methods markedly tougher.Therefore, ArxivMIA stands as an especially rigorous benchmark, highlighting the need for more sophisticated pre-training data detection methods.\n\nOur contributions can be summarized as follows:",
            "score": 0.48041158062733225,
            "section_title": "Introduction",
            "char_start_offset": 1926,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 279,
                    "end": 511
                },
                {
                    "start": 511,
                    "end": 619
                },
                {
                    "start": 619,
                    "end": 733
                },
                {
                    "start": 735,
                    "end": 889
                },
                {
                    "start": 889,
                    "end": 1004
                },
                {
                    "start": 1006,
                    "end": 1178
                },
                {
                    "start": 1180,
                    "end": 1298
                },
                {
                    "start": 1300,
                    "end": 1465
                },
                {
                    "start": 1465,
                    "end": 1635
                },
                {
                    "start": 1635,
                    "end": 1779
                },
                {
                    "start": 1779,
                    "end": 1905
                },
                {
                    "start": 1905,
                    "end": 2122
                },
                {
                    "start": 2122,
                    "end": 2267
                },
                {
                    "start": 2269,
                    "end": 2316
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9072265625
        },
        {
            "corpus_id": "265067240",
            "title": "First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models",
            "text": "However, these tests are plagued by data contamination: benchmark exposure during training has created illusory gains in tasks ranging from code generation (Khan et al., 2023) to theory-of-mind puzzles (Ullman, 2023). Clearly language models are improving, but we cannot say precisely how, or by how much.",
            "score": 0.47921875827762245,
            "section_title": "Remember small-scale problems.",
            "char_start_offset": 18079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 305
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11553955078125
        },
        {
            "corpus_id": "267412617",
            "title": "Evading Data Contamination Detection for Language Models is (too) Easy",
            "text": "The current practice among model providers to train language models in an honest-but-negligent fashion, combined with the risk of malicious actors actively contaminating models to achieve top benchmark performance, can make traditional benchmarks an unreliable indicator of model quality. We discuss several alternatives that circumvent the issues associated with static benchmarks, while still allowing for a comprehensive and reliable evaluation of the models. \n\nDynamic Benchmarks One of the main issues associated with traditional benchmarks is their static nature, which allows both honest-but-negligent and malicious contamination to occur. Therefore, a recent line of work (Huang et al., 2023;Li et al., 2023;Li & Flanigan, 2023;Roberts et al., 2023;Shi et al., 2023) has focused on a different type of evaluation using dynamic benchmarks. Specifically, dynamic benchmarks are periodically updated and therefore vary over time, allowing to measure model performance on benchmark data that was not available during training. Furthermore, these benchmarks can compare performance before and after model release and thus provide a simple and accurate way to measure contamination. \n\nHowever, their dynamic nature comes with considerable challenges. Specifically, high-quality benchmarks take considerable time and effort to create, thereby making dynamic benchmarks proposed by current works considerably less curated than traditional benchmarks. Furthermore, performance on dynamic benchmarks can vary considerably over time, making it harder to track progress. Especially the possibility of new models training on prior versions of the benchmark can lead to a false sense of progress. Finally, continued effort is required to ensure that the benchmark remains up-to-date and applicable to new models. \n\nHuman Evaluation Human evaluations provide the possibility for comprehensive model evaluation with limited risk of contamination over a wide range of tasks requiring expert knowledge (Chang et al., 2023;Freitag et al., 2021;Zheng et al., 2023). However, it is both time-consuming and expensive, requiring a large number of expert evaluators and a good experimental setup to prevent human biases from influencing the results (Chang et al., 2023;Zheng et al., 2023). Furthermore, human preferences can differ between individuals, cultural backgrounds and other factors (Peng et al., 1997).",
            "score": 0.4787364919683028,
            "section_title": "Discussion",
            "char_start_offset": 27921,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 462
                },
                {
                    "start": 465,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1806
                },
                {
                    "start": 1809,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2273
                },
                {
                    "start": 2274,
                    "end": 2396
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31591796875
        },
        {
            "corpus_id": "270217411",
            "title": "Probing Language Models for Pre-training Data Detection",
            "text": "Given a benchmark, we already trained a probe P \u03b8 , which can be used to detect whether a sample is included in the pre-training data.For a sample s, we preprocess it with the above prompt template, then feed it into the model M to get the activation x l .Finally, we can get the confidence score P \u03b8 (x l ), which represents the probability of the sample being a part of the pre-training dataset.This score is then utilized to classify the sample as a member or non-member based on a threshold \u03b3 :\n\nTo evaluate various pre-training data detection methods in a more challenging scenario, we introduce ArxivMIA, a new benchmark comprising abstracts from the fields of Computer Science (CS) and Mathematics (Math) sourced from Arxiv.In contrast to the existing WikiMIA (Shi et al., 2023) benchmark, which primarily utilizes event data from Wikipedia pages-thus prone to higher duplication rates in pre-training datasets-ArxivMIA presents a more challenging scenario.\n\nFor dataset construction, abstracts published post-2024 are designated as non-member data, while member data are derived from the Arxiv subset of the RedPajama dataset (Computer, 2023).The RedPajama dataset is the reproduction of the LLaMA (Touvron et al., 2023a) training dataset and is extensively utilized in pre-training LLMs (Zhang et al., 2024;Geng and Liu, 2023).This makes ArxivMIA particularly suited for testing LLMs pre-trained on the RedPajama dataset.Detailed information about ArxivMIA is presented in Table 1.In summary, ArxivMIA has three distinctive features: Firstly, it is a larger dataset with a total of 2000 samples.Secondly, it covers multiple fields, including CS and Math.Lastly, it features a longer average sentence length, with an average of 143.1 tokens per sample.These characteristics make ArxivMIA a more challenging benchmark for evaluating pre-training data detection methodologies.",
            "score": 0.4786765404102816,
            "section_title": "Pre-training Data Detection with Probe Classifier",
            "char_start_offset": 8692,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 134,
                    "end": 256
                },
                {
                    "start": 256,
                    "end": 397
                },
                {
                    "start": 397,
                    "end": 498
                },
                {
                    "start": 500,
                    "end": 731
                },
                {
                    "start": 731,
                    "end": 964
                },
                {
                    "start": 966,
                    "end": 1151
                },
                {
                    "start": 1151,
                    "end": 1336
                },
                {
                    "start": 1336,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1490
                },
                {
                    "start": 1490,
                    "end": 1604
                },
                {
                    "start": 1604,
                    "end": 1663
                },
                {
                    "start": 1663,
                    "end": 1760
                },
                {
                    "start": 1760,
                    "end": 1882
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64990234375
        },
        {
            "corpus_id": "267412617",
            "title": "Evading Data Contamination Detection for Language Models is (too) Easy",
            "text": "Before systematizing (de-)contamination practices and detection methods, we first need to formally define data contamination. We consider a (training) dataset D to be contaminated with some benchmark D \u2032 if there is an overlap between the two. We call models trained on a contaminated dataset contaminated models. For this purpose, we consider training to also include data augmentation. \n\nFrom a detection perspective, it is helpful to differentiate between sampleand benchmark-level data contamination. While sample-level contamination detection aims to determine whether a given sample x was contained in the training dataset D, benchmark-level detection aims to determine whether any subset of a benchmark D \u2032 was contained in the training set D without specifically aiming to provide this overlapping subset. More formally, we define sample-and benchmark-level contamination as follows: \n\nDefinition 1 (Sample-level Data Contamination). A dataset D is contaminated with a sample x from a benchmark \n\nSample-level detection methods provide fine-grained information on the amount of contamination in a dataset. This allows model providers to present evaluation results on a clean subset in the presence of contamination (Brown et al., 2020;Touvron et al., 2023b). However, as detection errors can significantly influence evaluation results and partial contamination can impact performance on the uncontaminated benchmark portion, it is questionable whether these results are comparable to an uncontaminated model. \n\nBenchmark-level contamination is particularly relevant to a benchmark's integrity as a performance metric. If a model has been contaminated with a benchmark, results will not be comparable to those of an uncontaminated model. However, benchmark-level methods do not provide fine-grained information regarding which or even how many samples were contaminated, making it challenging to assess the contamination's effect on model performance.",
            "score": 0.47834793162558115,
            "section_title": "Data Contamination",
            "char_start_offset": 3727,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 387
                },
                {
                    "start": 390,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1002
                },
                {
                    "start": 1005,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1516
                },
                {
                    "start": 1519,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1958
                }
            ],
            "ref_mentions": [
                {
                    "start": 1223,
                    "end": 1243,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69189453125
        },
        {
            "corpus_id": "271719982",
            "title": "StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation",
            "text": "Addressing data contamination becomes increasingly crucial for large language models as the training data grow exponentially with the data sources and processing recipes being obscure. \n\nTo demonstrate the effectiveness of StructEval in resisting the risk of data contamination, we compare the performance divergences of LLMs with and without data contamination, on the original benchmark, the data augmented benchmark and StructEval-constructed benchmark respectively. Specifically, for a seed benchmark and a base model, we use instruction fine-tuning (IFT) to train the model on both a clean dataset and a dataset contaminated with test data. To make a fair comparison, we ensure that both datasets maintain identical scale and similar data composition. Simultaneously, we integrate Alpaca-GPT-4 (Taori et al., 2023) dataset into both the training data to ensure data diversity and prevent training collapse. In this case, the contaminated set consists of Alpaca-GPT-4 and the test data, while the clean set consists of Alpaca-GPT-4 and an equal number of multichoice questions which are randomly sampled from an out-of-distribution benchmark Xiezhi (Gu et al., 2023). To ensure the robustness of our conclusions, we consider 5 wildly used base LLMs of various scales including LLaMa-7B&30B (Touvron et al., 2023a), LLaMa-2-7B&13B (Touvron et al., 2023b) and Mistral-7B (Jiang et al., 2023). Each model is trained through 3 epochs with batch size of 256 sequences, using Adam with learning rate 2e \u2212 5. \n\nWe also compare our method with the following augmentation-based approaches including character-level, word-level and instance-level, which are able to generate adversarial samples while ensuring the answerability and correctness of test instances: 1) CharDisturb (Morris et al., 2020): which randomly substitutes, deletes, inserts and swaps characters in original question. 2) Word-Net (Miller, 1992), which randomly replaces words with WordNet synonyms.",
            "score": 0.4777353697873917,
            "section_title": "Robustness of",
            "char_start_offset": 20841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 187,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1505
                },
                {
                    "start": 1508,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 1963
                }
            ],
            "ref_mentions": [
                {
                    "start": 1153,
                    "end": 1170,
                    "matchedPaperCorpusId": "259129613"
                },
                {
                    "start": 1334,
                    "end": 1357,
                    "matchedPaperCorpusId": "259950998"
                },
                {
                    "start": 1772,
                    "end": 1793,
                    "matchedPaperCorpusId": "220714040"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81005859375
        },
        {
            "corpus_id": "257038929",
            "title": "Towards Safer Generative Language Models: A Survey on Safety Risks, Evaluations, and Improvements",
            "text": "Recent instruction-following models like Chat-GPT have demonstrated the ability to act as automated agents, performing practical tasks and using tools (Xu et al., 2023b;Liu et al., 2023c). However, this has raised new safety concerns, illustrated by instances like ChaosGPT generating plans for human annihilation and GPT-4 manipulating humans to assist in CAPTCHA tests. Studies indicate that GPT-4 exhibits power-seeking behaviors such as autonomous replication and shutdown evasion (OpenAI, 2023a). Regarding such safety concerns, current safety evaluations mainly depend on manual observation. Given these models' real-world interactions, it is crucial to invest more effort in developing automated risk detectors for a more thorough monitoring of potential risks. \n\nIn the pre-training phase, language models learn from a vast array of data, which is often sourced from the Internet. While this enables the models to master complex language patterns and acquire a broad knowledge base, it also poses inherent risks. Specifically, the models may inadvertently learn and propagate biases or harmful content in the training data. As such, careful handling of data during the pre-training phase plays a critical role in mitigating models' safety risks. \n\nFiltering out undesired content from the training data is among the most commonly used approaches. This can be accomplished via heuristic rule-based methods, such as keyword matching, or by employing safety detectors with confidence scores. Safety issue detectors like BBF (Dinan et al., 2019) and Detoxify (Hanu and Unitary team, 2020), discussed in Section 3.3, can be selectively applied to identify and eliminate undesired content in the training data. Given that much of the data used for pre-training is gleaned from social media platforms, some researchers have explored author-based filtering methods. For instance, if certain authors are known for frequently posting harmful material, removing all posts from these authors can serve as an effective strategy to discard both explicit and implicit unsafe content (Dinan et al., 2019;Wang et al., 2020;Gu et al., 2022).",
            "score": 0.4766352244135116,
            "section_title": "Advanced Safety Evaluation",
            "char_start_offset": 26526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1253
                },
                {
                    "start": 1256,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 1529,
                    "end": 1549,
                    "matchedPaperCorpusId": "201070022"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6318359375
        },
        {
            "corpus_id": "265128736",
            "title": "Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models",
            "text": "Data contamination primarily occurs in two forms: verbatim, where dataset instances are directly included in the training data, and non-verbatim, such as metadata contamination. This study only focuses on estimating verbatim contamination in LLMs. Therefore, we encourage future research to develop methods for detecting and estimating other types of contamination beyond verbatim. \n\nWe also emphasize the importance of carefully considering data contamination when evaluating LLMs on benchmarks, particularly for reasoning tasks, where the goal is to assess genuine generalization abilities. Although there has been progress in reducing contamination risks through private or semi-private evaluations of newer models, recent findings suggest that contamination remains a serious concern, and private evaluations alone are neither a definitive nor a long-term solution. \n\nFor example, OpenAI's recent o3 model (Jaech et al. 2024; OpenAI 2024) achieved nearhuman performance on the ARC-AGI-1 dataset (Chollet 2019), representing a major leap in reasoning capabilities. However, it scored only in the single digits on the newly released ARC-AGI-2 dataset (Chollet et al. 2024), even though both datasets are of similar difficulty and considered relatively easy for humans, according to their creators (ARC Prize Team 2024). Since the only primary difference between the two datasets is their release time, this large performance gap raises concerns about the genuine generalization abilities of these models and the reliability of private evaluations in preventing contamination, as this longitudinal performance drop can be mostly attributed to contamination (Roberts et al. 2023). As a result, data contamination remains a major challenge in model evaluation and calls for further research.",
            "score": 0.4761807887797801,
            "section_title": "Limitations",
            "char_start_offset": 39952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 869
                },
                {
                    "start": 872,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1790
                }
            ],
            "ref_mentions": [
                {
                    "start": 999,
                    "end": 1013,
                    "matchedPaperCorpusId": "54137939"
                },
                {
                    "start": 1153,
                    "end": 1174,
                    "matchedPaperCorpusId": "54137939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "270737802",
            "title": "PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models",
            "text": "In this study, we introduce a novel approach named PaCoST (Paired Confidence Significance Testing) designed for the detection of benchmark contamination in open-source LLMs. Our method entails a three-step statistical analysis, capable of identifying benchmarks within the model's training data. Specifically, our approach involves constructing counterparts for each data instance with similar distribution, followed by statistical analysis of corresponding confidence scores to ascertain whether the model exhibits significantly higher confidence when presented with original benchmarks. We operate under the assumption that the model tends to demonstrate greater confidence when responding to questions it has been trained on. To validate our method rigorously, we conduct a series of controlled experiments. \n\nSubsequently, we employ PaCoST across a diverse array of publicly accessible LLMs, scrutinizing various benchmarks to reveal contamination outcomes. Our experimental observations indicate that, across the board, there are suspicions of contamination to varying degrees in both models and benchmarks. Consequently, we advocate for the adoption of a benchmark-free evaluation approach as a means to mitigate this contamination issue. \n\nOur contributions can be summarized as follows: \n\n\u2022 We propose several properties which a good benchmark contamination detection method should satisfy. \n\n\u2022 We introduce a simple yet effective method PaCoST to detect benchmark contamination in LLMs and validate its effectiveness and stability. \n\n\u2022 We conduct experiments on popular opensource LLMs and benchmarks and find suspected contamination on almost all tested models and benchmarks. \n\n2 Related Works",
            "score": 0.47592382574891234,
            "section_title": "Introduction",
            "char_start_offset": 2247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1244
                },
                {
                    "start": 1247,
                    "end": 1294
                },
                {
                    "start": 1297,
                    "end": 1398
                },
                {
                    "start": 1401,
                    "end": 1540
                },
                {
                    "start": 1543,
                    "end": 1686
                },
                {
                    "start": 1689,
                    "end": 1704
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9404296875
        },
        {
            "corpus_id": "236034103",
            "title": "Are Multilingual Models the Best Choice for Moderately Under-resourced Languages? A Comprehensive Assessment for Catalan",
            "text": "As studied in Brown et al. (2020), data contamination refers to the inclusion of content from test or development sets into the training corpora used for language model pre-training. Consequently, the effect of contamination might have an impact on the downstream tasks performances, distorting them. In our case, we measure the effect of data Note that all the compared models (mBERT, WikiBERT-ca, XLM-RoBERTa) have some degree of data contamination, since all of them include the Catalan Wikipedia as part of their training corpora. We show the results of BERTa along with BERTa-decontaminate in Table 4.",
            "score": 0.4757768092357937,
            "section_title": "Data contamination",
            "char_start_offset": 20416,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 606
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.428466796875
        },
        {
            "corpus_id": "218971783",
            "title": "Language Models are Few-Shot Learners",
            "text": "Since our training dataset is sourced from the internet, it is possible that our model was trained on some of our benchmark test sets. Accurately detecting test contamination from internet-scale datasets is a new area of research without established best practices. While it is common practice to train large models without investigating contamination, given the increasing scale of pretraining datasets, we believe this issue is becoming increasingly important to attend to.\n\nThis concern is not just hypothetical. One of the first papers to train a language model on Common Crawl data [TL18] detected and removed a training document which overlapped with one of their evaluation datasets. Other work such as GPT-2 [RWC + 19] also conducted post-hoc overlap analysis. Their study was relatively encouraging, finding that Poor English input: I eated the purple berries. Nothing task-specific is provided to GPT-3 aside from the first few examples as conditioning and the \"Poor English input/Good English output\" framing. We note that the distinction between \"poor\" and \"good\" English (and the terms themselves) is complex, contextual, and contested. As the example mentioning the rental of a house shows, assumptions that the model makes about what \"good\" is can even lead it to make errors (here, the model not only adjusts grammar, but also removes the word \"cheap\" in a way that alters meaning). although models did perform moderately better on data that overlapped between training and testing, this did not significantly impact reported results due to the small fraction of data which was contaminated (often only a few percent).\n\nGPT-3 operates in a somewhat different regime. On the one hand, the dataset and model size are about two orders of magnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential for contamination and memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B does not overfit its training set by a significant amount, measured relative to a held-out validation set with which it was deduplicated (Figure 4.1). Thus, we expect that contamination is likely to be frequent, but that its effects may not be as large as feared.\n\nWe initially tried to address the issue of contamination",
            "score": 0.4757768092357937,
            "section_title": "Measuring and Preventing Memorization Of Benchmarks",
            "char_start_offset": 69631,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70361328125
        },
        {
            "corpus_id": "258741333",
            "title": "Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks",
            "text": "We consider two independent scenarios of data contamination, and three assumptions underlying our mitigation strategies: \n\nScenario 1 (internet crawled corpora): The model to be evaluated is based on a training set derived automatically from the internet. The training set is closed or large enough that it is difficult to exhaustively and routinely search for all possible instances of data contamination from public test sets (both exact and approximate matches). Note that we include cases in which the model potentially trained on some form of the data's solution, even if it was not trained on the data verbatim. For example, in the task of sentiment classification of Amazon product reviews (Johnson and Zhang, 2014), the answer (the review's rating) is available on the internet, and possibly compromised in training, even if the dataset was processed and its final form was not available for the model. \n\nScenario 2 (closed API models): The model to be evaluated is a closed model behind an API, and there is no global or conditional guarantee of exclusion from future training. Any API call which contains test data compromises that test data for all models under the API holder and their affiliates.",
            "score": 0.4757768092357937,
            "section_title": "Setting",
            "char_start_offset": 2948,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 123,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1209
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2132568359375
        },
        {
            "corpus_id": "270703142",
            "title": "ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods",
            "text": "The rapid scaling of large language models (LLMs) has raised concerns about the transparency and fair use of the data used in their pretraining. Detecting such content is challenging due to the scale of the data and limited exposure of each instance during training. We propose ReCaLL (Relative Conditional Log-Likelihood), a novel membership inference attack (MIA) to detect LLMs\u2019 pretraining data by leveraging their conditional language modeling capabilities. ReCaLL examines the relative change in conditional log-likelihoods when prefixing target data points with non-member context. Our empirical findings show that conditioning member data on non-member prefixes induces a larger decrease in log-likelihood compared to non-member data. We conduct comprehensive experiments and show that ReCaLL achieves state-of-the-art performance on the WikiMIA dataset, even with random and synthetic prefixes, and can be further improved using an ensemble approach. Moreover, we conduct an in-depth analysis of LLMs\u2019 behavior with different membership contexts, providing insights into how LLMs leverage membership information for effective inference at both the sequence and token level.",
            "score": 0.4751642631838018,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91748046875
        },
        {
            "corpus_id": "269921283",
            "title": "Data Contamination Calibration for Black-box LLMs",
            "text": "As is widely acknowledged, the rapid advancements of Large Language Models (LLMs) in natural language tasks are largely attributed to the incredible expansion of the size of the training data (Kaplan et al., 2020).Despite the massive successes, this unmanaged expansion has introduced a series of significant issues that are yet explored, particularly data contamination.This issue arises notably when the benchmarking data is inadvertently included in the training sets.This contamination leads to misleading evaluation results (Zhou et al., 2023;Narayanan and Kapoor, 2023), thus deducing difficulties in acquiring effective and secure models.Additionally, training on datasets with copyrighted, private, or harmful content could violate laws, infringe on privacy, and introduce biases (Carlini et al., 2019;Nasr et al., 2018b).Unlike in earlier stages of machine learning, we posit that this problem would be much more prevalent in the age of the LLMs, very much due to the inevitable lack of scrutinization of the much-scaled -and often private -training data (Magar and Schwartz, 2022;Dodge et al., 2021).Consequently, PAC compares the polarized distance of z with its adjacent samples z, where the polarized distance is a spatial measurement jointly considering far and near probability regions.",
            "score": 0.47321859395524024,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 214,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 471
                },
                {
                    "start": 471,
                    "end": 645
                },
                {
                    "start": 645,
                    "end": 830
                },
                {
                    "start": 830,
                    "end": 1110
                },
                {
                    "start": 1110,
                    "end": 1301
                }
            ],
            "ref_mentions": [
                {
                    "start": 788,
                    "end": 810,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 810,
                    "end": 829,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1090,
                    "end": 1109,
                    "matchedPaperCorpusId": "237568724"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.254150390625
        },
        {
            "corpus_id": "277150897",
            "title": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination",
            "text": "Benchmark Data Contamination (BDC)-the inclusion of benchmark testing samples in the training set-has raised increasing concerns in Large Language Model (LLM) evaluation, leading to falsely inflated performance estimates and undermining evaluation reliability. To address this, researchers have proposed various mitigation strategies to update existing benchmarks, including modifying original questions or generating new ones based on them. However, a rigorous examination of the effectiveness of these mitigation strategies remains lacking. In this paper, we design a systematic and controlled pipeline along with two novel metrics-fidelity and contamination resistance-to provide a fine-grained and comprehensive assessment of existing BDC mitigation strategies. Previous assessment methods, such as accuracy drop and accuracy matching, focus solely on aggregate accuracy, often leading to incomplete or misleading conclusions. Our metrics address this limitation by emphasizing question-level evaluation result matching. Extensive experiments with 10 LLMs, 5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios reveal that no existing strategy significantly improves resistance over the vanilla case (i.e., no benchmark update) across all benchmarks, and none effectively balances fidelity and contamination resistance. These findings underscore the urgent need for designing more effective BDC mitigation strategies. Our code repository is available at https://github.com/ASTRAL-Group/BDC_mitigation_assessment.",
            "score": 0.4728648055840815,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76806640625
        },
        {
            "corpus_id": "266933004",
            "title": "Investigating Data Contamination for Pre-training Language Models",
            "text": "Data Contamination Definition and Investigation. The exploration of data contamination has been a consistent element in LLM reports, dating back to the initial discussions of the memorization problem in BERT [7]. Recent LLM reports [29,2,6,27,33,34] have delved deeper into how evaluation data may be duplicated within pre-training corpora. These studies typically analyze the robustness of models against data contamination through n-gram-based definitions; the analysis is also typically focused on the evaluation level as opposed to the pre-training level (recall \u00a74.1). However, such definitions may not accurately detect real contamination, casting doubt on the definitive conclusions drawn from these studies. Recent LLM studies also investigated the embedding-based contamination definitions. The contamination analysis explored in phi-1/1.5 [11,17] involves n-gram-based and embedding and syntax-based definitions but only focuses on code data. These studies represent a preliminary investigation in understanding the role of data contamination in the pre-training corpus. Another recent work [36] shows that the existing n-gram-based and embedding-based definitions can be easily evaded by applying simple paraphrasing of evaluation data, emphasizing the urgent necessity for proper definitions of contamination and reliable detection methods. \n\nData Contamination and Memorization. Memorization in neural networks has been a well-explored topic in machine learning. Previous work has studied how memorization connects to and differs from generalization [26,20,8], analyzed memorization in language models [4,25], and studied how memorization connects to privacy [14] and data extraction attacks [5,25]. Memorization is closely linked to data contamination as the model performance on evaluation data is no longer trustworthy if the evaluation data were memorized, regurgitated, and reasoned upon. Because of this connection, past work also explored membership inference attacks (MIA) for language models [21,15,23,3,22,31]. However, these methods can sometimes be computationally intensive, and more generally, examplebased matching can lead to false negatives in flagging contamination (e.g. detection can be evaded through paraphrasing [36]).",
            "score": 0.4725811443302766,
            "section_title": "Related Work",
            "char_start_offset": 23169,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 49,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2202
                },
                {
                    "start": 2203,
                    "end": 2254
                }
            ],
            "ref_mentions": [
                {
                    "start": 208,
                    "end": 211,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 232,
                    "end": 236,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 236,
                    "end": 238,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1563,
                    "end": 1567,
                    "matchedPaperCorpusId": "53953510"
                },
                {
                    "start": 1570,
                    "end": 1572,
                    "matchedPaperCorpusId": "186206616"
                },
                {
                    "start": 1672,
                    "end": 1676,
                    "matchedPaperCorpusId": "263610040"
                },
                {
                    "start": 1705,
                    "end": 1708,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 2026,
                    "end": 2029,
                    "matchedPaperCorpusId": "258967264"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83447265625
        },
        {
            "corpus_id": "237416585",
            "title": "Finetuned Language Models Are Zero-Shot Learners",
            "text": "investigate whether the performance of the model is in fact inflated by evaluating on examples that occurred in the pretraining dataset. \n\nOur data contamination procedure follows the setup of Brown et al. (2020), which, for each benchmark, produces a \"clean\" version that removes all potentially leaked examples, defined as examples for which any n-gram (n varies per dataset but is roughly 13) overlapped with anything in the pretraining corpus. We use the same n per dataset as Brown et al. (2020) and also split on spaces. We then evaluate our model on this clean subset, comparing against model performance on the original dataset (clean + dirty). Lower performance on the clean subset would suggest that data contamination leads to inflated results. \n\nFigure 12 summarizes these results, with the exact numbers given in Table 5. We see several trends very similar to those in the GPT-3 paper: (1) many datasets had a substantial number of examples that overlapped with the pretraining data, (2) across all datasets, we do not see a correlation that evaluating on clean data does worse than evaluating on the total dataset, and (3) as datasets had fewer clean examples, there was higher variance in the percent change in performance (likely due to a smaller number of clean examples). \n\nLike GPT-3, we also found that DROP and SQuADv2 had almost total overlap with the pretraining data. We follow their procedure of manually inspecting the data, and find that most overlapping ngrams were only in the contexts of examples (99.6% for DROP and 97.2% for SQuADv2). Overlaps never occurred in both the question and answer for DROP, and only occurred for both the question and answer for SQuADv2 in 5 of the 11,153 evaluation examples. Hence, for these two datasets, the ANLI R1 and R2 (Nie et al., 2020) also had almost complete data contamination, to a much higher degree than GPT-3. Upon further inspection, we see that most overlaps occur in example contexts and not hypotheses (97.3% for ANLI R1 and 98.2% for ANLI R2).",
            "score": 0.47108735754461806,
            "section_title": "Closed",
            "char_start_offset": 34633,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 139,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 755
                },
                {
                    "start": 758,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1289
                },
                {
                    "start": 1292,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 212,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 481,
                    "end": 500,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1786,
                    "end": 1803,
                    "matchedPaperCorpusId": "207756753"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81591796875
        },
        {
            "corpus_id": "261030626",
            "title": "A Methodology for Generative Spelling Correction via Natural Spelling Errors Emulation across Multiple Domains and Languages",
            "text": "To prepare pre-training datasets, we take correct samples and then corrupt them employing augmentation strategies described in 3.3. As for correct samples for experiments in Russian, we use twelve gigabytes (12GB) of raw Russian Wikipedia dumps and an open source dataset of transcripted videos in Russian6 of three and a half million (3.5M) texts. We remove all the sentences with characters other than Russian and English alphabets, digits, and punctuation or under forty characters. We balance both datasets to roughly 3.3 million sentences, resulting in a pre-training corpus of 6.611.990 texts. Then statistic-based spelling corruption is applied. We scan statistics from the train split of RUSpellRU (Sorokin et al., 2016), multiply the number of errors per sentence distribution by ten to ensure we induce a much denser noise in the pre-training corpus than it is in fine-tuning datasets, and apply to the pre-training corpus to get corrupted sentences. As a result, the pre-training dataset is a collection of 6.611.990 text pairs, each consisting of corrupted sentences and corresponding correct sentences. \n\nFor pre-training in the English language, we combine clean Leipzig Corpora Collection7 (News domain) and English Wikipedia dumps, clean them the way we applied for Russian and create a parallel corpus using a statistic-based augmentation technique based on a 5k subset of BEA60K. We result in six gigabytes (6 GB) of data for pre-training.",
            "score": 0.47108735754461806,
            "section_title": "Pre-training Data",
            "char_start_offset": 16531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1457
                }
            ],
            "ref_mentions": [
                {
                    "start": 706,
                    "end": 728,
                    "matchedPaperCorpusId": "201047249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0276947021484375
        },
        {
            "corpus_id": "270620798",
            "title": "Data Contamination Can Cross Language Barriers",
            "text": "Although we conducted extensive experiments on both the injection and detection of cross-lingual contamination, the investigation of this work has some limitations: (1) The injection of cross-lingual contamination is only based on 7B LLMs. Whether such cross-lingual contamination universally works on other sizes of LLMs is unclear. (2) The benchmarks we select are all multiple-choice questionsanswering, which limits the detection of contamination on other forms of benchmarks. We select the multiple-choice datasets as they are among the most widely adopted benchmarks for LLMs evaluation. (3) The contamination for different benchmarks and languages is injected separately, which may not reflect the real-world scenarios where multiple benchmarks and languages are blended. The main reason for not including such a multilingual and multi-benchmark mixture is the constraint on computation resources, as we employ full-parameter continual pre-training instead of parameter-efficient fine-tuning. We encourage future works to tackle these limitations and provide stronger detection methods to uncover the potential undisclosed contamination in the wild.",
            "score": 0.4705135288940269,
            "section_title": "Limitations",
            "char_start_offset": 25717,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1156
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.330810546875
        },
        {
            "corpus_id": "252907410",
            "title": "Sequential Learning Of Neural Networks for Prequential MDL",
            "text": "Test set contamination is an increasingly grave unsolved problem that becomes more pressing as machine learning research moves from curated academic benchmark data to large user provided or web-scraped data. In the recent GPT-3 paper on large scale language modeling the authors dedicate multiple pages in the Appendix to discuss the issue (Brown et al., 2020). The paper details the method used to detect training-/test set duplicates and presents experiments and the conclusion that their particular model and evaluation approach is relatively insensitive to contamination. Indeed, some of the contamination was only discovered after the model had been trained, and the model could not be retrained with decontaminated data due to the prohibitive (computational) cost. This prompted some follow-up discussion on how validation and test sets should be created and decontaminated. \n\nIn (S\u00f8gaard et al., 2021) the authors show that depending on the exact splitting procedure significantly different results can be obtained. The discussion lays out arguments in favour of either using deliberately biased splits, or forward-validating on future data. This work refers to prior work (Gorman & Bedrick, 2019), which also described inconsistent results between different test sets, and between the random split, but at large came to a different conclusion. \n\nTo the best of our knowledge these issues have been primarily discussed in the NLP community. \n\nWe mention them to point out that validation on held-out data can not blindly be regarded as the gold-standard for model evaluation when, at times, the procedure to create those splits is subject to intense discussion. Instead, careful consideration will be necessary in order to avoid misleading conclusions based on inappropriate validation and test sets. \n\nTest set contamination is not only an increasingly important problem but also in a certain sense an unsolvable one, because it is ill-defined. At what threshold for example should a document that verbatimly cites a training-set document be removed from the test-set? When are two images scraped from the internet essentially the same? \n\nOn the other side: Test set contamination is no problem when using any of the MDL variants for model evaluation. For NML and Bayes this is true because they use model complexity and marginal predictions for regularization instead of held-out sets to avoid overfitting.",
            "score": 0.4700012947609267,
            "section_title": "A PITFALLS OF USING TEST SPLITS",
            "char_start_offset": 26940,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1351
                },
                {
                    "start": 1354,
                    "end": 1447
                },
                {
                    "start": 1450,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1807
                },
                {
                    "start": 1810,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2144
                },
                {
                    "start": 2147,
                    "end": 2259
                },
                {
                    "start": 2260,
                    "end": 2415
                }
            ],
            "ref_mentions": [
                {
                    "start": 340,
                    "end": 360,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 886,
                    "end": 908,
                    "matchedPaperCorpusId": "218487319"
                },
                {
                    "start": 1180,
                    "end": 1204,
                    "matchedPaperCorpusId": "196189229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5078125
        },
        {
            "corpus_id": "262465154",
            "title": "BenLLM-Eval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP",
            "text": "Task contamination analysis (Li and Flanigan, 2023) is essential to ensure a fair model evaluation since it helps identify a model's prior exposure to test tasks on its training data. Inspired by the work of Li and Flanigan (2023), we include task contamination analysis in our evaluation to appropriately assess the performance of the LLMs. We utilize two methods: Task Example Extraction (TEE) and Membership Inference (for generative tasks like summarization and paraphrasing) to verify the evidence of task contamination. TEE involves retrieving task examples instruction-tuned models, although it doesn't require an exact match with existing training data, helps identify potential task contamination in zero and few-shot learning scenarios, as any task examples found indicate possible prior exposure. Additionally, Membership Inference is a method used for generation tasks, where the model's generated content is checked against the original dataset for exact matches. If a match is found, it implies that the content was part of the LLM's training data, indicating direct exposure rather than general learning ability. \n\nAt first, we applied TEE to all tasks without explicitly mentioning the dataset name. Our findings reveal that only GPT-3.5 could generate examples related to these tasks (except Natural Language Inference), while Claude-2 and LLaMA-2-13b-chat models failed to extract task examples for any tasks. Therefore, there is a possibility that such tasks were already included in the pre-training data of GPT-3.5. Regarding the BNLI dataset where no models could extract any task examples, we find that the premises, hypotheses, and labels generated by all LLMs for Bengali were significantly inaccurate, providing evidence that contamination did not occur. In terms of extracting task examples in the transliteration task, we find that among LLMs, only GPT-3.5 could extract the task examples for both word-level and sentence-level transliteration. \n\nFinally, we applied the Membership Inference technique in the generative tasks (e.g., summarization and paraphrase generation) to analyze task contamination. In the summarization task, none of the LLMs produced an output that aligned with the test data labels.",
            "score": 0.46876250321718466,
            "section_title": "Task Contamination Analysis",
            "char_start_offset": 16417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1127
                },
                {
                    "start": 1130,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1972
                },
                {
                    "start": 1975,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2235
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81884765625
        },
        {
            "corpus_id": "270688323",
            "title": "PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data",
            "text": "Large Language Models (LLMs) have made tremendous progress recently by excelling at several tasks (OpenAI et al., 2024;Zhang et al., 2023;Anil and Team, 2024;Reid and Team, 2024, interalia). However, it is not always clear what capabilities these models possess, leading to an increased interest in evaluation. Benchmarking is the defacto standard for evaluating LLMs, with several popular benchmarks used to validate the quality of models when they are released. \n\nHowever, standard benchmarking suffers from the following issues: many popular benchmarks are (2) We generate responses for the curated prompts from the selected models. (3) We evaluate generated responses in two settings (direct assessment and pairwise comparison) by both humans and an LLM. (4) We construct leaderboards using scores obtained and analyze the agreement between human and LLM evaluators. available on the web and have already been consumed in the training data of LLMs, rendering them unsuitable for fair evaluation. This phenomenon is known as test dataset contamination, and recent works (Ravaut et al., 2024;Golchin and Surdeanu, 2024;Dong et al., 2024;Oren et al., 2024;Deng et al., 2024) have suggested that contamination may occur not only during pre-training, but also during fine-tuning and evaluation (Balloccu et al., 2024). This calls for dynamic benchmarking with the help of humans (Chiang et al., 2024;Yang et al., 2023). Although, human evaluation is considered the gold standard, it can be expensive and time consuming. Due to this, the use of LLM-evaluators, where an LLM itself is used to evaluate the output of another LLM (sometimes the same LLM) has become very popular. \n\nMost studies on LLM training and evaluation focus on English. Recent works have shown that LLMs perform worse on non-English languages, particularly those written in scripts other than the Latin script, and under-resourced languages (Ahuja et al., 2023(Ahuja et al., , 2024;;Asai et al., 2024).",
            "score": 0.46872452363208494,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 463
                },
                {
                    "start": 466,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1971
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 138,
                    "matchedPaperCorpusId": "263835243"
                },
                {
                    "start": 1139,
                    "end": 1157,
                    "matchedPaperCorpusId": "264490730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13525390625
        },
        {
            "corpus_id": "270619707",
            "title": "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation",
            "text": "The emergence of large language models (LLMs) (Brown et al., 2020b;Touvron et al., 2023a;Zeng et al., 2023;Yang et al., 2023a;Cai et al., 2024;OpenAI, 2022;Taori et al., 2023;Chiang et al., 2023;Sun et al., 2024;Anthropic, 2023) has made the effectiveness of model capability evaluation crucial.Not only does it assist in ranking models, but it also helps in distinguishing valuable work and effective strategies for model improvement.Current LLMs are achieving increasingly better performance on various benchmarks.However, their leaked data Q: James decides to run 3 sprints 3 times a week.He runs 60 meters each sprint.How many total meters does he run a week?He sprints 3*3=9 times.So he runs 9*60=540 meters.\n\nHe runs 540 meters.performance in practical applications does not always match their benchmark results (Huang et al., 2023).This suggests that the superior performance of LLMs on benchmarks might be due to intentional or inadvertent data contamination (Golchin and Surdeanu, 2023b;Li and Flanigan, 2024;Yang et al., 2023c).LLMs are potential cheaters.\n\nThe impact of potential data contamination on model evaluation encourages researchers to establish new benchmarks for a more accurate assessment of model performance (White et al., 2024;Li et al., 2024;Zhang et al., 2024).However, any benchmark faces risk of leakage once it is publicly available.Many benchmarks are fully open, leading to varying degrees of data leakage that affect the authenticity and fairness of model evaluations.As models are trained on increasingly large datasets, it becomes likely that benchmark-related data contaminates the training sets, causing LLMs to inadvertently cheat.Additionally, creating new benchmarks is both labor-intensive and costly, making it a less favorable solution for addressing test data contamination.\n\nCan we reuse high quality leaked benchmarks for LLM evaluation?As shown in Figure 1, when a test sample is used for model training, there are two possibilities: 1.",
            "score": 0.46839958195404374,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 295
                },
                {
                    "start": 295,
                    "end": 435
                },
                {
                    "start": 435,
                    "end": 516
                },
                {
                    "start": 516,
                    "end": 592
                },
                {
                    "start": 592,
                    "end": 622
                },
                {
                    "start": 622,
                    "end": 663
                },
                {
                    "start": 663,
                    "end": 686
                },
                {
                    "start": 686,
                    "end": 713
                },
                {
                    "start": 715,
                    "end": 734
                },
                {
                    "start": 734,
                    "end": 839
                },
                {
                    "start": 839,
                    "end": 1038
                },
                {
                    "start": 1038,
                    "end": 1066
                },
                {
                    "start": 1068,
                    "end": 1290
                },
                {
                    "start": 1290,
                    "end": 1365
                },
                {
                    "start": 1365,
                    "end": 1503
                },
                {
                    "start": 1503,
                    "end": 1671
                },
                {
                    "start": 1671,
                    "end": 1820
                },
                {
                    "start": 1822,
                    "end": 1885
                },
                {
                    "start": 1885,
                    "end": 1985
                }
            ],
            "ref_mentions": [
                {
                    "start": 89,
                    "end": 107,
                    "matchedPaperCorpusId": "252715691"
                },
                {
                    "start": 996,
                    "end": 1018,
                    "matchedPaperCorpusId": "266573740"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.202392578125
        },
        {
            "corpus_id": "272593138",
            "title": "PingPong: A Benchmark for Role-Playing Language Models with User Emulation and Multi-Model Evaluation",
            "text": "Automatic evaluation with language models. LLM-as-a-Judge (Zheng et al., 2023) is an evaluation method that relies on language models, such as GPT-4, instead of humans. Popular benchmarks using this method include AlpacaEval, EQ-bench, Creative Writing, and BiGGen Bench (Dubois et al., 2024;Paech, 2023;Kim et al., 2024). The validity of these benchmarks is based on their high correlation with human annotations, specifically with Chatbot Arena (Chiang et al., 2024). \n\nMulti-model evaluation. However, all these benchmarks are based on a single model as a judge, which may introduce various biases, including selfevaluation bias (Panickssery et al., 2024;Xu et al., 2024). PoLL (Verga et al., 2024) authors aggregate evaluations from different language models similarly to us, with average pooling. They show that ensembling different models for evaluation increases the correlation with human annotations. There is another more agentic approach (Chan et al., 2024) with a referee team. \n\nMulti-turn evaluation. Most benchmarks are single-turn, which contrasts with the real-world usage of language models. There are multi-turn benchmarks, such as MT-Bench-101 (Bai et al., 2024) and MT-Eval (Kwan et al., 2024), though they focus on specific capabilities, and their evaluation procedures still differ from how humans implicitly rate language models. Data contamination. Another major problem for static public benchmarks is data leakage into the pre-training data sets of language models (Deng et al., 2024). It is challenging to avoid contamination since such tests are usually stored online and considered \"code\" during pre-training. This can occur even with no malicious intent from the model creators. The most obvious solution is to close the benchmarks completely, which requires trusting the benchmark organizers, which is difficult in a highly competitive environment. Alternative solutions include regularly updating benchmarks with new test data (White et al., 2024) or dynamically generating test data using existing language models.",
            "score": 0.4657219613057877,
            "section_title": "Related work",
            "char_start_offset": 5972,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2048
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 78,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 632,
                    "end": 658,
                    "matchedPaperCorpusId": "269293311"
                },
                {
                    "start": 658,
                    "end": 674,
                    "matchedPaperCorpusId": "267751249"
                },
                {
                    "start": 949,
                    "end": 968,
                    "matchedPaperCorpusId": "260887105"
                },
                {
                    "start": 1164,
                    "end": 1182,
                    "matchedPaperCorpusId": "267782920"
                },
                {
                    "start": 1492,
                    "end": 1511,
                    "matchedPaperCorpusId": "265220695"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.277587890625
        },
        {
            "corpus_id": "274515161",
            "title": "The Vulnerability of Language Model Benchmarks: Do They Accurately Reflect True LLM Performance?",
            "text": "The pursuit of leaderboard rankings in Large Language Models (LLMs) has created a fundamental paradox: models excel at standardized tests while failing to demonstrate genuine language understanding and adaptability. Our systematic analysis of NLP evaluation frameworks reveals pervasive vulnerabilities across the evaluation spectrum, from basic metrics to complex benchmarks like GLUE and MMLU. These vulnerabilities manifest through benchmark exploitation, dataset contamination, and evaluation bias, creating a false perception of progress in language understanding capabilities. Through extensive review of contemporary evaluation approaches, we identify significant limitations in static benchmark designs, human evaluation protocols, and LLM-as-judge frameworks, all of which compromise the reliability of current performance assessments. As LLM capabilities evolve and existing benchmarks become redundant, we lay the groundwork for new evaluation methods that resist manipulation, minimize data contamination, and assess domain-specific tasks. This requires frameworks that are adapted dynamically, addressing current limitations and providing a more accurate reflection of LLM performance.",
            "score": 0.4654511962551165,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23388671875
        },
        {
            "corpus_id": "266362809",
            "title": "LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction",
            "text": "Data contamination in evaluation is getting increasingly prevalent with the emergence of language models pre-trained on super large, automatically crawled corpora. This problem leads to significant challenges in the accurate assessment of model capabilities and generalisations. In this paper, we propose LatestEval, an automatic method that leverages the most recent texts to create uncontaminated reading comprehension evaluations. LatestEval avoids data contamination by only using texts published within a recent time window, ensuring no overlap with the training corpora of pre-trained language models. We develop the LatestEval automated pipeline to 1) gather the latest texts; 2) identify key information, and 3) construct questions targeting the information while removing the existing answers from the context. This encourages models to infer the answers themselves based on the remaining context, rather than just copy-paste. Our experiments demonstrate that language models exhibit negligible memorisation behaviours on LatestEval as opposed to previous benchmarks, suggesting a significantly reduced risk of data contamination and leading to a more robust evaluation. Data and code are publicly available at: https://github.com/liyucheng09/LatestEval.",
            "score": 0.4650615800636084,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7314453125
        },
        {
            "corpus_id": "249063028",
            "title": "Detecting Label Errors by Using Pre-Trained Language Models",
            "text": "Improving model performance in the presence of label errors comprises an area of active research (Song et al., 2022). However, existing methods focus on label errors in training data. Although seldom acknowledged, evaluation label errors are at least as pernicious as training label errors: pervasive errors in commonly used NLP benchmarks have been found to destabilize model performance (Malik and Bhardwaj, 2011;Northcutt et al., 2021b). Such findings suggest that improving training methods does not preclude the need for improving the underlying data. We propose a simple method for using large, pre-trained language models * Equal contribution. Figure 1: Precision-recall curves for label error detection: Large language models detect label errors with high precision, and far more effectively than a baseline word vector-based neural classifier. Overlaying a state-of-the-art model-agnostic error detection method, Confident Learning, results in little to no improvement (TweetNLP-5; \u00a77). \n\n(LLMs) to directly identify label errors for the purposes of correcting or removing them. \n\nThe majority of work in identifying label errors, and in data-centric artificial intelligence (DCAI) more broadly, focuses on image and healthcare data (DCAI Workshop, 2021). However, the success of the foundation model (FM) paradigm in applying pre-trained language models to a variety of NLP tasks (Bommasani et al., 2021;Reiss et al., 2020) suggests that FMs may be a powerful tool for detecting and correcting label errors in language datasets. Pre-training has been shown to imbue models with properties such as resistance to label errors, class imbalance (Karthik et al., 2021), out-of-distribution detection (Hendrycks et al., 2018), and confidence calibration (Desai and Durrett, 2020), while conferring ro-",
            "score": 0.46423488580480077,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 995
                },
                {
                    "start": 998,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1805
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 116,
                    "matchedPaperCorpusId": "220546541"
                },
                {
                    "start": 389,
                    "end": 415,
                    "matchedPaperCorpusId": "6565043"
                },
                {
                    "start": 1414,
                    "end": 1433,
                    "matchedPaperCorpusId": "226283626"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1600341796875
        },
        {
            "corpus_id": "271570943",
            "title": "Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens",
            "text": "Datasets. We perform experiments on three benchmarks, including two existing benchmarks WikiMIA [41] and MIMIR [17], as well as a new benchmark Dolma-Book that we developed upon a novel framework. WikiMIA [41] is a dataset that collect event pages from Wikipedia and distinguish training and non-training data based on a specific cutoff date. Moreover, WikiMIA1 provides categorized data based on different sentence lengths (i.e., 32, 64, 128, 256 words in one input separately), aiming to offer a detailed evaluation of varying input lengths. MIMIR [17] is benchmark that built based on seven diverse data sources in the Pile dataset [18]. Training and non-training data in this benchmark are directly sampled from the train and test sets of the Pile respectively. Moreover, MIMIR 2 provides data in different source domains with different n-gram overlap between the training and non-training data, and we keep all methods to use the \u2264 20% 7-gram overlap data groups in our evaluation since baseline methods show better performance. Dolma-Book Benchmark Construction. We construct a new benchmark Dolma-Book, which is developed upon a novel LLM framework -Open Language Model (OLMo) [20], to provide further evaluation of our method. Since it releases full pre-training data (i.e., Dolma) for the OLMo models, we could easily sample training data from the released data. However, we still need to collect nontraining data for the detection problem. Therefore, to construct our Dolma-Book benchmark: Step 1. Collect non-training book data from Project Gutenberg, which is a repository of over 70 thousand public domain books that are not protected under U.S. copyright law. Since Dolma collected this archive in April 2023, we set January 1, 2024 as the cutoff date and extract all books after this date directly from the website gutenberg.org 3 . There are 978 books in total. Step 2. For the pre-training data, we randomly sample from the groups of book data within Dolma 4 for the same number of books with the non-training ones.",
            "score": 0.4639921379419909,
            "section_title": "Setups",
            "char_start_offset": 16399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 2032
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 100,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "264451585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.452392578125
        },
        {
            "corpus_id": "265220695",
            "title": "Investigating Data Contamination in Modern Benchmarks for Large Language Models",
            "text": "Data Contamination in LLMs Rather than directly retrieving documents to assess potential data contamination in benchmarks, several contemporary studies have explored this issue from alternative angles. Golchin and Surdeanu (2023) introduce a method to discern the difference in output when prompting Large Language Models with the knowledge that they are evaluating a benchmark. Complementing this approach, other works have focused on utilizing data generated before and after model training as a starting point (Shi et al., 2023;Aiyappa et al., 2023;Roberts et al., 2023). Oren et al. (2023) present a probing method that hinges on the canonical order of data in the test set. Furthermore, recommendations to mitigate potential data leakage during the manipulation of benchmark test sets (Jacovi et al., 2023) and to perform dynamic evaluation (Zhu et al., 2023) have been suggested. Chang et al. (2023) provides a mask-based method to testify memorization in the books for LLMs. In contrast to these studies, our approach address this from two perspectives, offering a straightforward method applicable to both open-source and closed-source LLMs. Besides detecting data contamination, Magar and Schwartz (2022); Jiang et al. (2024) also investigate the correlation between pretrain phase memorization and downstream task performance.",
            "score": 0.46398813372112346,
            "section_title": "Related Work",
            "char_start_offset": 8130,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1336
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.634765625
        },
        {
            "corpus_id": "268819579",
            "title": "A Comprehensive Survey of Contamination Detection Methods in Large Language Models",
            "text": "Through the use case of fine-tuning LLMs on mathematical reasoning datasets like GSM8K (Cobbe et al., 2021), Tu et al. (2024) introduce a new paradigm in (closed-data) contamination detection. Their DICE approach consists in finding the layer of the fine-tuned LLM furthest away from its counterpart layer in the not fine-tuned same LLM. Then, a MLP binary classifier is inferred on the isolated layer's weights to predict contamination. This method is particularly effective for in-distribution contamination detection, a setup defined by the authors as fine-tuning on a dataset from the same distribution as the evaluation dataset. In this setup, DICE reaches near-perfect ROC-AUC, outperforming all other methods, including notably Min-K% Prob. \n\nLiu et al. ( 2024) also make use of the intermediate layers of a fine-tuned LLM. Their method first gathers a dataset of non-members and members ; where the former are pulled from after the pre-training cutoff or synthetically generated from ChatGPT. Then, the LLM is fine-tuned to classify membership between these two groups. Activations of intermediate layers of the fine-tuned LLM on both members and non-members are used as input of a probe classifier assessing membership. At inference time, the whole system feeds a target text to the fine-tuned LLM, extract its activations and feeds them to the probe classifier to check membership. The best layer to extract activation from is determined on a validation set. Results show greater ROC-AUC on WikiMIA (Shi et al., 2024) and ArxivMIA (Liu et al., 2024), and indicate that this probing system works better with larger models. (Fu et al., 2024a) MaxNorm (Meeus et al., 2024a) PREMIA (Feng et al., 2024) SPV-MIA (Fu et al., 2023) CAMIA (Chang et al., 2024) S2-MIA (Li et al., 2024) EM-MIA (Kim et al., 2024) Model Memorization Gemma Team Google DeepMind (2024)",
            "score": 0.46327292874900844,
            "section_title": "Probing Hidden Layers",
            "char_start_offset": 38362,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1864
                }
            ],
            "ref_mentions": [
                {
                    "start": 1509,
                    "end": 1527,
                    "matchedPaperCorpusId": "264451585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "273850342",
            "title": "Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?",
            "text": "Before diving into a deeper and benchmark-specific comparison between metrics, we first present the average maximum z-scores for each detection method, across all seven model-pre-training corpus pairs. In Table 2, we report both the averages as well as the number of model-corpus pairs for which significant EPG was found (column N ). In the following paragraphs, we discuss the main observations. \n\nNo significant EPG for DM contest, COPA, and SIQA A first observation that can be made from Table 2 is that no metric we considered found any meaningful contamination for DM Contest, COPA, and SIQA. In Appendix Figure 13 (top row), we can see that for the benchmark DM Contest, no method marked any data contaminated, suggesting that either none of the methods provides suitable signal for this \n\n6.8 37 6.9 36 7.7 33 6.3 54 \n\nTable 2: Maximum z-scores for each metric. We show the maximum z-scores for each metric and benchmark, across all models and pre-training corpus pairs where any significant EPG was found (avg) and the number of model-benchmark pairs over which this average was computed (N). The bottom row gives the per method weighted average z-score, and the total number of model-benchmark pairs for which significant EPG was found. All methods are run with their \"optimal\" hyperparameters (n = 8, mincount = 1, skip_budget = 0) -see \u00a7 6 for justification. \n\nmodel, or that the benchmark is truly entirely uncontaminated for the pre-training corpora at hand. For the benchmarks COPA, and SIQA, on the other hand (rows 9 and 12 in the same figure), up to around 50% contamination was detected, but there was no meaningful impact on performance for any of the models. \n\nLONGEST-MATCH provides overall best signal Secondly, we can see that by far the most significant EPG was found by the LONGEST-MATCH method, which finds significant EPG for 10 out of 13 benchmarks, for 54 out of, in total, 70 of the model-corpus pairs across these 10 benchmarks.",
            "score": 0.4629188922530053,
            "section_title": "At a numerical glance",
            "char_start_offset": 24481,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 824
                },
                {
                    "start": 827,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1370
                },
                {
                    "start": 1373,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1679
                },
                {
                    "start": 1682,
                    "end": 1960
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6796875
        },
        {
            "corpus_id": "264451585",
            "title": "Detecting Pretraining Data from Large Language Models",
            "text": "Chowdhery et al. (2022) further improves this by deeming an example contaminated if 70% of its 8-grams appear in the training data. Touvron et al. (2023b) builds on these methods by extending the framework to tokenized inputs and judging a token to be contaminated if it appears in any token n-gram longer than 10 tokens. However, their methods require access to retraining corpora, which is largely unavailable for recent model releases. Other approaches try to detect contamination without access to pretraining corpora. Sainz et al. (2023) simply prompts ChatGPT to generate examples from a dataset by providing the dataset's name and split. They found that the models generate verbatim instances from NLP datasets. Golchin & Surdeanu (2023) extends this framework to extract more memorized instances by incorporating partial instance content into the prompt. While these methods study contamination in closed-sourced models, they can not determine contamination on an instance level.",
            "score": 0.46186137106523967,
            "section_title": "RELATED WORK",
            "char_start_offset": 33798,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 987
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93701171875
        },
        {
            "corpus_id": "268680446",
            "title": "Concerned with Data Contamination? Assessing Countermeasures in Code Language Model",
            "text": "Large language models (LLMs) are increasingly utilized by state-of-the-art techniques to solve challenging software engineering problems [1], [2], [3], [4].Since such models were trained on a large amount of data from various code corpus, it is hard to tell whether the evaluation results of these techniques overfit the training data.In other words, the data used for evaluation may have already been inadvertently seen in the training data, resulting in exaggerated performance.The situation is identified as data contamination [5], [6], [7], [8] 1 .\n\nChallenge -Identifying the existence of data contamination is difficult.In traditional machine learning scenario where the dataset scale is not that large, the contamination could be avoid by splitting training/validation/testing sets and avoid leaking target variables during training.However, in the era of LLMs, where models are trained on vast corpora of data spanning a diverse range of topics and sources, avoiding the data contamination becomes expensive and sometimes unrealistic.First, Opacity of training source.Model developers are unwilling to disclose the training data and implementation details [14], [15], [16] due to various reasons (i.e., intellectual property, data privacy, license, and commercial competition), leaving no clues to the training data.Second, Indirect contamination.The data pipelines for LLMs are much more complex.The preprocessing steps, such as tokenization and data augmentation, can introduce contamination indirectly.Third, Scale of training data.Even if the training source is disclosed, the sheer volume of data makes it impractical to traverse the entire training set.Moreover, simply scanning the training data may not be reliable due to the differences in data format (e.g., XML, CSV) and their preprocessing (e.g., tokenization, and normalization).\n\nResearch gap -Given the uncontrollability of training process of LLMs, researchers shifted to proposing countermeasures to ensure the reliability of evaluation.Various countermeasures are introduced.For example, \u278a Using recent data (i.e., the data released after the deployed model's training cut-off date) will be less subject to contamination.",
            "score": 0.46186137106523967,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 335
                },
                {
                    "start": 335,
                    "end": 480
                },
                {
                    "start": 480,
                    "end": 552
                },
                {
                    "start": 554,
                    "end": 626
                },
                {
                    "start": 626,
                    "end": 840
                },
                {
                    "start": 840,
                    "end": 1042
                },
                {
                    "start": 1042,
                    "end": 1076
                },
                {
                    "start": 1076,
                    "end": 1324
                },
                {
                    "start": 1324,
                    "end": 1355
                },
                {
                    "start": 1355,
                    "end": 1405
                },
                {
                    "start": 1405,
                    "end": 1513
                },
                {
                    "start": 1513,
                    "end": 1543
                },
                {
                    "start": 1543,
                    "end": 1667
                },
                {
                    "start": 1667,
                    "end": 1850
                },
                {
                    "start": 1852,
                    "end": 2012
                },
                {
                    "start": 2012,
                    "end": 2051
                },
                {
                    "start": 2051,
                    "end": 2197
                }
            ],
            "ref_mentions": [
                {
                    "start": 137,
                    "end": 140,
                    "matchedPaperCorpusId": "255340904"
                },
                {
                    "start": 142,
                    "end": 145,
                    "matchedPaperCorpusId": "267523826"
                },
                {
                    "start": 147,
                    "end": 150,
                    "matchedPaperCorpusId": "260735598"
                },
                {
                    "start": 152,
                    "end": 155,
                    "matchedPaperCorpusId": "259860439"
                },
                {
                    "start": 530,
                    "end": 533,
                    "matchedPaperCorpusId": "260925501"
                },
                {
                    "start": 535,
                    "end": 538,
                    "matchedPaperCorpusId": "264555419"
                },
                {
                    "start": 540,
                    "end": 543,
                    "matchedPaperCorpusId": "266573740"
                },
                {
                    "start": 1170,
                    "end": 1174,
                    "matchedPaperCorpusId": "229156229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51220703125
        },
        {
            "corpus_id": "275906726",
            "title": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data Contamination in Large Language Models",
            "text": "Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP) with their remarkable performance across a wide range of tasks [1], [2]. These models rely on pretraining over massive-scale internet corpora, while many widely-used benchmarks are also derived from online resources, inevitably leading to the issue of data contamination [3]. Recent studies indicate that data contamination is prevalent in LLM evaluations [1], [4], [5], undermining the credibility of evaluation results and hindering fair comparisons between models. \n\nTo address data contamination, two primary solutions have been proposed: data contamination detection and dynamic data evaluation [6]. Data contamination detection aims to identify overlaps between model outputs and training data [7]. However, these methods face limitations, particularly for proprietary models such as the GPT series, which often incorporate special filtering mechanisms during generation. Additionally, detection methods primarily focus on extreme memorization (i.e., direct reproduction of training data), making it difficult to capture more subtle forms of contamination. Moreover, the training datasets of closed-source models are often treated as trade secrets, leaving external communities with limited ability to intervene directly. \n\nCompared to static benchmarks, dynamic data evaluation effectively circumvents the problem of data contamination, avoiding the inherent limitations of detection methods. Existing dynamic evaluation approaches have achieved notable progress in various domains. For instance, DYVAL [4] introduces dynamic data generation tailored for mathematical tasks; KIEval [6] incorporates multi-turn dialogue-based evaluation; LatestEval [8] excels in generating contamination-free datasets; SciEval [9] emphasizes testing scientific research capabilities through the design of entirely new questions; and [10] propose a practical strategy for generating evaluation data using simple heuristics. These methods explore the potential of dynamic evaluation from different perspectives, offering diverse tools for assessing the capabilities of LLMs.",
            "score": 0.46186137106523967,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1331
                },
                {
                    "start": 1334,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2166
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 175,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94580078125
        },
        {
            "corpus_id": "248986492",
            "title": "Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Understanding",
            "text": "We collect pre-training data from the following sources: common crawl (CC), news (NEWS, EL-KHAIR), and Wikipedia (WIKI). 3 Recent studies have highlighted the importance of cleaning up raw pre-training data for achieving better performance 3 See Appendix A.1 for data source specific details. on downstream tasks (Raffel et al., 2019;Brown et al., 2020). We developed a set of heuristics (see Appendix A.2) for cleaning our Arabic corpora that is able to filter out gibberish, noisy and duplicated texts.",
            "score": 0.46186137106523967,
            "section_title": "Data Collection and Processing",
            "char_start_offset": 8053,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 504
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5693359375
        },
        {
            "corpus_id": "267412617",
            "title": "Evading Data Contamination Detection for Language Models is (too) Easy",
            "text": "Evasively Malicious 0  Contamination Detection These benchmarks are generally made public to allow evaluation of new models. However, as LLMs are often trained on scraped web data, benchmark samples may inadvertently be part of the training dataset. This data contamination can lead to inflated benchmark performance and inaccurate evaluation results. To alleviate this issue, both model providers (Anil et al., 2023a;OpenAI, 2023;Touvron et al., 2023b) and third parties (Golchin & Surdeanu, 2023a;Oren et al., 2023;Shi et al., 2023) developed methods to detect and quantify the influence of data contamination on model performance. \n\nMalicious Actors However, high competitive pressure and significant financial stakes could incentivize malicious actors to actively contaminate their model to increase benchmark performance while evading detection. Crucially, this malicious setting is currently not considered at all when evaluating contamination detection methods. \n\nThis Work: Evading Detection We show that all current detection methods can be evaded while still boosting performance by training on rephrased benchmark samples (see Fig. 1). We believe this endangers the integrity of current benchmarks and highlights the need for a systematic study of contamination detection in the malicious setting.",
            "score": 0.4612098247005387,
            "section_title": "Openly Malicious",
            "char_start_offset": 622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 633
                },
                {
                    "start": 636,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1308
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.693359375
        },
        {
            "corpus_id": "271088808",
            "title": "Bucket Pre-training is All You Need",
            "text": "Large language models (LLMs) have achieved remarkable accomplishments across various natural language processing, coding and math benchmarks (Brown et al., 2020), as well as in tackling intricate real-world challenges (Ouyang et al., 2022).The significant advancements in large language models are attributed to their extensive pre-training on an extensive corpus of unlabeled documents.Since the amount of data for pre-training large language models is huge, it is crucial to organize the input data effectively during training.\n\nTheoretically, the optimal training approach would involve treating each document as an independent training instance, eliminating noise and truncation entirely.However, practical considerations such as training efficiency, context length, and batch processing make this approach infeasible.Currently, most existing methods organize the massive documents in the following two ways: (1) Each document is used as independent training data.During batch training, shorter sequences are padded with [PAD] tokens to match the length of the longest text in the batch, ensuring consistent input length.While common in traditional pretraining tasks, this method faces scalability challenges in the era of large language models, where datasets often reach terabyte scale.The significant amount of padding tokens introduced can reduce training efficiency.\n\n(2) All data are randomly concatenated, and then split (with a special token) according to fixed sequence lengths, e.g., 8192 (Rae et al., 2021;Zhang et al., 2022;Touvron et al., 2023;Pouransari et al., 2024).And hence, each of input data has a fixed length after segmentation, which is convenient for input into the model for batch training.However, we analyze the length distributions of three common pretraining corpora: web corpus, Wikipedia articles, and GitHub code.As shown in Figure 1, a significant majority of documents in these datasets are relatively short, with most falling within the 2k token range.This finding exposes a potential inefficiency in traditional fixedlength pretraining approaches.Using fixed lengths of 4096, 8192, or longer necessitates concatenating a substantial portion of short documents to fill the input sequence, potentially introducing noise during training.",
            "score": 0.4603831425297098,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 240,
                    "end": 387
                },
                {
                    "start": 387,
                    "end": 529
                },
                {
                    "start": 531,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 822
                },
                {
                    "start": 822,
                    "end": 968
                },
                {
                    "start": 968,
                    "end": 1125
                },
                {
                    "start": 1125,
                    "end": 1292
                },
                {
                    "start": 1292,
                    "end": 1375
                },
                {
                    "start": 1377,
                    "end": 1586
                },
                {
                    "start": 1586,
                    "end": 1719
                },
                {
                    "start": 1719,
                    "end": 1849
                },
                {
                    "start": 1849,
                    "end": 1991
                },
                {
                    "start": 1991,
                    "end": 2087
                },
                {
                    "start": 2087,
                    "end": 2274
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 161,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 218,
                    "end": 239,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1630859375
        },
        {
            "corpus_id": "276250274",
            "title": "LessLeak-Bench: A First Investigation of Data Leakage in LLMs Across 83 Software Engineering Benchmarks",
            "text": "By removing these leaked samples, we created a cleaned version of the SE benchmarks, named LessLeak-Bench. For researchers currently using or planning to use the benchmarks analyzed in our study, we recommend adopting LessLeak-Bench to ensure more reliable and trustworthy evaluation results. Developing more advanced methods to effectively detect leaked benchmark samples without access to LLM pre-training data remains an essential need. In our study, we explored the use of Perplexity scores to detect leaked samples (RQ4). While this approach showed potential, there remains significant room for improvement. To support progress in this area, we introduce the AutoDetectLeak-Bench dataset, which contains manually verified labels for both leaked and non-leaked samples. This benchmark serves as a foundation for future research aimed at developing more effective leakage detection methods, particularly in scenarios where LLM pre-training data is unavailable. As the first large-scale study on data leakage in SE benchmarks, our focus was on LLMs with publicly available pre-training data. This choice was necessary due to the current lack of highly effective automated tools capable of accurately identifying data leakage without knowledge of pre-training data in the SE community. To ensure the reliability of our findings, we selected LLMs that openly shared their pre-training data. Using this information, we constructed AutoDetectLeak-Bench. Before this work, the absence of a reliable, manually verified dataset made it challenging to assess the effectiveness of automated leakage detection tools. By providing AutoDetectLeak-Bench, we address this gap and offer a valuable resource for advancing research in this area. This dataset can support the development of accurate leakage detection methods, facilitating the identification and removal of leaked data even for models like Llama3 that do not disclose their pretraining data. Such advancements will help ensure more robust and fair evaluations in the future. \n\n6.2 Actionable Suggestions 6.2.1 Suggestions for LLM Providers. LLM providers like OpenAI and Meta have developed exceptional and powerful models. To address potential data leakage issues, a key measure is for providers to carefully curate their pre-training data, ensuring it does not contain duplicates or overlaps with SE benchmark samples.",
            "score": 0.4597172674933225,
            "section_title": "Implications",
            "char_start_offset": 55084,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2025
                },
                {
                    "start": 2028,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2174
                },
                {
                    "start": 2175,
                    "end": 2371
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "276617946",
            "title": "MathClean: A Benchmark for Synthetic Mathematical Data Cleaning",
            "text": "With the rapid development of large language models (LLMs), the quality of training data has become crucial. Among the various types of training data, mathematical data plays a key role in enabling LLMs to acquire strong reasoning abilities. While high-quality open-source data is important, it is often insufficient for pre-training, necessitating the addition of synthetic math problems. However, synthetic math questions and answers can introduce inaccuracies, which may degrade both the training data and web data. Therefore, an effective method for cleaning synthetic math data is essential. In this paper, we propose the MathClean benchmark to evaluate the effectiveness of math data cleaning models. The MathClean benchmark consists of 2,000 correct questions and 2,000 erroneous questions with additional 2,000 correct and erroneous answers sourced from augmented data based on GSM8K and MATH. Moreover, we also annotate error types for each question or answer, since it can assess whether models can correctly identify the error categories for future improvements. Finally, we present comprehensive evaluations using state-of-the-art (SOTA) models. Our results demonstrate that even strong models like GPT-o1 and DeepSeek-R1 perform poorly on this benchmark, highlighting the utility of MathClean. Our code and data is available at https://github.com/YuYingLi0/MathClean.",
            "score": 0.45950111259069615,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.410888671875
        },
        {
            "corpus_id": "253116873",
            "title": "Handling and Presenting Harmful Text in NLP Research",
            "text": "Text data is fundamental to nearly all areas of language processing, and good quality data is critical for better performing, safer and more robust models (Sambasivan et al., 2021;Xu et al., 2021). The shift towards widespread use of pre-trained models has intensified the search for more and higher quality data within the NLP community. To deal with the demands of deep learning, many data curators and researchers have turned to internet-scraped datasets, such as the Common Crawl Corpus or WebText (Radford et al., 2019). Large-scale language models pre-trained on such datasets have become popular because of their high performance and the ease with which they can be accessed, such as using Hugging Face Transformers (Wolf et al., 2020). They are now commonly-used 'out-of-thebox' for a variety of downstream tasks (Kirk et al., 2021). Most researchers can only fine-tune these models on small datasets because retraining a large model from scratch is infeasibly resource-intensive. Recent work has prioritised data acquisition over model complexity (Paullada et al., 2021) and forefronted the importance of data audits (Koch et al., 2021). While these developments are welcome, it may mean that researchers spend more time qualitatively inspecting datasets, auditing their content, and manually reviewing labels. This creates new venues for harm. However, exactly how harms transpire during data auditing and model training depends on whether harmful content is unsought i.e., encountered unexpectedly in the dataset, versus sought, i.e., collected as a feature of the research design to investigate harmful phenomena.",
            "score": 0.4591034944272301,
            "section_title": "How is Harm Encountered?",
            "char_start_offset": 9665,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1625
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 180,
                    "matchedPaperCorpusId": "231829607"
                },
                {
                    "start": 723,
                    "end": 742,
                    "matchedPaperCorpusId": "269498086"
                },
                {
                    "start": 821,
                    "end": 840,
                    "matchedPaperCorpusId": "236950797"
                },
                {
                    "start": 1056,
                    "end": 1079,
                    "matchedPaperCorpusId": "228084012"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1676025390625
        },
        {
            "corpus_id": "274859647",
            "title": "MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark",
            "text": "Multiple-choice question (MCQ) datasets like Massive Multitask Language Understanding (MMLU) are widely used to evaluate the commonsense, understanding, and problem-solving abilities of large language models (LLMs). However, the open-source nature of these benchmarks and the broad sources of training data for LLMs have inevitably led to benchmark contamination, resulting in unreliable evaluation results. To alleviate this issue, we propose a contamination-free and more challenging MCQ benchmark called MMLU-CF. This benchmark reassesses LLMs' understanding of world knowledge by averting both unintentional and malicious data leakage. To avoid unintentional data leakage, we source data from a broader domain and design three decontamination rules. To prevent malicious data leakage, we divide the benchmark into validation and test sets with similar difficulty and subject distributions. The test set remains closed-source to ensure reliable results, while the validation set is publicly available to promote transparency and facilitate independent verification. Our evaluation of mainstream LLMs reveals that the powerful GPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on the test set, which indicates the effectiveness of our approach in creating a more rigorous and contamination-free evaluation standard. The GitHub repository is available at https://github.com/microsoft/MMLU-CF and the dataset refers to https://huggingface.co/datasets/microsoft/MMLU-CF.",
            "score": 0.4577031517190348,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.272705078125
        },
        {
            "corpus_id": "274515161",
            "title": "The Vulnerability of Language Model Benchmarks: Do They Accurately Reflect True LLM Performance?",
            "text": "The race for state-of-the-art performance metrics in Large Language Models (LLMs) has exposed a critical vulnerability in how they are evaluated [7]. While LLMs achieve unprecedented scores on standardized tests, their susceptibility to dataset exploitation and pattern matching raises fundamental concerns about current evaluation methodologies. Rather than pursuing genuine language understanding, model developers increasingly exploit benchmark weaknesses, employing tactics like data contamination and climb leaderboards. Recent analyses reveal that LLMs can attain state-of-the-art results through such superficial optimization rather than deep linguistic comprehension [3], undermining the validity of existing bench-marking frameworks. This systematic failure in evaluation approaches threatens to misguide the field's progress, as models optimize for benchmarks that fail to capture genuine language understanding [1,5].",
            "score": 0.4576000536953331,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 928
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 148,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 675,
                    "end": 678,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 925,
                    "end": 927,
                    "matchedPaperCorpusId": "49313245"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10614013671875
        },
        {
            "corpus_id": "273233891",
            "title": "Detecting Training Data of Large Language Models via Expectation Maximization",
            "text": "We demonstrate that ReCaLL (Xie et al., 2024) has an over-reliance on prefix selection, while EM-MIA is designed to work without labeled data or benchmark-specific prior knowledge. EM-MIA remarkably outperforms all existing strong MIA baselines for LLMs, achieving state-of-the-art results on WikiMIA (Shi et al., 2023), the most popular benchmark for detecting LLM pre-training data. Experiments on our OLMoMIA highlight EM-MIA's robustness across diverse distributional conditions and underscore the importance of evaluating MIA methods under varied scenarios.",
            "score": 0.45749058308790697,
            "section_title": "Introduction",
            "char_start_offset": 3853,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 562
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46875
        },
        {
            "corpus_id": "270619707",
            "title": "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation",
            "text": "This paper explores eliciting truthful answers from a language model by addressing the impact of data contamination on model evaluations.We propose an inference-time decontamination method involving detection and iterative rewriting of contaminated data, leading to more accurate model performance assessments.Experiments on GSM8K and MMLU benchmarks suggest that our method can mitigate contamination effects, resulting in more reliable evaluation results.Our framework's detection, rewrite, and assurance stages allow for consistent and fair assessments without needing entirely new datasets.The reduction in contamination's impact highlights the promise of our approach in providing a realistic view of model capabilities.\n\nWe believe this work lays a foundation for future research in improving language model evaluations.Further exploration of advanced detection and rewriting techniques will continue to enhance the reliability and fairness of these assessments.",
            "score": 0.4574163084650018,
            "section_title": "Conclusion",
            "char_start_offset": 25936,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 137,
                    "end": 310
                },
                {
                    "start": 310,
                    "end": 457
                },
                {
                    "start": 457,
                    "end": 594
                },
                {
                    "start": 594,
                    "end": 725
                },
                {
                    "start": 727,
                    "end": 826
                },
                {
                    "start": 826,
                    "end": 968
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80712890625
        },
        {
            "corpus_id": "262055119",
            "title": "Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation",
            "text": "We propose a perplexity-based method to estimate language model benchmark contamination without full training data access. By comparing test perplexity to memorised and clean baselines, we can identify potential memorisation indicating leaked examples. Experiments on reading comprehension, summarisation, and multiple choice benchmarks demonstrate the approach can effectively detect contamination. This enables crucial contamination analysis when training data is unavailable, supporting credible evaluation.",
            "score": 0.45732318312132936,
            "section_title": "Conclusion",
            "char_start_offset": 16318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 510
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93212890625
        },
        {
            "corpus_id": "258741333",
            "title": "Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks",
            "text": "Common NLP models today are large language models trained on data crawled from the internet (Raffel et al., 2020;Bommasani et al., 2022;Touvron et al., 2023). This data is often malformed or obfuscated in ways that make it difficult to audit at scale (Bommasani et al., 2022;Mitchell et al., 2023). In particular, evaluation data that is also available on the web may be used as part of training, and it can be challenging to verify whether it was used in practice or not (OpenAI, 2023;Google, 2023a). 1 Worse, for many closed models, training data is considered a trade secret and thus unknown to the research community. Such models are being evaluated on data that cannot be certified to be unseen during training (Brown et al., 2020). Indeed, signs show that such models were exposed to test data (Dodge et al., 2021;Magar and Schwartz, 2022), and we refer to this as data contamination. \n\nThe above issue of internet-crawled training data is one of two prominent scenarios of data contamination we consider in this work. The second is in the access to closed models via APIs. 2 Such models are frequently used in research for various purposes (Wei et al., 2023;Qin et al., 2023;Moor et al., 2023), and thus they are also evaluated (Srivastava et al., 2022;Bubeck et al., 2023). In most cases, the institution behind the API reserves the option to use the data sent to them as training data in further iterations. 3 In this case, any valuable evaluation data sent to closed API models for any purpose is potentially compromised for any subsequent evaluations using the same data. \n\nThe NLP evaluation community is now witness to two urgent crises: Data contamination in training data crawled from the internet, and in training data collected from calls to a closed API. The implications are severe-not only is much of our evaluation methodology potentially compromised, but we also cannot fully identify the scope and magnitude of the contamination, even for open models.",
            "score": 0.45732318312132936,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1582
                },
                {
                    "start": 1585,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 113,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 716,
                    "end": 736,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1182,
                    "end": 1200,
                    "matchedPaperCorpusId": "258083369"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.314208984375
        },
        {
            "corpus_id": "265128736",
            "title": "Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models",
            "text": "Yang et al. (2023) further relaxed their settings by assuming access to training data to detect contamination. They used embedding similarity to find the top-k similar training samples for each dataset instance and used an LLM to verify if these samples were rephrased versions of the dataset instance. Similarly, Deng et al. (2023) developed a retrievalbased method that identified potential contamination by querying training corpora and assessing overlap with benchmark data. However, assuming access to training data is impractical, even for open-weight models. \n\nA few recent studies developed strategies for detecting contamination without access to training data and model weights/logits. Sainz et al. (2023a;b) prompted GPT-3.5 to generate the first instances of a dataset partition. However, this method faltered due to the sparseness of the signal in the prompt, coupled with the LLM's safety filters preventing the generation of copyrighted materials. \n\nIn contrast, Golchin and Surdeanu (2024) amplified the signal in the prompt by infusing a random-length initial segment of a dataset instance and directing the LLM to complete it. Dong et al. (2024) detected contamination by measuring the peakedness of the model's output distribution through edit distance to reveal contamination. Deng et al. (2023) proposed two methods: a mask-based approach, which detects contamination if the model can predict masked parts of dataset instances, and a question-based approach, which identifies contamination if the model generates a new answer to a question that already has an answer embedded in the prompt. However, none of these methods can estimate contamination.",
            "score": 0.45732318312132936,
            "section_title": "Related Work",
            "char_start_offset": 37383,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1670
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.869140625
        },
        {
            "corpus_id": "278164704",
            "title": "BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text",
            "text": "The advanced LLMs typically undergo extensive training on vast data, raising the possibility of unintended data exposure of benchmark. To assess this potential data contamination, we employed a text completion-based approach to detect possible leakage of benchmark data into the evaluated models' training corpora. 98 Specifically, we tokenized each test sample using model-specific tokenizers, truncating sequences at predetermined positions (tokens 10, 15, 20, 25, and 30) and prompting the LLM to predict the subsequent five tokens. The accuracy of predicted tokens compared to actual tokens (5-gram accuracy) was measured. A test sample was classified as potentially leaked if predictions exactly matched actual tokens at three or more truncation positions. To balance sufficient contextual information while avoiding overly simplistic long-context completions, truncation points began from token position 10, incrementing by intervals of 5 tokens.",
            "score": 0.45732318312132936,
            "section_title": "Data Contamination Analysis",
            "char_start_offset": 38506,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 952
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.669921875
        },
        {
            "corpus_id": "271097946",
            "title": "A Taxonomy for Data Contamination in Large Language Models",
            "text": "Our analysis highlights the importance of data format, with models performing better when pretraining data matches the evaluation format. We also observe task-specific effects, with certain contamination methods benefiting particular tasks more than others. Additionally, we find that some latestage pretraining contamination can actually be unhelpful to downstream performance, if it occurs in a substantially different format from the downstream task. Our findings underscore gaps in current decontamination practices, which primarily focus on full-dataset-level contamination and are often unable to detect approximate or noisy contamination. \n\nWe demonstrate that different types of contamination can have variable effects on model performance, highlighting the need for careful consideration during training and evaluation. With the creation of our taxonomy, we hope to promote standardization regarding the definition and categories of contamination within the research community, facilitating clear communication and collaboration, while also enabling precise detection and mitigation of contamination in pretraining data. We recommend researchers decontaminating pretraining corpora for LLMs prioritize developing techniques that address noisy evaluation data, while also ensuring rigorous scrutiny to prevent any shuffled or interleaved evaluation data from inadvertently persisting in the pretraining data. It is not enough to merely remove instances of the full test dataset in the pretraining corpus; fragments or noised versions of the test set can also inflate performance. We hope our work inspires future work on detecting and mitigating specific types of contamination.",
            "score": 0.45732318312132936,
            "section_title": "Conclusion",
            "char_start_offset": 22752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1686
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9443359375
        },
        {
            "corpus_id": "265220695",
            "title": "Investigating Data Contamination in Modern Benchmarks for Large Language Models",
            "text": "Retrieving from Large Corpora Retrieving from Large Corpus is an emerging topic in the era of LLMs. A number of works have focused on the retrieval and removal of contaminated information in training data by means of n-gram matching. Specifically, recent work has focused on building indexing tools for large corpora (Dodge et al., 2021;Piktus et al., 2023b,a;Elazar et al., 2023), which allows efficient retrieval. Additionally, previous work including GPT-3 (Appendix C;Brown et al., 2020) utilized a 13-gram tokenization strategy for both training and benchmark data for decontamination purposes. Similarly, PaLM (Chowdhery et al., 2022) employs an 8-gram approach, considering data as contaminated if there is a 70% overlap with 8-grams from the test set. Open-source models like Llama (Touvron et al., 2023a) adopt a methodology akin to GPT-3's, while Llama 2 (Touvron et al., 2023b) (Section A.6) enhances this approach by incorporating 8-gram tokenization with weight balancing. Moreover, Dodge et al. (2021) discusses documenting the large corpus C4 and benchmarking to detect data contamination, while Elazar et al. (2023) provides a detailed analysis of various aspects of open training data including C4, RedPajama, Pile, The Stack, etc, and providing analysis of potential contamination on GLUE and Su-perGLUE benchmarks. Besides the research conducted on English-only corpora, Blevins and Zettlemoyer (2022) investigate language contamination in cross-lingual settings. While n-gram matching can provide some level of detection for contaminated data, recent work has found that many test examples can remain undetected using such methods (Gunasekar et al., 2023). Futhermore, Riddell et al. (2024) also investigates code contamination both in the surface-level and semantic-level. \n\nData Contamination in LLMs Rather than directly retrieving documents to assess potential data contamination in benchmarks, several contemporary studies have explored this issue from alternative angles.",
            "score": 0.45691934201477946,
            "section_title": "Related Work",
            "char_start_offset": 6334,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1793
                },
                {
                    "start": 1796,
                    "end": 1997
                }
            ],
            "ref_mentions": [
                {
                    "start": 317,
                    "end": 337,
                    "matchedPaperCorpusId": "237568724"
                },
                {
                    "start": 996,
                    "end": 1015,
                    "matchedPaperCorpusId": "237568724"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69873046875
        },
        {
            "corpus_id": "270619707",
            "title": "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation",
            "text": "The training process of large language models (LLMs) often involves varying degrees of test data contamination. Although current LLMs are achieving increasingly better performance on various benchmarks, their performance in practical applications does not always match their benchmark results. Leakage of benchmarks can prevent the accurate assessment of LLMs' true performance. However, constructing new benchmarks is costly, labor-intensive and still carries the risk of leakage. Therefore, in this paper, we ask the question, Can we reuse these leaked benchmarks for LLM evaluation? We propose Inference-Time Decontamination (ITD) to address this issue by detecting and rewriting leaked samples without altering their difficulties. ITD can mitigate performance inflation caused by memorizing leaked benchmarks. Our proof-of-concept experiments demonstrate that ITD reduces inflated accuracy by 22.9% on GSM8K and 19.0% on MMLU. On MMLU, using Inference-time Decontamination can lead to a decrease in the results of Phi3 and Mistral by 6.7% and 3.6% respectively. We hope that ITD can provide more truthful evaluation results for large language models.",
            "score": 0.45643852808859053,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8427734375
        },
        {
            "paperId": "459c82205d2a27a8542bba7a4d478a8a23be2f5d",
            "corpusId": 258212638,
            "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "referenceCount": 46,
            "citationCount": 309,
            "influentialCitationCount": 45,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.09542",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.09542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153198380",
                    "name": "Weiwei Sun"
                },
                {
                    "authorId": "1387839383",
                    "name": "Lingyong Yan"
                },
                {
                    "authorId": "121875983",
                    "name": "Xinyu Ma"
                },
                {
                    "authorId": "1749477",
                    "name": "Pengjie Ren"
                },
                {
                    "authorId": "2136400100",
                    "name": "Dawei Yin"
                },
                {
                    "authorId": "2780667",
                    "name": "Z. Ren"
                }
            ],
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks, including search engines. However, existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge. In this paper, we first investigate generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks. Furthermore, to address concerns about data contamination of LLMs, we collect a new test set called NovelEval, based on the latest knowledge and aiming to verify the model's ability to rank unknown knowledge. Finally, to improve efficiency in real-world applications, we delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models using a permutation distillation scheme. Our evaluation results turn out that a distilled 440M model outperforms a 3B supervised model on the BEIR benchmark. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT.",
            "corpus_id": "258212638",
            "text": "Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks, including search engines. However, existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge. In this paper, we first investigate generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks. Furthermore, to address concerns about data contamination of LLMs, we collect a new test set called NovelEval, based on the latest knowledge and aiming to verify the model's ability to rank unknown knowledge. Finally, to improve efficiency in real-world applications, we delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models using a permutation distillation scheme. Our evaluation results turn out that a distilled 440M model outperforms a 3B supervised model on the BEIR benchmark. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.285888671875
        },
        {
            "paperId": "8ba39021c8c14a13664fcb14f9c37328e0995168",
            "corpusId": 279447681,
            "title": "Winter Soldier: Backdooring Language Models at Pre-Training with Indirect Data Poisoning",
            "venue": "",
            "year": 2025,
            "referenceCount": 38,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.14913, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1791507",
                    "name": "Wassim Bouaziz"
                },
                {
                    "authorId": "2093405908",
                    "name": "Mathurin Videau"
                },
                {
                    "authorId": "2306053042",
                    "name": "Nicolas Usunier"
                },
                {
                    "authorId": "1412478053",
                    "name": "El-Mahdi El-Mhamdi"
                }
            ],
            "abstract": "The pre-training of large language models (LLMs) relies on massive text datasets sourced from diverse and difficult-to-curate origins. Although membership inference attacks and hidden canaries have been explored to trace data usage, such methods rely on memorization of training data, which LM providers try to limit. In this work, we demonstrate that indirect data poisoning (where the targeted behavior is absent from training data) is not only feasible but also allow to effectively protect a dataset and trace its use. Using gradient-based optimization prompt-tuning, we make a model learn arbitrary secret sequences: secret responses to secret prompts that are absent from the training corpus. We validate our approach on language models pre-trained from scratch and show that less than 0.005% of poisoned tokens are sufficient to covertly make a LM learn a secret and detect it with extremely high confidence ($p<10^{-55}$) with a theoretically certifiable scheme. Crucially, this occurs without performance degradation (on LM benchmarks) and despite secrets never appearing in the training set.",
            "corpus_id": "279447681",
            "text": "The pre-training of large language models (LLMs) relies on massive text datasets sourced from diverse and difficult-to-curate origins. Although membership inference attacks and hidden canaries have been explored to trace data usage, such methods rely on memorization of training data, which LM providers try to limit. In this work, we demonstrate that indirect data poisoning (where the targeted behavior is absent from training data) is not only feasible but also allow to effectively protect a dataset and trace its use. Using gradient-based optimization prompt-tuning, we make a model learn arbitrary secret sequences: secret responses to secret prompts that are absent from the training corpus. We validate our approach on language models pre-trained from scratch and show that less than 0.005% of poisoned tokens are sufficient to covertly make a LM learn a secret and detect it with extremely high confidence ($p<10^{-55}$) with a theoretically certifiable scheme. Crucially, this occurs without performance degradation (on LM benchmarks) and despite secrets never appearing in the training set.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.80322265625
        },
        {
            "paperId": "f7ef135cf14f35da75311ecde77800737a65510d",
            "corpusId": 273798316,
            "title": "Scaling Up Membership Inference: When and How Attacks Succeed on Large Language Models",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 30,
            "citationCount": 3,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.00154, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1389023854",
                    "name": "Haritz Puerto"
                },
                {
                    "authorId": "35582593",
                    "name": "Martin Gubri"
                },
                {
                    "authorId": "2269993531",
                    "name": "Sangdoo Yun"
                },
                {
                    "authorId": "2329102667",
                    "name": "Seong Joon Oh"
                }
            ],
            "abstract": "Membership inference attacks (MIA) attempt to verify the membership of a given data sample in the training set for a model. MIA has become relevant in recent years, following the rapid development of large language models (LLM). Many are concerned about the usage of copyrighted materials for training them and call for methods for detecting such usage. However, recent research has largely concluded that current MIA methods do not work on LLMs. Even when they seem to work, it is usually because of the ill-designed experimental setup where other shortcut features enable\"cheating.\"In this work, we argue that MIA still works on LLMs, but only when multiple documents are presented for testing. We construct new benchmarks that measure the MIA performances at a continuous scale of data samples, from sentences (n-grams) to a collection of documents (multiple chunks of tokens). To validate the efficacy of current MIA approaches at greater scales, we adapt a recent work on Dataset Inference (DI) for the task of binary membership detection that aggregates paragraph-level MIA features to enable MIA at document and collection of documents level. This baseline achieves the first successful MIA on pre-trained and fine-tuned LLMs.",
            "corpus_id": "273798316",
            "text": "Membership inference attacks (MIA) attempt to verify the membership of a given data sample in the training set for a model. MIA has become relevant in recent years, following the rapid development of large language models (LLM). Many are concerned about the usage of copyrighted materials for training them and call for methods for detecting such usage. However, recent research has largely concluded that current MIA methods do not work on LLMs. Even when they seem to work, it is usually because of the ill-designed experimental setup where other shortcut features enable\"cheating.\"In this work, we argue that MIA still works on LLMs, but only when multiple documents are presented for testing. We construct new benchmarks that measure the MIA performances at a continuous scale of data samples, from sentences (n-grams) to a collection of documents (multiple chunks of tokens). To validate the efficacy of current MIA approaches at greater scales, we adapt a recent work on Dataset Inference (DI) for the task of binary membership detection that aggregates paragraph-level MIA features to enable MIA at document and collection of documents level. This baseline achieves the first successful MIA on pre-trained and fine-tuned LLMs.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.51171875
        },
        {
            "paperId": "6b45d0ecde8dcbcf4424ef34b7f26112aa43e9b5",
            "corpusId": 274859806,
            "title": "Memorization Over Reasoning? Exposing and Mitigating Verbatim Memorization in Large Language Models' Character Understanding Evaluation",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 51,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.14368, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2336324125",
                    "name": "Yuxuan Jiang"
                },
                {
                    "authorId": "2336094221",
                    "name": "Francis Ferraro"
                }
            ],
            "abstract": "Recently, Large Language Models (LLMs) have shown impressive performance in character understanding tasks, such as analyzing the roles, personalities, and relationships of fictional characters. However, the extensive pre-training corpora used by LLMs raise concerns that they may rely on memorizing popular fictional works rather than genuinely understanding and reasoning about them. In this work, we argue that 'gist memory'-capturing essential meaning - should be the primary mechanism for character understanding tasks, as opposed to 'verbatim memory' - exact match of a string. We introduce a simple yet effective method to mitigate mechanized memorization in character understanding evaluations while preserving the essential implicit cues needed for comprehension and reasoning. Our approach reduces memorization-driven performance on popular fictional works from 96% accuracy to 72% and results in up to an 18% drop in accuracy across various character understanding tasks. These findings underscore the issue of data contamination in existing benchmarks, which often measure memorization rather than true character understanding.",
            "corpus_id": "274859806",
            "text": "Recently, Large Language Models (LLMs) have shown impressive performance in character understanding tasks, such as analyzing the roles, personalities, and relationships of fictional characters. However, the extensive pre-training corpora used by LLMs raise concerns that they may rely on memorizing popular fictional works rather than genuinely understanding and reasoning about them. In this work, we argue that 'gist memory'-capturing essential meaning - should be the primary mechanism for character understanding tasks, as opposed to 'verbatim memory' - exact match of a string. We introduce a simple yet effective method to mitigate mechanized memorization in character understanding evaluations while preserving the essential implicit cues needed for comprehension and reasoning. Our approach reduces memorization-driven performance on popular fictional works from 96% accuracy to 72% and results in up to an 18% drop in accuracy across various character understanding tasks. These findings underscore the issue of data contamination in existing benchmarks, which often measure memorization rather than true character understanding.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.247802734375
        }
    ],
    "quotes": {
        "cost": 0.17827500000000004,
        "quotes": [
            {
                "idx": 0,
                "key": "[260925501 | Golchin et al. | 2023 | Citations: 108]",
                "snippets": "To address this issue, we propose an inexpensive and robust approach to detect data contamination for a given dataset partition automatically. Importantly, our approach functions under two realistic assumptions: (a) we lack direct access to the pre-training data of the LLMs, and (b) we have limited computational resources. Intuitively, our method starts by identifying potential contamination in individual instances that are drawn from a small random sample of the corresponding dataset partition (we use samples of 10 instances in this work).\n\nAt its core, our approach starts by identifying potential contamination at the instance level; using this information, our approach then assesses wider contamination at the partition level. To estimate contamination of individual instances, we employ\"guided instruction:\"a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or nearly matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset partition as contaminated if the average overlap score with the reference instances (as measured by ROUGE-L or BLEURT) is statistically significantly better with the completions from guided instruction compared to a\"general instruction\"that does not include the dataset and partition name. The second idea marks a dataset partition as contaminated if a classifier based on GPT-4 with few-shot in-context learning prompt marks multiple generated completions as exact/near-exact matches of the corresponding reference instances. Our best method achieves an accuracy between 92% and 100% in detecting if an LLM is contaminated with seven datasets, containing train and test/validation partitions, when contrasted with manual evaluation by human experts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "To address this issue, we propose an inexpensive and robust approach to detect data contamination for a given dataset partition automatically. Importantly, our approach functions under two realistic assumptions: (a) we lack direct access to the pre-training data of the LLMs, and (b) we have limited computational resources. Intuitively, our method starts by identifying potential contamination in individual instances that are drawn from a small random sample of the corresponding dataset partition (we use samples of 10 instances in this work).\n\nAt its core, our approach starts by identifying potential contamination at the instance level; using this information, our approach then assesses wider contamination at the partition level. To estimate contamination of individual instances, we employ\"guided instruction:\"a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or nearly matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset partition as contaminated if the average overlap score with the reference instances (as measured by ROUGE-L or BLEURT) is statistically significantly better with the completions from guided instruction compared to a\"general instruction\"that does not include the dataset and partition name. The second idea marks a dataset partition as contaminated if a classifier based on GPT-4 with few-shot in-context learning prompt marks multiple generated completions as exact/near-exact matches of the corresponding reference instances. Our best method achieves an accuracy between 92% and 100% in detecting if an LLM is contaminated with seven datasets, containing train and test/validation partitions, when contrasted with manual evaluation by human experts.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[262055119 | Li | 2023 | Citations: 35]",
                "snippets": "In this paper, we propose a novel approach to quantify potential contamination in language model evaluation benchmarks without accessing the entire training data. Instead of identifying n-gram overlaps between training and test set, we directly observe whether models exhibit memorisation behaviour on test instances.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1956,
                        "end": 2273,
                        "sentence_offsets": [
                            {
                                "start": 1956,
                                "end": 2118
                            },
                            {
                                "start": 2119,
                                "end": 2273
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this paper, we propose a novel approach to quantify potential contamination in language model evaluation benchmarks without accessing the entire training data. Instead of identifying n-gram overlaps between training and test set, we directly observe whether models exhibit memorisation behaviour on test instances."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[264451585 | Shi et al. | 2023 | Citations: 201]",
                "snippets": "To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous methods.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 691,
                        "end": 1464,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous methods."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[264490730 | Oren et al. | 2023 | Citations: 146]",
                "snippets": "In this work, we show it is possible to go beyond heuristics and provide provable guarantees of test set contamination in black box language models. More specifically, we provide a statistical test that can identify the presence of a benchmark in the pre-training dataset of a language model with provable false positive rate guarantees and without access to the model's training data or weights.\n\nTo achieve these guarantees, we exploit the fact that many datasets have a property known as exchangeability, where the order of examples in the dataset can be shuffled without affecting its joint distribution. Our key insight is that if a language model shows a preference for any particular ordering of the dataset -such as a canonical ordering that appears in publicly available repositories- this violates exchangeability and can only occur by observing the dataset during training (Figure 1). We leverage this insight to propose a set of tests that compares the language model's log probability on the 'canonical' ordering (taken from public repositories) to the log probability on a dataset with shuffled examples, and flag a dataset if the two log probabilities have statistically significant differences.\n\nUsing these ideas, we propose a computationally efficient and statistically powerful test for contamination which shards the dataset into smaller segments and performs a series of log probability comparisons within each shard. We prove that this sharded test provides control over the false positive rate, enables computationally efficient parallel tests, and substantially improves the power of the test for small p-values.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 376,
                        "end": 2012,
                        "sentence_offsets": [
                            {
                                "start": 270,
                                "end": 482
                            },
                            {
                                "start": 485,
                                "end": 633
                            },
                            {
                                "start": 634,
                                "end": 881
                            },
                            {
                                "start": 884,
                                "end": 1094
                            },
                            {
                                "start": 1095,
                                "end": 1379
                            },
                            {
                                "start": 1380,
                                "end": 1694
                            },
                            {
                                "start": 1697,
                                "end": 1923
                            },
                            {
                                "start": 1924,
                                "end": 2121
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this work, we show it is possible to go beyond heuristics and provide provable guarantees of test set contamination in black box language models. More specifically, we provide a statistical test that can identify the presence of a benchmark in the pre-training dataset of a language model with provable false positive rate guarantees and without access to the model's training data or weights.\n\nTo achieve these guarantees, we exploit the fact that many datasets have a property known as exchangeability, where the order of examples in the dataset can be shuffled without affecting its joint distribution. Our key insight is that if a language model shows a preference for any particular ordering of the dataset -such as a canonical ordering that appears in publicly available repositories- this violates exchangeability and can only occur by observing the dataset during training (Figure 1). We leverage this insight to propose a set of tests that compares the language model's log probability on the 'canonical' ordering (taken from public repositories) to the log probability on a dataset with shuffled examples, and flag a dataset if the two log probabilities have statistically significant differences.\n\nUsing these ideas, we propose a computationally efficient and statistically powerful test for contamination which shards the dataset into smaller segments and performs a series of log probability comparisons within each shard. We prove that this sharded test provides control over the false positive rate, enables computationally efficient parallel tests, and substantially improves the power of the test for small p-values."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[266359062 | Li | 2023 | Citations: 19]",
                "snippets": "The central goal of data contamination analysis is to categorise test samples as either clean or contaminated and then evaluate models separately on the clean and contaminated samples to assess the impact of contamination on the performance metrics. In this section, we describe our methodology to identify contaminated test samples. The basic idea of detecting contaminated examples in our method is to check whether test examples appear verbatim in Common Crawl. We base on Common Crawl because it is completely open-sourced and often comprises the majority of pre-training data for large language models, e.g., Common Crawl weights over 80% in GPT-3 and LLaMA training data (Brown et al., 2020;Touvron et al., 2023a).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Our Approach",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 720,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 249
                            },
                            {
                                "start": 250,
                                "end": 333
                            },
                            {
                                "start": 334,
                                "end": 464
                            },
                            {
                                "start": 465,
                                "end": 720
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The central goal of data contamination analysis is to categorise test samples as either clean or contaminated and then evaluate models separately on the clean and contaminated samples to assess the impact of contamination on the performance metrics. In this section, we describe our methodology to identify contaminated test samples. The basic idea of detecting contaminated examples in our method is to check whether test examples appear verbatim in Common Crawl. We base on Common Crawl because it is completely open-sourced and often comprises the majority of pre-training data for large language models, e.g., Common Crawl weights over 80% in GPT-3 and LLaMA training data (Brown et al., 2020;Touvron et al., 2023a)."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[266933004 | Jiang et al. | 2024 | Citations: 72]",
                "snippets": "Numerous studies on large language models (LLMs) have explored and investigated the concept of data contamination and demonstrated the robustness of these models against potential contamination in their evaluation datasets (Radford et al., 2019)(Brown et al., 2020)6,27,33,34,11]. Most definitions proposed in the existing studies are based on n-gram duplication between pre-training data and evaluation data. For instance, PaLM [6] divides the evaluation data into two categories-\"clean\" and \"contaminated\"-based on whether at least 70% of all possible 8-grams in the evaluation sample were seen at least once in the pre-training corpus. Llama 2 [34] provides a more fine-grained definition: a token is considered contaminated if it appears in any token n-gram longer than 10 tokens in both the evaluation sample and the training set, and the contamination percentage of an evaluation sample is defined to be the percentage of tokens contaminated; the evaluation data are then divided into 4 buckets-\"Clean\", \"Not Clean\", \"Not Dirty\", and \"Dirty\"-based on the contamination percentage of each evaluation sample. While intuitive, these contamination definitions primarily revolve around n-gram or token overlaps, which only target direct duplications present in both training and evaluation datasets and might provide both high false positive rate (since many semantically different texts have overlaps) and false negative rate (since simple paraphrasing can evade detection [36]).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
                },
                "metadata": [
                    {
                        "section_title": "Contamination Definitions",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1445,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 244
                            },
                            {
                                "start": 245,
                                "end": 373
                            },
                            {
                                "start": 374,
                                "end": 602
                            },
                            {
                                "start": 603,
                                "end": 1076
                            },
                            {
                                "start": 1077,
                                "end": 1445
                            }
                        ],
                        "ref_mentions": [
                            "160025533",
                            "218971783"
                        ],
                        "quote": "Numerous studies on large language models (LLMs) have explored and investigated the concept of data contamination and demonstrated the robustness of these models against potential contamination in their evaluation datasets (Radford et al., 2019)(Brown et al., 2020)6,27,33,34,11]. Most definitions proposed in the existing studies are based on n-gram duplication between pre-training data and evaluation data. For instance, PaLM [6] divides the evaluation data into two categories-\"clean\" and \"contaminated\"-based on whether at least 70% of all possible 8-grams in the evaluation sample were seen at least once in the pre-training corpus. Llama 2 [34] provides a more fine-grained definition: a token is considered contaminated if it appears in any token n-gram longer than 10 tokens in both the evaluation sample and the training set, and the contamination percentage of an evaluation sample is defined to be the percentage of tokens contaminated; the evaluation data are then divided into 4 buckets-\"Clean\", \"Not Clean\", \"Not Dirty\", and \"Dirty\"-based on the contamination percentage of each evaluation sample. While intuitive, these contamination definitions primarily revolve around n-gram or token overlaps, which only target direct duplications present in both training and evaluation datasets and might provide both high false positive rate (since many semantically different texts have overlaps) and false negative rate (since simple paraphrasing can evade detection [36])."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[267897557 | Yu et al. | 2024 | Citations: 28]",
                "snippets": "Detecting data contamination, a form of Membership Inference Attack (MIA), poses challenges for large language models (LLMs) due to their training on vast corpora and the difficulty of conducting ablation studies (Shi et al., 2023). To detect such contamination of LLMs, Wei et al. (2023) suggested comparing average loss values between training and test datasets, while Shi et al. (2023) introduced Min-K% Prob based on loss values to identify texts used in training. Our experiments show these methods are effective for pre-training but not for detecting contamination during fine-tuning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Addressing Data Contamination of LLMs",
                        "pdf_hash": "",
                        "start": 344,
                        "end": 934,
                        "sentence_offsets": [
                            {
                                "start": 344,
                                "end": 576
                            },
                            {
                                "start": 577,
                                "end": 812
                            },
                            {
                                "start": 813,
                                "end": 934
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Detecting data contamination, a form of Membership Inference Attack (MIA), poses challenges for large language models (LLMs) due to their training on vast corpora and the difficulty of conducting ablation studies (Shi et al., 2023). To detect such contamination of LLMs, Wei et al. (2023) suggested comparing average loss values between training and test datasets, while Shi et al. (2023) introduced Min-K% Prob based on loss values to identify texts used in training. Our experiments show these methods are effective for pre-training but not for detecting contamination during fine-tuning."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[268819579 | Ravaut et al. | 2024 | Citations: 7]",
                "snippets": "Min-K%++ (Zhang et al., 2024a) proposes another extension of the idea of Min-K% Prob: rather than simply getting next-token predicted probabilities, the authors propose a score which subtracts the expected log-probability and divides the score by the variance; in other words, normalizing the initial log-probability. The sequence scoring mechanism is then identical to the one of Min-K% Prob...Min-K%++ reaches state-of-the-art on WikiMIA, outperforming Min-K% Prob by up to 10 points.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Likelihood",
                        "pdf_hash": "",
                        "start": 204,
                        "end": 596,
                        "sentence_offsets": [
                            {
                                "start": 204,
                                "end": 521
                            },
                            {
                                "start": 522,
                                "end": 597
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Min-K%++ (Zhang et al., 2024a) proposes another extension of the idea of Min-K% Prob: rather than simply getting next-token predicted probabilities, the authors propose a score which subtracts the expected log-probability and divides the score by the variance; in other words, normalizing the initial log-probability. The sequence scoring mechanism is then identical to the one of Min-K% Prob"
                    },
                    {
                        "section_title": "Likelihood",
                        "pdf_hash": "",
                        "start": 1006,
                        "end": 1098,
                        "sentence_offsets": [
                            {
                                "start": 1006,
                                "end": 1097
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Min-K%++ reaches state-of-the-art on WikiMIA, outperforming Min-K% Prob by up to 10 points."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[270063170 | Dekoninck et al. | 2024 | Citations: 8]",
                "snippets": "Traditional Contamination Detection Existing contamination detection methods [18,23,24,30,(Mattern et al., 2023)36,39,40,48] aim to detect the inclusion of benchmark samples in the training data as a measure of contamination.However, these approaches show limited success, cannot quantify the contamination's effect on model performance, and have to make strict assumptions about the contamination process, making them easy to evade [17].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258967264 | Mattern et al. | 2023 | Citations: 189]": "Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend to assign higher probabilities to their training samples than non-training points. However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample. Recent work has demonstrated that reference-based attacks which compare model scores to those obtained from a reference model trained on similar data can substantially improve the performance of MIAs. However, in order to train reference models, attacks of this kind make the strong and arguably unrealistic assumption that an adversary has access to samples closely resembling the original training data. Therefore, we investigate their performance in more realistic scenarios and find that they are highly fragile in relation to the data distribution used to train reference models. To investigate whether this fragility provides a layer of safety, we propose and evaluate neighbourhood attacks, which compare model scores for a given sample to scores of synthetically generated neighbour texts and therefore eliminate the need for access to the training data distribution. We show that, in addition to being competitive with reference-based attacks that have perfect knowledge about the training data distribution, our attack clearly outperforms existing reference-free attacks as well as reference-based attacks with imperfect knowledge, which demonstrates the need for a reevaluation of the threat model of adversarial attacks."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1768,
                        "end": 2187,
                        "sentence_offsets": [
                            {
                                "start": 1768,
                                "end": 1974
                            },
                            {
                                "start": 1974,
                                "end": 2187
                            }
                        ],
                        "ref_mentions": [
                            "258967264"
                        ],
                        "quote": "Traditional Contamination Detection Existing contamination detection methods [18,23,24,30,(Mattern et al., 2023)36,39,40,48] aim to detect the inclusion of benchmark samples in the training data as a measure of contamination.However, these approaches show limited success, cannot quantify the contamination's effect on model performance, and have to make strict assumptions about the contamination process, making them easy to evade [17]."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[270217411 | Liu et al. | 2024 | Citations: 13]",
                "snippets": "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 830,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[270285708 | Xu et al. | 2024 | Citations: 56]",
                "snippets": "Efficient identification of BDC in evaluation benchmarks represents a fundamental aspect in ensuring the reliability and integrity of LLMs, while also providing the basis for developing effective mitigation strategies.This section provides a comprehensive review of literature related to the detection of BDC.We categorize the methodologies into two distinct strategies: matching-based and comparison-based, discussed in Sections 3.1 and 3.2, respectively...Deng et al. [30] proposed two novel methods to detect potential overlaps between evaluation benchmarks and pre-training corpora, tailored for both open-source and proprietary LLMs.They introduced a retrieval-based system and a Testset Slot Guessing (TS-Guessing) protocol, which involves masking incorrect answers in multiple-choice questions and prompting the model to fill in the gaps...Golchin and Surdeanu [46] presented another novel approach, the Data Contamination Quiz (DCQ), to detect and quantify BDC in LLMs.The DCQ is a series of multiple-choice questions with three perturbed versions of each dataset instance, including only word-level changes.The LLM's ability to identify the original instance among the perturbed ones indicates potential exposure to the data during pre-training...Golchin and Surdeanu (Golchin et al., 2023) presented a technique that combines instance-level identification using guided instruction prompts with partition-level assessment through overlap score comparison and classifier-based detection...Li et al. [91] presented a comprehensive report on BDC across over 15 LLMs and six multiplechoice QA benchmarks.They introduced an open-source pipeline to conduct contamination analysis on customized data and models...Dong et al. [34] focused on the distribution of generated content, they proposed a novel method, CDD (Contamination Detection via output Distribution), which uses the output distribution of LLMs to detect BDC...The paper also presents two new benchmarks, DetCon and ComiEval, for assessing BDC and mitigation methods...Li [89] proposed a novel method to detect BDC in language model evaluation without requiring access to the full training set.The technique uses perplexity to measure the extent of contamination...Oren et al. (Oren et al., 2023) presented a method to detect test set contamination in language models without needing access to the model's pre-training data or weights.The authors used a statistical test to identify contamination by comparing the likelihood of a benchmark dataset's canonical ordering against a shuffled version.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[260925501 | Golchin et al. | 2023 | Citations: 108]": "Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in measuring LLMs' real effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination at the instance level; using this information, our approach then assesses wider contamination at the partition level. To estimate contamination of individual instances, we employ\"guided instruction:\"a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or nearly matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset partition as contaminated if the average overlap score with the reference instances (as measured by ROUGE-L or BLEURT) is statistically significantly better with the completions from guided instruction compared to a\"general instruction\"that does not include the dataset and partition name. The second idea marks a dataset partition as contaminated if a classifier based on GPT-4 with few-shot in-context learning prompt marks multiple generated completions as exact/near-exact matches of the corresponding reference instances. Our best method achieves an accuracy between 92% and 100% in detecting if an LLM is contaminated with seven datasets, containing train and test/validation partitions, when contrasted with manual evaluation by human experts. Further, our findings indicate that GPT-4 is contaminated with AG News, WNLI, and XSum datasets.",
                    "[264490730 | Oren et al. | 2023 | Citations: 146]": "Large language models are trained on vast amounts of internet data, prompting concerns and speculation that they have memorized public benchmarks. Going from speculation to proof of contamination is challenging, as the pretraining data used by proprietary models are often not publicly accessible. We show that it is possible to provide provable guarantees of test set contamination in language models without access to pretraining data or model weights. Our approach leverages the fact that when there is no data contamination, all orderings of an exchangeable benchmark should be equally likely. In contrast, the tendency for language models to memorize example order means that a contaminated language model will find certain canonical orderings to be much more likely than others. Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples. We demonstrate that our procedure is sensitive enough to reliably prove test set contamination in challenging situations, including models as small as 1.4 billion parameters, on small test sets of only 1000 examples, and datasets that appear only a few times in the pretraining corpus. Using our test, we audit five popular publicly accessible language models for test set contamination and find little evidence for pervasive contamination."
                },
                "metadata": [
                    {
                        "section_title": "Related Tasks",
                        "pdf_hash": "",
                        "start": 891,
                        "end": 1346,
                        "sentence_offsets": [
                            {
                                "start": 637,
                                "end": 1109
                            },
                            {
                                "start": 1109,
                                "end": 1200
                            },
                            {
                                "start": 1200,
                                "end": 1347
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Efficient identification of BDC in evaluation benchmarks represents a fundamental aspect in ensuring the reliability and integrity of LLMs, while also providing the basis for developing effective mitigation strategies.This section provides a comprehensive review of literature related to the detection of BDC.We categorize the methodologies into two distinct strategies: matching-based and comparison-based, discussed in Sections 3.1 and 3.2, respectively"
                    },
                    {
                        "section_title": "Pretrain Corpus",
                        "pdf_hash": "",
                        "start": 42,
                        "end": 428,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 222
                            },
                            {
                                "start": 222,
                                "end": 429
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Deng et al. [30] proposed two novel methods to detect potential overlaps between evaluation benchmarks and pre-training corpora, tailored for both open-source and proprietary LLMs.They introduced a retrieval-based system and a Testset Slot Guessing (TS-Guessing) protocol, which involves masking incorrect answers in multiple-choice questions and prompting the model to fill in the gaps"
                    },
                    {
                        "section_title": "Pretrain Corpus",
                        "pdf_hash": "",
                        "start": 582,
                        "end": 988,
                        "sentence_offsets": [
                            {
                                "start": 582,
                                "end": 712
                            },
                            {
                                "start": 712,
                                "end": 851
                            },
                            {
                                "start": 851,
                                "end": 989
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Golchin and Surdeanu [46] presented another novel approach, the Data Contamination Quiz (DCQ), to detect and quantify BDC in LLMs.The DCQ is a series of multiple-choice questions with three perturbed versions of each dataset instance, including only word-level changes.The LLM's ability to identify the original instance among the perturbed ones indicates potential exposure to the data during pre-training"
                    },
                    {
                        "section_title": "Pretrain Corpus",
                        "pdf_hash": "",
                        "start": 1232,
                        "end": 1452,
                        "sentence_offsets": [
                            {
                                "start": 1232,
                                "end": 1453
                            }
                        ],
                        "ref_mentions": [
                            "260925501"
                        ],
                        "quote": "Golchin and Surdeanu (Golchin et al., 2023) presented a technique that combines instance-level identification using guided instruction prompts with partition-level assessment through overlap score comparison and classifier-based detection"
                    },
                    {
                        "section_title": "Pretrain Corpus",
                        "pdf_hash": "",
                        "start": 1674,
                        "end": 1889,
                        "sentence_offsets": [
                            {
                                "start": 1674,
                                "end": 1786
                            },
                            {
                                "start": 1786,
                                "end": 1890
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Li et al. [91] presented a comprehensive report on BDC across over 15 LLMs and six multiplechoice QA benchmarks.They introduced an open-source pipeline to conduct contamination analysis on customized data and models"
                    },
                    {
                        "section_title": "Comparison-based Methods",
                        "pdf_hash": "",
                        "start": 633,
                        "end": 841,
                        "sentence_offsets": [
                            {
                                "start": 633,
                                "end": 842
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Dong et al. [34] focused on the distribution of generated content, they proposed a novel method, CDD (Contamination Detection via output Distribution), which uses the output distribution of LLMs to detect BDC"
                    },
                    {
                        "section_title": "Comparison-based Methods",
                        "pdf_hash": "",
                        "start": 1187,
                        "end": 1292,
                        "sentence_offsets": [
                            {
                                "start": 1187,
                                "end": 1293
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The paper also presents two new benchmarks, DetCon and ComiEval, for assessing BDC and mitigation methods"
                    },
                    {
                        "section_title": "Comparison-based Methods",
                        "pdf_hash": "",
                        "start": 1465,
                        "end": 1658,
                        "sentence_offsets": [
                            {
                                "start": 1432,
                                "end": 1590
                            },
                            {
                                "start": 1590,
                                "end": 1761
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Li [89] proposed a novel method to detect BDC in language model evaluation without requiring access to the full training set.The technique uses perplexity to measure the extent of contamination"
                    },
                    {
                        "section_title": "Comparison-based Methods",
                        "pdf_hash": "",
                        "start": 2172,
                        "end": 2489,
                        "sentence_offsets": [
                            {
                                "start": 2172,
                                "end": 2328
                            },
                            {
                                "start": 2328,
                                "end": 2489
                            }
                        ],
                        "ref_mentions": [
                            "264490730"
                        ],
                        "quote": "Oren et al. (Oren et al., 2023) presented a method to detect test set contamination in language models without needing access to the model's pre-training data or weights.The authors used a statistical test to identify contamination by comparing the likelihood of a benchmark dataset's canonical ordering against a shuffled version."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[270619707 | Zhu et al. | 2024 | Citations: 7]",
                "snippets": "Traditional contamination detection methods directly calculate the overlap between pre-training data and evaluation datasets, including n-gram analysis (Touvron et al., 2023b;OpenAI, 2023;Team et al., 2023;Bai et al., 2023) and BM25 (Jiang et al., 2024) for indexing and matching.However, as pre-training data grows exponentially, even simple n-gram statistics become extremely resource-intensive.Yang et al. (2023c); Gunasekar et al. (2023) find n-gram detection unreliable due to unintentional contamination risks.More importantly, training corpora for mainstream LLMs are mostly inaccessible, so recent research has turned to focus on: i)-exploiting the distributional differences between the benchmark training set and the test set to evaluated (Xu et al., 2024).ii)-Evaluate sample-level contamination by providing text segments and black-box access to the LLM (Shi et al., 2023).Other work evaluates contamination through LLM-generated content, limited by the LLM's comprehension abilities to instrurction (Deng et al., 2023;Golchin and Surdeanu, 2023a).Some studies test if models can coherently continue a given sample part (Golchin and Surdeanu, 2023b).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 24,
                        "end": 1186,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 304
                            },
                            {
                                "start": 304,
                                "end": 421
                            },
                            {
                                "start": 421,
                                "end": 540
                            },
                            {
                                "start": 540,
                                "end": 791
                            },
                            {
                                "start": 791,
                                "end": 909
                            },
                            {
                                "start": 909,
                                "end": 1084
                            },
                            {
                                "start": 1084,
                                "end": 1186
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Traditional contamination detection methods directly calculate the overlap between pre-training data and evaluation datasets, including n-gram analysis (Touvron et al., 2023b;OpenAI, 2023;Team et al., 2023;Bai et al., 2023) and BM25 (Jiang et al., 2024) for indexing and matching.However, as pre-training data grows exponentially, even simple n-gram statistics become extremely resource-intensive.Yang et al. (2023c); Gunasekar et al. (2023) find n-gram detection unreliable due to unintentional contamination risks.More importantly, training corpora for mainstream LLMs are mostly inaccessible, so recent research has turned to focus on: i)-exploiting the distributional differences between the benchmark training set and the test set to evaluated (Xu et al., 2024).ii)-Evaluate sample-level contamination by providing text segments and black-box access to the LLM (Shi et al., 2023).Other work evaluates contamination through LLM-generated content, limited by the LLM's comprehension abilities to instrurction (Deng et al., 2023;Golchin and Surdeanu, 2023a).Some studies test if models can coherently continue a given sample part (Golchin and Surdeanu, 2023b)."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[270620798 | Yao et al. | 2024 | Citations: 12]",
                "snippets": "Existing contamination detection methods are typically based on the text overlap between training and evaluation data, which can be too superficial to reflect deeper forms of contamination.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 164,
                        "end": 353,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Existing contamination detection methods are typically based on the text overlap between training and evaluation data, which can be too superficial to reflect deeper forms of contamination."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[270737802 | Zhang et al. | 2024 | Citations: 0]",
                "snippets": "In this study, we introduce a novel approach named PaCoST (Paired Confidence Significance Testing) designed for the detection of benchmark contamination in open-source LLMs. Our method entails a three-step statistical analysis, capable of identifying benchmarks within the model's training data. Specifically, our approach involves constructing counterparts for each data instance with similar distribution, followed by statistical analysis of corresponding confidence scores to ascertain whether the model exhibits significantly higher confidence when presented with original benchmarks. We operate under the assumption that the model tends to demonstrate greater confidence when responding to questions it has been trained on. To validate our method rigorously, we conduct a series of controlled experiments.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 810,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 173
                            },
                            {
                                "start": 174,
                                "end": 295
                            },
                            {
                                "start": 296,
                                "end": 588
                            },
                            {
                                "start": 589,
                                "end": 728
                            },
                            {
                                "start": 729,
                                "end": 810
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this study, we introduce a novel approach named PaCoST (Paired Confidence Significance Testing) designed for the detection of benchmark contamination in open-source LLMs. Our method entails a three-step statistical analysis, capable of identifying benchmarks within the model's training data. Specifically, our approach involves constructing counterparts for each data instance with similar distribution, followed by statistical analysis of corresponding confidence scores to ascertain whether the model exhibits significantly higher confidence when presented with original benchmarks. We operate under the assumption that the model tends to demonstrate greater confidence when responding to questions it has been trained on. To validate our method rigorously, we conduct a series of controlled experiments."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[271097946 | Palavalli et al. | 2024 | Citations: 4]",
                "snippets": "Methods with access to pretraining data Early research on LLM data contamination primarily employed methods akin to high-order n-gram overlap detection between pretraining and evaluation data (Radford et al., 2019a;Brown et al., 2020;Wei et al., 2021;Touvron et al., 2023). Tools for qualitative analysis on large-scale corpora (such as Data Portraits (Marone and Durme, 2023) and the ROOTS Search Tool (Piktus et al., 2023)) have further increased the practicality of this type of contamination detection. However, these approaches have several limitations: they remain fairly computationally expensive, assume access to pretraining data, and generally can only detect contamination when a cluster of several test set examples co-occur (as most methods leverage data sketching (Broder, 1997) tools that are only effective for sequences above a certain length). Yang et al. (2023) proposes an LLM-based decontamination method, which leverages embedding similarity search followed by evaluation with a strong language model (e.g. GPT-4), to identify and mitigate contamination. This is computationally costly but can identify noisy contamination Methods without access to pretraining data Some approaches are capable of detecting contamination without direct access to pretraining data, but assume that the test data has not been modified or distributed across the pretraining corpus. These methods leverage metadata from the dataset to detect contamination, e.g. by leveraging dataset ordering (Sainz et al., 2023b) or the assignment of examples to specific data splits (Golchin and Surdeanu, 2023). Golchin and Surdeanu (2024) introduce the Data Contamination Quiz, a streamlined method that efficiently detects and estimates verbatim contamination in LLMs by crafting multiple choice questions that prompt a model to correctly dataset-specific content among similar but noisy alternatives.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Detecting Data Contamination",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1891,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 273
                            },
                            {
                                "start": 274,
                                "end": 506
                            },
                            {
                                "start": 507,
                                "end": 861
                            },
                            {
                                "start": 862,
                                "end": 1028
                            },
                            {
                                "start": 1029,
                                "end": 1076
                            },
                            {
                                "start": 1077,
                                "end": 1383
                            },
                            {
                                "start": 1384,
                                "end": 1599
                            },
                            {
                                "start": 1600,
                                "end": 1891
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Methods with access to pretraining data Early research on LLM data contamination primarily employed methods akin to high-order n-gram overlap detection between pretraining and evaluation data (Radford et al., 2019a;Brown et al., 2020;Wei et al., 2021;Touvron et al., 2023). Tools for qualitative analysis on large-scale corpora (such as Data Portraits (Marone and Durme, 2023) and the ROOTS Search Tool (Piktus et al., 2023)) have further increased the practicality of this type of contamination detection. However, these approaches have several limitations: they remain fairly computationally expensive, assume access to pretraining data, and generally can only detect contamination when a cluster of several test set examples co-occur (as most methods leverage data sketching (Broder, 1997) tools that are only effective for sequences above a certain length). Yang et al. (2023) proposes an LLM-based decontamination method, which leverages embedding similarity search followed by evaluation with a strong language model (e.g. GPT-4), to identify and mitigate contamination. This is computationally costly but can identify noisy contamination Methods without access to pretraining data Some approaches are capable of detecting contamination without direct access to pretraining data, but assume that the test data has not been modified or distributed across the pretraining corpus. These methods leverage metadata from the dataset to detect contamination, e.g. by leveraging dataset ordering (Sainz et al., 2023b) or the assignment of examples to specific data splits (Golchin and Surdeanu, 2023). Golchin and Surdeanu (2024) introduce the Data Contamination Quiz, a streamlined method that efficiently detects and estimates verbatim contamination in LLMs by crafting multiple choice questions that prompt a model to correctly dataset-specific content among similar but noisy alternatives."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[271570943 | Zhang et al. | 2024 | Citations: 6]",
                "snippets": "Current solutions to this problem leverage techniques explored in machine learning privacy such as Membership Inference Attacks (MIAs), which heavily depend on LLMs' capability of verbatim memorization. However, this reliance presents challenges, especially given the vast amount of training data and the restricted number of effective training epochs. In this paper, we propose an adaptive pre-training data detection method which alleviates this reliance and effectively amplify the identification. Our method adaptively locates \\textit{surprising tokens} of the input. A token is surprising to a LLM if the prediction on the token is\"certain but wrong\", which refers to low Shannon entropy of the probability distribution and low probability of the ground truth token at the same time. By using the prediction probability of surprising tokens to measure \\textit{surprising}, the detection method is achieved based on the simple hypothesis that seeing seen data is less surprising for the model compared with seeing unseen data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 232,
                        "end": 1262,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Current solutions to this problem leverage techniques explored in machine learning privacy such as Membership Inference Attacks (MIAs), which heavily depend on LLMs' capability of verbatim memorization. However, this reliance presents challenges, especially given the vast amount of training data and the restricted number of effective training epochs. In this paper, we propose an adaptive pre-training data detection method which alleviates this reliance and effectively amplify the identification. Our method adaptively locates \\textit{surprising tokens} of the input. A token is surprising to a LLM if the prediction on the token is\"certain but wrong\", which refers to low Shannon entropy of the probability distribution and low probability of the ground truth token at the same time. By using the prediction probability of surprising tokens to measure \\textit{surprising}, the detection method is achieved based on the simple hypothesis that seeing seen data is less surprising for the model compared with seeing unseen data."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[272423548 | Wang et al. | 2024 | Citations: 6]",
                "snippets": "Existing methods typically analyze target text in isolation or solely with non-member contexts, overlooking potential insights from simultaneously considering both member and non-member contexts. While previous work suggested that member contexts provide little information due to the minor distributional shift they induce, our analysis reveals that these subtle shifts can be effectively leveraged when contrasted with non-member contexts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Existing methods typically analyze target text in isolation or solely with non-member contexts, overlooking potential insights from simultaneously considering both member and non-member contexts. While previous work suggested that member contexts provide little information due to the minor distributional shift they induce, our analysis reveals that these subtle shifts can be effectively leveraged when contrasted with non-member contexts.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[272689919 | Samuel et al. | 2024 | Citations: 8]",
                "snippets": "To address this gap and provide a dual investigation of SOTA LLM contamination status and detection method robustness, we evaluate five contamination detection approaches with four state-of-the-art LLMs across eight challenging datasets often used in modern LLM evaluation.\n\nWe examine five distinct approaches to detecting data contamination in LLMs, including three state-of-the-art techniques from ICLR (2023-2024) and two exploratory prompt-based approaches. Two approaches are based on sequence probabilities and require access to model parameters. Figure 1 illustrates the visual overview of each approach. For each method, we also note the limitations we identified during our examination. The overview of these methods are illustrated in Figure 1.\n\n\u2022 Min-K% Prob (Shi et al., 2023) assesses whether a text was in an LLM's pre-training data by calculating the average log-likelihood of the k% lowestprobability tokens, with a high result suggesting the text's presence in the training data. Limitations: (1) The authors report AUC based on the proposed WiKiMIA dataset, in which they regarded data events before the model release as contaminated data. Such a strong assumption on the ground truth may require more justification. (2) They did not provide the threshold to determine the value of min-K%-prob in the paper since they claim they can use AUC; however, in real-world settings, we do not always have the oracle to determine AUC -instead, we need a metric for determining whether arbitrary datasets are contaminated. (3) The code is not available.\n\n\u2022 Canonical Order Statistical Testing (Oren et al., 2024) identifies contamination in a pre-training dataset by checking if the model shows a preference for the canonical order of examples over random shuffling. This preference is tested by comparing their log probabilities, with results aggregated across datasets to ensure a low false positive rate. Limitations: When an individual data example's length is long, the combination of sample/shards/permutations in the setup can be costly.\n\n\u2022 Token Completion Overlap Score (Golchin and Surdeanu, 2024b) detects contamination by prompting the LLM with a dataset name, partition type, and a random initial segment of a reference instance. If the LLM's output closely matches the latter part of the reference, the instance is flagged as contaminated.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "To address this gap and provide a dual investigation of SOTA LLM contamination status and detection method robustness, we evaluate five contamination detection approaches with four state-of-the-art LLMs across eight challenging datasets often used in modern LLM evaluation.\n\nWe examine five distinct approaches to detecting data contamination in LLMs, including three state-of-the-art techniques from ICLR (2023-2024) and two exploratory prompt-based approaches. Two approaches are based on sequence probabilities and require access to model parameters. Figure 1 illustrates the visual overview of each approach. For each method, we also note the limitations we identified during our examination. The overview of these methods are illustrated in Figure 1.\n\n\u2022 Min-K% Prob (Shi et al., 2023) assesses whether a text was in an LLM's pre-training data by calculating the average log-likelihood of the k% lowestprobability tokens, with a high result suggesting the text's presence in the training data. Limitations: (1) The authors report AUC based on the proposed WiKiMIA dataset, in which they regarded data events before the model release as contaminated data. Such a strong assumption on the ground truth may require more justification. (2) They did not provide the threshold to determine the value of min-K%-prob in the paper since they claim they can use AUC; however, in real-world settings, we do not always have the oracle to determine AUC -instead, we need a metric for determining whether arbitrary datasets are contaminated. (3) The code is not available.\n\n\u2022 Canonical Order Statistical Testing (Oren et al., 2024) identifies contamination in a pre-training dataset by checking if the model shows a preference for the canonical order of examples over random shuffling. This preference is tested by comparing their log probabilities, with results aggregated across datasets to ensure a low false positive rate. Limitations: When an individual data example's length is long, the combination of sample/shards/permutations in the setup can be costly.\n\n\u2022 Token Completion Overlap Score (Golchin and Surdeanu, 2024b) detects contamination by prompting the LLM with a dataset name, partition type, and a random initial segment of a reference instance. If the LLM's output closely matches the latter part of the reference, the instance is flagged as contaminated.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[273350935 | Zhang et al. | 2024 | Citations: 1]",
                "snippets": "Current methods differentiate members and non-members by designing scoring functions, like Perplexity and Min-k%. However, the diversity and complexity of training data magnifies the difficulty of distinguishing, leading to suboptimal performance in detecting pretraining data.\n\nOn the one hand, model developers can remove evaluation benchmark data from training data by retrieval-based methods with access to pertaining data (Ravaut et al., 2024;Chowdhery et al., 2023). Specifically, those methods employ n-gram tokenization and string-matching for detecting data contamination (Brown et al., 2020;Touvron et al., 2023b;Team et al., 2023;Radford et al., 2019). On the other hand, researchers utilize prompting techniques (Golchin & Surdeanu, 2024), performance analysis (Ye et al., 2024;Debenedetti et al., 2024), model likelihood (Oren et al., 2024;Shi et al., 2024;Xu et al., 2024b) to detect potential contamination without access to the training data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Current methods differentiate members and non-members by designing scoring functions, like Perplexity and Min-k%. However, the diversity and complexity of training data magnifies the difficulty of distinguishing, leading to suboptimal performance in detecting pretraining data.\n\nOn the one hand, model developers can remove evaluation benchmark data from training data by retrieval-based methods with access to pertaining data (Ravaut et al., 2024;Chowdhery et al., 2023). Specifically, those methods employ n-gram tokenization and string-matching for detecting data contamination (Brown et al., 2020;Touvron et al., 2023b;Team et al., 2023;Radford et al., 2019). On the other hand, researchers utilize prompting techniques (Golchin & Surdeanu, 2024), performance analysis (Ye et al., 2024;Debenedetti et al., 2024), model likelihood (Oren et al., 2024;Shi et al., 2024;Xu et al., 2024b) to detect potential contamination without access to the training data.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[273850342 | Singh et al. | 2024 | Citations: 13]",
                "snippets": "Hampering the interpretation of benchmark scores, evaluation data contamination has become a growing concern in the evaluation of LLMs, and an active area of research studies its effects. While evaluation data contamination is easily understood intuitively, it is surprisingly difficult to define precisely which samples should be considered contaminated and, consequently, how it impacts benchmark scores. We propose that these questions should be addressed together and that contamination metrics can be assessed based on whether models benefit from the examples they mark contaminated. We propose a novel analysis method called ConTAM, and show with a large scale survey of existing and novel n-gram based contamination metrics across 13 benchmarks and 7 models from 2 different families that ConTAM can be used to better understand evaluation data contamination and its effects.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Hampering the interpretation of benchmark scores, evaluation data contamination has become a growing concern in the evaluation of LLMs, and an active area of research studies its effects. While evaluation data contamination is easily understood intuitively, it is surprisingly difficult to define precisely which samples should be considered contaminated and, consequently, how it impacts benchmark scores. We propose that these questions should be addressed together and that contamination metrics can be assessed based on whether models benefit from the examples they mark contaminated. We propose a novel analysis method called ConTAM, and show with a large scale survey of existing and novel n-gram based contamination metrics across 13 benchmarks and 7 models from 2 different families that ConTAM can be used to better understand evaluation data contamination and its effects.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[275906726 | Fan | 2025 | Citations: 0]",
                "snippets": "To address data contamination, two primary solutions have been proposed: data contamination detection and dynamic data evaluation [6]. Data contamination detection aims to identify overlaps between model outputs and training data [7]. However, these methods face limitations, particularly for proprietary models such as the GPT series, which often incorporate special filtering mechanisms during generation. Additionally, detection methods primarily focus on extreme memorization (i.e., direct reproduction of training data), making it difficult to capture more subtle forms of contamination.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "I. INTRODUCTION",
                        "pdf_hash": "",
                        "start": 574,
                        "end": 1166,
                        "sentence_offsets": [
                            {
                                "start": 574,
                                "end": 708
                            },
                            {
                                "start": 709,
                                "end": 808
                            },
                            {
                                "start": 809,
                                "end": 981
                            },
                            {
                                "start": 982,
                                "end": 1166
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To address data contamination, two primary solutions have been proposed: data contamination detection and dynamic data evaluation [6]. Data contamination detection aims to identify overlaps between model outputs and training data [7]. However, these methods face limitations, particularly for proprietary models such as the GPT series, which often incorporate special filtering mechanisms during generation. Additionally, detection methods primarily focus on extreme memorization (i.e., direct reproduction of training data), making it difficult to capture more subtle forms of contamination."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[276575227 | Sander et al. | 2025 | Citations: 1]",
                "snippets": "Benchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect \"radioactivity\", i.e., traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, e.g., p-val = 10 \u22123 for +5% on ARC-Easy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 998,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Benchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect \"radioactivity\", i.e., traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, e.g., p-val = 10 \u22123 for +5% on ARC-Easy."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.029934,
        "cot": "For this query on detecting pre-training data contamination in large language models and evaluating benchmarks like WIKIMIA, I'll organize the quotes into meaningful dimensions.\n\n1. \"Introduction and Background\" - This section will provide essential context about data contamination in LLMs, explaining why it matters and basic concepts. This should be in synthesis format to give readers a coherent understanding of the problem space.\n\n2. \"Matching-Based Detection Methods\" - These are methods that directly try to find overlaps between training data and evaluation data. A synthesis format makes sense here to explain the approaches and their connections.\n\n3. \"Statistical and Probability-Based Detection Methods\" - These methods focus on statistical signals and probability patterns to detect contamination. A synthesis paragraph is appropriate to explain the underlying principles and how they work together.\n\n4. \"Min-K% Prob and its Variants\" - This specific approach appears multiple times in the quotes and seems particularly important, especially in relation to WIKIMIA. A synthesis format will help explain how it works and its advantages.\n\n5. \"Benchmark Datasets for Contamination Detection\" - This section will cover WIKIMIA and other benchmarks designed to evaluate contamination detection methods. A list format works well here to clearly present the different benchmarks.\n\n6. \"Limitations of Current Detection Methods\" - A synthesis format is best to analyze the various limitations across different approaches, showing patterns and common challenges.\n\n7. \"Novel and Alternative Detection Approaches\" - This dimension covers emerging or specialized methods. A list format helps present these distinct approaches clearly.\n\nThis organization follows a logical flow from basic concepts to specific methods, benchmarks, limitations, and finally novel approaches, matching how a comprehensive report would be structured.",
        "plan": {
            "Introduction and Background (synthesis)": [
                5,
                8,
                19,
                20
            ],
            "Matching-Based Detection Methods (synthesis)": [
                4,
                10,
                11,
                14
            ],
            "Statistical and Probability-Based Detection Methods (synthesis)": [
                1,
                3,
                6,
                13,
                14,
                17,
                18
            ],
            "Min-K% Prob and its Variants (synthesis)": [
                2,
                6,
                7,
                15,
                16,
                17
            ],
            "Benchmark Datasets for Contamination Detection (list)": [
                2,
                9,
                10,
                19,
                21
            ],
            "Limitations of Current Detection Methods (synthesis)": [
                5,
                8,
                11,
                12,
                17,
                18,
                20
            ],
            "Novel and Alternative Detection Approaches (list)": [
                0,
                9,
                10,
                13,
                17,
                21
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction and Background",
                "tldr": "Data contamination in large language models (LLMs) occurs when evaluation benchmarks overlap with training data, artificially inflating performance metrics. Various detection methods have emerged to identify contamination, primarily focusing on n-gram matching between training and evaluation datasets. (4 sources)",
                "text": "\nData contamination has become a growing concern in the evaluation of large language models (LLMs), hampering the interpretation of benchmark scores and raising questions about the reliability of reported model performance <Paper corpusId=\"273850342\" paperTitle=\"(Singh et al., 2024)\" isShortName></Paper>. The issue arises when evaluation data has been inadvertently included in the model's training corpus, potentially enabling the model to simply recall memorized information rather than demonstrate true generalization capabilities.\n\nNumerous studies have investigated this phenomenon, with most existing definitions of contamination being based on n-gram duplication between pre-training and evaluation datasets <Paper corpusId=\"266933004\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>. For example, some research teams have established specific thresholds: PaLM researchers classified evaluation data as \"contaminated\" if at least 70% of all possible 8-grams in an evaluation sample appeared at least once in the pre-training corpus, while Llama 2 developers provided a more fine-grained definition based on the percentage of tokens that appear in both training and evaluation sets <Paper corpusId=\"266933004\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>.\n\nHowever, these traditional n-gram-based approaches have significant limitations. They primarily target direct duplications, which can lead to both high false positive rates (as semantically different texts may have overlaps) and false negative rates (since simple paraphrasing can evade detection) <Paper corpusId=\"266933004\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>. Additionally, existing contamination detection methods have shown limited success and cannot effectively quantify the contamination's impact on model performance <Paper corpusId=\"270063170\" paperTitle=\"(Dekoninck et al., 2024)\" isShortName></Paper>. These methods also make strict assumptions about the contamination process, making them relatively easy to circumvent <Paper corpusId=\"270063170\" paperTitle=\"(Dekoninck et al., 2024)\" isShortName></Paper>.\n\nTwo primary solutions have been proposed to address data contamination: data contamination detection and dynamic data evaluation <Paper corpusId=\"275906726\" paperTitle=\"(Fan, 2025)\" isShortName></Paper>. While detection methods aim to identify overlaps between model outputs and training data, they face considerable challenges with proprietary models like the GPT series, which often incorporate special filtering mechanisms during generation <Paper corpusId=\"275906726\" paperTitle=\"(Fan, 2025)\" isShortName></Paper>. Furthermore, current detection approaches primarily focus on extreme memorization (direct reproduction of training data), making it difficult to capture more subtle forms of contamination that might still affect evaluation results <Paper corpusId=\"275906726\" paperTitle=\"(Fan, 2025)\" isShortName></Paper>.\n\nThe complexity of defining and detecting contamination has led researchers to propose that contamination metrics should be assessed based on whether models benefit from the examples they mark as contaminated, suggesting a more outcome-oriented approach to understanding this phenomenon <Paper corpusId=\"273850342\" paperTitle=\"(Singh et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Singh et al., 2024)",
                        "snippets": [
                            "Hampering the interpretation of benchmark scores, evaluation data contamination has become a growing concern in the evaluation of LLMs, and an active area of research studies its effects. While evaluation data contamination is easily understood intuitively, it is surprisingly difficult to define precisely which samples should be considered contaminated and, consequently, how it impacts benchmark scores. We propose that these questions should be addressed together and that contamination metrics can be assessed based on whether models benefit from the examples they mark contaminated. We propose a novel analysis method called ConTAM, and show with a large scale survey of existing and novel n-gram based contamination metrics across 13 benchmarks and 7 models from 2 different families that ConTAM can be used to better understand evaluation data contamination and its effects."
                        ],
                        "paper": {
                            "corpus_id": 273850342,
                            "title": "Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?",
                            "authors": [
                                {
                                    "authorId": "2306863572",
                                    "name": "Aaditya K. Singh"
                                },
                                {
                                    "authorId": "1665851460",
                                    "name": "Muhammed Yusuf Kocyigit"
                                },
                                {
                                    "authorId": "2282542314",
                                    "name": "Andrew Poulton"
                                },
                                {
                                    "authorId": "71039937",
                                    "name": "David Esiobu"
                                },
                                {
                                    "authorId": "2253400960",
                                    "name": "Maria Lomeli"
                                },
                                {
                                    "authorId": "2253402270",
                                    "name": "Gergely Szilvasy"
                                },
                                {
                                    "authorId": "3449411",
                                    "name": "Dieuwke Hupkes"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 13
                        },
                        "score": 0.92529296875
                    },
                    {
                        "id": "(Jiang et al., 2024)",
                        "snippets": [
                            "Numerous studies on large language models (LLMs) have explored and investigated the concept of data contamination and demonstrated the robustness of these models against potential contamination in their evaluation datasets (Radford et al., 2019)(Brown et al., 2020)6,27,33,34,11]. Most definitions proposed in the existing studies are based on n-gram duplication between pre-training data and evaluation data. For instance, PaLM [6] divides the evaluation data into two categories-\"clean\" and \"contaminated\"-based on whether at least 70% of all possible 8-grams in the evaluation sample were seen at least once in the pre-training corpus. Llama 2 [34] provides a more fine-grained definition: a token is considered contaminated if it appears in any token n-gram longer than 10 tokens in both the evaluation sample and the training set, and the contamination percentage of an evaluation sample is defined to be the percentage of tokens contaminated; the evaluation data are then divided into 4 buckets-\"Clean\", \"Not Clean\", \"Not Dirty\", and \"Dirty\"-based on the contamination percentage of each evaluation sample. While intuitive, these contamination definitions primarily revolve around n-gram or token overlaps, which only target direct duplications present in both training and evaluation datasets and might provide both high false positive rate (since many semantically different texts have overlaps) and false negative rate (since simple paraphrasing can evade detection [36])."
                        ],
                        "paper": {
                            "corpus_id": 266933004,
                            "title": "Investigating Data Contamination for Pre-training Language Models",
                            "authors": [
                                {
                                    "authorId": "2800541",
                                    "name": "Minhao Jiang"
                                },
                                {
                                    "authorId": "2298016051",
                                    "name": "Ken Ziyu Liu"
                                },
                                {
                                    "authorId": "1606040932",
                                    "name": "Ming Zhong"
                                },
                                {
                                    "authorId": "1749176844",
                                    "name": "Rylan Schaeffer"
                                },
                                {
                                    "authorId": "2260339714",
                                    "name": "Siru Ouyang"
                                },
                                {
                                    "authorId": "2259869648",
                                    "name": "Jiawei Han"
                                },
                                {
                                    "authorId": "123593472",
                                    "name": "Sanmi Koyejo"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 72
                        },
                        "score": 0.9453125
                    },
                    {
                        "id": "(Dekoninck et al., 2024)",
                        "snippets": [
                            "Traditional Contamination Detection Existing contamination detection methods [18,23,24,30,(Mattern et al., 2023)36,39,40,48] aim to detect the inclusion of benchmark samples in the training data as a measure of contamination.However, these approaches show limited success, cannot quantify the contamination's effect on model performance, and have to make strict assumptions about the contamination process, making them easy to evade [17]."
                        ],
                        "paper": {
                            "corpus_id": 270063170,
                            "title": "ConStat: Performance-Based Contamination Detection in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2268310707",
                                    "name": "Jasper Dekoninck"
                                },
                                {
                                    "authorId": "2116235329",
                                    "name": "Mark Niklas M\u00fcller"
                                },
                                {
                                    "authorId": "1736447",
                                    "name": "Martin T. Vechev"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 8
                        },
                        "score": 0.97705078125
                    },
                    {
                        "id": "(Fan, 2025)",
                        "snippets": [
                            "To address data contamination, two primary solutions have been proposed: data contamination detection and dynamic data evaluation [6]. Data contamination detection aims to identify overlaps between model outputs and training data [7]. However, these methods face limitations, particularly for proprietary models such as the GPT series, which often incorporate special filtering mechanisms during generation. Additionally, detection methods primarily focus on extreme memorization (i.e., direct reproduction of training data), making it difficult to capture more subtle forms of contamination."
                        ],
                        "paper": {
                            "corpus_id": 275906726,
                            "title": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data Contamination in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2342797440",
                                    "name": "Yang Fan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.94580078125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Matching-Based Detection Methods",
                "tldr": "Matching-based detection methods identify contamination by directly comparing evaluation data with pre-training corpora or by assessing model behavior when prompted with segments of evaluation data, with approaches ranging from n-gram overlap analysis to more sophisticated retrieval-based systems. (6 sources)",
                "text": "\nMatching-based detection methods form a significant category of techniques for identifying benchmark data contamination (BDC) in large language models. These approaches typically involve direct comparisons between evaluation datasets and pre-training corpora, or analyzing model behavior when presented with portions of evaluation data.\n\nThe most traditional approach involves calculating n-gram overlap between pre-training and evaluation datasets, which has been widely employed in the development of models like GPT-3, LLaMA, and Claude <Paper corpusId=\"270619707\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>. This method directly checks whether test examples appear verbatim in pre-training corpora, with Common Crawl often serving as a reference point because it comprises a majority of pre-training data for many LLMs <Paper corpusId=\"266359062\" paperTitle=\"(Li, 2023)\" isShortName></Paper>. Beyond simple n-gram analysis, some researchers have employed BM25 for indexing and matching across datasets <Paper corpusId=\"270619707\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>.\n\nHowever, these direct matching approaches face significant limitations. As pre-training datasets grow exponentially, even basic n-gram statistics become extremely resource-intensive to compute. Furthermore, these methods often prove unreliable due to unintentional contamination risks <Paper corpusId=\"270619707\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>. They also typically assume access to pre-training data and can only detect contamination when multiple test examples co-occur, as they rely on data sketching tools that are only effective for sequences above certain lengths <Paper corpusId=\"271097946\" paperTitle=\"(Palavalli et al., 2024)\" isShortName></Paper>.\n\nTo address scenarios where pre-training data is inaccessible (especially with proprietary models), researchers have developed more sophisticated detection techniques. Deng et al. proposed a retrieval-based system and a \"Testset Slot Guessing\" protocol that involves masking incorrect answers in multiple-choice questions and prompting the model to fill in the gaps <Paper corpusId=\"270285708\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>. Similarly, Golchin and Surdeanu introduced the \"Data Contamination Quiz\" (DCQ), which presents models with multiple-choice questions containing three perturbed versions of each dataset instance. The model's ability to identify the original instance among the perturbed ones indicates potential exposure during pre-training <Paper corpusId=\"270285708\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"260925501\" paperTitle=\"(Golchin et al., 2023)\" isShortName></Paper>.\n\nAnother notable approach is the \"guided instruction\" method, which identifies potential contamination by prompting an LLM with the dataset name, partition type, and a random-length initial segment of a reference instance, then asking it to complete the text. An instance is flagged as contaminated if the model's output closely matches the latter segment of the reference <Paper corpusId=\"260925501\" paperTitle=\"(Golchin et al., 2023)\" isShortName></Paper>. Some researchers have also leveraged dataset metadata for detection, such as dataset ordering or the assignment of examples to specific data splits <Paper corpusId=\"271097946\" paperTitle=\"(Palavalli et al., 2024)\" isShortName></Paper>.\n\nOren et al. proposed a method that provides provable guarantees of test set contamination without requiring access to pre-training data or model weights. Their approach is based on the principle that in uncontaminated models, all orderings of an exchangeable benchmark should be equally likely. By comparing the likelihood of a canonically ordered benchmark dataset with its shuffled version, they can detect potential contamination <Paper corpusId=\"270285708\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264490730\" paperTitle=\"(Oren et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Zhu et al., 2024)",
                        "snippets": [
                            "Traditional contamination detection methods directly calculate the overlap between pre-training data and evaluation datasets, including n-gram analysis (Touvron et al., 2023b;OpenAI, 2023;Team et al., 2023;Bai et al., 2023) and BM25 (Jiang et al., 2024) for indexing and matching.However, as pre-training data grows exponentially, even simple n-gram statistics become extremely resource-intensive.Yang et al. (2023c); Gunasekar et al. (2023) find n-gram detection unreliable due to unintentional contamination risks.More importantly, training corpora for mainstream LLMs are mostly inaccessible, so recent research has turned to focus on: i)-exploiting the distributional differences between the benchmark training set and the test set to evaluated (Xu et al., 2024).ii)-Evaluate sample-level contamination by providing text segments and black-box access to the LLM (Shi et al., 2023).Other work evaluates contamination through LLM-generated content, limited by the LLM's comprehension abilities to instrurction (Deng et al., 2023;Golchin and Surdeanu, 2023a).Some studies test if models can coherently continue a given sample part (Golchin and Surdeanu, 2023b)."
                        ],
                        "paper": {
                            "corpus_id": 270619707,
                            "title": "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation",
                            "authors": [
                                {
                                    "authorId": "2307992515",
                                    "name": "Qin Zhu"
                                },
                                {
                                    "authorId": "2290056525",
                                    "name": "Qingyuan Cheng"
                                },
                                {
                                    "authorId": "2307468261",
                                    "name": "Runyu Peng"
                                },
                                {
                                    "authorId": "50080067",
                                    "name": "Xiaonan Li"
                                },
                                {
                                    "authorId": "2136108329",
                                    "name": "Tengxiao Liu"
                                },
                                {
                                    "authorId": "2307468261",
                                    "name": "Runyu Peng"
                                },
                                {
                                    "authorId": "2282972251",
                                    "name": "Xipeng Qiu"
                                },
                                {
                                    "authorId": "2284750473",
                                    "name": "Xuanjing Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 7
                        },
                        "score": 0.92431640625
                    },
                    {
                        "id": "(Li, 2023)",
                        "snippets": [
                            "The central goal of data contamination analysis is to categorise test samples as either clean or contaminated and then evaluate models separately on the clean and contaminated samples to assess the impact of contamination on the performance metrics. In this section, we describe our methodology to identify contaminated test samples. The basic idea of detecting contaminated examples in our method is to check whether test examples appear verbatim in Common Crawl. We base on Common Crawl because it is completely open-sourced and often comprises the majority of pre-training data for large language models, e.g., Common Crawl weights over 80% in GPT-3 and LLaMA training data (Brown et al., 2020;Touvron et al., 2023a)."
                        ],
                        "paper": {
                            "corpus_id": 266359062,
                            "title": "An Open-Source Data Contamination Report for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "1527099159",
                                    "name": "Yucheng Li"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 19
                        },
                        "score": 0.92138671875
                    },
                    {
                        "id": "(Palavalli et al., 2024)",
                        "snippets": [
                            "Methods with access to pretraining data Early research on LLM data contamination primarily employed methods akin to high-order n-gram overlap detection between pretraining and evaluation data (Radford et al., 2019a;Brown et al., 2020;Wei et al., 2021;Touvron et al., 2023). Tools for qualitative analysis on large-scale corpora (such as Data Portraits (Marone and Durme, 2023) and the ROOTS Search Tool (Piktus et al., 2023)) have further increased the practicality of this type of contamination detection. However, these approaches have several limitations: they remain fairly computationally expensive, assume access to pretraining data, and generally can only detect contamination when a cluster of several test set examples co-occur (as most methods leverage data sketching (Broder, 1997) tools that are only effective for sequences above a certain length). Yang et al. (2023) proposes an LLM-based decontamination method, which leverages embedding similarity search followed by evaluation with a strong language model (e.g. GPT-4), to identify and mitigate contamination. This is computationally costly but can identify noisy contamination Methods without access to pretraining data Some approaches are capable of detecting contamination without direct access to pretraining data, but assume that the test data has not been modified or distributed across the pretraining corpus. These methods leverage metadata from the dataset to detect contamination, e.g. by leveraging dataset ordering (Sainz et al., 2023b) or the assignment of examples to specific data splits (Golchin and Surdeanu, 2023). Golchin and Surdeanu (2024) introduce the Data Contamination Quiz, a streamlined method that efficiently detects and estimates verbatim contamination in LLMs by crafting multiple choice questions that prompt a model to correctly dataset-specific content among similar but noisy alternatives."
                        ],
                        "paper": {
                            "corpus_id": 271097946,
                            "title": "A Taxonomy for Data Contamination in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2220962946",
                                    "name": "Medha Palavalli"
                                },
                                {
                                    "authorId": "2138301112",
                                    "name": "Amanda Bertsch"
                                },
                                {
                                    "authorId": "1762110",
                                    "name": "Matthew R. Gormley"
                                }
                            ],
                            "year": 2024,
                            "venue": "CONDA",
                            "n_citations": 4
                        },
                        "score": 0.95166015625
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Efficient identification of BDC in evaluation benchmarks represents a fundamental aspect in ensuring the reliability and integrity of LLMs, while also providing the basis for developing effective mitigation strategies.This section provides a comprehensive review of literature related to the detection of BDC.We categorize the methodologies into two distinct strategies: matching-based and comparison-based, discussed in Sections 3.1 and 3.2, respectively",
                            "Deng et al. [30] proposed two novel methods to detect potential overlaps between evaluation benchmarks and pre-training corpora, tailored for both open-source and proprietary LLMs.They introduced a retrieval-based system and a Testset Slot Guessing (TS-Guessing) protocol, which involves masking incorrect answers in multiple-choice questions and prompting the model to fill in the gaps",
                            "Golchin and Surdeanu [46] presented another novel approach, the Data Contamination Quiz (DCQ), to detect and quantify BDC in LLMs.The DCQ is a series of multiple-choice questions with three perturbed versions of each dataset instance, including only word-level changes.The LLM's ability to identify the original instance among the perturbed ones indicates potential exposure to the data during pre-training",
                            "Golchin and Surdeanu (Golchin et al., 2023) presented a technique that combines instance-level identification using guided instruction prompts with partition-level assessment through overlap score comparison and classifier-based detection",
                            "Li et al. [91] presented a comprehensive report on BDC across over 15 LLMs and six multiplechoice QA benchmarks.They introduced an open-source pipeline to conduct contamination analysis on customized data and models",
                            "Dong et al. [34] focused on the distribution of generated content, they proposed a novel method, CDD (Contamination Detection via output Distribution), which uses the output distribution of LLMs to detect BDC",
                            "The paper also presents two new benchmarks, DetCon and ComiEval, for assessing BDC and mitigation methods",
                            "Li [89] proposed a novel method to detect BDC in language model evaluation without requiring access to the full training set.The technique uses perplexity to measure the extent of contamination",
                            "Oren et al. (Oren et al., 2023) presented a method to detect test set contamination in language models without needing access to the model's pre-training data or weights.The authors used a statistical test to identify contamination by comparing the likelihood of a benchmark dataset's canonical ordering against a shuffled version."
                        ],
                        "paper": {
                            "corpus_id": 270285708,
                            "title": "Benchmark Data Contamination of Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2153079650",
                                    "name": "Cheng Xu"
                                },
                                {
                                    "authorId": "2304954016",
                                    "name": "Shuhao Guan"
                                },
                                {
                                    "authorId": "2304952028",
                                    "name": "Derek Greene"
                                },
                                {
                                    "authorId": "2266906186",
                                    "name": "Mohand-Tahar Kechadi"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 56
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Golchin et al., 2023)",
                        "snippets": [
                            "To address this issue, we propose an inexpensive and robust approach to detect data contamination for a given dataset partition automatically. Importantly, our approach functions under two realistic assumptions: (a) we lack direct access to the pre-training data of the LLMs, and (b) we have limited computational resources. Intuitively, our method starts by identifying potential contamination in individual instances that are drawn from a small random sample of the corresponding dataset partition (we use samples of 10 instances in this work).\n\nAt its core, our approach starts by identifying potential contamination at the instance level; using this information, our approach then assesses wider contamination at the partition level. To estimate contamination of individual instances, we employ\"guided instruction:\"a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or nearly matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset partition as contaminated if the average overlap score with the reference instances (as measured by ROUGE-L or BLEURT) is statistically significantly better with the completions from guided instruction compared to a\"general instruction\"that does not include the dataset and partition name. The second idea marks a dataset partition as contaminated if a classifier based on GPT-4 with few-shot in-context learning prompt marks multiple generated completions as exact/near-exact matches of the corresponding reference instances. Our best method achieves an accuracy between 92% and 100% in detecting if an LLM is contaminated with seven datasets, containing train and test/validation partitions, when contrasted with manual evaluation by human experts."
                        ],
                        "paper": {
                            "corpus_id": 260925501,
                            "title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "65754049",
                                    "name": "Shahriar Golchin"
                                },
                                {
                                    "authorId": "1760868",
                                    "name": "M. Surdeanu"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 108
                        },
                        "score": 0.95458984375
                    },
                    {
                        "id": "(Oren et al., 2023)",
                        "snippets": [
                            "In this work, we show it is possible to go beyond heuristics and provide provable guarantees of test set contamination in black box language models. More specifically, we provide a statistical test that can identify the presence of a benchmark in the pre-training dataset of a language model with provable false positive rate guarantees and without access to the model's training data or weights.\n\nTo achieve these guarantees, we exploit the fact that many datasets have a property known as exchangeability, where the order of examples in the dataset can be shuffled without affecting its joint distribution. Our key insight is that if a language model shows a preference for any particular ordering of the dataset -such as a canonical ordering that appears in publicly available repositories- this violates exchangeability and can only occur by observing the dataset during training (Figure 1). We leverage this insight to propose a set of tests that compares the language model's log probability on the 'canonical' ordering (taken from public repositories) to the log probability on a dataset with shuffled examples, and flag a dataset if the two log probabilities have statistically significant differences.\n\nUsing these ideas, we propose a computationally efficient and statistically powerful test for contamination which shards the dataset into smaller segments and performs a series of log probability comparisons within each shard. We prove that this sharded test provides control over the false positive rate, enables computationally efficient parallel tests, and substantially improves the power of the test for small p-values."
                        ],
                        "paper": {
                            "corpus_id": 264490730,
                            "title": "Proving Test Set Contamination in Black Box Language Models",
                            "authors": [
                                {
                                    "authorId": "153163779",
                                    "name": "Yonatan Oren"
                                },
                                {
                                    "authorId": "2261737865",
                                    "name": "Nicole Meister"
                                },
                                {
                                    "authorId": "22193324",
                                    "name": "Niladri S. Chatterji"
                                },
                                {
                                    "authorId": "8759332",
                                    "name": "Faisal Ladhak"
                                },
                                {
                                    "authorId": "2244446164",
                                    "name": "Tatsunori Hashimoto"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 146
                        },
                        "score": 0.95849609375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Statistical and Probability-Based Detection Methods",
                "tldr": "Statistical and probability-based detection methods identify contamination by analyzing model behaviors like output probabilities and confidence patterns, without requiring direct access to pre-training data, making them applicable to proprietary models. (7 sources)",
                "text": "\nStatistical and probability-based detection methods offer an alternative approach to identifying data contamination by focusing on model behaviors rather than direct corpus matching. These techniques are particularly valuable when pre-training data is inaccessible, as is often the case with proprietary models.\n\nOne of the most prominent statistical approaches is Min-K% Prob, which assesses whether a text was present in an LLM's pre-training data by calculating the average log-likelihood of the lowest-probability tokens in the sequence. A high result suggests the text's presence in the training data <Paper corpusId=\"272689919\" paperTitle=\"(Samuel et al., 2024)\" isShortName></Paper>. However, this method faces limitations in real-world applications, including the lack of established thresholds for determining contamination and challenges in implementation due to unavailable code <Paper corpusId=\"272689919\" paperTitle=\"(Samuel et al., 2024)\" isShortName></Paper>.\n\nAnother significant statistical method is the Canonical Order Statistical Testing approach proposed by Oren et al., which provides provable guarantees of test set contamination without requiring access to training data or model weights. This method exploits the exchangeability property of datasets, where the order of examples should not affect the joint distribution. By comparing the language model's log probability on a \"canonical\" ordering versus a shuffled version, the test can detect if a model shows a statistically significant preference for a particular ordering, which would indicate contamination <Paper corpusId=\"264490730\" paperTitle=\"(Oren et al., 2023)\" isShortName></Paper>. Despite its statistical rigor, this approach can be computationally intensive, especially when analyzing longer data examples <Paper corpusId=\"272689919\" paperTitle=\"(Samuel et al., 2024)\" isShortName></Paper>.\n\nResearchers have also explored direct observation of model memorization behaviors as an indicator of contamination. Li proposed quantifying potential contamination by examining whether models exhibit memorization behavior on test instances, rather than identifying n-gram overlaps between training and test sets <Paper corpusId=\"262055119\" paperTitle=\"(Li_1, 2023)\" isShortName></Paper>. Similarly, Zhang et al. introduced PaCoST (Paired Confidence Significance Testing), which constructs counterparts for each data instance and analyzes corresponding confidence scores to determine if a model demonstrates significantly higher confidence when presented with original benchmarks, operating under the assumption that models show greater confidence when responding to questions they've been trained on <Paper corpusId=\"270737802\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nWhile these methods have shown promise, their effectiveness varies depending on the context. Yu et al. found that loss-value comparison methods like Min-K% Prob are effective for detecting pre-training contamination but less reliable for identifying contamination during fine-tuning <Paper corpusId=\"267897557\" paperTitle=\"(Yu et al., 2024)\" isShortName></Paper>. Additionally, the diversity and complexity of training data can amplify the difficulty of distinguishing between members and non-members, leading to suboptimal performance in detecting pre-training data contamination <Paper corpusId=\"273350935\" paperTitle=\"(Zhang et al._1, 2024)\" isShortName></Paper>.\n\nBeyond these primary methods, researchers have developed approaches that leverage dataset metadata to detect contamination without direct access to pre-training data. These include methods that exploit dataset ordering or the assignment of examples to specific data splits <Paper corpusId=\"271097946\" paperTitle=\"(Palavalli et al., 2024)\" isShortName></Paper>. The Token Completion Overlap Score is one such technique that prompts an LLM with dataset name, partition type, and a random initial segment of a reference instance, flagging it as contaminated if the model's output closely matches the latter part of the reference <Paper corpusId=\"272689919\" paperTitle=\"(Samuel et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Samuel et al., 2024)",
                        "snippets": [
                            "To address this gap and provide a dual investigation of SOTA LLM contamination status and detection method robustness, we evaluate five contamination detection approaches with four state-of-the-art LLMs across eight challenging datasets often used in modern LLM evaluation.\n\nWe examine five distinct approaches to detecting data contamination in LLMs, including three state-of-the-art techniques from ICLR (2023-2024) and two exploratory prompt-based approaches. Two approaches are based on sequence probabilities and require access to model parameters. Figure 1 illustrates the visual overview of each approach. For each method, we also note the limitations we identified during our examination. The overview of these methods are illustrated in Figure 1.\n\n\u2022 Min-K% Prob (Shi et al., 2023) assesses whether a text was in an LLM's pre-training data by calculating the average log-likelihood of the k% lowestprobability tokens, with a high result suggesting the text's presence in the training data. Limitations: (1) The authors report AUC based on the proposed WiKiMIA dataset, in which they regarded data events before the model release as contaminated data. Such a strong assumption on the ground truth may require more justification. (2) They did not provide the threshold to determine the value of min-K%-prob in the paper since they claim they can use AUC; however, in real-world settings, we do not always have the oracle to determine AUC -instead, we need a metric for determining whether arbitrary datasets are contaminated. (3) The code is not available.\n\n\u2022 Canonical Order Statistical Testing (Oren et al., 2024) identifies contamination in a pre-training dataset by checking if the model shows a preference for the canonical order of examples over random shuffling. This preference is tested by comparing their log probabilities, with results aggregated across datasets to ensure a low false positive rate. Limitations: When an individual data example's length is long, the combination of sample/shards/permutations in the setup can be costly.\n\n\u2022 Token Completion Overlap Score (Golchin and Surdeanu, 2024b) detects contamination by prompting the LLM with a dataset name, partition type, and a random initial segment of a reference instance. If the LLM's output closely matches the latter part of the reference, the instance is flagged as contaminated."
                        ],
                        "paper": {
                            "corpus_id": 272689919,
                            "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges",
                            "authors": [
                                {
                                    "authorId": "2298040858",
                                    "name": "Vinay Samuel"
                                },
                                {
                                    "authorId": "2261321156",
                                    "name": "Yue Zhou"
                                },
                                {
                                    "authorId": "2261285492",
                                    "name": "Henry Peng Zou"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 8
                        },
                        "score": 0.9736328125
                    },
                    {
                        "id": "(Oren et al., 2023)",
                        "snippets": [
                            "In this work, we show it is possible to go beyond heuristics and provide provable guarantees of test set contamination in black box language models. More specifically, we provide a statistical test that can identify the presence of a benchmark in the pre-training dataset of a language model with provable false positive rate guarantees and without access to the model's training data or weights.\n\nTo achieve these guarantees, we exploit the fact that many datasets have a property known as exchangeability, where the order of examples in the dataset can be shuffled without affecting its joint distribution. Our key insight is that if a language model shows a preference for any particular ordering of the dataset -such as a canonical ordering that appears in publicly available repositories- this violates exchangeability and can only occur by observing the dataset during training (Figure 1). We leverage this insight to propose a set of tests that compares the language model's log probability on the 'canonical' ordering (taken from public repositories) to the log probability on a dataset with shuffled examples, and flag a dataset if the two log probabilities have statistically significant differences.\n\nUsing these ideas, we propose a computationally efficient and statistically powerful test for contamination which shards the dataset into smaller segments and performs a series of log probability comparisons within each shard. We prove that this sharded test provides control over the false positive rate, enables computationally efficient parallel tests, and substantially improves the power of the test for small p-values."
                        ],
                        "paper": {
                            "corpus_id": 264490730,
                            "title": "Proving Test Set Contamination in Black Box Language Models",
                            "authors": [
                                {
                                    "authorId": "153163779",
                                    "name": "Yonatan Oren"
                                },
                                {
                                    "authorId": "2261737865",
                                    "name": "Nicole Meister"
                                },
                                {
                                    "authorId": "22193324",
                                    "name": "Niladri S. Chatterji"
                                },
                                {
                                    "authorId": "8759332",
                                    "name": "Faisal Ladhak"
                                },
                                {
                                    "authorId": "2244446164",
                                    "name": "Tatsunori Hashimoto"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 146
                        },
                        "score": 0.95849609375
                    },
                    {
                        "id": "(Li_1, 2023)",
                        "snippets": [
                            "In this paper, we propose a novel approach to quantify potential contamination in language model evaluation benchmarks without accessing the entire training data. Instead of identifying n-gram overlaps between training and test set, we directly observe whether models exhibit memorisation behaviour on test instances."
                        ],
                        "paper": {
                            "corpus_id": 262055119,
                            "title": "Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation",
                            "authors": [
                                {
                                    "authorId": "1527099159",
                                    "name": "Yucheng Li"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 35
                        },
                        "score": 0.93212890625
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "In this study, we introduce a novel approach named PaCoST (Paired Confidence Significance Testing) designed for the detection of benchmark contamination in open-source LLMs. Our method entails a three-step statistical analysis, capable of identifying benchmarks within the model's training data. Specifically, our approach involves constructing counterparts for each data instance with similar distribution, followed by statistical analysis of corresponding confidence scores to ascertain whether the model exhibits significantly higher confidence when presented with original benchmarks. We operate under the assumption that the model tends to demonstrate greater confidence when responding to questions it has been trained on. To validate our method rigorously, we conduct a series of controlled experiments."
                        ],
                        "paper": {
                            "corpus_id": 270737802,
                            "title": "PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2288559437",
                                    "name": "Huixuan Zhang"
                                },
                                {
                                    "authorId": "2297335276",
                                    "name": "Yun Lin"
                                },
                                {
                                    "authorId": "2288537941",
                                    "name": "Xiaojun Wan"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 0
                        },
                        "score": 0.9736328125
                    },
                    {
                        "id": "(Yu et al., 2024)",
                        "snippets": [
                            "Detecting data contamination, a form of Membership Inference Attack (MIA), poses challenges for large language models (LLMs) due to their training on vast corpora and the difficulty of conducting ablation studies (Shi et al., 2023). To detect such contamination of LLMs, Wei et al. (2023) suggested comparing average loss values between training and test datasets, while Shi et al. (2023) introduced Min-K% Prob based on loss values to identify texts used in training. Our experiments show these methods are effective for pre-training but not for detecting contamination during fine-tuning."
                        ],
                        "paper": {
                            "corpus_id": 267897557,
                            "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2164113313",
                                    "name": "Zhuohao Yu"
                                },
                                {
                                    "authorId": "2287659901",
                                    "name": "Chang Gao"
                                },
                                {
                                    "authorId": "2286328804",
                                    "name": "Wenjin Yao"
                                },
                                {
                                    "authorId": "2108024279",
                                    "name": "Yidong Wang"
                                },
                                {
                                    "authorId": "145235149",
                                    "name": "Wei Ye"
                                },
                                {
                                    "authorId": "2273553706",
                                    "name": "Jindong Wang"
                                },
                                {
                                    "authorId": "2249681654",
                                    "name": "Xing Xie"
                                },
                                {
                                    "authorId": "2250437942",
                                    "name": "Yue Zhang"
                                },
                                {
                                    "authorId": "1705434",
                                    "name": "Shikun Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 28
                        },
                        "score": 0.962890625
                    },
                    {
                        "id": "(Zhang et al._1, 2024)",
                        "snippets": [
                            "Current methods differentiate members and non-members by designing scoring functions, like Perplexity and Min-k%. However, the diversity and complexity of training data magnifies the difficulty of distinguishing, leading to suboptimal performance in detecting pretraining data.\n\nOn the one hand, model developers can remove evaluation benchmark data from training data by retrieval-based methods with access to pertaining data (Ravaut et al., 2024;Chowdhery et al., 2023). Specifically, those methods employ n-gram tokenization and string-matching for detecting data contamination (Brown et al., 2020;Touvron et al., 2023b;Team et al., 2023;Radford et al., 2019). On the other hand, researchers utilize prompting techniques (Golchin & Surdeanu, 2024), performance analysis (Ye et al., 2024;Debenedetti et al., 2024), model likelihood (Oren et al., 2024;Shi et al., 2024;Xu et al., 2024b) to detect potential contamination without access to the training data."
                        ],
                        "paper": {
                            "corpus_id": 273350935,
                            "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2316591285",
                                    "name": "Hengxiang Zhang"
                                },
                                {
                                    "authorId": "2266803682",
                                    "name": "Songxin Zhang"
                                },
                                {
                                    "authorId": "2283306681",
                                    "name": "Bingyi Jing"
                                },
                                {
                                    "authorId": "2325203633",
                                    "name": "Hongxin Wei"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.95556640625
                    },
                    {
                        "id": "(Palavalli et al., 2024)",
                        "snippets": [
                            "Methods with access to pretraining data Early research on LLM data contamination primarily employed methods akin to high-order n-gram overlap detection between pretraining and evaluation data (Radford et al., 2019a;Brown et al., 2020;Wei et al., 2021;Touvron et al., 2023). Tools for qualitative analysis on large-scale corpora (such as Data Portraits (Marone and Durme, 2023) and the ROOTS Search Tool (Piktus et al., 2023)) have further increased the practicality of this type of contamination detection. However, these approaches have several limitations: they remain fairly computationally expensive, assume access to pretraining data, and generally can only detect contamination when a cluster of several test set examples co-occur (as most methods leverage data sketching (Broder, 1997) tools that are only effective for sequences above a certain length). Yang et al. (2023) proposes an LLM-based decontamination method, which leverages embedding similarity search followed by evaluation with a strong language model (e.g. GPT-4), to identify and mitigate contamination. This is computationally costly but can identify noisy contamination Methods without access to pretraining data Some approaches are capable of detecting contamination without direct access to pretraining data, but assume that the test data has not been modified or distributed across the pretraining corpus. These methods leverage metadata from the dataset to detect contamination, e.g. by leveraging dataset ordering (Sainz et al., 2023b) or the assignment of examples to specific data splits (Golchin and Surdeanu, 2023). Golchin and Surdeanu (2024) introduce the Data Contamination Quiz, a streamlined method that efficiently detects and estimates verbatim contamination in LLMs by crafting multiple choice questions that prompt a model to correctly dataset-specific content among similar but noisy alternatives."
                        ],
                        "paper": {
                            "corpus_id": 271097946,
                            "title": "A Taxonomy for Data Contamination in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2220962946",
                                    "name": "Medha Palavalli"
                                },
                                {
                                    "authorId": "2138301112",
                                    "name": "Amanda Bertsch"
                                },
                                {
                                    "authorId": "1762110",
                                    "name": "Matthew R. Gormley"
                                }
                            ],
                            "year": 2024,
                            "venue": "CONDA",
                            "n_citations": 4
                        },
                        "score": 0.95166015625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Min-K% Prob and its Variants",
                "tldr": "Min-K% Prob detects contamination by identifying low-probability tokens in text sequences, with the intuition that models assign higher probabilities to previously seen content. Several variants have emerged that improve upon this method, including Min-K%++ and approaches that focus on \"surprising tokens\" or contextual comparisons. (6 sources)",
                "text": "\nMin-K% Prob has emerged as a significant approach for detecting data contamination in large language models. Introduced by Shi et al., this method operates on a simple yet effective hypothesis: \"an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities\" <Paper corpusId=\"264451585\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. What makes Min-K% Prob particularly valuable is that it requires neither knowledge about the pre-training corpus nor additional training, distinguishing it from previous detection methods that demanded training reference models on data similar to pre-training data <Paper corpusId=\"264451585\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>.\n\nThe effectiveness of Min-K% Prob has been demonstrated on the WIKIMIA benchmark, where it achieved a 7.4% improvement over previous methods <Paper corpusId=\"264451585\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. This benchmark was specifically designed to support gold truth detection by using data created before and after model training <Paper corpusId=\"264451585\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. However, while Min-K% Prob has shown effectiveness for detecting pre-training contamination, research by Yu et al. indicates it may be less reliable for identifying contamination that occurs during fine-tuning <Paper corpusId=\"267897557\" paperTitle=\"(Yu et al., 2024)\" isShortName></Paper>.\n\nSeveral variants have been developed to address limitations in the original Min-K% Prob approach. Min-K%++ represents a significant enhancement that normalizes the initial log-probability by subtracting the expected log-probability and dividing by the variance <Paper corpusId=\"268819579\" paperTitle=\"(Ravaut et al., 2024)\" isShortName></Paper>. This adaptation has yielded substantial improvements, with Min-K%++ outperforming the original Min-K% Prob by up to 10 points on the WIKIMIA benchmark <Paper corpusId=\"268819579\" paperTitle=\"(Ravaut et al., 2024)\" isShortName></Paper>.\n\nZhang et al. proposed another adaptive pre-training data detection method that focuses on \"surprising tokens\" <Paper corpusId=\"271570943\" paperTitle=\"(Zhang et al._2, 2024)\" isShortName></Paper>. This approach defines a token as surprising when the model's prediction is \"certain but wrong\" \u2013 characterized by low Shannon entropy in the probability distribution and a low probability assigned to the ground truth token <Paper corpusId=\"271570943\" paperTitle=\"(Zhang et al._2, 2024)\" isShortName></Paper>. The method builds on the hypothesis that seen data is less surprising to the model than unseen data, thereby reducing dependence on verbatim memorization for detection <Paper corpusId=\"271570943\" paperTitle=\"(Zhang et al._2, 2024)\" isShortName></Paper>.\n\nA contextual contrast approach developed by Wang et al. offers yet another variation, arguing that both member and non-member contexts should be considered simultaneously <Paper corpusId=\"272423548\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. Contrary to previous assumptions that member contexts provide minimal information due to small distributional shifts, their analysis revealed that these subtle shifts can be effectively leveraged when contrasted with non-member contexts <Paper corpusId=\"272423548\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nDespite these advances, Min-K% Prob and its variants face several practical limitations. Samuel et al. identified three key challenges with the original approach: (1) the reliance on assumptions about ground truth in the WIKIMIA dataset that may require further justification, (2) the absence of established thresholds for determining contamination in real-world applications where oracle AUC measurements are unavailable, and (3) a lack of publicly available code for implementation <Paper corpusId=\"272689919\" paperTitle=\"(Samuel et al., 2024)\" isShortName></Paper>. These limitations highlight ongoing challenges in applying these detection methods to real-world contamination scenarios.",
                "citations": [
                    {
                        "id": "(Shi et al., 2023)",
                        "snippets": [
                            "To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous methods."
                        ],
                        "paper": {
                            "corpus_id": 264451585,
                            "title": "Detecting Pretraining Data from Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2254168373",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "2218438150",
                                    "name": "Anirudh Ajith"
                                },
                                {
                                    "authorId": "67284811",
                                    "name": "Mengzhou Xia"
                                },
                                {
                                    "authorId": "108053318",
                                    "name": "Yangsibo Huang"
                                },
                                {
                                    "authorId": "2261780806",
                                    "name": "Daogao Liu"
                                },
                                {
                                    "authorId": "3443287",
                                    "name": "Terra Blevins"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "2137813791",
                                    "name": "Luke S. Zettlemoyer"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 201
                        },
                        "score": 0.9736328125
                    },
                    {
                        "id": "(Yu et al., 2024)",
                        "snippets": [
                            "Detecting data contamination, a form of Membership Inference Attack (MIA), poses challenges for large language models (LLMs) due to their training on vast corpora and the difficulty of conducting ablation studies (Shi et al., 2023). To detect such contamination of LLMs, Wei et al. (2023) suggested comparing average loss values between training and test datasets, while Shi et al. (2023) introduced Min-K% Prob based on loss values to identify texts used in training. Our experiments show these methods are effective for pre-training but not for detecting contamination during fine-tuning."
                        ],
                        "paper": {
                            "corpus_id": 267897557,
                            "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2164113313",
                                    "name": "Zhuohao Yu"
                                },
                                {
                                    "authorId": "2287659901",
                                    "name": "Chang Gao"
                                },
                                {
                                    "authorId": "2286328804",
                                    "name": "Wenjin Yao"
                                },
                                {
                                    "authorId": "2108024279",
                                    "name": "Yidong Wang"
                                },
                                {
                                    "authorId": "145235149",
                                    "name": "Wei Ye"
                                },
                                {
                                    "authorId": "2273553706",
                                    "name": "Jindong Wang"
                                },
                                {
                                    "authorId": "2249681654",
                                    "name": "Xing Xie"
                                },
                                {
                                    "authorId": "2250437942",
                                    "name": "Yue Zhang"
                                },
                                {
                                    "authorId": "1705434",
                                    "name": "Shikun Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 28
                        },
                        "score": 0.962890625
                    },
                    {
                        "id": "(Ravaut et al., 2024)",
                        "snippets": [
                            "Min-K%++ (Zhang et al., 2024a) proposes another extension of the idea of Min-K% Prob: rather than simply getting next-token predicted probabilities, the authors propose a score which subtracts the expected log-probability and divides the score by the variance; in other words, normalizing the initial log-probability. The sequence scoring mechanism is then identical to the one of Min-K% Prob",
                            "Min-K%++ reaches state-of-the-art on WikiMIA, outperforming Min-K% Prob by up to 10 points."
                        ],
                        "paper": {
                            "corpus_id": 268819579,
                            "title": "A Comprehensive Survey of Contamination Detection Methods in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "14038850",
                                    "name": "Mathieu Ravaut"
                                },
                                {
                                    "authorId": "2064493724",
                                    "name": "Bosheng Ding"
                                },
                                {
                                    "authorId": "1689176705",
                                    "name": "Fangkai Jiao"
                                },
                                {
                                    "authorId": "2258571998",
                                    "name": "Hailin Chen"
                                },
                                {
                                    "authorId": "2155447436",
                                    "name": "Xingxuan Li"
                                },
                                {
                                    "authorId": "2091437375",
                                    "name": "Ruochen Zhao"
                                },
                                {
                                    "authorId": "2084609980",
                                    "name": "Chengwei Qin"
                                },
                                {
                                    "authorId": "2267728986",
                                    "name": "Caiming Xiong"
                                },
                                {
                                    "authorId": "2708940",
                                    "name": "Shafiq R. Joty"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 7
                        },
                        "score": 0.955078125
                    },
                    {
                        "id": "(Zhang et al._2, 2024)",
                        "snippets": [
                            "Current solutions to this problem leverage techniques explored in machine learning privacy such as Membership Inference Attacks (MIAs), which heavily depend on LLMs' capability of verbatim memorization. However, this reliance presents challenges, especially given the vast amount of training data and the restricted number of effective training epochs. In this paper, we propose an adaptive pre-training data detection method which alleviates this reliance and effectively amplify the identification. Our method adaptively locates \\textit{surprising tokens} of the input. A token is surprising to a LLM if the prediction on the token is\"certain but wrong\", which refers to low Shannon entropy of the probability distribution and low probability of the ground truth token at the same time. By using the prediction probability of surprising tokens to measure \\textit{surprising}, the detection method is achieved based on the simple hypothesis that seeing seen data is less surprising for the model compared with seeing unseen data."
                        ],
                        "paper": {
                            "corpus_id": 271570943,
                            "title": "Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens",
                            "authors": [
                                {
                                    "authorId": "2313922585",
                                    "name": "Anqi Zhang"
                                },
                                {
                                    "authorId": "2314513929",
                                    "name": "Chaofeng Wu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.93603515625
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Existing methods typically analyze target text in isolation or solely with non-member contexts, overlooking potential insights from simultaneously considering both member and non-member contexts. While previous work suggested that member contexts provide little information due to the minor distributional shift they induce, our analysis reveals that these subtle shifts can be effectively leveraged when contrasted with non-member contexts."
                        ],
                        "paper": {
                            "corpus_id": 272423548,
                            "title": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding",
                            "authors": [
                                {
                                    "authorId": "2319955259",
                                    "name": "Cheng Wang"
                                },
                                {
                                    "authorId": "2280103482",
                                    "name": "Yiwei Wang"
                                },
                                {
                                    "authorId": "2305483565",
                                    "name": "Bryan Hooi"
                                },
                                {
                                    "authorId": "1928716951",
                                    "name": "Yujun Cai"
                                },
                                {
                                    "authorId": "2256996328",
                                    "name": "Nanyun Peng"
                                },
                                {
                                    "authorId": "2257127887",
                                    "name": "Kai-Wei Chang"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 6
                        },
                        "score": 0.9306640625
                    },
                    {
                        "id": "(Samuel et al., 2024)",
                        "snippets": [
                            "To address this gap and provide a dual investigation of SOTA LLM contamination status and detection method robustness, we evaluate five contamination detection approaches with four state-of-the-art LLMs across eight challenging datasets often used in modern LLM evaluation.\n\nWe examine five distinct approaches to detecting data contamination in LLMs, including three state-of-the-art techniques from ICLR (2023-2024) and two exploratory prompt-based approaches. Two approaches are based on sequence probabilities and require access to model parameters. Figure 1 illustrates the visual overview of each approach. For each method, we also note the limitations we identified during our examination. The overview of these methods are illustrated in Figure 1.\n\n\u2022 Min-K% Prob (Shi et al., 2023) assesses whether a text was in an LLM's pre-training data by calculating the average log-likelihood of the k% lowestprobability tokens, with a high result suggesting the text's presence in the training data. Limitations: (1) The authors report AUC based on the proposed WiKiMIA dataset, in which they regarded data events before the model release as contaminated data. Such a strong assumption on the ground truth may require more justification. (2) They did not provide the threshold to determine the value of min-K%-prob in the paper since they claim they can use AUC; however, in real-world settings, we do not always have the oracle to determine AUC -instead, we need a metric for determining whether arbitrary datasets are contaminated. (3) The code is not available.\n\n\u2022 Canonical Order Statistical Testing (Oren et al., 2024) identifies contamination in a pre-training dataset by checking if the model shows a preference for the canonical order of examples over random shuffling. This preference is tested by comparing their log probabilities, with results aggregated across datasets to ensure a low false positive rate. Limitations: When an individual data example's length is long, the combination of sample/shards/permutations in the setup can be costly.\n\n\u2022 Token Completion Overlap Score (Golchin and Surdeanu, 2024b) detects contamination by prompting the LLM with a dataset name, partition type, and a random initial segment of a reference instance. If the LLM's output closely matches the latter part of the reference, the instance is flagged as contaminated."
                        ],
                        "paper": {
                            "corpus_id": 272689919,
                            "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges",
                            "authors": [
                                {
                                    "authorId": "2298040858",
                                    "name": "Vinay Samuel"
                                },
                                {
                                    "authorId": "2261321156",
                                    "name": "Yue Zhou"
                                },
                                {
                                    "authorId": "2261285492",
                                    "name": "Henry Peng Zou"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 8
                        },
                        "score": 0.9736328125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Benchmark Datasets for Contamination Detection",
                "tldr": "Several benchmark datasets have been developed specifically for evaluating contamination detection methods, including WIKIMIA, ArxivMIA, DetCon, and ComiEval, each providing different advantages for contamination analysis in various domains. (6 sources)",
                "text": "\nA number of specialized benchmark datasets have been created to evaluate the effectiveness of contamination detection methods in large language models:\n\n- **WIKIMIA**: Introduced by Shi et al., WIKIMIA is a dynamic benchmark that uses data created both before and after model training to support \"gold truth\" detection. This benchmark has become particularly valuable because it provides known ground truth about which data was available during pre-training, allowing researchers to definitively evaluate detection methods. Min-K% Prob demonstrated a 7.4% improvement over previous methods when tested on this benchmark. <Paper corpusId=\"264451585\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>\n\n- **ArxivMIA**: Liu et al. proposed this challenging benchmark comprising arXiv abstracts from Computer Science and Mathematics categories. Rather than focusing on superficial features like perplexities in generated texts, ArxivMIA was designed to support more sophisticated detection methods that examine a model's internal activations through probing techniques. <Paper corpusId=\"270217411\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>\n\n- **DetCon and ComiEval**: Introduced by Dong et al., these benchmarks were specifically designed for assessing both benchmark data contamination (BDC) and the effectiveness of contamination mitigation methods. DetCon focuses on evaluating detection techniques, while ComiEval provides a framework for testing mitigation strategies. <Paper corpusId=\"270285708\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>\n\n- **Canonically Ordered Benchmarks**: Oren et al. developed a methodology that leverages existing benchmarks in a novel way by comparing the likelihood of canonically ordered datasets versus shuffled versions. While not a new dataset per se, this approach transforms standard benchmarks into contamination detection tools. This method has been successfully applied to detect contamination in models as small as 1.4 billion parameters and on small test sets of only 1,000 examples. <Paper corpusId=\"270285708\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264490730\" paperTitle=\"(Oren et al., 2023)\" isShortName></Paper>\n\n- **ConTAM**: Singh et al. proposed this novel analysis method that evaluates contamination metrics based on whether models actually benefit from examples marked as contaminated. Through a large-scale survey across 13 benchmarks and 7 models from 2 different families, ConTAM provides a framework for better understanding evaluation data contamination and its effects. <Paper corpusId=\"273850342\" paperTitle=\"(Singh et al., 2024)\" isShortName></Paper>\n\n- **Watermarked Benchmarks**: Sander et al. introduced an innovative approach that deliberately watermarks benchmark datasets before their release. This method reformulates original questions with a watermarked LLM without altering benchmark utility, then detects \"radioactivity\" traces during evaluation. Testing on benchmarks like ARC-Easy, ARC-Challenge, and MMLU demonstrated successful contamination detection when models were contaminated enough to enhance performance. <Paper corpusId=\"276575227\" paperTitle=\"(Sander et al., 2025)\" isShortName></Paper>\n\nThese benchmark datasets have enabled researchers to systematically evaluate contamination detection methods across diverse contexts and model architectures, providing essential tools for addressing the critical challenge of benchmark data contamination.",
                "citations": [
                    {
                        "id": "(Shi et al., 2023)",
                        "snippets": [
                            "To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous methods."
                        ],
                        "paper": {
                            "corpus_id": 264451585,
                            "title": "Detecting Pretraining Data from Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2254168373",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "2218438150",
                                    "name": "Anirudh Ajith"
                                },
                                {
                                    "authorId": "67284811",
                                    "name": "Mengzhou Xia"
                                },
                                {
                                    "authorId": "108053318",
                                    "name": "Yangsibo Huang"
                                },
                                {
                                    "authorId": "2261780806",
                                    "name": "Daogao Liu"
                                },
                                {
                                    "authorId": "3443287",
                                    "name": "Terra Blevins"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "2137813791",
                                    "name": "Luke S. Zettlemoyer"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 201
                        },
                        "score": 0.9736328125
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories."
                        ],
                        "paper": {
                            "corpus_id": 270217411,
                            "title": "Probing Language Models for Pre-training Data Detection",
                            "authors": [
                                {
                                    "authorId": "2294376388",
                                    "name": "Zhenhua Liu"
                                },
                                {
                                    "authorId": "1914586128",
                                    "name": "Tong Zhu"
                                },
                                {
                                    "authorId": "2186374155",
                                    "name": "Chuanyuan Tan"
                                },
                                {
                                    "authorId": "2304460083",
                                    "name": "Haonan Lu"
                                },
                                {
                                    "authorId": "2330946427",
                                    "name": "Bing Liu"
                                },
                                {
                                    "authorId": "2265943980",
                                    "name": "Wenliang Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 13
                        },
                        "score": 0.9775390625
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Efficient identification of BDC in evaluation benchmarks represents a fundamental aspect in ensuring the reliability and integrity of LLMs, while also providing the basis for developing effective mitigation strategies.This section provides a comprehensive review of literature related to the detection of BDC.We categorize the methodologies into two distinct strategies: matching-based and comparison-based, discussed in Sections 3.1 and 3.2, respectively",
                            "Deng et al. [30] proposed two novel methods to detect potential overlaps between evaluation benchmarks and pre-training corpora, tailored for both open-source and proprietary LLMs.They introduced a retrieval-based system and a Testset Slot Guessing (TS-Guessing) protocol, which involves masking incorrect answers in multiple-choice questions and prompting the model to fill in the gaps",
                            "Golchin and Surdeanu [46] presented another novel approach, the Data Contamination Quiz (DCQ), to detect and quantify BDC in LLMs.The DCQ is a series of multiple-choice questions with three perturbed versions of each dataset instance, including only word-level changes.The LLM's ability to identify the original instance among the perturbed ones indicates potential exposure to the data during pre-training",
                            "Golchin and Surdeanu (Golchin et al., 2023) presented a technique that combines instance-level identification using guided instruction prompts with partition-level assessment through overlap score comparison and classifier-based detection",
                            "Li et al. [91] presented a comprehensive report on BDC across over 15 LLMs and six multiplechoice QA benchmarks.They introduced an open-source pipeline to conduct contamination analysis on customized data and models",
                            "Dong et al. [34] focused on the distribution of generated content, they proposed a novel method, CDD (Contamination Detection via output Distribution), which uses the output distribution of LLMs to detect BDC",
                            "The paper also presents two new benchmarks, DetCon and ComiEval, for assessing BDC and mitigation methods",
                            "Li [89] proposed a novel method to detect BDC in language model evaluation without requiring access to the full training set.The technique uses perplexity to measure the extent of contamination",
                            "Oren et al. (Oren et al., 2023) presented a method to detect test set contamination in language models without needing access to the model's pre-training data or weights.The authors used a statistical test to identify contamination by comparing the likelihood of a benchmark dataset's canonical ordering against a shuffled version."
                        ],
                        "paper": {
                            "corpus_id": 270285708,
                            "title": "Benchmark Data Contamination of Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2153079650",
                                    "name": "Cheng Xu"
                                },
                                {
                                    "authorId": "2304954016",
                                    "name": "Shuhao Guan"
                                },
                                {
                                    "authorId": "2304952028",
                                    "name": "Derek Greene"
                                },
                                {
                                    "authorId": "2266906186",
                                    "name": "Mohand-Tahar Kechadi"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 56
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Oren et al., 2023)",
                        "snippets": [
                            "In this work, we show it is possible to go beyond heuristics and provide provable guarantees of test set contamination in black box language models. More specifically, we provide a statistical test that can identify the presence of a benchmark in the pre-training dataset of a language model with provable false positive rate guarantees and without access to the model's training data or weights.\n\nTo achieve these guarantees, we exploit the fact that many datasets have a property known as exchangeability, where the order of examples in the dataset can be shuffled without affecting its joint distribution. Our key insight is that if a language model shows a preference for any particular ordering of the dataset -such as a canonical ordering that appears in publicly available repositories- this violates exchangeability and can only occur by observing the dataset during training (Figure 1). We leverage this insight to propose a set of tests that compares the language model's log probability on the 'canonical' ordering (taken from public repositories) to the log probability on a dataset with shuffled examples, and flag a dataset if the two log probabilities have statistically significant differences.\n\nUsing these ideas, we propose a computationally efficient and statistically powerful test for contamination which shards the dataset into smaller segments and performs a series of log probability comparisons within each shard. We prove that this sharded test provides control over the false positive rate, enables computationally efficient parallel tests, and substantially improves the power of the test for small p-values."
                        ],
                        "paper": {
                            "corpus_id": 264490730,
                            "title": "Proving Test Set Contamination in Black Box Language Models",
                            "authors": [
                                {
                                    "authorId": "153163779",
                                    "name": "Yonatan Oren"
                                },
                                {
                                    "authorId": "2261737865",
                                    "name": "Nicole Meister"
                                },
                                {
                                    "authorId": "22193324",
                                    "name": "Niladri S. Chatterji"
                                },
                                {
                                    "authorId": "8759332",
                                    "name": "Faisal Ladhak"
                                },
                                {
                                    "authorId": "2244446164",
                                    "name": "Tatsunori Hashimoto"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 146
                        },
                        "score": 0.95849609375
                    },
                    {
                        "id": "(Singh et al., 2024)",
                        "snippets": [
                            "Hampering the interpretation of benchmark scores, evaluation data contamination has become a growing concern in the evaluation of LLMs, and an active area of research studies its effects. While evaluation data contamination is easily understood intuitively, it is surprisingly difficult to define precisely which samples should be considered contaminated and, consequently, how it impacts benchmark scores. We propose that these questions should be addressed together and that contamination metrics can be assessed based on whether models benefit from the examples they mark contaminated. We propose a novel analysis method called ConTAM, and show with a large scale survey of existing and novel n-gram based contamination metrics across 13 benchmarks and 7 models from 2 different families that ConTAM can be used to better understand evaluation data contamination and its effects."
                        ],
                        "paper": {
                            "corpus_id": 273850342,
                            "title": "Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?",
                            "authors": [
                                {
                                    "authorId": "2306863572",
                                    "name": "Aaditya K. Singh"
                                },
                                {
                                    "authorId": "1665851460",
                                    "name": "Muhammed Yusuf Kocyigit"
                                },
                                {
                                    "authorId": "2282542314",
                                    "name": "Andrew Poulton"
                                },
                                {
                                    "authorId": "71039937",
                                    "name": "David Esiobu"
                                },
                                {
                                    "authorId": "2253400960",
                                    "name": "Maria Lomeli"
                                },
                                {
                                    "authorId": "2253402270",
                                    "name": "Gergely Szilvasy"
                                },
                                {
                                    "authorId": "3449411",
                                    "name": "Dieuwke Hupkes"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 13
                        },
                        "score": 0.92529296875
                    },
                    {
                        "id": "(Sander et al., 2025)",
                        "snippets": [
                            "Benchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect \"radioactivity\", i.e., traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, e.g., p-val = 10 \u22123 for +5% on ARC-Easy."
                        ],
                        "paper": {
                            "corpus_id": 276575227,
                            "title": "Detecting Benchmark Contamination Through Watermarking",
                            "authors": [
                                {
                                    "authorId": "2283934407",
                                    "name": "Tom Sander"
                                },
                                {
                                    "authorId": "2322442744",
                                    "name": "Pierre Fernandez"
                                },
                                {
                                    "authorId": "2290845753",
                                    "name": "Saeed Mahloujifar"
                                },
                                {
                                    "authorId": "2283933043",
                                    "name": "A. Durmus"
                                },
                                {
                                    "authorId": "2290241478",
                                    "name": "Chuan Guo"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.955078125
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Limitations of Current Detection Methods",
                "tldr": "Current contamination detection methods face significant challenges including resource intensity, dependence on inaccessible pre-training data, inability to quantify performance impacts, and vulnerability to evasion through paraphrasing or subtle contamination. (7 sources)",
                "text": "\nDespite substantial progress in developing methods to detect data contamination in large language models, existing approaches face several critical limitations that hinder their effectiveness. Traditional contamination detection methods, which primarily rely on calculating textual overlap between pre-training and evaluation data, have become increasingly impractical as pre-training datasets grow exponentially in size. Even computing basic n-gram statistics has become extremely resource-intensive <Paper corpusId=\"270619707\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>. This computational burden significantly constrains researchers' ability to comprehensively assess contamination in modern LLMs.\n\nA fundamental challenge is that most detection methods assume access to pre-training data, which is rarely available for proprietary models like the GPT series <Paper corpusId=\"275906726\" paperTitle=\"(Fan, 2025)\" isShortName></Paper>. This inaccessibility has led researchers to develop black-box approaches, but these often focus on extreme cases of memorization, making it difficult to identify more subtle forms of contamination that might still significantly impact evaluation results <Paper corpusId=\"275906726\" paperTitle=\"(Fan, 2025)\" isShortName></Paper>. Current detection methods also typically cannot quantify the contamination's effect on model performance, limiting their practical utility <Paper corpusId=\"270063170\" paperTitle=\"(Dekoninck et al., 2024)\" isShortName></Paper>.\n\nThe standard n-gram-based detection approaches suffer from both high false positive and false negative rates, as semantically different texts may share n-gram overlaps while simple paraphrasing can easily evade detection <Paper corpusId=\"266933004\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>. This superficiality fails to capture deeper forms of contamination that may affect model performance <Paper corpusId=\"270620798\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>. Additionally, existing methods must make strict assumptions about the contamination process, making them relatively easy to circumvent <Paper corpusId=\"270063170\" paperTitle=\"(Dekoninck et al., 2024)\" isShortName></Paper>.\n\nEven state-of-the-art detection methods have significant limitations. Min-K% Prob, while innovative, lacks established thresholds for determining contamination in real-world applications where oracle AUC measurements are unavailable <Paper corpusId=\"272689919\" paperTitle=\"(Samuel et al., 2024)\" isShortName></Paper>. The Canonical Order Statistical Testing method becomes prohibitively expensive when analyzing longer data examples due to the combinatorial explosion of sample/shard/permutation combinations <Paper corpusId=\"272689919\" paperTitle=\"(Samuel et al., 2024)\" isShortName></Paper>. Token Completion Overlap Score and other prompt-based approaches are limited by the LLM's comprehension abilities and response to instructions <Paper corpusId=\"270619707\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>.\n\nA particularly challenging issue is that the diversity and complexity of training data magnify the difficulty of distinguishing between members and non-members of the training set, leading to suboptimal performance in detecting pre-training data contamination <Paper corpusId=\"273350935\" paperTitle=\"(Zhang et al._1, 2024)\" isShortName></Paper>. This challenge is exacerbated by proprietary models that incorporate special filtering mechanisms during generation, further complicating detection efforts <Paper corpusId=\"275906726\" paperTitle=\"(Fan, 2025)\" isShortName></Paper>.\n\nThese limitations collectively underscore the need for more robust, efficient, and nuanced approaches to contamination detection that can address the full spectrum of contamination scenarios while being practical to implement at scale.",
                "citations": [
                    {
                        "id": "(Zhu et al., 2024)",
                        "snippets": [
                            "Traditional contamination detection methods directly calculate the overlap between pre-training data and evaluation datasets, including n-gram analysis (Touvron et al., 2023b;OpenAI, 2023;Team et al., 2023;Bai et al., 2023) and BM25 (Jiang et al., 2024) for indexing and matching.However, as pre-training data grows exponentially, even simple n-gram statistics become extremely resource-intensive.Yang et al. (2023c); Gunasekar et al. (2023) find n-gram detection unreliable due to unintentional contamination risks.More importantly, training corpora for mainstream LLMs are mostly inaccessible, so recent research has turned to focus on: i)-exploiting the distributional differences between the benchmark training set and the test set to evaluated (Xu et al., 2024).ii)-Evaluate sample-level contamination by providing text segments and black-box access to the LLM (Shi et al., 2023).Other work evaluates contamination through LLM-generated content, limited by the LLM's comprehension abilities to instrurction (Deng et al., 2023;Golchin and Surdeanu, 2023a).Some studies test if models can coherently continue a given sample part (Golchin and Surdeanu, 2023b)."
                        ],
                        "paper": {
                            "corpus_id": 270619707,
                            "title": "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation",
                            "authors": [
                                {
                                    "authorId": "2307992515",
                                    "name": "Qin Zhu"
                                },
                                {
                                    "authorId": "2290056525",
                                    "name": "Qingyuan Cheng"
                                },
                                {
                                    "authorId": "2307468261",
                                    "name": "Runyu Peng"
                                },
                                {
                                    "authorId": "50080067",
                                    "name": "Xiaonan Li"
                                },
                                {
                                    "authorId": "2136108329",
                                    "name": "Tengxiao Liu"
                                },
                                {
                                    "authorId": "2307468261",
                                    "name": "Runyu Peng"
                                },
                                {
                                    "authorId": "2282972251",
                                    "name": "Xipeng Qiu"
                                },
                                {
                                    "authorId": "2284750473",
                                    "name": "Xuanjing Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 7
                        },
                        "score": 0.92431640625
                    },
                    {
                        "id": "(Fan, 2025)",
                        "snippets": [
                            "To address data contamination, two primary solutions have been proposed: data contamination detection and dynamic data evaluation [6]. Data contamination detection aims to identify overlaps between model outputs and training data [7]. However, these methods face limitations, particularly for proprietary models such as the GPT series, which often incorporate special filtering mechanisms during generation. Additionally, detection methods primarily focus on extreme memorization (i.e., direct reproduction of training data), making it difficult to capture more subtle forms of contamination."
                        ],
                        "paper": {
                            "corpus_id": 275906726,
                            "title": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data Contamination in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2342797440",
                                    "name": "Yang Fan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.94580078125
                    },
                    {
                        "id": "(Dekoninck et al., 2024)",
                        "snippets": [
                            "Traditional Contamination Detection Existing contamination detection methods [18,23,24,30,(Mattern et al., 2023)36,39,40,48] aim to detect the inclusion of benchmark samples in the training data as a measure of contamination.However, these approaches show limited success, cannot quantify the contamination's effect on model performance, and have to make strict assumptions about the contamination process, making them easy to evade [17]."
                        ],
                        "paper": {
                            "corpus_id": 270063170,
                            "title": "ConStat: Performance-Based Contamination Detection in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2268310707",
                                    "name": "Jasper Dekoninck"
                                },
                                {
                                    "authorId": "2116235329",
                                    "name": "Mark Niklas M\u00fcller"
                                },
                                {
                                    "authorId": "1736447",
                                    "name": "Martin T. Vechev"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 8
                        },
                        "score": 0.97705078125
                    },
                    {
                        "id": "(Jiang et al., 2024)",
                        "snippets": [
                            "Numerous studies on large language models (LLMs) have explored and investigated the concept of data contamination and demonstrated the robustness of these models against potential contamination in their evaluation datasets (Radford et al., 2019)(Brown et al., 2020)6,27,33,34,11]. Most definitions proposed in the existing studies are based on n-gram duplication between pre-training data and evaluation data. For instance, PaLM [6] divides the evaluation data into two categories-\"clean\" and \"contaminated\"-based on whether at least 70% of all possible 8-grams in the evaluation sample were seen at least once in the pre-training corpus. Llama 2 [34] provides a more fine-grained definition: a token is considered contaminated if it appears in any token n-gram longer than 10 tokens in both the evaluation sample and the training set, and the contamination percentage of an evaluation sample is defined to be the percentage of tokens contaminated; the evaluation data are then divided into 4 buckets-\"Clean\", \"Not Clean\", \"Not Dirty\", and \"Dirty\"-based on the contamination percentage of each evaluation sample. While intuitive, these contamination definitions primarily revolve around n-gram or token overlaps, which only target direct duplications present in both training and evaluation datasets and might provide both high false positive rate (since many semantically different texts have overlaps) and false negative rate (since simple paraphrasing can evade detection [36])."
                        ],
                        "paper": {
                            "corpus_id": 266933004,
                            "title": "Investigating Data Contamination for Pre-training Language Models",
                            "authors": [
                                {
                                    "authorId": "2800541",
                                    "name": "Minhao Jiang"
                                },
                                {
                                    "authorId": "2298016051",
                                    "name": "Ken Ziyu Liu"
                                },
                                {
                                    "authorId": "1606040932",
                                    "name": "Ming Zhong"
                                },
                                {
                                    "authorId": "1749176844",
                                    "name": "Rylan Schaeffer"
                                },
                                {
                                    "authorId": "2260339714",
                                    "name": "Siru Ouyang"
                                },
                                {
                                    "authorId": "2259869648",
                                    "name": "Jiawei Han"
                                },
                                {
                                    "authorId": "123593472",
                                    "name": "Sanmi Koyejo"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 72
                        },
                        "score": 0.9453125
                    },
                    {
                        "id": "(Yao et al., 2024)",
                        "snippets": [
                            "Existing contamination detection methods are typically based on the text overlap between training and evaluation data, which can be too superficial to reflect deeper forms of contamination."
                        ],
                        "paper": {
                            "corpus_id": 270620798,
                            "title": "Data Contamination Can Cross Language Barriers",
                            "authors": [
                                {
                                    "authorId": "2297810571",
                                    "name": "Feng Yao"
                                },
                                {
                                    "authorId": "1505801820",
                                    "name": "Yufan Zhuang"
                                },
                                {
                                    "authorId": "2307478708",
                                    "name": "Zihao Sun"
                                },
                                {
                                    "authorId": "2307559061",
                                    "name": "Sunan Xu"
                                },
                                {
                                    "authorId": "2297831856",
                                    "name": "Animesh Kumar"
                                },
                                {
                                    "authorId": "2297773933",
                                    "name": "Jingbo Shang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 12
                        },
                        "score": 0.92822265625
                    },
                    {
                        "id": "(Samuel et al., 2024)",
                        "snippets": [
                            "To address this gap and provide a dual investigation of SOTA LLM contamination status and detection method robustness, we evaluate five contamination detection approaches with four state-of-the-art LLMs across eight challenging datasets often used in modern LLM evaluation.\n\nWe examine five distinct approaches to detecting data contamination in LLMs, including three state-of-the-art techniques from ICLR (2023-2024) and two exploratory prompt-based approaches. Two approaches are based on sequence probabilities and require access to model parameters. Figure 1 illustrates the visual overview of each approach. For each method, we also note the limitations we identified during our examination. The overview of these methods are illustrated in Figure 1.\n\n\u2022 Min-K% Prob (Shi et al., 2023) assesses whether a text was in an LLM's pre-training data by calculating the average log-likelihood of the k% lowestprobability tokens, with a high result suggesting the text's presence in the training data. Limitations: (1) The authors report AUC based on the proposed WiKiMIA dataset, in which they regarded data events before the model release as contaminated data. Such a strong assumption on the ground truth may require more justification. (2) They did not provide the threshold to determine the value of min-K%-prob in the paper since they claim they can use AUC; however, in real-world settings, we do not always have the oracle to determine AUC -instead, we need a metric for determining whether arbitrary datasets are contaminated. (3) The code is not available.\n\n\u2022 Canonical Order Statistical Testing (Oren et al., 2024) identifies contamination in a pre-training dataset by checking if the model shows a preference for the canonical order of examples over random shuffling. This preference is tested by comparing their log probabilities, with results aggregated across datasets to ensure a low false positive rate. Limitations: When an individual data example's length is long, the combination of sample/shards/permutations in the setup can be costly.\n\n\u2022 Token Completion Overlap Score (Golchin and Surdeanu, 2024b) detects contamination by prompting the LLM with a dataset name, partition type, and a random initial segment of a reference instance. If the LLM's output closely matches the latter part of the reference, the instance is flagged as contaminated."
                        ],
                        "paper": {
                            "corpus_id": 272689919,
                            "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges",
                            "authors": [
                                {
                                    "authorId": "2298040858",
                                    "name": "Vinay Samuel"
                                },
                                {
                                    "authorId": "2261321156",
                                    "name": "Yue Zhou"
                                },
                                {
                                    "authorId": "2261285492",
                                    "name": "Henry Peng Zou"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 8
                        },
                        "score": 0.9736328125
                    },
                    {
                        "id": "(Zhang et al._1, 2024)",
                        "snippets": [
                            "Current methods differentiate members and non-members by designing scoring functions, like Perplexity and Min-k%. However, the diversity and complexity of training data magnifies the difficulty of distinguishing, leading to suboptimal performance in detecting pretraining data.\n\nOn the one hand, model developers can remove evaluation benchmark data from training data by retrieval-based methods with access to pertaining data (Ravaut et al., 2024;Chowdhery et al., 2023). Specifically, those methods employ n-gram tokenization and string-matching for detecting data contamination (Brown et al., 2020;Touvron et al., 2023b;Team et al., 2023;Radford et al., 2019). On the other hand, researchers utilize prompting techniques (Golchin & Surdeanu, 2024), performance analysis (Ye et al., 2024;Debenedetti et al., 2024), model likelihood (Oren et al., 2024;Shi et al., 2024;Xu et al., 2024b) to detect potential contamination without access to the training data."
                        ],
                        "paper": {
                            "corpus_id": 273350935,
                            "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2316591285",
                                    "name": "Hengxiang Zhang"
                                },
                                {
                                    "authorId": "2266803682",
                                    "name": "Songxin Zhang"
                                },
                                {
                                    "authorId": "2283306681",
                                    "name": "Bingyi Jing"
                                },
                                {
                                    "authorId": "2325203633",
                                    "name": "Hongxin Wei"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.95556640625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Novel and Alternative Detection Approaches",
                "tldr": "Researchers have developed innovative contamination detection methods that overcome limitations of traditional approaches, including probing techniques that examine model internals, statistical tests comparing output distributions, and even deliberate watermarking of benchmark datasets. (6 sources)",
                "text": "\nSeveral novel contamination detection approaches have emerged to address the limitations of traditional methods:\n\n- **Guided Instruction and Data Contamination Quiz (DCQ)**: Golchin and Surdeanu developed a method that prompts an LLM with dataset name, partition type, and a random-length initial segment of a reference instance, flagging contamination if the model's output closely matches the latter segment. They also introduced DCQ, which presents models with multiple-choice questions containing perturbed versions of dataset instances to detect contamination. Their approach achieved 92-100% accuracy across seven datasets when compared with expert evaluations. <Paper corpusId=\"260925501\" paperTitle=\"(Golchin et al., 2023)\" isShortName></Paper>\n\n- **Probing Techniques**: Liu et al. proposed using probing techniques to examine a model's internal activations rather than focusing on superficial features like perplexities in generated texts. They demonstrated this approach on their ArxivMIA benchmark, which contains challenging arXiv abstracts from Computer Science and Mathematics categories. <Paper corpusId=\"270217411\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>\n\n- **Contamination Detection via Output Distribution (CDD)**: Dong et al. focused on analyzing the distribution of generated content to detect contamination, introducing DetCon and ComiEval benchmarks specifically for assessing contamination detection and mitigation methods. <Paper corpusId=\"270285708\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>\n\n- **Paired Confidence Significance Testing (PaCoST)**: Zhang et al. introduced this approach which constructs counterparts for each data instance with similar distribution, then analyzes confidence scores to determine if a model shows significantly higher confidence when presented with original benchmarks. The method operates on the assumption that models demonstrate greater confidence when responding to questions they've been trained on. <Paper corpusId=\"270737802\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>\n\n- **Benchmark Watermarking**: Sander et al. proposed an innovative preemptive solution by watermarking benchmarks before their release. This approach reformulates original questions with a watermarked LLM without altering benchmark utility, then detects \"radioactivity\" traces during evaluation. Testing on benchmarks like ARC-Easy, ARC-Challenge, and MMLU demonstrated successful contamination detection when models were contaminated enough to enhance performance. <Paper corpusId=\"276575227\" paperTitle=\"(Sander et al., 2025)\" isShortName></Paper>\n\n- **Testset Slot Guessing (TS-Guessing)**: Deng et al. developed this protocol which masks incorrect answers in multiple-choice questions and prompts the model to fill in the gaps, using the model's response patterns to identify potential contamination. <Paper corpusId=\"270285708\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>\n\nWhile these approaches represent significant advances, they still face limitations. Token Completion Overlap Score is constrained by the LLM's comprehension abilities and response to instructions, while Canonical Order Statistical Testing becomes prohibitively expensive when analyzing longer data examples due to combinatorial explosion of sample/shard/permutation combinations. <Paper corpusId=\"272689919\" paperTitle=\"(Samuel et al., 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Golchin et al., 2023)",
                        "snippets": [
                            "To address this issue, we propose an inexpensive and robust approach to detect data contamination for a given dataset partition automatically. Importantly, our approach functions under two realistic assumptions: (a) we lack direct access to the pre-training data of the LLMs, and (b) we have limited computational resources. Intuitively, our method starts by identifying potential contamination in individual instances that are drawn from a small random sample of the corresponding dataset partition (we use samples of 10 instances in this work).\n\nAt its core, our approach starts by identifying potential contamination at the instance level; using this information, our approach then assesses wider contamination at the partition level. To estimate contamination of individual instances, we employ\"guided instruction:\"a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or nearly matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset partition as contaminated if the average overlap score with the reference instances (as measured by ROUGE-L or BLEURT) is statistically significantly better with the completions from guided instruction compared to a\"general instruction\"that does not include the dataset and partition name. The second idea marks a dataset partition as contaminated if a classifier based on GPT-4 with few-shot in-context learning prompt marks multiple generated completions as exact/near-exact matches of the corresponding reference instances. Our best method achieves an accuracy between 92% and 100% in detecting if an LLM is contaminated with seven datasets, containing train and test/validation partitions, when contrasted with manual evaluation by human experts."
                        ],
                        "paper": {
                            "corpus_id": 260925501,
                            "title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "65754049",
                                    "name": "Shahriar Golchin"
                                },
                                {
                                    "authorId": "1760868",
                                    "name": "M. Surdeanu"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 108
                        },
                        "score": 0.95458984375
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories."
                        ],
                        "paper": {
                            "corpus_id": 270217411,
                            "title": "Probing Language Models for Pre-training Data Detection",
                            "authors": [
                                {
                                    "authorId": "2294376388",
                                    "name": "Zhenhua Liu"
                                },
                                {
                                    "authorId": "1914586128",
                                    "name": "Tong Zhu"
                                },
                                {
                                    "authorId": "2186374155",
                                    "name": "Chuanyuan Tan"
                                },
                                {
                                    "authorId": "2304460083",
                                    "name": "Haonan Lu"
                                },
                                {
                                    "authorId": "2330946427",
                                    "name": "Bing Liu"
                                },
                                {
                                    "authorId": "2265943980",
                                    "name": "Wenliang Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 13
                        },
                        "score": 0.9775390625
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Efficient identification of BDC in evaluation benchmarks represents a fundamental aspect in ensuring the reliability and integrity of LLMs, while also providing the basis for developing effective mitigation strategies.This section provides a comprehensive review of literature related to the detection of BDC.We categorize the methodologies into two distinct strategies: matching-based and comparison-based, discussed in Sections 3.1 and 3.2, respectively",
                            "Deng et al. [30] proposed two novel methods to detect potential overlaps between evaluation benchmarks and pre-training corpora, tailored for both open-source and proprietary LLMs.They introduced a retrieval-based system and a Testset Slot Guessing (TS-Guessing) protocol, which involves masking incorrect answers in multiple-choice questions and prompting the model to fill in the gaps",
                            "Golchin and Surdeanu [46] presented another novel approach, the Data Contamination Quiz (DCQ), to detect and quantify BDC in LLMs.The DCQ is a series of multiple-choice questions with three perturbed versions of each dataset instance, including only word-level changes.The LLM's ability to identify the original instance among the perturbed ones indicates potential exposure to the data during pre-training",
                            "Golchin and Surdeanu (Golchin et al., 2023) presented a technique that combines instance-level identification using guided instruction prompts with partition-level assessment through overlap score comparison and classifier-based detection",
                            "Li et al. [91] presented a comprehensive report on BDC across over 15 LLMs and six multiplechoice QA benchmarks.They introduced an open-source pipeline to conduct contamination analysis on customized data and models",
                            "Dong et al. [34] focused on the distribution of generated content, they proposed a novel method, CDD (Contamination Detection via output Distribution), which uses the output distribution of LLMs to detect BDC",
                            "The paper also presents two new benchmarks, DetCon and ComiEval, for assessing BDC and mitigation methods",
                            "Li [89] proposed a novel method to detect BDC in language model evaluation without requiring access to the full training set.The technique uses perplexity to measure the extent of contamination",
                            "Oren et al. (Oren et al., 2023) presented a method to detect test set contamination in language models without needing access to the model's pre-training data or weights.The authors used a statistical test to identify contamination by comparing the likelihood of a benchmark dataset's canonical ordering against a shuffled version."
                        ],
                        "paper": {
                            "corpus_id": 270285708,
                            "title": "Benchmark Data Contamination of Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2153079650",
                                    "name": "Cheng Xu"
                                },
                                {
                                    "authorId": "2304954016",
                                    "name": "Shuhao Guan"
                                },
                                {
                                    "authorId": "2304952028",
                                    "name": "Derek Greene"
                                },
                                {
                                    "authorId": "2266906186",
                                    "name": "Mohand-Tahar Kechadi"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 56
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "In this study, we introduce a novel approach named PaCoST (Paired Confidence Significance Testing) designed for the detection of benchmark contamination in open-source LLMs. Our method entails a three-step statistical analysis, capable of identifying benchmarks within the model's training data. Specifically, our approach involves constructing counterparts for each data instance with similar distribution, followed by statistical analysis of corresponding confidence scores to ascertain whether the model exhibits significantly higher confidence when presented with original benchmarks. We operate under the assumption that the model tends to demonstrate greater confidence when responding to questions it has been trained on. To validate our method rigorously, we conduct a series of controlled experiments."
                        ],
                        "paper": {
                            "corpus_id": 270737802,
                            "title": "PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2288559437",
                                    "name": "Huixuan Zhang"
                                },
                                {
                                    "authorId": "2297335276",
                                    "name": "Yun Lin"
                                },
                                {
                                    "authorId": "2288537941",
                                    "name": "Xiaojun Wan"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 0
                        },
                        "score": 0.9736328125
                    },
                    {
                        "id": "(Sander et al., 2025)",
                        "snippets": [
                            "Benchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect \"radioactivity\", i.e., traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, e.g., p-val = 10 \u22123 for +5% on ARC-Easy."
                        ],
                        "paper": {
                            "corpus_id": 276575227,
                            "title": "Detecting Benchmark Contamination Through Watermarking",
                            "authors": [
                                {
                                    "authorId": "2283934407",
                                    "name": "Tom Sander"
                                },
                                {
                                    "authorId": "2322442744",
                                    "name": "Pierre Fernandez"
                                },
                                {
                                    "authorId": "2290845753",
                                    "name": "Saeed Mahloujifar"
                                },
                                {
                                    "authorId": "2283933043",
                                    "name": "A. Durmus"
                                },
                                {
                                    "authorId": "2290241478",
                                    "name": "Chuan Guo"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.955078125
                    },
                    {
                        "id": "(Samuel et al., 2024)",
                        "snippets": [
                            "To address this gap and provide a dual investigation of SOTA LLM contamination status and detection method robustness, we evaluate five contamination detection approaches with four state-of-the-art LLMs across eight challenging datasets often used in modern LLM evaluation.\n\nWe examine five distinct approaches to detecting data contamination in LLMs, including three state-of-the-art techniques from ICLR (2023-2024) and two exploratory prompt-based approaches. Two approaches are based on sequence probabilities and require access to model parameters. Figure 1 illustrates the visual overview of each approach. For each method, we also note the limitations we identified during our examination. The overview of these methods are illustrated in Figure 1.\n\n\u2022 Min-K% Prob (Shi et al., 2023) assesses whether a text was in an LLM's pre-training data by calculating the average log-likelihood of the k% lowestprobability tokens, with a high result suggesting the text's presence in the training data. Limitations: (1) The authors report AUC based on the proposed WiKiMIA dataset, in which they regarded data events before the model release as contaminated data. Such a strong assumption on the ground truth may require more justification. (2) They did not provide the threshold to determine the value of min-K%-prob in the paper since they claim they can use AUC; however, in real-world settings, we do not always have the oracle to determine AUC -instead, we need a metric for determining whether arbitrary datasets are contaminated. (3) The code is not available.\n\n\u2022 Canonical Order Statistical Testing (Oren et al., 2024) identifies contamination in a pre-training dataset by checking if the model shows a preference for the canonical order of examples over random shuffling. This preference is tested by comparing their log probabilities, with results aggregated across datasets to ensure a low false positive rate. Limitations: When an individual data example's length is long, the combination of sample/shards/permutations in the setup can be costly.\n\n\u2022 Token Completion Overlap Score (Golchin and Surdeanu, 2024b) detects contamination by prompting the LLM with a dataset name, partition type, and a random initial segment of a reference instance. If the LLM's output closely matches the latter part of the reference, the instance is flagged as contaminated."
                        ],
                        "paper": {
                            "corpus_id": 272689919,
                            "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges",
                            "authors": [
                                {
                                    "authorId": "2298040858",
                                    "name": "Vinay Samuel"
                                },
                                {
                                    "authorId": "2261321156",
                                    "name": "Yue Zhou"
                                },
                                {
                                    "authorId": "2261285492",
                                    "name": "Henry Peng Zou"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 8
                        },
                        "score": 0.9736328125
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.199083
    }
}