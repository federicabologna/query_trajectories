{
    "query": "What research papers present specific synthetic data generation methods for complex reasoning tasks in natural language processing, such as logical inference or multi-step question answering?",
    "user_id": "lib_user",
    "task_id": "c9668cc4-6dac-4035-a079-3c44ab75a831",
    "timestamp": "2025-06-23T22:05:57.335069",
    "n_retrieval": 256,
    "n_retrieved": 272,
    "n_candidates": 45,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.504807,
    "decomposed_query": {
        "rewritten_query": "Synthetic data generation methods for complex reasoning tasks in natural language processing, such as logical inference or multi-step question answering.",
        "keyword_query": "synthetic data generation methods complex reasoning tasks natural language processing logical inference multi-step question answering",
        "search_filters": {
            "fieldsOfStudy": "Computer Science,Linguistics"
        },
        "cost": 0.0,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.08239, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2223643390",
                    "name": "A. Lupidi"
                },
                {
                    "authorId": "2320807392",
                    "name": "Carlos Gemmell"
                },
                {
                    "authorId": "2313189469",
                    "name": "Nicola Cancedda"
                },
                {
                    "authorId": "2173509991",
                    "name": "Jane Dwivedi-Yu"
                },
                {
                    "authorId": "2267341626",
                    "name": "Jason E Weston"
                },
                {
                    "authorId": "2320805323",
                    "name": "Jakob Foerster"
                },
                {
                    "authorId": "48647153",
                    "name": "R. Raileanu"
                },
                {
                    "authorId": "2253400960",
                    "name": "Maria Lomeli"
                }
            ],
            "abstract": "Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines.",
            "corpus_id": 272600015,
            "sentences": [
                {
                    "corpus_id": "272600015",
                    "title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources",
                    "text": "Large Language Models (LLMs) (Devlin et al., 2019;Chowdhery et al., 2022;Brown et al., 2020;Vaswani et al., 2017) have risen to popularity due to their remarkable ability to digest and generate human-like text (Radford et al., 2018). However, LLMs still struggle with more complex tasks such as multi-step reasoning, tool use and manipulating or processing structured data. For many of these tasks there exists source data, such as existing structured data on the web, but little data of how to use it to solve a task.In principle, one can achieve performance improvements during fine-tuning by collecting human annotated data of such tasks. However, this is an expensive and time-consuming process (Touvron et al., 2023). \n\nIn this paper, we propose Source2Synth, a general approach to generate synthetic data grounded in external real-world sources. Grounding the data generation process in real-world sources steers the examples to be more realistic, diverse, and factually correct. We showcase our method on two challenging tasks: multi-hop questions based on sources from the web, and tabular question answering using SQL as a tool. In both cases it achieves improved performance without relying on human annotations, resulting in a scalable data generation method for complex tasks. \n\nSource2Synth consists of three stages: Dataset Generation and Dataset Curation, followed by Model Finetuning, see Figure 1. At the Data Generation stage, we start by selecting a data source (such as tables on the web, or related Wikipedia articles) to ground our synthetic data generation in realistic information for a specific task. Then, to generate a given example, our method first selects a seed topic to condition the generation -for example a specific entity in a Wikipedia article or a factual statement about a table. Given the seed topic, the method then generates the full example: the instruction (e.g., question), the reasoning chain to arrive at the answer (e.g., the steps of multi-hop question answering, or tool use) and the answer itself.",
                    "score": 0.5541473068744649,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 233
                        },
                        {
                            "start": 234,
                            "end": 373
                        },
                        {
                            "start": 374,
                            "end": 641
                        },
                        {
                            "start": 642,
                            "end": 722
                        },
                        {
                            "start": 725,
                            "end": 851
                        },
                        {
                            "start": 852,
                            "end": 985
                        },
                        {
                            "start": 986,
                            "end": 1137
                        },
                        {
                            "start": 1138,
                            "end": 1288
                        },
                        {
                            "start": 1291,
                            "end": 1414
                        },
                        {
                            "start": 1415,
                            "end": 1625
                        },
                        {
                            "start": 1626,
                            "end": 1818
                        },
                        {
                            "start": 1819,
                            "end": 2048
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 92,
                            "end": 113,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9638671875
                }
            ],
            "relevance_judgement": 0.9638671875,
            "relevance_judgment_input_expanded": "# Title: Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources\n# Venue: arXiv.org\n# Authors: A. Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason E Weston, Jakob Foerster, R. Raileanu, Maria Lomeli\n## Abstract\nLarge Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines.\n## Introduction\nLarge Language Models (LLMs) (Devlin et al., 2019;Chowdhery et al., 2022;Brown et al., 2020;Vaswani et al., 2017) have risen to popularity due to their remarkable ability to digest and generate human-like text (Radford et al., 2018). However, LLMs still struggle with more complex tasks such as multi-step reasoning, tool use and manipulating or processing structured data. For many of these tasks there exists source data, such as existing structured data on the web, but little data of how to use it to solve a task.In principle, one can achieve performance improvements during fine-tuning by collecting human annotated data of such tasks. However, this is an expensive and time-consuming process (Touvron et al., 2023). \n\nIn this paper, we propose Source2Synth, a general approach to generate synthetic data grounded in external real-world sources. Grounding the data generation process in real-world sources steers the examples to be more realistic, diverse, and factually correct. We showcase our method on two challenging tasks: multi-hop questions based on sources from the web, and tabular question answering using SQL as a tool. In both cases it achieves improved performance without relying on human annotations, resulting in a scalable data generation method for complex tasks. \n\nSource2Synth consists of three stages: Dataset Generation and Dataset Curation, followed by Model Finetuning, see Figure 1. At the Data Generation stage, we start by selecting a data source (such as tables on the web, or related Wikipedia articles) to ground our synthetic data generation in realistic information for a specific task. Then, to generate a given example, our method first selects a seed topic to condition the generation -for example a specific entity in a Wikipedia article or a factual statement about a table. Given the seed topic, the method then generates the full example: the instruction (e.g., question), the reasoning chain to arrive at the answer (e.g., the steps of multi-hop question answering, or tool use) and the answer itself.",
            "reference_string": "[272600015 | Lupidi et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Leveraging QA Datasets to Improve Generative Data Augmentation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 54,
            "citation_count": 18,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.emnlp-main.660.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.12604, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "7565696",
                    "name": "Dheeraj Mekala"
                },
                {
                    "authorId": "144244743",
                    "name": "Tu Vu"
                },
                {
                    "authorId": "32246932",
                    "name": "Timo Schick"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                }
            ],
            "abstract": "The ability of generative language models (GLMs) to generate text has improved considerably in the last few years, enabling their use for generative data augmentation. In this work, we propose CONDA, an approach to further improve GLM\u2019s ability to generate synthetic data by reformulating data generation as context generation for a given question-answer (QA) pair and leveraging QA datasets for training context generators. Then, we cast downstream tasks into the same question answering format and adapt the fine-tuned context generators to the target task domain. Finally, we use the fine-tuned GLM to generate relevant contexts, which are in turn used as synthetic training data for their corresponding tasks. We perform extensive experiments on multiple classification datasets and demonstrate substantial improvements in performance for both few- and zero-shot settings. Our analysis reveals that QA datasets that require high-level reasoning abilities (e.g., abstractive and common-sense QA datasets) tend to give the best boost in performance in both few-shot and zero-shot settings.",
            "corpus_id": 253107930,
            "sentences": [
                {
                    "corpus_id": "253107930",
                    "title": "Leveraging QA Datasets to Improve Generative Data Augmentation",
                    "text": "The ability of generative language models (GLMs) to generate text has improved considerably in the last few years, enabling their use for generative data augmentation. In this work, we propose CONDA, an approach to further improve GLM\u2019s ability to generate synthetic data by reformulating data generation as context generation for a given question-answer (QA) pair and leveraging QA datasets for training context generators. Then, we cast downstream tasks into the same question answering format and adapt the fine-tuned context generators to the target task domain. Finally, we use the fine-tuned GLM to generate relevant contexts, which are in turn used as synthetic training data for their corresponding tasks. We perform extensive experiments on multiple classification datasets and demonstrate substantial improvements in performance for both few- and zero-shot settings. Our analysis reveals that QA datasets that require high-level reasoning abilities (e.g., abstractive and common-sense QA datasets) tend to give the best boost in performance in both few-shot and zero-shot settings.",
                    "score": 0.44781811627713985,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95458984375
                },
                {
                    "corpus_id": "253107930",
                    "title": "Leveraging QA Datasets to Improve Generative Data Augmentation",
                    "text": "Finally, we generate synthetic training data for the target task by generating contexts for questions and answers pertaining to the respective dataset. Then, we add the generated samples to the few-shot supervision and train a target task model on the augmented data. \n\nWe perform extensive experiments on multiple sentiment analysis and topic classification datasets with several abstractive, extractive, and commonsense reasoning QA datasets. Through rigorous experiments and thorough analysis, we observe that QA datasets that require high-level reasoning abilities such as abstractive and common-sense QA datasets suit the best for generating high-quality data. \n\nOur contributions are summarized as follows: \n\n\u2022 We propose to use QA datasets for training generative language models to be context generators for a given question and answer. \u2022 We formulate various classification tasks into a QA format and model synthetic training data generation for these tasks as context generation. \u2022 We perform experiments on multiple sentiment analysis and topic classification datasets to demonstrate the effectiveness of our method in zero-and few-shot settings. \n\n\u2022 We release the code on Github1 .",
                    "score": 0.4610242289102009,
                    "section_title": "General text classification format",
                    "char_start_offset": 2771,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 267
                        },
                        {
                            "start": 270,
                            "end": 444
                        },
                        {
                            "start": 445,
                            "end": 665
                        },
                        {
                            "start": 668,
                            "end": 712
                        },
                        {
                            "start": 715,
                            "end": 844
                        },
                        {
                            "start": 845,
                            "end": 989
                        },
                        {
                            "start": 990,
                            "end": 1157
                        },
                        {
                            "start": 1160,
                            "end": 1194
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8671875
                }
            ],
            "relevance_judgement": 0.95458984375,
            "relevance_judgment_input_expanded": "# Title: Leveraging QA Datasets to Improve Generative Data Augmentation\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Dheeraj Mekala, Tu Vu, Timo Schick, Jingbo Shang\n## Abstract\nThe ability of generative language models (GLMs) to generate text has improved considerably in the last few years, enabling their use for generative data augmentation. In this work, we propose CONDA, an approach to further improve GLM\u2019s ability to generate synthetic data by reformulating data generation as context generation for a given question-answer (QA) pair and leveraging QA datasets for training context generators. Then, we cast downstream tasks into the same question answering format and adapt the fine-tuned context generators to the target task domain. Finally, we use the fine-tuned GLM to generate relevant contexts, which are in turn used as synthetic training data for their corresponding tasks. We perform extensive experiments on multiple classification datasets and demonstrate substantial improvements in performance for both few- and zero-shot settings. Our analysis reveals that QA datasets that require high-level reasoning abilities (e.g., abstractive and common-sense QA datasets) tend to give the best boost in performance in both few-shot and zero-shot settings.\n## General text classification format\nFinally, we generate synthetic training data for the target task by generating contexts for questions and answers pertaining to the respective dataset. Then, we add the generated samples to the few-shot supervision and train a target task model on the augmented data. \n\nWe perform extensive experiments on multiple sentiment analysis and topic classification datasets with several abstractive, extractive, and commonsense reasoning QA datasets. Through rigorous experiments and thorough analysis, we observe that QA datasets that require high-level reasoning abilities such as abstractive and common-sense QA datasets suit the best for generating high-quality data. \n\nOur contributions are summarized as follows: \n\n\u2022 We propose to use QA datasets for training generative language models to be context generators for a given question and answer. \u2022 We formulate various classification tasks into a QA format and model synthetic training data generation for these tasks as context generation. \u2022 We perform experiments on multiple sentiment analysis and topic classification datasets to demonstrate the effectiveness of our method in zero-and few-shot settings. \n\n\u2022 We release the code on Github1 .",
            "reference_string": "[253107930 | Mekala et al. | 2022 | Citations: 18]"
        },
        {
            "title": "Synthetic Data Generation Using Large Language Models: Advances in Text and Code",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 62,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.14023, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2350754914",
                    "name": "Mihai Nadas"
                },
                {
                    "authorId": "2306585020",
                    "name": "Laura Dio\u015fan"
                },
                {
                    "authorId": "2350756350",
                    "name": "Andreea Tomescu"
                }
            ],
            "abstract": "Large language models (LLMs) have unlocked new possibilities for generating synthetic training data in both natural language and code. By producing artificial but task-relevant examples, these models can significantly augment or even replace real-world datasets, especially when labeled data is scarce or sensitive. This paper surveys recent advances in using LLMs to create synthetic text and code, emphasizing prompt-based generation, retrieval-augmented pipelines, and iterative self-refinement. We show how these methods enrich low-resource tasks such as classification and question answering, as well as code-centric applications such as instruction tuning, code translation, and bug repair, by enabling automated verification of functional correctness. Alongside potential benefits like cost-effectiveness, broad coverage, and controllable diversity, we address challenges such as factual inaccuracies in generated text, lack of stylistic realism, and the risk of bias amplification. Proposed mitigations include filtering and weighting outputs and reinforcement learning with execution feedback for code. We conclude with open research directions like automated prompt engineering, cross-modal data synthesis, and robust evaluation frameworks, highlighting the importance of LLM-generated synthetic data in advancing AI while emphasizing ethical and quality safeguards.",
            "corpus_id": 277104955,
            "sentences": [
                {
                    "corpus_id": "277104955",
                    "title": "Synthetic Data Generation Using Large Language Models: Advances in Text and Code",
                    "text": "A bootstrapping algorithm where a pretrained LM generates new tasks and solutions from a few seed examples. Initially applied to create Stanford Alpaca dataset (52k instructions) using text-davinci-003. This method showed that an LLM can recursively teach itself new tasks, yielding high-quality multi-turn instruction data without manual writing. Inspired many subsequent works in synthetic instruction generation for both text and code. Empirical Study [35] Text Classification (6 tasks) Systematic analysis of GPT-3.5 generated data for classification. Showed significant gains (3-26%) with synthetic augmentation at 100 real samples, diminishing returns as data increases. Compared prompting methods: finding that topic-guided generation improves diversity and performance. Provided practical recommendations on how much data to generate and which prompting strategy to use in various scenarios. AugGPT: Leveraging ChatGPT for Text Data Augmentation [9] Few-shot text classification (Amazon Reviews, PubMed20K, Symptoms Dataset) \n\nAugGPT rephrases sentences into multiple diverse yet semantically consistent versions, improving training data quality. Evaluation shows AugGPT significantly improves model accuracy compared to traditional augmentation methods (e.g., synonym replacement, back-translation). Findings suggest ChatGPT-generated synthetic data is a scalable, highquality augmentation strategy, making it a cost-effective alternative to manual data expansion in NLP. Self-Ask [47] Multi-hop Question Answering (QA), Reasoning \n\nIntroduced an LLM-based self-asking strategy where models recursively break down complex questions into simpler sub-questions. Demonstrated improved reasoning performance on multi-step inference tasks by leveraging self-generated sub-questions for enhanced accuracy. CoT [13] Scientific QA, Logical Reasoning \n\nExamined how Chain-of-Thought (CoT) prompting improves LLM reasoning capabilities. Proposed Sci-CoT, a two-stage framework where intermediate rationales are generated separately from answer inference, improving efficiency and model performance. \n\nEnd of Table 3.",
                    "score": 0.5423484238828125,
                    "section_title": "Various approaches for",
                    "char_start_offset": 20773,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 107
                        },
                        {
                            "start": 108,
                            "end": 202
                        },
                        {
                            "start": 203,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 438
                        },
                        {
                            "start": 439,
                            "end": 555
                        },
                        {
                            "start": 556,
                            "end": 676
                        },
                        {
                            "start": 677,
                            "end": 777
                        },
                        {
                            "start": 778,
                            "end": 899
                        },
                        {
                            "start": 900,
                            "end": 1032
                        },
                        {
                            "start": 1035,
                            "end": 1154
                        },
                        {
                            "start": 1155,
                            "end": 1308
                        },
                        {
                            "start": 1309,
                            "end": 1480
                        },
                        {
                            "start": 1481,
                            "end": 1539
                        },
                        {
                            "start": 1542,
                            "end": 1668
                        },
                        {
                            "start": 1669,
                            "end": 1808
                        },
                        {
                            "start": 1809,
                            "end": 1850
                        },
                        {
                            "start": 1853,
                            "end": 1935
                        },
                        {
                            "start": 1936,
                            "end": 2097
                        },
                        {
                            "start": 2100,
                            "end": 2115
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1813,
                            "end": 1817,
                            "matchedPaperCorpusId": "264288838"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94775390625
                }
            ],
            "relevance_judgement": 0.94775390625,
            "relevance_judgment_input_expanded": "# Title: Synthetic Data Generation Using Large Language Models: Advances in Text and Code\n# Venue: arXiv.org\n# Authors: Mihai Nadas, Laura Dio\u015fan, Andreea Tomescu\n## Abstract\nLarge language models (LLMs) have unlocked new possibilities for generating synthetic training data in both natural language and code. By producing artificial but task-relevant examples, these models can significantly augment or even replace real-world datasets, especially when labeled data is scarce or sensitive. This paper surveys recent advances in using LLMs to create synthetic text and code, emphasizing prompt-based generation, retrieval-augmented pipelines, and iterative self-refinement. We show how these methods enrich low-resource tasks such as classification and question answering, as well as code-centric applications such as instruction tuning, code translation, and bug repair, by enabling automated verification of functional correctness. Alongside potential benefits like cost-effectiveness, broad coverage, and controllable diversity, we address challenges such as factual inaccuracies in generated text, lack of stylistic realism, and the risk of bias amplification. Proposed mitigations include filtering and weighting outputs and reinforcement learning with execution feedback for code. We conclude with open research directions like automated prompt engineering, cross-modal data synthesis, and robust evaluation frameworks, highlighting the importance of LLM-generated synthetic data in advancing AI while emphasizing ethical and quality safeguards.\n## Various approaches for\nA bootstrapping algorithm where a pretrained LM generates new tasks and solutions from a few seed examples. Initially applied to create Stanford Alpaca dataset (52k instructions) using text-davinci-003. This method showed that an LLM can recursively teach itself new tasks, yielding high-quality multi-turn instruction data without manual writing. Inspired many subsequent works in synthetic instruction generation for both text and code. Empirical Study [35] Text Classification (6 tasks) Systematic analysis of GPT-3.5 generated data for classification. Showed significant gains (3-26%) with synthetic augmentation at 100 real samples, diminishing returns as data increases. Compared prompting methods: finding that topic-guided generation improves diversity and performance. Provided practical recommendations on how much data to generate and which prompting strategy to use in various scenarios. AugGPT: Leveraging ChatGPT for Text Data Augmentation [9] Few-shot text classification (Amazon Reviews, PubMed20K, Symptoms Dataset) \n\nAugGPT rephrases sentences into multiple diverse yet semantically consistent versions, improving training data quality. Evaluation shows AugGPT significantly improves model accuracy compared to traditional augmentation methods (e.g., synonym replacement, back-translation). Findings suggest ChatGPT-generated synthetic data is a scalable, highquality augmentation strategy, making it a cost-effective alternative to manual data expansion in NLP. Self-Ask [47] Multi-hop Question Answering (QA), Reasoning \n\nIntroduced an LLM-based self-asking strategy where models recursively break down complex questions into simpler sub-questions. Demonstrated improved reasoning performance on multi-step inference tasks by leveraging self-generated sub-questions for enhanced accuracy. CoT [13] Scientific QA, Logical Reasoning \n\nExamined how Chain-of-Thought (CoT) prompting improves LLM reasoning capabilities. Proposed Sci-CoT, a two-stage framework where intermediate rationales are generated separately from answer inference, improving efficiency and model performance. \n\nEnd of Table 3.",
            "reference_string": "[277104955 | Nadas et al. | 2025 | Citations: 3]"
        },
        {
            "title": "Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 63,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13064, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47893214",
                    "name": "Haoran Li"
                },
                {
                    "authorId": "2047143813",
                    "name": "Qingxiu Dong"
                },
                {
                    "authorId": "2284834012",
                    "name": "Zhengyang Tang"
                },
                {
                    "authorId": "2144522700",
                    "name": "Chaojun Wang"
                },
                {
                    "authorId": "2284863493",
                    "name": "Xingxing Zhang"
                },
                {
                    "authorId": "15086992",
                    "name": "Haoyang Huang"
                },
                {
                    "authorId": "3110003",
                    "name": "Shaohan Huang"
                },
                {
                    "authorId": "2116768132",
                    "name": "Xiaolong Huang"
                },
                {
                    "authorId": "2284830389",
                    "name": "Zeqiang Huang"
                },
                {
                    "authorId": "2273919921",
                    "name": "Dongdong Zhang"
                },
                {
                    "authorId": "2116405624",
                    "name": "Yuxian Gu"
                },
                {
                    "authorId": "2193630544",
                    "name": "Xin Cheng"
                },
                {
                    "authorId": "2193104542",
                    "name": "Xun Wang"
                },
                {
                    "authorId": "2263708536",
                    "name": "Si-Qing Chen"
                },
                {
                    "authorId": "145307652",
                    "name": "Li Dong"
                },
                {
                    "authorId": "2261087197",
                    "name": "Wei Lu"
                },
                {
                    "authorId": "3335836",
                    "name": "Zhifang Sui"
                },
                {
                    "authorId": "2284827140",
                    "name": "Benyou Wang"
                },
                {
                    "authorId": "2273358252",
                    "name": "Wai Lam"
                },
                {
                    "authorId": "2253471545",
                    "name": "Furu Wei"
                }
            ],
            "abstract": "We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines. Specifically, inspired by the systematic structure in human education system, we build the taxonomy by decomposing human knowledge and capabilities to various fields, sub-fields and ultimately, distinct disciplines semi-automatically, facilitated by LLMs. Subsequently, we generate a comprehensive list of subjects for every discipline and proceed to design a syllabus tailored to each subject, again utilizing LLMs. With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate diverse instructions with a broad coverage across the entire spectrum of human knowledge and skills. Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks. In addition, GLAN allows for easy customization and new fields or skills can be added by simply incorporating a new node into our taxonomy.",
            "corpus_id": 267759981,
            "sentences": [
                {
                    "corpus_id": "267759981",
                    "title": "Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models",
                    "text": "We propose GLAN, a general and scalable method for synthesizing instruction data. Experiments show GLAN can help large language models improve their capabilities in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following. Currently, our synthetic data is based on the taxonomy of human knowledge and capabilities and there are other types of useful data not been covered. We are interested to design methods with boarder coverage. Our current instruction data are mostly question answer pairs and in the next step, we plan to generate synthetic data of multi-turn conversations and long documents.",
                    "score": 0.4675020670149115,
                    "section_title": "Conclusions",
                    "char_start_offset": 28410,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 81
                        },
                        {
                            "start": 82,
                            "end": 289
                        },
                        {
                            "start": 290,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 498
                        },
                        {
                            "start": 499,
                            "end": 665
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94140625
                }
            ],
            "relevance_judgement": 0.94140625,
            "relevance_judgment_input_expanded": "# Title: Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models\n# Venue: arXiv.org\n# Authors: Haoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang, Haoyang Huang, Shaohan Huang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang, Yuxian Gu, Xin Cheng, Xun Wang, Si-Qing Chen, Li Dong, Wei Lu, Zhifang Sui, Benyou Wang, Wai Lam, Furu Wei\n## Abstract\nWe introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines. Specifically, inspired by the systematic structure in human education system, we build the taxonomy by decomposing human knowledge and capabilities to various fields, sub-fields and ultimately, distinct disciplines semi-automatically, facilitated by LLMs. Subsequently, we generate a comprehensive list of subjects for every discipline and proceed to design a syllabus tailored to each subject, again utilizing LLMs. With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate diverse instructions with a broad coverage across the entire spectrum of human knowledge and skills. Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks. In addition, GLAN allows for easy customization and new fields or skills can be added by simply incorporating a new node into our taxonomy.\n## Conclusions\nWe propose GLAN, a general and scalable method for synthesizing instruction data. Experiments show GLAN can help large language models improve their capabilities in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following. Currently, our synthetic data is based on the taxonomy of human knowledge and capabilities and there are other types of useful data not been covered. We are interested to design methods with boarder coverage. Our current instruction data are mostly question answer pairs and in the next step, we plan to generate synthetic data of multi-turn conversations and long documents.",
            "reference_string": "[267759981 | Li et al. | 2024 | Citations: 63]"
        },
        {
            "title": "Teaching Broad Reasoning Skills for Multi-Step QA by Generating Hard Contexts",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 45,
            "citation_count": 11,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.emnlp-main.439.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.12496, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "6365809",
                    "name": "H. Trivedi"
                },
                {
                    "authorId": "35217367",
                    "name": "Niranjan Balasubramanian"
                },
                {
                    "authorId": "2236429",
                    "name": "Tushar Khot"
                },
                {
                    "authorId": "48229640",
                    "name": "Ashish Sabharwal"
                }
            ],
            "abstract": "Question-answering datasets require a broad set of reasoning skills. We show how to use question decompositions to teach language models these broad reasoning skills in a robust fashion. Specifically, we use widely available QDMR representations to programmatically create hard-to-cheat synthetic contexts for real questions in six multi-step reasoning datasets. These contexts are carefully designed to avoid common reasoning shortcuts prevalent in real contexts that prevent models from learning the right skills. This results in a pretraining dataset, named TeaBReaC, containing 525K multi-step questions (with associated formal programs) covering about 900 reasoning patterns. We show that pretraining standard language models (LMs) on TeaBReaC before fine-tuning them on target datasets improves their performance by up to 13 F1 points across 4 multi-step QA datasets, with up to 21 point gain on more complex questions. The resulting models also demonstrate higher robustness, with a 5-8 F1 point improvement on two contrast sets. Furthermore, TeaBReaC pretraining substantially improves model performance and robustness even when starting with numerate LMs pretrained using recent methods (e.g., PReasM, POET). Our work thus shows how to effectively use decomposition-guided contexts to robustly teach multi-step reasoning.",
            "corpus_id": 253370208,
            "sentences": [
                {
                    "corpus_id": "253370208",
                    "title": "Teaching Broad Reasoning Skills for Multi-Step QA by Generating Hard Contexts",
                    "text": "Despite large LMs' impressive reading abilities and the availability of large scale multi-step QA datasets requiring a rich set of reasoning skills, LMbased QA models do not reliably learn to use such skills for answering complex questions. In this work, we show that the greater control that synthetic contexts offer can be leveraged to create a teaching dataset where models can learn a broad range of reasoning skills in a reliable manner, especially for more complex questions. \n\nOur transfer results from synthetic data to ac-  tual QA datasets add to the growing line of work that shows synthetic datasets can in fact be used to inject useful skills are valuable for real, natural language tasks. Given the artifact issues in real datasets (specifically, in their contexts) and the difficulty in controlling for them via perturbations, we present a viable alternative: leveraging existing multi-step questions for their broad reasoning patterns but using synthetic contexts for carefully constructing teaching datasets, where models can learn the right way to reason.",
                    "score": 0.45874844972740453,
                    "section_title": "Conclusions",
                    "char_start_offset": 29310,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 240
                        },
                        {
                            "start": 241,
                            "end": 481
                        },
                        {
                            "start": 484,
                            "end": 702
                        },
                        {
                            "start": 703,
                            "end": 1073
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93310546875
                },
                {
                    "corpus_id": "253370208",
                    "title": "Teaching Broad Reasoning Skills for Multi-Step QA by Generating Hard Contexts",
                    "text": "Further, context generation also enables us to balance out the distribution of reasoning patterns, e.g., by synthesizing additional contexts (and thereby examples) for questions from the long-tail of underrepresented reasoning patterns. \n\nOur use of synthetic contexts to reliably teach broad skills is inspired by three strands of recent RC QA research. One strand has shown that skills learnt over synthetic data can indeed transfer to real datasets (Geva et al., 2020;Yang et al., 2021;Yoran et al., 2022;Pi et al., 2022). A second strand has shown that perturbing the existing (natural) contexts of RC instances in a targeted fashion can reduce artifact-based reasoning (Jia and Liang, 2017;Trivedi et al., 2020). A third strand has shown that carefully constructing contexts (for synthetic questions) to have sufficient distractors can reduce artifacts (Trivedi et al., 2022;Khot et al., 2022a). \n\nBuilding upon these three strands, we introduce TEABREAC,2 a teaching dataset that includes carefully constructed synthetic contexts for a broad set of real multi-step questions sourced from six existing datasets. TEABREAC was designed with the goals of strong control over cheatability and balanced coverage of reasoning patterns. To identify the intended reasoning, we leverage question decomposition annotations, specifically Question Decomposition Meaning Representation or QDMR annotations which are widely available for a broad set of datasets (Wolfson et al., 2020). \n\nFigure 1 shows the overview of our construction process for TEABREAC. Our approach relies on treating a question decomposition as an unambiguous typed program that can be used to generate a synthetic context and can be executed to provide an answer. To this end, we first turn natural language QDMRs into a precise typed program. We then construct a synthetic context by asserting a set of facts that relate to various parts of the multi-step question. We do this by grounding the predicates of QDMR (e.g., field goals of Shayne Graham in Fig. 1) with randomly generated entities. We also add distractor statements to the context to ensure that bypassing reasoning steps results in an incorrect answer.",
                    "score": 0.5010452032659285,
                    "section_title": "Introduction",
                    "char_start_offset": 2090,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 236
                        },
                        {
                            "start": 239,
                            "end": 354
                        },
                        {
                            "start": 355,
                            "end": 525
                        },
                        {
                            "start": 526,
                            "end": 717
                        },
                        {
                            "start": 718,
                            "end": 900
                        },
                        {
                            "start": 903,
                            "end": 1116
                        },
                        {
                            "start": 1117,
                            "end": 1234
                        },
                        {
                            "start": 1235,
                            "end": 1476
                        },
                        {
                            "start": 1479,
                            "end": 1548
                        },
                        {
                            "start": 1549,
                            "end": 1728
                        },
                        {
                            "start": 1729,
                            "end": 1808
                        },
                        {
                            "start": 1809,
                            "end": 1931
                        },
                        {
                            "start": 1932,
                            "end": 2059
                        },
                        {
                            "start": 2060,
                            "end": 2181
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 452,
                            "end": 471,
                            "matchedPaperCorpusId": "215548225"
                        },
                        {
                            "start": 489,
                            "end": 508,
                            "matchedPaperCorpusId": "235898880"
                        },
                        {
                            "start": 674,
                            "end": 695,
                            "matchedPaperCorpusId": "7228830"
                        },
                        {
                            "start": 695,
                            "end": 716,
                            "matchedPaperCorpusId": "221749191"
                        },
                        {
                            "start": 858,
                            "end": 880,
                            "matchedPaperCorpusId": "236771976"
                        },
                        {
                            "start": 880,
                            "end": 899,
                            "matchedPaperCorpusId": "248666080"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84521484375
                }
            ],
            "relevance_judgement": 0.93310546875,
            "relevance_judgment_input_expanded": "# Title: Teaching Broad Reasoning Skills for Multi-Step QA by Generating Hard Contexts\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: H. Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal\n## Abstract\nQuestion-answering datasets require a broad set of reasoning skills. We show how to use question decompositions to teach language models these broad reasoning skills in a robust fashion. Specifically, we use widely available QDMR representations to programmatically create hard-to-cheat synthetic contexts for real questions in six multi-step reasoning datasets. These contexts are carefully designed to avoid common reasoning shortcuts prevalent in real contexts that prevent models from learning the right skills. This results in a pretraining dataset, named TeaBReaC, containing 525K multi-step questions (with associated formal programs) covering about 900 reasoning patterns. We show that pretraining standard language models (LMs) on TeaBReaC before fine-tuning them on target datasets improves their performance by up to 13 F1 points across 4 multi-step QA datasets, with up to 21 point gain on more complex questions. The resulting models also demonstrate higher robustness, with a 5-8 F1 point improvement on two contrast sets. Furthermore, TeaBReaC pretraining substantially improves model performance and robustness even when starting with numerate LMs pretrained using recent methods (e.g., PReasM, POET). Our work thus shows how to effectively use decomposition-guided contexts to robustly teach multi-step reasoning.\n## Introduction\nFurther, context generation also enables us to balance out the distribution of reasoning patterns, e.g., by synthesizing additional contexts (and thereby examples) for questions from the long-tail of underrepresented reasoning patterns. \n\nOur use of synthetic contexts to reliably teach broad skills is inspired by three strands of recent RC QA research. One strand has shown that skills learnt over synthetic data can indeed transfer to real datasets (Geva et al., 2020;Yang et al., 2021;Yoran et al., 2022;Pi et al., 2022). A second strand has shown that perturbing the existing (natural) contexts of RC instances in a targeted fashion can reduce artifact-based reasoning (Jia and Liang, 2017;Trivedi et al., 2020). A third strand has shown that carefully constructing contexts (for synthetic questions) to have sufficient distractors can reduce artifacts (Trivedi et al., 2022;Khot et al., 2022a). \n\nBuilding upon these three strands, we introduce TEABREAC,2 a teaching dataset that includes carefully constructed synthetic contexts for a broad set of real multi-step questions sourced from six existing datasets. TEABREAC was designed with the goals of strong control over cheatability and balanced coverage of reasoning patterns. To identify the intended reasoning, we leverage question decomposition annotations, specifically Question Decomposition Meaning Representation or QDMR annotations which are widely available for a broad set of datasets (Wolfson et al., 2020). \n\nFigure 1 shows the overview of our construction process for TEABREAC. Our approach relies on treating a question decomposition as an unambiguous typed program that can be used to generate a synthetic context and can be executed to provide an answer. To this end, we first turn natural language QDMRs into a precise typed program. We then construct a synthetic context by asserting a set of facts that relate to various parts of the multi-step question. We do this by grounding the predicates of QDMR (e.g., field goals of Shayne Graham in Fig. 1) with randomly generated entities. We also add distractor statements to the context to ensure that bypassing reasoning steps results in an incorrect answer.\n\n## Conclusions\nDespite large LMs' impressive reading abilities and the availability of large scale multi-step QA datasets requiring a rich set of reasoning skills, LMbased QA models do not reliably learn to use such skills for answering complex questions. In this work, we show that the greater control that synthetic contexts offer can be leveraged to create a teaching dataset where models can learn a broad range of reasoning skills in a reliable manner, especially for more complex questions. \n\nOur transfer results from synthetic data to ac-  tual QA datasets add to the growing line of work that shows synthetic datasets can in fact be used to inject useful skills are valuable for real, natural language tasks. Given the artifact issues in real datasets (specifically, in their contexts) and the difficulty in controlling for them via perturbations, we present a viable alternative: leveraging existing multi-step questions for their broad reasoning patterns but using synthetic contexts for carefully constructing teaching datasets, where models can learn the right way to reason.",
            "reference_string": "[253370208 | Trivedi et al. | 2022 | Citations: 11]"
        },
        {
            "title": "Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.12437, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2297828274",
                    "name": "Jiaming Zhou"
                },
                {
                    "authorId": "2321867953",
                    "name": "Abbas Ghaddar"
                },
                {
                    "authorId": "2321875142",
                    "name": "Ge Zhang"
                },
                {
                    "authorId": "1892081076",
                    "name": "Liheng Ma"
                },
                {
                    "authorId": "2288403553",
                    "name": "Yaochen Hu"
                },
                {
                    "authorId": "38939190",
                    "name": "Soumyasundar Pal"
                },
                {
                    "authorId": "2287938359",
                    "name": "Mark Coates"
                },
                {
                    "authorId": "2321911323",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "2275529643",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "2307072418",
                    "name": "Jianye Hao"
                }
            ],
            "abstract": "Despite recent advances in training and prompting strategies for Large Language Models (LLMs), these models continue to face challenges with complex logical reasoning tasks that involve long reasoning chains. In this work, we explore the potential and limitations of using graph-based synthetic reasoning data as training signals to enhance LLMs' reasoning capabilities. Our extensive experiments, conducted on two established natural language reasoning tasks -- inductive reasoning and spatial reasoning -- demonstrate that supervised fine-tuning (SFT) with synthetic graph-based reasoning data effectively enhances LLMs' reasoning performance without compromising their effectiveness on other standard evaluation benchmarks.",
            "corpus_id": 272753147,
            "sentences": [
                {
                    "corpus_id": "272753147",
                    "title": "Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data",
                    "text": "The reasoning capabilities of Large Language Models (LLMs) (Touvron et al., 2023;Jiang et al., 2023;Dubey et al., 2024) can be greatly enhanced by post-training techniques (Ouyang et al., 2022;Zhang et al., 2023) and prompting strategies (Wei et al., 2022b;Yao et al., 2023;Madaan et al., 2023). However, even with the aforementioned techniques, the multi-hop reasoning tasks remain challenging (Touvron et al., 2023;Jiang et al., 2023;Dubey et al., 2024): LLMs struggle to reason over steps (Agrawal et al., 2024;Zhao and Zhang, 2024), and are fragile to minor perturbations (Ullman, 2023;Chen et al., 2024) in the input prompt. \n\nRecently, several works (Xu et al., 2024;Abdin et al., 2024;Anil et al., 2023) have demonstrated the efficacy of boosting the LLMs' reasoning capacity via fine-tuning on synthetic data generated by stronger LLMs. However, how to make such synthetic data generation effective and controllable for specific applications remains an open question. \n\nExtensive prompt engineering and quality filtering are required to guide LLMs' generation, yet the quality of generated reasoning questions and their labels remains uncertain (Gudibande et al., 2023;Wang et al., 2023;Tan et al., 2024). Motivated by the fact that natural language reasoning tasks can be represented as structured data with finite nodes and edges (Jin et al., 2024), and inspired by existing works on constructing reasoning benchmarks (Fatemi et al., 2024;Kazemi et al., 2023;Agrawal et al., 2024), we propose to leverage synthetic graph-based data for task-specific posttraining adaptation to improve the correctness of the generated reasoning questions and labels. \n\nIn this paper, we carefully design a random walk sampling algorithm on graphs and introduce a new prompting strategy that first extracts a reasoning chain and then derives the answer.",
                    "score": 0.5043763906136552,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 295
                        },
                        {
                            "start": 296,
                            "end": 629
                        },
                        {
                            "start": 632,
                            "end": 844
                        },
                        {
                            "start": 845,
                            "end": 975
                        },
                        {
                            "start": 978,
                            "end": 1213
                        },
                        {
                            "start": 1214,
                            "end": 1659
                        },
                        {
                            "start": 1662,
                            "end": 1845
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 590,
                            "end": 608,
                            "matchedPaperCorpusId": "267657940"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9267578125
                },
                {
                    "corpus_id": "272753147",
                    "title": "Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data",
                    "text": "Reasoning tasks in natural language often involve a structured representation of facts that can be abstracted into a graph following predefined rules (Ji et al., 2022). For example, family relationships can be systematically represented in a logical graph, where nodes denote family members and edges define their relationships, all governed by logical rules. Let G=(V, E, R) represent a relational graph with a set of nodes V, a set of edges E between the nodes, and a set of relations R expressed in first-order logic. In this graph, vertices (e.g., family members) are denoted as v i \u2208V, and directed edges (e.g., familial relationships) are represented as (v i , r i,j , v j ) \u2208 E, with relations r i,j \u2208R. Our goal is to generate synthetic examples of graphstructured data from such relation graphs to adapt LLMs for targeted reasoning tasks. Existing graphbased data generation methods, such as (Sinha et al., 2019), may produce large amounts of data, but this data is frequently redundant and lacks the necessary complexity (see Figure 1). Therefore, we propose a random-walk-based algorithm (Lov\u00e1sz, 1993) that produces a manageable yet diverse set of examples by sampling sub-graphs from G. In the remainder of this section, we describe our algorithm for constructing G in \u00a7 2.1, the process of generating synthetic reasoning data in \u00a7 2.2, and their deployment to enhance LLM capabilities in natural language reasoning tasks in \u00a7 2.3.",
                    "score": 0.5099859762098198,
                    "section_title": "Methodology",
                    "char_start_offset": 2524,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 168
                        },
                        {
                            "start": 169,
                            "end": 359
                        },
                        {
                            "start": 360,
                            "end": 520
                        },
                        {
                            "start": 521,
                            "end": 710
                        },
                        {
                            "start": 711,
                            "end": 847
                        },
                        {
                            "start": 848,
                            "end": 1046
                        },
                        {
                            "start": 1047,
                            "end": 1444
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 150,
                            "end": 167,
                            "matchedPaperCorpusId": "211010433"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92041015625
                }
            ],
            "relevance_judgement": 0.9267578125,
            "relevance_judgment_input_expanded": "# Title: Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data\n# Venue: arXiv.org\n# Authors: Jiaming Zhou, Abbas Ghaddar, Ge Zhang, Liheng Ma, Yaochen Hu, Soumyasundar Pal, Mark Coates, Bin Wang, Yingxue Zhang, Jianye Hao\n## Abstract\nDespite recent advances in training and prompting strategies for Large Language Models (LLMs), these models continue to face challenges with complex logical reasoning tasks that involve long reasoning chains. In this work, we explore the potential and limitations of using graph-based synthetic reasoning data as training signals to enhance LLMs' reasoning capabilities. Our extensive experiments, conducted on two established natural language reasoning tasks -- inductive reasoning and spatial reasoning -- demonstrate that supervised fine-tuning (SFT) with synthetic graph-based reasoning data effectively enhances LLMs' reasoning performance without compromising their effectiveness on other standard evaluation benchmarks.\n## Introduction\nThe reasoning capabilities of Large Language Models (LLMs) (Touvron et al., 2023;Jiang et al., 2023;Dubey et al., 2024) can be greatly enhanced by post-training techniques (Ouyang et al., 2022;Zhang et al., 2023) and prompting strategies (Wei et al., 2022b;Yao et al., 2023;Madaan et al., 2023). However, even with the aforementioned techniques, the multi-hop reasoning tasks remain challenging (Touvron et al., 2023;Jiang et al., 2023;Dubey et al., 2024): LLMs struggle to reason over steps (Agrawal et al., 2024;Zhao and Zhang, 2024), and are fragile to minor perturbations (Ullman, 2023;Chen et al., 2024) in the input prompt. \n\nRecently, several works (Xu et al., 2024;Abdin et al., 2024;Anil et al., 2023) have demonstrated the efficacy of boosting the LLMs' reasoning capacity via fine-tuning on synthetic data generated by stronger LLMs. However, how to make such synthetic data generation effective and controllable for specific applications remains an open question. \n\nExtensive prompt engineering and quality filtering are required to guide LLMs' generation, yet the quality of generated reasoning questions and their labels remains uncertain (Gudibande et al., 2023;Wang et al., 2023;Tan et al., 2024). Motivated by the fact that natural language reasoning tasks can be represented as structured data with finite nodes and edges (Jin et al., 2024), and inspired by existing works on constructing reasoning benchmarks (Fatemi et al., 2024;Kazemi et al., 2023;Agrawal et al., 2024), we propose to leverage synthetic graph-based data for task-specific posttraining adaptation to improve the correctness of the generated reasoning questions and labels. \n\nIn this paper, we carefully design a random walk sampling algorithm on graphs and introduce a new prompting strategy that first extracts a reasoning chain and then derives the answer.\n\n## Methodology\nReasoning tasks in natural language often involve a structured representation of facts that can be abstracted into a graph following predefined rules (Ji et al., 2022). For example, family relationships can be systematically represented in a logical graph, where nodes denote family members and edges define their relationships, all governed by logical rules. Let G=(V, E, R) represent a relational graph with a set of nodes V, a set of edges E between the nodes, and a set of relations R expressed in first-order logic. In this graph, vertices (e.g., family members) are denoted as v i \u2208V, and directed edges (e.g., familial relationships) are represented as (v i , r i,j , v j ) \u2208 E, with relations r i,j \u2208R. Our goal is to generate synthetic examples of graphstructured data from such relation graphs to adapt LLMs for targeted reasoning tasks. Existing graphbased data generation methods, such as (Sinha et al., 2019), may produce large amounts of data, but this data is frequently redundant and lacks the necessary complexity (see Figure 1). Therefore, we propose a random-walk-based algorithm (Lov\u00e1sz, 1993) that produces a manageable yet diverse set of examples by sampling sub-graphs from G. In the remainder of this section, we describe our algorithm for constructing G in \u00a7 2.1, the process of generating synthetic reasoning data in \u00a7 2.2, and their deployment to enhance LLM capabilities in natural language reasoning tasks in \u00a7 2.3.",
            "reference_string": "[272753147 | Zhou et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 32,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.04736, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46684455",
                    "name": "Anna Goldie"
                },
                {
                    "authorId": "1861312",
                    "name": "Azalia Mirhoseini"
                },
                {
                    "authorId": "2354343696",
                    "name": "Hao Zhou"
                },
                {
                    "authorId": "2354178736",
                    "name": "Irene Cai"
                },
                {
                    "authorId": "2262643057",
                    "name": "Christopher D. Manning"
                }
            ],
            "abstract": "Reinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus shifts toward more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8K (a math dataset) by a relative 16.9%.",
            "corpus_id": 277622127,
            "sentences": [
                {
                    "corpus_id": "277622127",
                    "title": "Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use",
                    "text": "Reinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus shifts toward more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8K (a math dataset) by a relative 16.9%.",
                    "score": 0.4946349513207618,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91943359375
                }
            ],
            "relevance_judgement": 0.91943359375,
            "relevance_judgment_input_expanded": "# Title: Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use\n# Venue: arXiv.org\n# Authors: Anna Goldie, Azalia Mirhoseini, Hao Zhou, Irene Cai, Christopher D. Manning\n## Abstract\nReinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus shifts toward more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8K (a math dataset) by a relative 16.9%.\n",
            "reference_string": "[277622127 | Goldie et al. | 2025 | Citations: 11]"
        },
        {
            "title": "WHERE and WHICH: Iterative Debate for Biomedical Synthetic Data Augmentation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 33,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.23673, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "98287352",
                    "name": "Zhengyi Zhao"
                },
                {
                    "authorId": "2331363849",
                    "name": "Shubo Zhang"
                },
                {
                    "authorId": "2292198028",
                    "name": "Bin Liang"
                },
                {
                    "authorId": "1707937",
                    "name": "Binyang Li"
                },
                {
                    "authorId": "2314397204",
                    "name": "Kam-Fai Wong"
                }
            ],
            "abstract": "In Biomedical Natural Language Processing (BioNLP) tasks, such as Relation Extraction, Named Entity Recognition, and Text Classification, the scarcity of high-quality data remains a significant challenge. This limitation poisons large language models to correctly understand relationships between biological entities, such as molecules and diseases, or drug interactions, and further results in potential misinterpretation of biomedical documents. To address this issue, current approaches generally adopt the Synthetic Data Augmentation method which involves similarity computation followed by word replacement, but counterfactual data are usually generated. As a result, these methods disrupt meaningful word sets or produce sentences with meanings that deviate substantially from the original context, rendering them ineffective in improving model performance. To this end, this paper proposes a biomedical-dedicated rationale-based synthetic data augmentation method. Beyond the naive lexicon similarity, specific bio-relation similarity is measured to hold the augmented instance having a strong correlation with bio-relation instead of simply increasing the diversity of augmented data. Moreover, a multi-agents-involved reflection mechanism helps the model iteratively distinguish different usage of similar entities to escape falling into the mis-replace trap. We evaluate our method on the BLURB and BigBIO benchmark, which includes 9 common datasets spanning four major BioNLP tasks. Our experimental results demonstrate consistent performance improvements across all tasks, highlighting the effectiveness of our approach in addressing the challenges associated with data scarcity and enhancing the overall performance of biomedical NLP models.",
            "corpus_id": 277452727,
            "sentences": [
                {
                    "corpus_id": "277452727",
                    "title": "WHERE and WHICH: Iterative Debate for Biomedical Synthetic Data Augmentation",
                    "text": "The synthetic data augmentation aims to generate pseudo instances corresponding to the given instances in semantics but with diverse syntax. The existing methods can be divided into two categories: (a) Amendment-based methods. This kind of method tries to amend the exact token or order of input sentences according to pre-defined rules to augment the data. [8] adopts synonym replacement, random insertion, random swap, and random deletion to augment the original sentences. \n\nBeyond the token-level, [16] interpolates the embeddings and labels of two or more sentences from representation-level. and (b) Generation-based methods. These years, leveraging generative large language models to generate kinds of data has become popular. [17] and [18] fine-tune multiple generative models for each relation type to generate augmentations. [10] proposes a sophisticated generation-based method that generates augmented data by incorporating new linguistic patterns. Beyond that, [19] introduced ChatGPT to make great progress in nearly all kinds of NLP tasks. It has proved to be effective utilizing ChatGPT as a data augmentation technique to enrich the data instances [20]. \n\nHowever, the existing data augmentation methods generally lean to generate instances leaving faithfulness and factuality alone which poison the model in understanding the interaction among biomedical entities and relations, and further, misleading models cannot be adapted to the real scenarios.",
                    "score": 0.4439982908604859,
                    "section_title": "B. Synthetic Data Augmentation in BioNLP",
                    "char_start_offset": 8102,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 140
                        },
                        {
                            "start": 141,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 357
                        },
                        {
                            "start": 358,
                            "end": 475
                        },
                        {
                            "start": 478,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 734
                        },
                        {
                            "start": 735,
                            "end": 835
                        },
                        {
                            "start": 836,
                            "end": 961
                        },
                        {
                            "start": 962,
                            "end": 1055
                        },
                        {
                            "start": 1056,
                            "end": 1171
                        },
                        {
                            "start": 1174,
                            "end": 1469
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 358,
                            "end": 361,
                            "matchedPaperCorpusId": "59523656"
                        },
                        {
                            "start": 735,
                            "end": 739,
                            "matchedPaperCorpusId": "212821571"
                        },
                        {
                            "start": 836,
                            "end": 840,
                            "matchedPaperCorpusId": "232379879"
                        },
                        {
                            "start": 1166,
                            "end": 1170,
                            "matchedPaperCorpusId": "260063065"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91650390625
                }
            ],
            "relevance_judgement": 0.91650390625,
            "relevance_judgment_input_expanded": "# Title: WHERE and WHICH: Iterative Debate for Biomedical Synthetic Data Augmentation\n# Venue: arXiv.org\n# Authors: Zhengyi Zhao, Shubo Zhang, Bin Liang, Binyang Li, Kam-Fai Wong\n## Abstract\nIn Biomedical Natural Language Processing (BioNLP) tasks, such as Relation Extraction, Named Entity Recognition, and Text Classification, the scarcity of high-quality data remains a significant challenge. This limitation poisons large language models to correctly understand relationships between biological entities, such as molecules and diseases, or drug interactions, and further results in potential misinterpretation of biomedical documents. To address this issue, current approaches generally adopt the Synthetic Data Augmentation method which involves similarity computation followed by word replacement, but counterfactual data are usually generated. As a result, these methods disrupt meaningful word sets or produce sentences with meanings that deviate substantially from the original context, rendering them ineffective in improving model performance. To this end, this paper proposes a biomedical-dedicated rationale-based synthetic data augmentation method. Beyond the naive lexicon similarity, specific bio-relation similarity is measured to hold the augmented instance having a strong correlation with bio-relation instead of simply increasing the diversity of augmented data. Moreover, a multi-agents-involved reflection mechanism helps the model iteratively distinguish different usage of similar entities to escape falling into the mis-replace trap. We evaluate our method on the BLURB and BigBIO benchmark, which includes 9 common datasets spanning four major BioNLP tasks. Our experimental results demonstrate consistent performance improvements across all tasks, highlighting the effectiveness of our approach in addressing the challenges associated with data scarcity and enhancing the overall performance of biomedical NLP models.\n## B. Synthetic Data Augmentation in BioNLP\nThe synthetic data augmentation aims to generate pseudo instances corresponding to the given instances in semantics but with diverse syntax. The existing methods can be divided into two categories: (a) Amendment-based methods. This kind of method tries to amend the exact token or order of input sentences according to pre-defined rules to augment the data. [8] adopts synonym replacement, random insertion, random swap, and random deletion to augment the original sentences. \n\nBeyond the token-level, [16] interpolates the embeddings and labels of two or more sentences from representation-level. and (b) Generation-based methods. These years, leveraging generative large language models to generate kinds of data has become popular. [17] and [18] fine-tune multiple generative models for each relation type to generate augmentations. [10] proposes a sophisticated generation-based method that generates augmented data by incorporating new linguistic patterns. Beyond that, [19] introduced ChatGPT to make great progress in nearly all kinds of NLP tasks. It has proved to be effective utilizing ChatGPT as a data augmentation technique to enrich the data instances [20]. \n\nHowever, the existing data augmentation methods generally lean to generate instances leaving faithfulness and factuality alone which poison the model in understanding the interaction among biomedical entities and relations, and further, misleading models cannot be adapted to the real scenarios.",
            "reference_string": "[277452727 | Zhao et al. | 2025 | Citations: 0]"
        },
        {
            "title": "nvBench 2.0: A Benchmark for Natural Language to Visualization under Ambiguity",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 54,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2503.12880?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2503.12880, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324719591",
                    "name": "Tianqi Luo"
                },
                {
                    "authorId": "2351107046",
                    "name": "Chuhan Huang"
                },
                {
                    "authorId": "2314809440",
                    "name": "Leixian Shen"
                },
                {
                    "authorId": "2304516904",
                    "name": "Boyan Li"
                },
                {
                    "authorId": "2316124858",
                    "name": "Shuyu Shen"
                },
                {
                    "authorId": "2351038617",
                    "name": "Wei Zeng"
                },
                {
                    "authorId": "2301154561",
                    "name": "Nan Tang"
                },
                {
                    "authorId": "2324528358",
                    "name": "Yuyu Luo"
                }
            ],
            "abstract": "Natural Language to Visualization (nl2vis) enables users to create visualizations from natural language queries, making data insights more accessible. However, nl2vis faces challenges in interpreting ambiguous queries, as users often express their visualization needs in imprecise language. To address this challenge, we introduce nvBench 2.0, a new benchmark designed to evaluate nl2vis systems in scenarios involving ambiguous queries. nvBench 2.0 includes 7,878 natural language queries and 24,076 corresponding visualizations, derived from 780 tables across 153 domains. It is built using a controlled ambiguity-injection pipeline that generates ambiguous queries through a reverse-generation workflow. By starting with unambiguous seed visualizations and selectively injecting ambiguities, the pipeline yields multiple valid interpretations for each query, with each ambiguous query traceable to its corresponding visualization through step-wise reasoning paths. We evaluate various Large Language Models (LLMs) on their ability to perform ambiguous nl2vis tasks using nvBench 2.0. We also propose Step-nl2vis, an LLM-based model trained on nvBench 2.0, which enhances performance in ambiguous scenarios through step-wise preference optimization. Our results show that Step-nl2vis outperforms all baselines, setting a new state-of-the-art for ambiguous nl2vis tasks. PVLDB Artifact Availability:",
            "corpus_id": 277065796,
            "sentences": [
                {
                    "corpus_id": "277065796",
                    "title": "nvBench 2.0: A Benchmark for Natural Language to Visualization under Ambiguity",
                    "text": "Recently, the use of LLMs for data synthesis or data augmentation has become increasingly prevalent. Many studies leverage LLMgenerated data for training models [9,36,53,56,59], as well as for evaluating the performance of other trained models [29]. In the NLP domain, researchers have utilized LLMs to generate synthetic data for tasks like text classification [1,3,55]. These works showcase that LLM-generated data can enhance data diversity, thereby improving model generalization and robustness. Building on this, VL2NL [13] extends LLMs to nl2vis domain, generating NL descriptions (e.g., L1 and L2 captions, and user commands) from Vega-Lite specifications. Similarly, the application of LLMs for tabular data or database-related tasks has gained attraction. Common approaches for generating nl2sql or table question answering datasets often involve generating nl queries first, followed by sql generation [2,30]. ScienceBenchmark [58] takes a reverse approach by starting with seed sql queries, then generating new queries from the domain schema, and translating them into NL using fine-tuned LLMs. We follow this reverse construction philosophy in developing nvBench 2.0. Specifically, we begin by extracting vql from seed charts and then use LLMs to reverse engineer the corresponding NL descriptions. The advantage of this approach is that vql clearly defines each step and the ambiguity types involved, allowing us to better capture one-to-many (nl, vis) pairs. By leveraging LLMs to generate multi-step reasoning data, the performance of models on long-chain and complex reasoning tasks can be further improved. As demonstrated by Hunter et al. [18], process supervision via multi-step reasoning significantly enhances model reliability on tasks such as mathematical problem-solving. Similarly, Step-DPO [14] shows that generating step-wise reasoning data enables models to better capture intermediate steps, resulting in improved accuracy. Following this approach, we also generate multi-step reasoning data for tasks in the nl2vis domain, where each step of the reasoning process is explicitly defined, contributing to more accurate and interpretable model predictions.",
                    "score": 0.499595526384076,
                    "section_title": "LLMs Data Synthesis",
                    "char_start_offset": 48619,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 100
                        },
                        {
                            "start": 101,
                            "end": 249
                        },
                        {
                            "start": 250,
                            "end": 371
                        },
                        {
                            "start": 372,
                            "end": 499
                        },
                        {
                            "start": 500,
                            "end": 663
                        },
                        {
                            "start": 664,
                            "end": 919
                        },
                        {
                            "start": 920,
                            "end": 1105
                        },
                        {
                            "start": 1106,
                            "end": 1179
                        },
                        {
                            "start": 1180,
                            "end": 1310
                        },
                        {
                            "start": 1311,
                            "end": 1472
                        },
                        {
                            "start": 1473,
                            "end": 1623
                        },
                        {
                            "start": 1624,
                            "end": 1795
                        },
                        {
                            "start": 1796,
                            "end": 1952
                        },
                        {
                            "start": 1953,
                            "end": 2183
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 161,
                            "end": 164,
                            "matchedPaperCorpusId": "247519233"
                        },
                        {
                            "start": 244,
                            "end": 248,
                            "matchedPaperCorpusId": "248779886"
                        },
                        {
                            "start": 367,
                            "end": 370,
                            "matchedPaperCorpusId": "233296100"
                        },
                        {
                            "start": 524,
                            "end": 528,
                            "matchedPaperCorpusId": "262054595"
                        },
                        {
                            "start": 912,
                            "end": 915,
                            "matchedPaperCorpusId": "264405947"
                        },
                        {
                            "start": 937,
                            "end": 941,
                            "matchedPaperCorpusId": "259108708"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91552734375
                }
            ],
            "relevance_judgement": 0.91552734375,
            "relevance_judgment_input_expanded": "# Title: nvBench 2.0: A Benchmark for Natural Language to Visualization under Ambiguity\n# Venue: arXiv.org\n# Authors: Tianqi Luo, Chuhan Huang, Leixian Shen, Boyan Li, Shuyu Shen, Wei Zeng, Nan Tang, Yuyu Luo\n## Abstract\nNatural Language to Visualization (nl2vis) enables users to create visualizations from natural language queries, making data insights more accessible. However, nl2vis faces challenges in interpreting ambiguous queries, as users often express their visualization needs in imprecise language. To address this challenge, we introduce nvBench 2.0, a new benchmark designed to evaluate nl2vis systems in scenarios involving ambiguous queries. nvBench 2.0 includes 7,878 natural language queries and 24,076 corresponding visualizations, derived from 780 tables across 153 domains. It is built using a controlled ambiguity-injection pipeline that generates ambiguous queries through a reverse-generation workflow. By starting with unambiguous seed visualizations and selectively injecting ambiguities, the pipeline yields multiple valid interpretations for each query, with each ambiguous query traceable to its corresponding visualization through step-wise reasoning paths. We evaluate various Large Language Models (LLMs) on their ability to perform ambiguous nl2vis tasks using nvBench 2.0. We also propose Step-nl2vis, an LLM-based model trained on nvBench 2.0, which enhances performance in ambiguous scenarios through step-wise preference optimization. Our results show that Step-nl2vis outperforms all baselines, setting a new state-of-the-art for ambiguous nl2vis tasks. PVLDB Artifact Availability:\n## LLMs Data Synthesis\nRecently, the use of LLMs for data synthesis or data augmentation has become increasingly prevalent. Many studies leverage LLMgenerated data for training models [9,36,53,56,59], as well as for evaluating the performance of other trained models [29]. In the NLP domain, researchers have utilized LLMs to generate synthetic data for tasks like text classification [1,3,55]. These works showcase that LLM-generated data can enhance data diversity, thereby improving model generalization and robustness. Building on this, VL2NL [13] extends LLMs to nl2vis domain, generating NL descriptions (e.g., L1 and L2 captions, and user commands) from Vega-Lite specifications. Similarly, the application of LLMs for tabular data or database-related tasks has gained attraction. Common approaches for generating nl2sql or table question answering datasets often involve generating nl queries first, followed by sql generation [2,30]. ScienceBenchmark [58] takes a reverse approach by starting with seed sql queries, then generating new queries from the domain schema, and translating them into NL using fine-tuned LLMs. We follow this reverse construction philosophy in developing nvBench 2.0. Specifically, we begin by extracting vql from seed charts and then use LLMs to reverse engineer the corresponding NL descriptions. The advantage of this approach is that vql clearly defines each step and the ambiguity types involved, allowing us to better capture one-to-many (nl, vis) pairs. By leveraging LLMs to generate multi-step reasoning data, the performance of models on long-chain and complex reasoning tasks can be further improved. As demonstrated by Hunter et al. [18], process supervision via multi-step reasoning significantly enhances model reliability on tasks such as mathematical problem-solving. Similarly, Step-DPO [14] shows that generating step-wise reasoning data enables models to better capture intermediate steps, resulting in improved accuracy. Following this approach, we also generate multi-step reasoning data for tasks in the nl2vis domain, where each step of the reasoning process is explicitly defined, contributing to more accurate and interpretable model predictions.",
            "reference_string": "[277065796 | Luo et al. | 2025 | Citations: 3]"
        },
        {
            "title": "GraDA: Graph Generative Data Augmentation for Commonsense Reasoning",
            "venue": "International Conference on Computational Linguistics",
            "year": 2022,
            "reference_count": 85,
            "citation_count": 96,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.dlg4nlp-1.6.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2022.coling-1.397, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "8785371",
                    "name": "Adyasha Maharana"
                },
                {
                    "authorId": "143977268",
                    "name": "Mohit Bansal"
                }
            ],
            "abstract": "Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions. Our approach improves performance for SocialIQA, CODAH, HellaSwag and CommonsenseQA, and works well for generative tasks like ProtoQA. We show improvement in robustness to semantic adversaries after training with GraDA and provide human evaluation of the quality of synthetic datasets in terms of factuality and answerability. Our work provides evidence and encourages future research into graph-based generative data augmentation.",
            "corpus_id": 250390686,
            "sentences": [
                {
                    "corpus_id": "250390686",
                    "title": "GraDA: Graph Generative Data Augmentation for Commonsense Reasoning",
                    "text": "Recent work has seen the emergence of several datasets for improving commonsense reasoning of language models through tasks like question answering (QA) (Sap et al., 2019b;Talmor et al., 2019;Bisk et al., 2020) and natural language inference (Bhagavatula et al., 2020;Zellers et al., 2019;Sakaguchi et al., 2020). Some of these datasets are based on existing knowledge graphs that represent different aspects of commonsense through entities and relations. For example, annotators for SocialIQA (Sap et al., 2019b) were shown an event from the inferential knowledge graph ATOMIC (Sap et al., 2019a) and instructed to turn it into a sentence by adding names, filling placeholders and adding context, etc. For multiple-choice QA datasets, annotators are also instructed to write distractor choices for each question. These useful datasets are collected through a time-taking and money-intensive crowdsourcing process which is hard to scale. Large pretrained models like GPT2 (Radford et al., 2018) can be finetuned to generate sentences from narrow data distributions, and it has recently been leveraged to augment datasets for text classification (Anaby-Tavor et al., 2020) and question answering (Puri et al., 2020;Yang et al., 2020). However, it is challenging to generate augmentation data for commonsense reasoning because the generated questions and answers (referred to as \"synthetic\" in rest of the paper) need to depict plausible real-world scenarios accurately. Hence, we develop GRADA, a graph-based generative data augmentation framework to generate synthetic samples from existing knowledge graphs that encode information about the real world. We focus on generating synthetic samples for models that perform discriminative and generative commonsense question answering.\n\nEach sample in commonsense reasoning datasets comprises a question which describes a real-world scenario and can be mapped to a set of predefined entities and relations from knowledge bases like ConceptNet and ATOMIC. For instance, the question \"Besides a mattress, name something people sleep on.\" from the ProtoQA dataset (Boratko et al",
                    "score": 0.45400762385614113,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91162109375
                }
            ],
            "relevance_judgement": 0.91162109375,
            "relevance_judgment_input_expanded": "# Title: GraDA: Graph Generative Data Augmentation for Commonsense Reasoning\n# Venue: International Conference on Computational Linguistics\n# Authors: Adyasha Maharana, Mohit Bansal\n## Abstract\nRecent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions. Our approach improves performance for SocialIQA, CODAH, HellaSwag and CommonsenseQA, and works well for generative tasks like ProtoQA. We show improvement in robustness to semantic adversaries after training with GraDA and provide human evaluation of the quality of synthetic datasets in terms of factuality and answerability. Our work provides evidence and encourages future research into graph-based generative data augmentation.\n## Introduction\nRecent work has seen the emergence of several datasets for improving commonsense reasoning of language models through tasks like question answering (QA) (Sap et al., 2019b;Talmor et al., 2019;Bisk et al., 2020) and natural language inference (Bhagavatula et al., 2020;Zellers et al., 2019;Sakaguchi et al., 2020). Some of these datasets are based on existing knowledge graphs that represent different aspects of commonsense through entities and relations. For example, annotators for SocialIQA (Sap et al., 2019b) were shown an event from the inferential knowledge graph ATOMIC (Sap et al., 2019a) and instructed to turn it into a sentence by adding names, filling placeholders and adding context, etc. For multiple-choice QA datasets, annotators are also instructed to write distractor choices for each question. These useful datasets are collected through a time-taking and money-intensive crowdsourcing process which is hard to scale. Large pretrained models like GPT2 (Radford et al., 2018) can be finetuned to generate sentences from narrow data distributions, and it has recently been leveraged to augment datasets for text classification (Anaby-Tavor et al., 2020) and question answering (Puri et al., 2020;Yang et al., 2020). However, it is challenging to generate augmentation data for commonsense reasoning because the generated questions and answers (referred to as \"synthetic\" in rest of the paper) need to depict plausible real-world scenarios accurately. Hence, we develop GRADA, a graph-based generative data augmentation framework to generate synthetic samples from existing knowledge graphs that encode information about the real world. We focus on generating synthetic samples for models that perform discriminative and generative commonsense question answering.\n\nEach sample in commonsense reasoning datasets comprises a question which describes a real-world scenario and can be mapped to a set of predefined entities and relations from knowledge bases like ConceptNet and ATOMIC. For instance, the question \"Besides a mattress, name something people sleep on.\" from the ProtoQA dataset (Boratko et al",
            "reference_string": "[250390686 | Maharana et al. | 2022 | Citations: 96]"
        },
        {
            "title": "Synthetic Data Generation with Large Language Models for Personalized Community Question Answering",
            "venue": "2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.22182, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2227483106",
                    "name": "Marco Braga"
                },
                {
                    "authorId": "2187856026",
                    "name": "Pranav Kasela"
                },
                {
                    "authorId": "3106437",
                    "name": "Alessandro Raganato"
                },
                {
                    "authorId": "2243333911",
                    "name": "Gabriella Pasi"
                }
            ],
            "abstract": "Personalization in Information Retrieval (IR) is a topic studied by the research community since a long time. However, there is still a lack of datasets to conduct large-scale evaluations of personalized IR; this is mainly due to the fact that collecting and curating high-quality user-related information requires significant costs and time investment. Furthermore, the creation of datasets for Personalized IR (PIR) tasks is affected by both privacy concerns and the need for accurate user-related data, which are often not publicly available. Recently, researchers have started to explore the use of Large Language Models (LLMs) to generate synthetic datasets, which is a possible solution to generate data for low-resource tasks. In this paper, we investigate the potential of Large Language Models (LLMs) for generating synthetic documents to train an IR system for a Personalized Community Question Answering task. To study the effectiveness of IR models fine-tuned on LLM-generated data, we introduce a new dataset, named Sy-SE-PQA. We build Sy-SE-PQA based on an existing dataset, SE-PQA11https://zenodo.org/records/10679181, which consists of questions and answers posted on the popular StackExchange communities. Starting from questions in SE-PQA, we generate synthetic answers using different prompt techniques and LLMs. Our findings suggest that LLMs have high potential in generating data tailored to users' needs. The synthetic data can replace human-written training data, even if the generated data may contain incorrect information. The code is publicly available22https://github.com/pkase1a!SY_SE-PQA.",
            "corpus_id": 273662200,
            "sentences": [
                {
                    "corpus_id": "273662200",
                    "title": "Synthetic Data Generation with Large Language Models for Personalized Community Question Answering",
                    "text": "Recent advancements in Generative Artificial Intelligence have motivated researchers to explore the potential of generative models to create synthetic data for training machine learning models, especially in Computer Vision and NLP tasks. In the context of this paper, we focus on the generation of textual data. Researchers have explored the ability of LLMs to generate data for various text classification tasks, with mixed results regarding their effectiveness [30]- [38]. Puri et al. [39] explore question and answer generation as a data augmentation method to enhance QA models, addressing the challenge of limited human-labelled data. The authors point out that the synthetic data alone overcome the performance of the original training set in the Exact Match (EM) and F1 scores. Li et al. [40] investigate the effectiveness of synthetic data generated by LLMs in training text classification models across tasks with varying levels of subjectivity. They compare models trained on real-world data with those trained on LLMgenerated data under zero-shot and few-shot settings. Their findings reveal that while models trained on synthetic data perform comparably on low-subjectivity tasks, e.g. news and topic classification, there is a notable performance drop on high-subjectivity tasks, such as sarcasm detection. Regarding the generation of synthetic data for IR tasks, LLMs can be used to generate synthetic queries [16] or documents [17]. In [41] the authors propose to generate documents by few-shot prompting LLMs and concatenate them with the original query to form a new query. \n\nWe focus our work on personalization tasks, which aim to tailor a model's outcomes for a specific user (or group of users) based on the knowledge of her/his interests and behaviour. For example, as the use of LLMs in real-world applications evolves, recent work has highlighted the impact and concerns associated with personalizing LLMs [42]. Salemi et al. [12] introduce the LaMP benchmark, a synthetic dataset designed to evaluate the ability of LLMs to produce personalized outcome. LaMP encompasses seven distinct tasks, divided into three Text Classification tasks (Citation Identification, Movie Tagging and Product Rating) and four Text Generation tasks (News Headline, Scholarly Title, Email Subject and Tweet Paraphrasing).",
                    "score": 0.46186137106523967,
                    "section_title": "B. Synthetic Data Generation",
                    "char_start_offset": 6148,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 238
                        },
                        {
                            "start": 239,
                            "end": 312
                        },
                        {
                            "start": 313,
                            "end": 475
                        },
                        {
                            "start": 476,
                            "end": 640
                        },
                        {
                            "start": 641,
                            "end": 785
                        },
                        {
                            "start": 786,
                            "end": 955
                        },
                        {
                            "start": 956,
                            "end": 1081
                        },
                        {
                            "start": 1082,
                            "end": 1198
                        },
                        {
                            "start": 1199,
                            "end": 1320
                        },
                        {
                            "start": 1321,
                            "end": 1448
                        },
                        {
                            "start": 1449,
                            "end": 1591
                        },
                        {
                            "start": 1594,
                            "end": 1775
                        },
                        {
                            "start": 1776,
                            "end": 1936
                        },
                        {
                            "start": 1937,
                            "end": 2079
                        },
                        {
                            "start": 2080,
                            "end": 2326
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 470,
                            "end": 474,
                            "matchedPaperCorpusId": "259096160"
                        },
                        {
                            "start": 1425,
                            "end": 1429,
                            "matchedPaperCorpusId": "250340449"
                        },
                        {
                            "start": 1452,
                            "end": 1456,
                            "matchedPaperCorpusId": "257505063"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90771484375
                }
            ],
            "relevance_judgement": 0.90771484375,
            "relevance_judgment_input_expanded": "# Title: Synthetic Data Generation with Large Language Models for Personalized Community Question Answering\n# Venue: 2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)\n# Authors: Marco Braga, Pranav Kasela, Alessandro Raganato, Gabriella Pasi\n## Abstract\nPersonalization in Information Retrieval (IR) is a topic studied by the research community since a long time. However, there is still a lack of datasets to conduct large-scale evaluations of personalized IR; this is mainly due to the fact that collecting and curating high-quality user-related information requires significant costs and time investment. Furthermore, the creation of datasets for Personalized IR (PIR) tasks is affected by both privacy concerns and the need for accurate user-related data, which are often not publicly available. Recently, researchers have started to explore the use of Large Language Models (LLMs) to generate synthetic datasets, which is a possible solution to generate data for low-resource tasks. In this paper, we investigate the potential of Large Language Models (LLMs) for generating synthetic documents to train an IR system for a Personalized Community Question Answering task. To study the effectiveness of IR models fine-tuned on LLM-generated data, we introduce a new dataset, named Sy-SE-PQA. We build Sy-SE-PQA based on an existing dataset, SE-PQA11https://zenodo.org/records/10679181, which consists of questions and answers posted on the popular StackExchange communities. Starting from questions in SE-PQA, we generate synthetic answers using different prompt techniques and LLMs. Our findings suggest that LLMs have high potential in generating data tailored to users' needs. The synthetic data can replace human-written training data, even if the generated data may contain incorrect information. The code is publicly available22https://github.com/pkase1a!SY_SE-PQA.\n## B. Synthetic Data Generation\nRecent advancements in Generative Artificial Intelligence have motivated researchers to explore the potential of generative models to create synthetic data for training machine learning models, especially in Computer Vision and NLP tasks. In the context of this paper, we focus on the generation of textual data. Researchers have explored the ability of LLMs to generate data for various text classification tasks, with mixed results regarding their effectiveness [30]- [38]. Puri et al. [39] explore question and answer generation as a data augmentation method to enhance QA models, addressing the challenge of limited human-labelled data. The authors point out that the synthetic data alone overcome the performance of the original training set in the Exact Match (EM) and F1 scores. Li et al. [40] investigate the effectiveness of synthetic data generated by LLMs in training text classification models across tasks with varying levels of subjectivity. They compare models trained on real-world data with those trained on LLMgenerated data under zero-shot and few-shot settings. Their findings reveal that while models trained on synthetic data perform comparably on low-subjectivity tasks, e.g. news and topic classification, there is a notable performance drop on high-subjectivity tasks, such as sarcasm detection. Regarding the generation of synthetic data for IR tasks, LLMs can be used to generate synthetic queries [16] or documents [17]. In [41] the authors propose to generate documents by few-shot prompting LLMs and concatenate them with the original query to form a new query. \n\nWe focus our work on personalization tasks, which aim to tailor a model's outcomes for a specific user (or group of users) based on the knowledge of her/his interests and behaviour. For example, as the use of LLMs in real-world applications evolves, recent work has highlighted the impact and concerns associated with personalizing LLMs [42]. Salemi et al. [12] introduce the LaMP benchmark, a synthetic dataset designed to evaluate the ability of LLMs to produce personalized outcome. LaMP encompasses seven distinct tasks, divided into three Text Classification tasks (Citation Identification, Movie Tagging and Product Rating) and four Text Generation tasks (News Headline, Scholarly Title, Email Subject and Tweet Paraphrasing).",
            "reference_string": "[273662200 | Braga et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 50,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2211.02971",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.02971, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2140791766",
                    "name": "Mobashir Sadat"
                },
                {
                    "authorId": "2140493460",
                    "name": "Cornelia Caragea"
                }
            ],
            "abstract": "Natural Language Inference (NLI) or Recognizing Textual Entailment (RTE) aims at predicting the relation between a pair of sentences (premise and hypothesis) as entailment, contradiction or semantic independence. Although deep learning models have shown promising performance for NLI in recent years, they rely on large scale expensive human-annotated datasets. Semi-supervised learning (SSL) is a popular technique for reducing the reliance on human annotation by leveraging unlabeled data for training. However, despite its substantial success on single sentence classification tasks where the challenge in making use of unlabeled data is to assign\"good enough\"pseudo-labels, for NLI tasks, the nature of unlabeled data is more complex: one of the sentences in the pair (usually the hypothesis) along with the class label are missing from the data and require human annotations, which makes SSL for NLI more challenging. In this paper, we propose a novel way to incorporate unlabeled data in SSL for NLI where we use a conditional language model, BART to generate the hypotheses for the unlabeled sentences (used as premises). Our experiments show that our SSL framework successfully exploits unlabeled data and substantially improves the performance of four NLI datasets in low-resource settings. We release our code at: https://github.com/msadat3/SSL_for_NLI.",
            "corpus_id": 253384598,
            "sentences": [
                {
                    "corpus_id": "253384598",
                    "title": "Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference",
                    "text": "With the advancement of text generation models in recent years (Radford et al., 2019;Brown et al., 2020;Lewis et al., 2020), researchers have started to adopt synthetic text generation as a data augmentation strategy for various tasks such as commonsense reasoning (Yang et al., 2020), sentence classification (Anaby-Tavor et al., 2020) and question answering (Puri et al., 2020). More recently, synthetic text generation methods have also been employed to remove spurious correlations in human annotated data for fact verification (Lee et al., 2021) and NLI (Wu et al., 2022). However, to our knowledge, no prior work aims at tackling low-resource NLI scenarios nor do they address the unavailability of hypotheses for unlabeled premises by using synthetic data generation methods.",
                    "score": 0.5585385263552101,
                    "section_title": "Synthetic Data Generation",
                    "char_start_offset": 28074,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 63,
                            "end": 85,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 104,
                            "end": 123,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 265,
                            "end": 284,
                            "matchedPaperCorpusId": "250390686"
                        },
                        {
                            "start": 360,
                            "end": 379,
                            "matchedPaperCorpusId": "211258652"
                        },
                        {
                            "start": 532,
                            "end": 550,
                            "matchedPaperCorpusId": "238227205"
                        },
                        {
                            "start": 559,
                            "end": 576,
                            "matchedPaperCorpusId": "247628095"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90380859375
                }
            ],
            "relevance_judgement": 0.90380859375,
            "relevance_judgment_input_expanded": "# Title: Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Mobashir Sadat, Cornelia Caragea\n## Abstract\nNatural Language Inference (NLI) or Recognizing Textual Entailment (RTE) aims at predicting the relation between a pair of sentences (premise and hypothesis) as entailment, contradiction or semantic independence. Although deep learning models have shown promising performance for NLI in recent years, they rely on large scale expensive human-annotated datasets. Semi-supervised learning (SSL) is a popular technique for reducing the reliance on human annotation by leveraging unlabeled data for training. However, despite its substantial success on single sentence classification tasks where the challenge in making use of unlabeled data is to assign\"good enough\"pseudo-labels, for NLI tasks, the nature of unlabeled data is more complex: one of the sentences in the pair (usually the hypothesis) along with the class label are missing from the data and require human annotations, which makes SSL for NLI more challenging. In this paper, we propose a novel way to incorporate unlabeled data in SSL for NLI where we use a conditional language model, BART to generate the hypotheses for the unlabeled sentences (used as premises). Our experiments show that our SSL framework successfully exploits unlabeled data and substantially improves the performance of four NLI datasets in low-resource settings. We release our code at: https://github.com/msadat3/SSL_for_NLI.\n## Synthetic Data Generation\nWith the advancement of text generation models in recent years (Radford et al., 2019;Brown et al., 2020;Lewis et al., 2020), researchers have started to adopt synthetic text generation as a data augmentation strategy for various tasks such as commonsense reasoning (Yang et al., 2020), sentence classification (Anaby-Tavor et al., 2020) and question answering (Puri et al., 2020). More recently, synthetic text generation methods have also been employed to remove spurious correlations in human annotated data for fact verification (Lee et al., 2021) and NLI (Wu et al., 2022). However, to our knowledge, no prior work aims at tackling low-resource NLI scenarios nor do they address the unavailability of hypotheses for unlabeled premises by using synthetic data generation methods.",
            "reference_string": "[253384598 | Sadat et al. | 2022 | Citations: 2]"
        },
        {
            "title": "VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 43,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.06737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323820473",
                    "name": "Thomas Zeng"
                },
                {
                    "authorId": "2190935184",
                    "name": "Shuibai Zhang"
                },
                {
                    "authorId": "2344794414",
                    "name": "Shutong Wu"
                },
                {
                    "authorId": "2344760610",
                    "name": "Christian Classen"
                },
                {
                    "authorId": "2253659910",
                    "name": "Daewon Chae"
                },
                {
                    "authorId": "2323781863",
                    "name": "Ethan Ewer"
                },
                {
                    "authorId": "2345324748",
                    "name": "Minjae Lee"
                },
                {
                    "authorId": "2344831673",
                    "name": "Heeju Kim"
                },
                {
                    "authorId": "2282472476",
                    "name": "Wonjun Kang"
                },
                {
                    "authorId": "2334358539",
                    "name": "Jackson Kunde"
                },
                {
                    "authorId": "2344807639",
                    "name": "Ying Fan"
                },
                {
                    "authorId": "2344788904",
                    "name": "Jungtaek Kim"
                },
                {
                    "authorId": "2282471620",
                    "name": "Hyung Il Koo"
                },
                {
                    "authorId": "144161012",
                    "name": "K. Ramchandran"
                },
                {
                    "authorId": "107579958",
                    "name": "Dimitris Papailiopoulos"
                },
                {
                    "authorId": "2323790154",
                    "name": "Kangwook Lee"
                }
            ],
            "abstract": "Process Reward Models (PRMs) have proven effective at enhancing mathematical reasoning for Large Language Models (LLMs) by leveraging increased inference-time computation. However, they are predominantly trained on mathematical data and their generalizability to non-mathematical domains has not been rigorously studied. In response, this work first shows that current PRMs have poor performance in other domains. To address this limitation, we introduce VersaPRM, a multi-domain PRM trained on synthetic reasoning data generated using our novel data generation and annotation method. VersaPRM achieves consistent performance gains across diverse domains. For instance, in the MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a 7.9% performance gain over the majority voting baseline -- surpassing Qwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community by open-sourcing all data, code and models for VersaPRM.",
            "corpus_id": 276250232,
            "sentences": [
                {
                    "corpus_id": "276250232",
                    "title": "VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data",
                    "text": "Reasoning Data with Labels \n\nIn order to obtain step-wise reasoning data for non-Math domains, we devise a pipeline, as outlined in Figure 2, to generate synthetic reasoning CoTs from existing questionanswering data. These CoTs are then given step-wise labels based on reasoning correctness. We detail the synthetic data generation process in Sections 5.1 and 5.2, including methods to create and annotate reasoning steps. We also provide additional analysis on the quality of the generation pipeline in Section 5.3.",
                    "score": 0.46644915669159726,
                    "section_title": "Automatic Generation of Multi-Domain",
                    "char_start_offset": 14358,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 26
                        },
                        {
                            "start": 29,
                            "end": 216
                        },
                        {
                            "start": 217,
                            "end": 291
                        },
                        {
                            "start": 292,
                            "end": 422
                        },
                        {
                            "start": 423,
                            "end": 516
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90185546875
                }
            ],
            "relevance_judgement": 0.90185546875,
            "relevance_judgment_input_expanded": "# Title: VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data\n# Venue: arXiv.org\n# Authors: Thomas Zeng, Shuibai Zhang, Shutong Wu, Christian Classen, Daewon Chae, Ethan Ewer, Minjae Lee, Heeju Kim, Wonjun Kang, Jackson Kunde, Ying Fan, Jungtaek Kim, Hyung Il Koo, K. Ramchandran, Dimitris Papailiopoulos, Kangwook Lee\n## Abstract\nProcess Reward Models (PRMs) have proven effective at enhancing mathematical reasoning for Large Language Models (LLMs) by leveraging increased inference-time computation. However, they are predominantly trained on mathematical data and their generalizability to non-mathematical domains has not been rigorously studied. In response, this work first shows that current PRMs have poor performance in other domains. To address this limitation, we introduce VersaPRM, a multi-domain PRM trained on synthetic reasoning data generated using our novel data generation and annotation method. VersaPRM achieves consistent performance gains across diverse domains. For instance, in the MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a 7.9% performance gain over the majority voting baseline -- surpassing Qwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community by open-sourcing all data, code and models for VersaPRM.\n## Automatic Generation of Multi-Domain\nReasoning Data with Labels \n\nIn order to obtain step-wise reasoning data for non-Math domains, we devise a pipeline, as outlined in Figure 2, to generate synthetic reasoning CoTs from existing questionanswering data. These CoTs are then given step-wise labels based on reasoning correctness. We detail the synthetic data generation process in Sections 5.1 and 5.2, including methods to create and annotate reasoning steps. We also provide additional analysis on the quality of the generation pipeline in Section 5.3.",
            "reference_string": "[276250232 | Zeng et al. | 2025 | Citations: 4]"
        },
        {
            "title": "PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering",
            "venue": "",
            "year": 2025,
            "reference_count": 41,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.09414, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2366433044",
                    "name": "Xiujun Zhou"
                },
                {
                    "authorId": "2197970055",
                    "name": "Pingjian Zhang"
                },
                {
                    "authorId": "2366668933",
                    "name": "Deyou Tang"
                }
            ],
            "abstract": "Knowledge Graph Question Answering (KGQA) is a crucial task in natural language processing that requires reasoning over knowledge graphs (KGs) to answer natural language questions. Recent methods utilizing large language models (LLMs) have shown remarkable semantic parsing capabilities but are limited by the scarcity of diverse annotated data and multi-hop reasoning samples. Traditional data augmentation approaches are focus mainly on single-hop questions and prone to semantic distortion, while LLM-based methods primarily address semantic distortion but usually neglect multi-hop reasoning, thus limiting data diversity. The scarcity of multi-hop samples further weakens models' generalization. To address these issues, we propose PGDA-KGQA, a prompt-guided generative framework with multiple data augmentation strategies for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by crafting meticulously engineered prompts that integrate the provided textual content, it leverages LLMs to generate large-scale (question, logical form) pairs for model training. Specifically, PGDA-KGQA enriches its training set by: (1) generating single-hop pseudo questions to improve the alignment of question semantics with KG relations; (2) applying semantic-preserving question rewriting to improve robustness against linguistic variations; (3) employing answer-guided reverse path exploration to create realistic multi-hop questions. By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA utilizes the augmented data to enhance the accuracy of logical form generation and thus improve answer retrieval performance. Experiments demonstrate that outperforms state-of-the-art methods on standard KGQA datasets, achieving improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by 1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.",
            "corpus_id": 279305725,
            "sentences": [],
            "relevance_judgement": 0.89501953125,
            "relevance_judgment_input_expanded": "# Title: PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering\n# Venue: \n# Authors: Xiujun Zhou, Pingjian Zhang, Deyou Tang\n## Abstract\nKnowledge Graph Question Answering (KGQA) is a crucial task in natural language processing that requires reasoning over knowledge graphs (KGs) to answer natural language questions. Recent methods utilizing large language models (LLMs) have shown remarkable semantic parsing capabilities but are limited by the scarcity of diverse annotated data and multi-hop reasoning samples. Traditional data augmentation approaches are focus mainly on single-hop questions and prone to semantic distortion, while LLM-based methods primarily address semantic distortion but usually neglect multi-hop reasoning, thus limiting data diversity. The scarcity of multi-hop samples further weakens models' generalization. To address these issues, we propose PGDA-KGQA, a prompt-guided generative framework with multiple data augmentation strategies for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by crafting meticulously engineered prompts that integrate the provided textual content, it leverages LLMs to generate large-scale (question, logical form) pairs for model training. Specifically, PGDA-KGQA enriches its training set by: (1) generating single-hop pseudo questions to improve the alignment of question semantics with KG relations; (2) applying semantic-preserving question rewriting to improve robustness against linguistic variations; (3) employing answer-guided reverse path exploration to create realistic multi-hop questions. By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA utilizes the augmented data to enhance the accuracy of logical form generation and thus improve answer retrieval performance. Experiments demonstrate that outperforms state-of-the-art methods on standard KGQA datasets, achieving improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by 1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.\n",
            "reference_string": "[279305725 | Zhou et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 70,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00552/2087854/tacl_a_00552.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.tacl-1.22, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "10680347",
                    "name": "Yilin Niu"
                },
                {
                    "authorId": "152159016",
                    "name": "Fei Huang"
                },
                {
                    "authorId": "46641573",
                    "name": "W. Liu"
                },
                {
                    "authorId": "3061725",
                    "name": "Jianwei Cui"
                },
                {
                    "authorId": "2152593593",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "1730108",
                    "name": "Minlie Huang"
                }
            ],
            "abstract": "Semantic parsing maps natural language questions into logical forms, which can be executed against a knowledge base for answers. In real-world applications, the performance of a parser is often limited by the lack of training data. To facilitate zero-shot learning, data synthesis has been widely studied to automatically generate paired questions and logical forms. However, data synthesis methods can hardly cover the diverse structures in natural languages, leading to a large gap in sentence structure between synthetic and natural questions. In this paper, we propose a decomposition-based method to unify the sentence structures of questions, which benefits the generalization to natural questions. Experiments demonstrate that our method significantly improves the semantic parser trained on synthetic data (+7.9% on KQA and +8.9% on ComplexWebQuestions in terms of exact match accuracy). Extensive analysis demonstrates that our method can better generalize to natural questions with novel text expressions compared with baselines. Besides semantic parsing, our idea potentially benefits other semantic understanding tasks by mitigating the distracting structure features. To illustrate this, we extend our method to the task of sentence embedding learning, and observe substantial improvements on sentence retrieval (+13.1% for Hit@1).",
            "corpus_id": 258558899,
            "sentences": [
                {
                    "corpus_id": "258558899",
                    "title": "Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing",
                    "text": "Data synthesis plays an important role in semantic parsing, especially in dataset construction and model training. Many datasets are constructed by first generating synthetic data by rules and then manually rewriting the questions for diverse expressions (Wang et al., 2015;Cao et al., 2022;Gu et al., 2021). Since the generation of synthetic data is controllable, researchers can restrict the distribution of logical forms in order to examine or improve various generalization capabilities of models, such as compositional generalization and scalability (Lake and Baroni, 2018;Gu et al., 2021;Oren et al., 2021). In many scenarios, where manually rewriting is laborious or unavailable, training on synthetic data can also alleviate the problems of cold-start and privacy (Yang et al., 2022b;Yu et al., 2021). Limited by stereotyped expressions, semantic parsers trained with only synthetic data are difficult to generalize to diverse natural questions. Wu et al. (2021a) and Xu et al. (2020c) automatically rewrite synthetic questions with paraphrasing models, which introduces various phrase-level expressions but change little in sentence structures. Instead of rule-based synthesis, Zhong et al. (2020) and Yang et al. (2021) train a neural data synthesizer with natural data from other domains, and then generate training data for the target domain in a way of domain transfer. Such methods heavily rely on multi-domain resources, and thus their applicability is limited. There are some other studies attempting to bridge the data gap through mapping synthetic and natural questions into a common embedding space. Berant and Liang (2014) and Marzoev et al. (2020) leverage sentence embeddings or other paraphrase detection technologies to measure the semantic similarity between the input question and candidate logical forms. However, existing sentence embedding methods are poor at accurately capturing the semantic meanings of complex sentences, making them unsuitable for some challenging semantic parsing tasks (e.g., KQA [Cao et al., 2022]).",
                    "score": 0.4582782095995556,
                    "section_title": "Data Synthesis for Semantic Parsing",
                    "char_start_offset": 4488,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 114
                        },
                        {
                            "start": 115,
                            "end": 308
                        },
                        {
                            "start": 309,
                            "end": 613
                        },
                        {
                            "start": 614,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 953
                        },
                        {
                            "start": 954,
                            "end": 1153
                        },
                        {
                            "start": 1154,
                            "end": 1382
                        },
                        {
                            "start": 1383,
                            "end": 1476
                        },
                        {
                            "start": 1477,
                            "end": 1618
                        },
                        {
                            "start": 1619,
                            "end": 1831
                        },
                        {
                            "start": 1832,
                            "end": 2052
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 255,
                            "end": 274,
                            "matchedPaperCorpusId": "14472576"
                        },
                        {
                            "start": 274,
                            "end": 291,
                            "matchedPaperCorpusId": "247362971"
                        },
                        {
                            "start": 291,
                            "end": 307,
                            "matchedPaperCorpusId": "226965153"
                        },
                        {
                            "start": 565,
                            "end": 578,
                            "matchedPaperCorpusId": "46761158"
                        },
                        {
                            "start": 578,
                            "end": 594,
                            "matchedPaperCorpusId": "226965153"
                        },
                        {
                            "start": 594,
                            "end": 612,
                            "matchedPaperCorpusId": "237420426"
                        },
                        {
                            "start": 772,
                            "end": 792,
                            "matchedPaperCorpusId": "248779994"
                        },
                        {
                            "start": 792,
                            "end": 808,
                            "matchedPaperCorpusId": "226965153"
                        },
                        {
                            "start": 954,
                            "end": 971,
                            "matchedPaperCorpusId": "235417352"
                        },
                        {
                            "start": 976,
                            "end": 993,
                            "matchedPaperCorpusId": "221819674"
                        },
                        {
                            "start": 1187,
                            "end": 1206,
                            "matchedPaperCorpusId": "221739233"
                        },
                        {
                            "start": 1619,
                            "end": 1642,
                            "matchedPaperCorpusId": "1336493"
                        },
                        {
                            "start": 2032,
                            "end": 2050,
                            "matchedPaperCorpusId": "247362971"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89404296875
                }
            ],
            "relevance_judgement": 0.89404296875,
            "relevance_judgment_input_expanded": "# Title: Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: Yilin Niu, Fei Huang, W. Liu, Jianwei Cui, Bin Wang, Minlie Huang\n## Abstract\nSemantic parsing maps natural language questions into logical forms, which can be executed against a knowledge base for answers. In real-world applications, the performance of a parser is often limited by the lack of training data. To facilitate zero-shot learning, data synthesis has been widely studied to automatically generate paired questions and logical forms. However, data synthesis methods can hardly cover the diverse structures in natural languages, leading to a large gap in sentence structure between synthetic and natural questions. In this paper, we propose a decomposition-based method to unify the sentence structures of questions, which benefits the generalization to natural questions. Experiments demonstrate that our method significantly improves the semantic parser trained on synthetic data (+7.9% on KQA and +8.9% on ComplexWebQuestions in terms of exact match accuracy). Extensive analysis demonstrates that our method can better generalize to natural questions with novel text expressions compared with baselines. Besides semantic parsing, our idea potentially benefits other semantic understanding tasks by mitigating the distracting structure features. To illustrate this, we extend our method to the task of sentence embedding learning, and observe substantial improvements on sentence retrieval (+13.1% for Hit@1).\n## Data Synthesis for Semantic Parsing\nData synthesis plays an important role in semantic parsing, especially in dataset construction and model training. Many datasets are constructed by first generating synthetic data by rules and then manually rewriting the questions for diverse expressions (Wang et al., 2015;Cao et al., 2022;Gu et al., 2021). Since the generation of synthetic data is controllable, researchers can restrict the distribution of logical forms in order to examine or improve various generalization capabilities of models, such as compositional generalization and scalability (Lake and Baroni, 2018;Gu et al., 2021;Oren et al., 2021). In many scenarios, where manually rewriting is laborious or unavailable, training on synthetic data can also alleviate the problems of cold-start and privacy (Yang et al., 2022b;Yu et al., 2021). Limited by stereotyped expressions, semantic parsers trained with only synthetic data are difficult to generalize to diverse natural questions. Wu et al. (2021a) and Xu et al. (2020c) automatically rewrite synthetic questions with paraphrasing models, which introduces various phrase-level expressions but change little in sentence structures. Instead of rule-based synthesis, Zhong et al. (2020) and Yang et al. (2021) train a neural data synthesizer with natural data from other domains, and then generate training data for the target domain in a way of domain transfer. Such methods heavily rely on multi-domain resources, and thus their applicability is limited. There are some other studies attempting to bridge the data gap through mapping synthetic and natural questions into a common embedding space. Berant and Liang (2014) and Marzoev et al. (2020) leverage sentence embeddings or other paraphrase detection technologies to measure the semantic similarity between the input question and candidate logical forms. However, existing sentence embedding methods are poor at accurately capturing the semantic meanings of complex sentences, making them unsuitable for some challenging semantic parsing tasks (e.g., KQA [Cao et al., 2022]).",
            "reference_string": "[258558899 | Niu et al. | 2023 | Citations: 6]"
        },
        {
            "title": "Best Practices and Lessons Learned on Synthetic Data for Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 190,
            "citation_count": 96,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.07503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "7247867",
                    "name": "Ruibo Liu"
                },
                {
                    "authorId": "2254934700",
                    "name": "Jerry Wei"
                },
                {
                    "authorId": "144097210",
                    "name": "Fangyu Liu"
                },
                {
                    "authorId": "2290019998",
                    "name": "Chenglei Si"
                },
                {
                    "authorId": "2121290295",
                    "name": "Yanzhe Zhang"
                },
                {
                    "authorId": "2238635148",
                    "name": "Jinmeng Rao"
                },
                {
                    "authorId": "2275574241",
                    "name": "Steven Zheng"
                },
                {
                    "authorId": "2283933546",
                    "name": "Daiyi Peng"
                },
                {
                    "authorId": "2254124342",
                    "name": "Diyi Yang"
                },
                {
                    "authorId": "2263221363",
                    "name": "Denny Zhou"
                },
                {
                    "authorId": "2253757717",
                    "name": "Andrew M. Dai"
                }
            ],
            "abstract": "The success of AI models relies on the availability of large, diverse, and high-quality datasets, which can be challenging to obtain due to data scarcity, privacy concerns, and high costs. Synthetic data has emerged as a promising solution by generating artificial data that mimics real-world patterns. This paper provides an overview of synthetic data research, discussing its applications, challenges, and future directions. We present empirical evidence from prior art to demonstrate its effectiveness and highlight the importance of ensuring its factuality, fidelity, and unbiasedness. We emphasize the need for responsible use of synthetic data to build more powerful, inclusive, and trustworthy language models.",
            "corpus_id": 269042851,
            "sentences": [
                {
                    "corpus_id": "269042851",
                    "title": "Best Practices and Lessons Learned on Synthetic Data for Language Models",
                    "text": "Math. Recent advancements in mathematical reasoning for language models (LMs) have led to the development of various approaches to improve performance on mathrelated tasks. One approach is to train on math-targeted pre-training data, such as Minerva (Lewkowycz et al., 2022), Llemma (Azerbayev et al., 2023), and DeepSeek-Math (Shao et al., 2024). Another mainstream method is to generate synthetic questions and answers to imitate the training or validation set of target benchmarks. For instance, WizardMath (Luo et al., 2023a) leverages a series of operations to increase the complexity of questions and answers using GPT-3.5, while MetaMath (Yu et al., 2023) bootstraps the questions in MATH and GSM8K by rewriting them in different ways, such as semantic rephrasing, self-verification, and backward reasoning. GAIR-Abel (Chern et al., 2023) found that the format of the augmented answers is crucial to final performance, with answers that begin with a paraphrasing of the question followed by a step-by-step solution showing better performance than those in vanilla format. Xwin-Math (Li et al., 2024) further scaled up synthetic SFT data to one million examples and found that the LLaMA-2 7B model (Touvron et al., 2023) can still benefit from data scaling. MMIQC (Liu & Yao, 2024) composed a bundle of datasets that infuse SFT style data (via question-answer rephrasing or directly taken from MetaMath) with a subset of high-quality mathematical pre-training data, such as OpenWebMath (Paster et al., 2023). \n\nOther reasoning tasks. Synthetic data also leads to impressive performance in other reasoning tasks. For instance, Wei et al. (2023a) augmented existing natural language datasets by replacing natural language labels with arbitrary symbols, generating over 500k synthetic examples. Using these synthetic data for supervised finetuning significantly improved model performance on unseen in-context learning and algorithmic-reasoning tasks.",
                    "score": 0.4821610507811901,
                    "section_title": "Reasoning",
                    "char_start_offset": 4189,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 5
                        },
                        {
                            "start": 6,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 484
                        },
                        {
                            "start": 485,
                            "end": 814
                        },
                        {
                            "start": 815,
                            "end": 1078
                        },
                        {
                            "start": 1079,
                            "end": 1263
                        },
                        {
                            "start": 1264,
                            "end": 1514
                        },
                        {
                            "start": 1517,
                            "end": 1539
                        },
                        {
                            "start": 1540,
                            "end": 1617
                        },
                        {
                            "start": 1618,
                            "end": 1797
                        },
                        {
                            "start": 1798,
                            "end": 1954
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89013671875
                }
            ],
            "relevance_judgement": 0.89013671875,
            "relevance_judgment_input_expanded": "# Title: Best Practices and Lessons Learned on Synthetic Data for Language Models\n# Venue: arXiv.org\n# Authors: Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, Andrew M. Dai\n## Abstract\nThe success of AI models relies on the availability of large, diverse, and high-quality datasets, which can be challenging to obtain due to data scarcity, privacy concerns, and high costs. Synthetic data has emerged as a promising solution by generating artificial data that mimics real-world patterns. This paper provides an overview of synthetic data research, discussing its applications, challenges, and future directions. We present empirical evidence from prior art to demonstrate its effectiveness and highlight the importance of ensuring its factuality, fidelity, and unbiasedness. We emphasize the need for responsible use of synthetic data to build more powerful, inclusive, and trustworthy language models.\n## Reasoning\nMath. Recent advancements in mathematical reasoning for language models (LMs) have led to the development of various approaches to improve performance on mathrelated tasks. One approach is to train on math-targeted pre-training data, such as Minerva (Lewkowycz et al., 2022), Llemma (Azerbayev et al., 2023), and DeepSeek-Math (Shao et al., 2024). Another mainstream method is to generate synthetic questions and answers to imitate the training or validation set of target benchmarks. For instance, WizardMath (Luo et al., 2023a) leverages a series of operations to increase the complexity of questions and answers using GPT-3.5, while MetaMath (Yu et al., 2023) bootstraps the questions in MATH and GSM8K by rewriting them in different ways, such as semantic rephrasing, self-verification, and backward reasoning. GAIR-Abel (Chern et al., 2023) found that the format of the augmented answers is crucial to final performance, with answers that begin with a paraphrasing of the question followed by a step-by-step solution showing better performance than those in vanilla format. Xwin-Math (Li et al., 2024) further scaled up synthetic SFT data to one million examples and found that the LLaMA-2 7B model (Touvron et al., 2023) can still benefit from data scaling. MMIQC (Liu & Yao, 2024) composed a bundle of datasets that infuse SFT style data (via question-answer rephrasing or directly taken from MetaMath) with a subset of high-quality mathematical pre-training data, such as OpenWebMath (Paster et al., 2023). \n\nOther reasoning tasks. Synthetic data also leads to impressive performance in other reasoning tasks. For instance, Wei et al. (2023a) augmented existing natural language datasets by replacing natural language labels with arbitrary symbols, generating over 500k synthetic examples. Using these synthetic data for supervised finetuning significantly improved model performance on unseen in-context learning and algorithmic-reasoning tasks.",
            "reference_string": "[269042851 | Liu et al. | 2024 | Citations: 96]"
        },
        {
            "title": "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 38,
            "citation_count": 47,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.02317",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.02317, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2215927057",
                    "name": "Daniel Philip Rose"
                },
                {
                    "authorId": "2215926792",
                    "name": "Vaishnavi Himakunthala"
                },
                {
                    "authorId": "2215914015",
                    "name": "Andy Ouyang"
                },
                {
                    "authorId": "2215915076",
                    "name": "Ryan He"
                },
                {
                    "authorId": "2185480449",
                    "name": "Alex Mei"
                },
                {
                    "authorId": "47006228",
                    "name": "Yujie Lu"
                },
                {
                    "authorId": "48227633",
                    "name": "Michael Stephen Saxon"
                },
                {
                    "authorId": "26961225",
                    "name": "Chinmay Sonar"
                },
                {
                    "authorId": "1705929",
                    "name": "Diba Mirza"
                },
                {
                    "authorId": "1682479",
                    "name": "William Yang Wang"
                }
            ],
            "abstract": "Recent advances in large language models elicit reasoning in a chain-of-thought that allows models to decompose problems in a human-like fashion. Though this paradigm improves multi-step reasoning ability in language models, it is limited by being unimodal and applied mainly to question-answering tasks. We claim that incorporating visual augmentation into reasoning is essential, especially for complex, imaginative tasks. Consequently, we introduce VCoT, a novel method that leverages chain-of-thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data. Our method uses visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning, as well as provide interpretability into models' multi-step reasoning. We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evaluation that VCoT offers novel and consistent synthetic data augmentation beating chain-of-thought baselines, which can be used to enhance downstream performance.",
            "corpus_id": 258461502,
            "sentences": [
                {
                    "corpus_id": "258461502",
                    "title": "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings",
                    "text": "Recent advances in large language models elicit reasoning in a chain-of-thought that allows models to decompose problems in a human-like fashion. Though this paradigm improves multi-step reasoning ability in language models, it is limited by being unimodal and applied mainly to question-answering tasks. We claim that incorporating visual augmentation into reasoning is essential, especially for complex, imaginative tasks. Consequently, we introduce VCoT, a novel method that leverages chain-of-thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data. Our method uses visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning, as well as provide interpretability into models' multi-step reasoning. We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evaluation that VCoT offers novel and consistent synthetic data augmentation beating chain-of-thought baselines, which can be used to enhance downstream performance.",
                    "score": 0.5013432754647728,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8818359375
                }
            ],
            "relevance_judgement": 0.8818359375,
            "relevance_judgment_input_expanded": "# Title: Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings\n# Venue: arXiv.org\n# Authors: Daniel Philip Rose, Vaishnavi Himakunthala, Andy Ouyang, Ryan He, Alex Mei, Yujie Lu, Michael Stephen Saxon, Chinmay Sonar, Diba Mirza, William Yang Wang\n## Abstract\nRecent advances in large language models elicit reasoning in a chain-of-thought that allows models to decompose problems in a human-like fashion. Though this paradigm improves multi-step reasoning ability in language models, it is limited by being unimodal and applied mainly to question-answering tasks. We claim that incorporating visual augmentation into reasoning is essential, especially for complex, imaginative tasks. Consequently, we introduce VCoT, a novel method that leverages chain-of-thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data. Our method uses visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning, as well as provide interpretability into models' multi-step reasoning. We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evaluation that VCoT offers novel and consistent synthetic data augmentation beating chain-of-thought baselines, which can be used to enhance downstream performance.\n",
            "reference_string": "[258461502 | Rose et al. | 2023 | Citations: 47]"
        },
        {
            "title": "Optimization Techniques for Unsupervised Complex Table Reasoning via Self-Training Framework",
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "year": 2022,
            "reference_count": 67,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2212.10097",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.10097, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2155354022",
                    "name": "Zhenyu Li"
                },
                {
                    "authorId": "2116521868",
                    "name": "Xiuxing Li"
                },
                {
                    "authorId": "2272781120",
                    "name": "Sunqi Fan"
                },
                {
                    "authorId": "2115642141",
                    "name": "Jianyong Wang"
                }
            ],
            "abstract": "Structured tabular data is a fundamental data type in numerous fields, and the capacity to reason over tables is crucial for answering questions and validating hypotheses. However, constructing labeled data for complex reasoning tasks is labor-intensive, and the quantity of annotated data remains insufficient to support the intricate demands of real-world applications. To address the insufficient annotation challenge, we present a self-training framework for unsupervised complex tabular reasoning (UCTR-ST) by generating diverse synthetic data with complex logic. Specifically, UCTR-ST incorporates several essential techniques: we aggregate diverse programs and execute them on tables based on a \u201cProgram-Management\u201d component, and we bridge the gap between programs and text with a powerful \u201cProgram-Transformation\u201d module that generates natural language sentences with complex logic. Furthermore, we optimize the procedure using \u201cTable-Text Manipulator\u201d to handle joint table-text reasoning scenarios. The entire framework utilizes self-training techniques to leverage the unlabeled training data, which results in significant performance improvements when tested on real-world data. Experimental results demonstrate that UCTR-ST achieves above 90% of the supervised model performance on different tasks and domains, reducing the dependence on manual annotation. Additionally, our approach can serve as a data augmentation technique, significantly boosting the performance of supervised models in low-resourced domains.",
            "corpus_id": 254877131,
            "sentences": [
                {
                    "corpus_id": "254877131",
                    "title": "Optimization Techniques for Unsupervised Complex Table Reasoning via Self-Training Framework",
                    "text": "Unsupervised data generation has been extensively studied on various tasks like question answering and natural language inference, and has shown surprising performances [51], [53] [54]. Recently, methods for synthesizing human-like tabular reasoning samples have also been proposed [52], [56]. Chemmengath et al. [46] sample complex SQL queries and generate natural language questions in a seq2seq manner. Eisenschlos et al. [29] generate factual claims leveraging context-free grammar (CFG) and counterfactual heuristics. Unfortunately, these methods focus on a specific task or scenario. Based on the modules and predefined operators, our approach can convert different types of programs into natural language questions or claims with tabular evidence or a hybrid of tabular and textual evidence. \n\nSelf-training has been widely explored in the realm of semi-supervised learning [60], [63], [65]. For example, Li et al. [66] proposed FlexKBQA, a method that combines selftraining and synthetic data to improve the performance of few-shot knowledge based question answering. Most works employ self-training techniques in the few-shot setting, where a small number of labeled samples are available. However, this study effectively combines data generation methods with selftraining to achieve good results in an unsupervised scenario.",
                    "score": 0.5937405089648865,
                    "section_title": "B. Unsupervised Data Generation and Self-Training.",
                    "char_start_offset": 45826,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 293
                        },
                        {
                            "start": 294,
                            "end": 405
                        },
                        {
                            "start": 406,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 589
                        },
                        {
                            "start": 590,
                            "end": 798
                        },
                        {
                            "start": 801,
                            "end": 898
                        },
                        {
                            "start": 899,
                            "end": 1075
                        },
                        {
                            "start": 1076,
                            "end": 1198
                        },
                        {
                            "start": 1199,
                            "end": 1334
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 881,
                            "end": 885,
                            "matchedPaperCorpusId": "250451588"
                        },
                        {
                            "start": 887,
                            "end": 891,
                            "matchedPaperCorpusId": "7890982"
                        },
                        {
                            "start": 893,
                            "end": 897,
                            "matchedPaperCorpusId": "18507866"
                        },
                        {
                            "start": 922,
                            "end": 926,
                            "matchedPaperCorpusId": "261076103"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88037109375
                },
                {
                    "corpus_id": "254877131",
                    "title": "Optimization Techniques for Unsupervised Complex Table Reasoning via Self-Training Framework",
                    "text": "(2) Methods based on synthesizing human-like data through heuristics or data-to-text models. Eisenschlos et al. [29] generate claims using context-free grammar (CFG) templates and counterfactual heuristics. Recently, Pan et al. [30] propose an unsupervised learning framework named MQA-QG. MQA-QG generates multi-hop questions for both the tabular and textual data, which is the most relevant work in this tendency. \n\nNevertheless, several critical issues remain unsolved in the realm of unsupervised complex tabular reasoning: (1) Existing methods for data generation mainly use heuristics or shallow data-to-text methods (e.g., converting a row to a sentence). Thus, they can merely generate relatively simple instances shown in Figure 2, limiting the model's effectiveness on complex reasoning samples, which require a deep understanding of the semantics and logic relationships between multiple table cells. (2) Previous works only focus on a single scenario but cannot be expanded to other tabular reasoning tasks. This is because they design heuristics based on specific data characteristics or the form of the task, and these methods cannot be transferred to other tasks flexibly. Therefore, these models struggle to handle the complex and diverse scenarios encountered in the real world. \n\nTo address these issues, we introduce UCTR-ST (Unsupervised Complex Tabular Reasoning using Self-Training), an advanced self-training framework designed explicitly for unsupervised complex tabular reasoning. More specifically, UCTR-ST primarily leverages a random sampling strategy to collect different types of programs. These programs consist of sequences of symbols that can be executed on tables, including SQL queries, logical forms, and arithmetic expressions, encompassing a wide range of reasoning types. Subsequently, we design a \"Program-Management\" module that generates program-answer pairs by leveraging numerous tables within the domain. To bridge the gap between the programs and natural language sentences, we develop a powerful \"Program-Transformation\" module based on generative language models that turns the programs into human-like natural questions or claims with complex logic.",
                    "score": 0.46369215152959614,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 4256,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 92
                        },
                        {
                            "start": 93,
                            "end": 206
                        },
                        {
                            "start": 207,
                            "end": 289
                        },
                        {
                            "start": 290,
                            "end": 415
                        },
                        {
                            "start": 418,
                            "end": 662
                        },
                        {
                            "start": 663,
                            "end": 911
                        },
                        {
                            "start": 912,
                            "end": 1019
                        },
                        {
                            "start": 1020,
                            "end": 1187
                        },
                        {
                            "start": 1188,
                            "end": 1295
                        },
                        {
                            "start": 1298,
                            "end": 1505
                        },
                        {
                            "start": 1506,
                            "end": 1619
                        },
                        {
                            "start": 1620,
                            "end": 1810
                        },
                        {
                            "start": 1811,
                            "end": 1949
                        },
                        {
                            "start": 1950,
                            "end": 2198
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80908203125
                }
            ],
            "relevance_judgement": 0.88037109375,
            "relevance_judgment_input_expanded": "# Title: Optimization Techniques for Unsupervised Complex Table Reasoning via Self-Training Framework\n# Venue: IEEE Transactions on Knowledge and Data Engineering\n# Authors: Zhenyu Li, Xiuxing Li, Sunqi Fan, Jianyong Wang\n## Abstract\nStructured tabular data is a fundamental data type in numerous fields, and the capacity to reason over tables is crucial for answering questions and validating hypotheses. However, constructing labeled data for complex reasoning tasks is labor-intensive, and the quantity of annotated data remains insufficient to support the intricate demands of real-world applications. To address the insufficient annotation challenge, we present a self-training framework for unsupervised complex tabular reasoning (UCTR-ST) by generating diverse synthetic data with complex logic. Specifically, UCTR-ST incorporates several essential techniques: we aggregate diverse programs and execute them on tables based on a \u201cProgram-Management\u201d component, and we bridge the gap between programs and text with a powerful \u201cProgram-Transformation\u201d module that generates natural language sentences with complex logic. Furthermore, we optimize the procedure using \u201cTable-Text Manipulator\u201d to handle joint table-text reasoning scenarios. The entire framework utilizes self-training techniques to leverage the unlabeled training data, which results in significant performance improvements when tested on real-world data. Experimental results demonstrate that UCTR-ST achieves above 90% of the supervised model performance on different tasks and domains, reducing the dependence on manual annotation. Additionally, our approach can serve as a data augmentation technique, significantly boosting the performance of supervised models in low-resourced domains.\n## I. INTRODUCTION\n(2) Methods based on synthesizing human-like data through heuristics or data-to-text models. Eisenschlos et al. [29] generate claims using context-free grammar (CFG) templates and counterfactual heuristics. Recently, Pan et al. [30] propose an unsupervised learning framework named MQA-QG. MQA-QG generates multi-hop questions for both the tabular and textual data, which is the most relevant work in this tendency. \n\nNevertheless, several critical issues remain unsolved in the realm of unsupervised complex tabular reasoning: (1) Existing methods for data generation mainly use heuristics or shallow data-to-text methods (e.g., converting a row to a sentence). Thus, they can merely generate relatively simple instances shown in Figure 2, limiting the model's effectiveness on complex reasoning samples, which require a deep understanding of the semantics and logic relationships between multiple table cells. (2) Previous works only focus on a single scenario but cannot be expanded to other tabular reasoning tasks. This is because they design heuristics based on specific data characteristics or the form of the task, and these methods cannot be transferred to other tasks flexibly. Therefore, these models struggle to handle the complex and diverse scenarios encountered in the real world. \n\nTo address these issues, we introduce UCTR-ST (Unsupervised Complex Tabular Reasoning using Self-Training), an advanced self-training framework designed explicitly for unsupervised complex tabular reasoning. More specifically, UCTR-ST primarily leverages a random sampling strategy to collect different types of programs. These programs consist of sequences of symbols that can be executed on tables, including SQL queries, logical forms, and arithmetic expressions, encompassing a wide range of reasoning types. Subsequently, we design a \"Program-Management\" module that generates program-answer pairs by leveraging numerous tables within the domain. To bridge the gap between the programs and natural language sentences, we develop a powerful \"Program-Transformation\" module based on generative language models that turns the programs into human-like natural questions or claims with complex logic.\n\n## B. Unsupervised Data Generation and Self-Training.\nUnsupervised data generation has been extensively studied on various tasks like question answering and natural language inference, and has shown surprising performances [51], [53] [54]. Recently, methods for synthesizing human-like tabular reasoning samples have also been proposed [52], [56]. Chemmengath et al. [46] sample complex SQL queries and generate natural language questions in a seq2seq manner. Eisenschlos et al. [29] generate factual claims leveraging context-free grammar (CFG) and counterfactual heuristics. Unfortunately, these methods focus on a specific task or scenario. Based on the modules and predefined operators, our approach can convert different types of programs into natural language questions or claims with tabular evidence or a hybrid of tabular and textual evidence. \n\nSelf-training has been widely explored in the realm of semi-supervised learning [60], [63], [65]. For example, Li et al. [66] proposed FlexKBQA, a method that combines selftraining and synthetic data to improve the performance of few-shot knowledge based question answering. Most works employ self-training techniques in the few-shot setting, where a small number of labeled samples are available. However, this study effectively combines data generation methods with selftraining to achieve good results in an unsupervised scenario.",
            "reference_string": "[254877131 | Li et al. | 2022 | Citations: 6]"
        },
        {
            "title": "ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for Reasoning Tasks",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 58,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.02390, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2348427354",
                    "name": "Heng Zhou"
                },
                {
                    "authorId": "2232782866",
                    "name": "Hejia Geng"
                },
                {
                    "authorId": "2320221475",
                    "name": "Xiangyuan Xue"
                },
                {
                    "authorId": "2348879787",
                    "name": "Zhenfei Yin"
                },
                {
                    "authorId": "2350152497",
                    "name": "Lei Bai"
                }
            ],
            "abstract": "Multi-agent systems (MAS) have emerged as a promising approach for enhancing the reasoning capabilities of large language models in complex problem-solving; however, current MAS frameworks suffer from poor flexibility and scalability with underdeveloped optimization strategies. To address these challenges, we propose ReSo, which integrates task graph generation with a reward-driven two-stage agent selection process centered on our Collaborative Reward Model that provides fine-grained reward signals to optimize MAS cooperation. We also introduce an automated data synthesis framework for generating MAS benchmarks without any human annotations. Experimental results show that ReSo matches or outperforms existing methods, achieving 33.7 percent accuracy on Math-MAS and 32.3 percent accuracy on SciBench-MAS, where other approaches completely fail.",
            "corpus_id": 276768281,
            "sentences": [
                {
                    "corpus_id": "276768281",
                    "title": "ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for Reasoning Tasks",
                    "text": "For each edge, we generate a textual description e jk , such as \"in this question, c k = previous answer + 3.\" Formally, it is an algorithm that constructs a string from two numbers: e ij = f (a j , c k ). f can be implemented using elementary arithmetic and text templates, ensuring that no answers or parameters in the original subtask need to be manually modified. Once the DAG is fully constructed, we refine node descriptions by removing any explicitly given numerical constants {c i } that are now dependent on the results of prior nodes. Finally, an entire graph described in natural language is a piece of synthetic data. \n\nThe proposed data synthesis framework generates structured, multi-step reasoning tasks with adjustable sizes, ensuring diverse and scalable problem structures. The synthesized dataset supports both training and testing, enabling fine-grained evaluation without human annotations.",
                    "score": 0.48749243607477033,
                    "section_title": "Data Synthesis",
                    "char_start_offset": 15932,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 205
                        },
                        {
                            "start": 206,
                            "end": 367
                        },
                        {
                            "start": 368,
                            "end": 544
                        },
                        {
                            "start": 545,
                            "end": 629
                        },
                        {
                            "start": 632,
                            "end": 791
                        },
                        {
                            "start": 792,
                            "end": 911
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87646484375
                }
            ],
            "relevance_judgement": 0.87646484375,
            "relevance_judgment_input_expanded": "# Title: ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for Reasoning Tasks\n# Venue: arXiv.org\n# Authors: Heng Zhou, Hejia Geng, Xiangyuan Xue, Zhenfei Yin, Lei Bai\n## Abstract\nMulti-agent systems (MAS) have emerged as a promising approach for enhancing the reasoning capabilities of large language models in complex problem-solving; however, current MAS frameworks suffer from poor flexibility and scalability with underdeveloped optimization strategies. To address these challenges, we propose ReSo, which integrates task graph generation with a reward-driven two-stage agent selection process centered on our Collaborative Reward Model that provides fine-grained reward signals to optimize MAS cooperation. We also introduce an automated data synthesis framework for generating MAS benchmarks without any human annotations. Experimental results show that ReSo matches or outperforms existing methods, achieving 33.7 percent accuracy on Math-MAS and 32.3 percent accuracy on SciBench-MAS, where other approaches completely fail.\n## Data Synthesis\nFor each edge, we generate a textual description e jk , such as \"in this question, c k = previous answer + 3.\" Formally, it is an algorithm that constructs a string from two numbers: e ij = f (a j , c k ). f can be implemented using elementary arithmetic and text templates, ensuring that no answers or parameters in the original subtask need to be manually modified. Once the DAG is fully constructed, we refine node descriptions by removing any explicitly given numerical constants {c i } that are now dependent on the results of prior nodes. Finally, an entire graph described in natural language is a piece of synthetic data. \n\nThe proposed data synthesis framework generates structured, multi-step reasoning tasks with adjustable sizes, ensuring diverse and scalable problem structures. The synthesized dataset supports both training and testing, enabling fine-grained evaluation without human annotations.",
            "reference_string": "[276768281 | Zhou et al. | 2025 | Citations: 6]"
        },
        {
            "title": "RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 38,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.20426, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2302813110",
                    "name": "Jiapeng Wang"
                },
                {
                    "authorId": "2118240359",
                    "name": "Jinhao Jiang"
                },
                {
                    "authorId": "2358106383",
                    "name": "Zhiqiang Zhang"
                },
                {
                    "authorId": "2279870653",
                    "name": "Jun Zhou"
                },
                {
                    "authorId": "2294811281",
                    "name": "Wayne Xin Zhao"
                }
            ],
            "abstract": "The advancement of reasoning capabilities in Large Language Models (LLMs) requires substantial amounts of high-quality reasoning data, particularly in mathematics. Existing data synthesis methods, such as data augmentation from annotated training sets or direct question generation based on relevant knowledge points and documents, have expanded datasets but face challenges in mastering the inner logic of the problem during generation and ensuring the verifiability of the solutions. To address these issues, we propose RV-Syn, a novel Rational and Verifiable mathematical Synthesis approach. RV-Syn constructs a structured mathematical operation function library based on initial seed problems and generates computational graphs as solutions by combining Python-formatted functions from this library. These graphs are then back-translated into complex problems. Based on the constructed computation graph, we achieve solution-guided logic-aware problem generation. Furthermore, the executability of the computational graph ensures the verifiability of the solving process. Experimental results show that RV-Syn surpasses existing synthesis methods, including those involving human-generated problems, achieving greater efficient data scaling. This approach provides a scalable framework for generating high-quality reasoning datasets.",
            "corpus_id": 278171321,
            "sentences": [
                {
                    "corpus_id": "278171321",
                    "title": "RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library",
                    "text": "The development of advanced reasoning Large Language Models (LLMs) (Zhao et al., 2023;OpenAI, 2024) has markedly improved their ability to address complex tasks across domains such as mathematics, science, and coding. This highlights the importance of synthesizing complex reasoning data to drive further advancements, given the limited availability of high-quality annotated instructions (Shah et al., 2024;Yuan et al., 2023). \n\nTo address this scarcity, researchers have explored various synthesizing methods, particularly in the mathematics domain. The mainstream methods involve data augmentation based on existing annotated training sets, such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), ranging from self-evolving instructions (Xu et al., 2024;Zeng et al., 2024) and question paraphrasing (Yu et al., 2024), to solution augmentation (Lu et al., 2024). However, these methods are limited by the available training data, constraining the synthesis diversity (Li et al., 2024c(Li et al., , 2023)). To enhance diversity, recent approaches enable LLMs to generate a large scale of questions from various mathematicsrelated sources, including web pages (Yue et al., 2024) and knowledge points (Tang et al., 2024) from web corpora or textbooks. \n\nHowever, as shown in Figure 1, these direct problem generation methods encounter challenges in lacking a deep understanding and simulation of the underlying reasoning processes. As a result, the synthesized problems may appear superficially plausible but actually suffer from insufficient reasoning depth and logical inconsistencies (such as internal contradictions), especially when generating complex multi-step reasoning problems. Furthermore, a long-standing challenge in synthetic data generation is the difficulty in supporting the validation of solution correctness during the synthesis process, ultimately compromising training efficiency. \n\nTo address the aforementioned challenges, we draw inspiration from how human educators create problems. Just as one cannot directly write down an Olympic-level math problem without deep consideration and careful curation of the underlying problem-solving process, the same holds true for models.",
                    "score": 0.5037788418459601,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 217
                        },
                        {
                            "start": 218,
                            "end": 427
                        },
                        {
                            "start": 430,
                            "end": 551
                        },
                        {
                            "start": 552,
                            "end": 878
                        },
                        {
                            "start": 879,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1264
                        },
                        {
                            "start": 1267,
                            "end": 1444
                        },
                        {
                            "start": 1445,
                            "end": 1700
                        },
                        {
                            "start": 1701,
                            "end": 1914
                        },
                        {
                            "start": 1917,
                            "end": 2020
                        },
                        {
                            "start": 2021,
                            "end": 2212
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 688,
                            "end": 712,
                            "matchedPaperCorpusId": "268247488"
                        },
                        {
                            "start": 771,
                            "end": 789,
                            "matchedPaperCorpusId": "270213007"
                        },
                        {
                            "start": 1214,
                            "end": 1233,
                            "matchedPaperCorpusId": "268247902"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87451171875
                }
            ],
            "relevance_judgement": 0.87451171875,
            "relevance_judgment_input_expanded": "# Title: RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library\n# Venue: arXiv.org\n# Authors: Jiapeng Wang, Jinhao Jiang, Zhiqiang Zhang, Jun Zhou, Wayne Xin Zhao\n## Abstract\nThe advancement of reasoning capabilities in Large Language Models (LLMs) requires substantial amounts of high-quality reasoning data, particularly in mathematics. Existing data synthesis methods, such as data augmentation from annotated training sets or direct question generation based on relevant knowledge points and documents, have expanded datasets but face challenges in mastering the inner logic of the problem during generation and ensuring the verifiability of the solutions. To address these issues, we propose RV-Syn, a novel Rational and Verifiable mathematical Synthesis approach. RV-Syn constructs a structured mathematical operation function library based on initial seed problems and generates computational graphs as solutions by combining Python-formatted functions from this library. These graphs are then back-translated into complex problems. Based on the constructed computation graph, we achieve solution-guided logic-aware problem generation. Furthermore, the executability of the computational graph ensures the verifiability of the solving process. Experimental results show that RV-Syn surpasses existing synthesis methods, including those involving human-generated problems, achieving greater efficient data scaling. This approach provides a scalable framework for generating high-quality reasoning datasets.\n## Introduction\nThe development of advanced reasoning Large Language Models (LLMs) (Zhao et al., 2023;OpenAI, 2024) has markedly improved their ability to address complex tasks across domains such as mathematics, science, and coding. This highlights the importance of synthesizing complex reasoning data to drive further advancements, given the limited availability of high-quality annotated instructions (Shah et al., 2024;Yuan et al., 2023). \n\nTo address this scarcity, researchers have explored various synthesizing methods, particularly in the mathematics domain. The mainstream methods involve data augmentation based on existing annotated training sets, such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), ranging from self-evolving instructions (Xu et al., 2024;Zeng et al., 2024) and question paraphrasing (Yu et al., 2024), to solution augmentation (Lu et al., 2024). However, these methods are limited by the available training data, constraining the synthesis diversity (Li et al., 2024c(Li et al., , 2023)). To enhance diversity, recent approaches enable LLMs to generate a large scale of questions from various mathematicsrelated sources, including web pages (Yue et al., 2024) and knowledge points (Tang et al., 2024) from web corpora or textbooks. \n\nHowever, as shown in Figure 1, these direct problem generation methods encounter challenges in lacking a deep understanding and simulation of the underlying reasoning processes. As a result, the synthesized problems may appear superficially plausible but actually suffer from insufficient reasoning depth and logical inconsistencies (such as internal contradictions), especially when generating complex multi-step reasoning problems. Furthermore, a long-standing challenge in synthetic data generation is the difficulty in supporting the validation of solution correctness during the synthesis process, ultimately compromising training efficiency. \n\nTo address the aforementioned challenges, we draw inspiration from how human educators create problems. Just as one cannot directly write down an Olympic-level math problem without deep consideration and careful curation of the underlying problem-solving process, the same holds true for models.",
            "reference_string": "[278171321 | Wang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Abstract Meaning Representation-Based Logic-Driven Data Augmentation for Logical Reasoning",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 48,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2305.12599",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.12599, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1491516930",
                    "name": "Qiming Bao"
                },
                {
                    "authorId": "2064347311",
                    "name": "A. Peng"
                },
                {
                    "authorId": "2244794",
                    "name": "Zhenyun Deng"
                },
                {
                    "authorId": "81970097",
                    "name": "Wanjun Zhong"
                },
                {
                    "authorId": "2151063121",
                    "name": "N. Tan"
                },
                {
                    "authorId": "2159676415",
                    "name": "Nathan Young"
                },
                {
                    "authorId": "2144353312",
                    "name": "Yang Chen"
                },
                {
                    "authorId": "2116513959",
                    "name": "Yonghua Zhu"
                },
                {
                    "authorId": "2582677",
                    "name": "Michael Witbrock"
                },
                {
                    "authorId": "2108326935",
                    "name": "Jiamou Liu"
                }
            ],
            "abstract": "Combining large language models with logical reasoning enhances their capacity to address problems in a robust and reliable manner. Nevertheless, the intricate nature of logical reasoning poses challenges when gathering reliable data from the web to build comprehensive training datasets, subsequently affecting performance on downstream tasks. To address this, we introduce a novel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the original text into an Abstract Meaning Representation (AMR) graph, a structured semantic representation that encapsulates the logical structure of the sentence, upon which operations are performed to generate logically modified AMR graphs. The modified AMR graphs are subsequently converted back into text to create augmented data. Notably, our methodology is architecture-agnostic and enhances both generative large language models, such as GPT-3.5 and GPT-4, through prompt augmentation, and discriminative large language models through contrastive learning with logic-driven data augmentation. Empirical evidence underscores the efficacy of our proposed method with improvement in performance across seven downstream tasks, such as reading comprehension requiring logical reasoning, textual entailment, and natural language inference. Furthermore, our method leads on the ReClor leaderboard at https://eval.ai/web/challenges/challenge-page/503/leaderboard/1347. The source code and data are publicly available at https://github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning.",
            "corpus_id": 258833579,
            "sentences": [
                {
                    "corpus_id": "258833579",
                    "title": "Abstract Meaning Representation-Based Logic-Driven Data Augmentation for Logical Reasoning",
                    "text": "ReClor (Yu et al., 2020) and LogiQA (Liu et al., 2021) are two challenging logical reasoning datasets. ReClor is collected from the Graduate Management Admission Test (GMAT) and Law School Admission Test (LSAT). LogiQA is collected from the National Civil Service Examination (Liu et al., 2021). Additionally, we performed evaluation on five datasets for natural language inference and textual entailment tasks: MNLI (Williams et al., 2018), RTE (Wang et al., 2018), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), and QQP (Wang et al., 2018). MNLI, RTE, and MRPC assess the relationship between two sentences, while QNLI focuses on the relationship between a question and a sentence, and QQP evaluates the relationship between two questions. \n\nSynthetic Data for Contrastive Learning In this paper, we performed contrastive learning for discriminative large language models on sentences augmented from a synthetic dataset. This dataset contains 14,962 sentences with different combinations of 23 entities, 2 relations and 40 attributes. Synthetic data was used to generate more controllable logical sentences. More details about the synthetic dataset can be found in the Appendix Section E.",
                    "score": 0.46842754747492155,
                    "section_title": "Datasets",
                    "char_start_offset": 16281,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 102
                        },
                        {
                            "start": 103,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 295
                        },
                        {
                            "start": 296,
                            "end": 559
                        },
                        {
                            "start": 560,
                            "end": 758
                        },
                        {
                            "start": 761,
                            "end": 939
                        },
                        {
                            "start": 940,
                            "end": 1053
                        },
                        {
                            "start": 1054,
                            "end": 1126
                        },
                        {
                            "start": 1127,
                            "end": 1207
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 7,
                            "end": 24,
                            "matchedPaperCorpusId": "209485573"
                        },
                        {
                            "start": 36,
                            "end": 54,
                            "matchedPaperCorpusId": "220483148"
                        },
                        {
                            "start": 276,
                            "end": 294,
                            "matchedPaperCorpusId": "220483148"
                        },
                        {
                            "start": 417,
                            "end": 440,
                            "matchedPaperCorpusId": "3432876"
                        },
                        {
                            "start": 446,
                            "end": 465,
                            "matchedPaperCorpusId": "5034059"
                        },
                        {
                            "start": 472,
                            "end": 498,
                            "matchedPaperCorpusId": "16639476"
                        },
                        {
                            "start": 505,
                            "end": 529,
                            "matchedPaperCorpusId": "11816014"
                        },
                        {
                            "start": 539,
                            "end": 558,
                            "matchedPaperCorpusId": "5034059"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86328125
                }
            ],
            "relevance_judgement": 0.86328125,
            "relevance_judgment_input_expanded": "# Title: Abstract Meaning Representation-Based Logic-Driven Data Augmentation for Logical Reasoning\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Qiming Bao, A. Peng, Zhenyun Deng, Wanjun Zhong, N. Tan, Nathan Young, Yang Chen, Yonghua Zhu, Michael Witbrock, Jiamou Liu\n## Abstract\nCombining large language models with logical reasoning enhances their capacity to address problems in a robust and reliable manner. Nevertheless, the intricate nature of logical reasoning poses challenges when gathering reliable data from the web to build comprehensive training datasets, subsequently affecting performance on downstream tasks. To address this, we introduce a novel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the original text into an Abstract Meaning Representation (AMR) graph, a structured semantic representation that encapsulates the logical structure of the sentence, upon which operations are performed to generate logically modified AMR graphs. The modified AMR graphs are subsequently converted back into text to create augmented data. Notably, our methodology is architecture-agnostic and enhances both generative large language models, such as GPT-3.5 and GPT-4, through prompt augmentation, and discriminative large language models through contrastive learning with logic-driven data augmentation. Empirical evidence underscores the efficacy of our proposed method with improvement in performance across seven downstream tasks, such as reading comprehension requiring logical reasoning, textual entailment, and natural language inference. Furthermore, our method leads on the ReClor leaderboard at https://eval.ai/web/challenges/challenge-page/503/leaderboard/1347. The source code and data are publicly available at https://github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning.\n## Datasets\nReClor (Yu et al., 2020) and LogiQA (Liu et al., 2021) are two challenging logical reasoning datasets. ReClor is collected from the Graduate Management Admission Test (GMAT) and Law School Admission Test (LSAT). LogiQA is collected from the National Civil Service Examination (Liu et al., 2021). Additionally, we performed evaluation on five datasets for natural language inference and textual entailment tasks: MNLI (Williams et al., 2018), RTE (Wang et al., 2018), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), and QQP (Wang et al., 2018). MNLI, RTE, and MRPC assess the relationship between two sentences, while QNLI focuses on the relationship between a question and a sentence, and QQP evaluates the relationship between two questions. \n\nSynthetic Data for Contrastive Learning In this paper, we performed contrastive learning for discriminative large language models on sentences augmented from a synthetic dataset. This dataset contains 14,962 sentences with different combinations of 23 entities, 2 relations and 40 attributes. Synthetic data was used to generate more controllable logical sentences. More details about the synthetic dataset can be found in the Appendix Section E.",
            "reference_string": "[258833579 | Bao et al. | 2023 | Citations: 4]"
        },
        {
            "title": "Does Synthetic Data Make Large Language Models More Efficient?",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 28,
            "citation_count": 15,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.07830",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.07830, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2141639835",
                    "name": "Sia Gholami"
                },
                {
                    "authorId": "2239197098",
                    "name": "Marwan Omar"
                }
            ],
            "abstract": "Natural Language Processing (NLP) has undergone transformative changes with the advent of deep learning methodologies. One challenge persistently confronting researchers is the scarcity of high-quality, annotated datasets that drive these models. This paper explores the nuances of synthetic data generation in NLP, with a focal point on template-based question generation. By assessing its advantages, including data augmentation potential and the introduction of structured variety, we juxtapose these benefits against inherent limitations, such as the risk of overfitting and the constraints posed by pre-defined templates. Drawing from empirical evaluations, we demonstrate the impact of template-based synthetic data on the performance of modern transformer models. We conclude by emphasizing the delicate balance required between synthetic and real-world data, and the future trajectories of integrating synthetic data in model training pipelines. The findings aim to guide NLP practitioners in harnessing synthetic data's potential, ensuring optimal model performance in diverse applications.",
            "corpus_id": 263909600,
            "sentences": [
                {
                    "corpus_id": "263909600",
                    "title": "Does Synthetic Data Make Large Language Models More Efficient?",
                    "text": "In the burgeoning field of Natural Language Processing (NLP), acquiring substantial data for training and fine-tuning models is a continual challenge [Vaswani et al., 2017]. While real-world annotated datasets are invaluable, their availability is often constrained, making them expensive to produce, and they can sometimes carry inherent biases from their collection methods [Bowman et al., 2015]. This context underscores the potential of synthetic data generation techniques, with synthetic questionanswer pairs emerging as a notable subset [Rajpurkar et al., 2016]. Among the diverse strategies available, template-based question generation, recognized for its rule-driven approach, provides a systematic avenue for data generation [Chen et al., 2016]. \n\nHowever, as with many techniques in the realm of computational linguistics, the adoption of templatebased generation in transformer models within NLP presents a complex landscape to navigate [Devlin et al., 2018]. This paper seeks to illuminate the intricacies of this approach, offering insights into its methodologies, advantages, and inherent limitations. Through our examination, our aim is to equip readers with a nuanced understanding of the technique, its impact on transformer architectures, and the potential avenues for its evolution in NLP research. \n\nThe implementation of template-based question generation for creating synthetic question-answer pairs can significantly impact the performance of a LLM in several ways: \n\n1. Data Augmentation: The most direct impact is the increase in training data. When you create synthetic question-answer pairs from existing text, you're effectively augmenting your dataset, which can be particularly useful when dealing with tasks where the amount of available labeled data is limited. This increased data volume helps the model better understand language patterns and variations, which can enhance the model's ability to generalize, ultimately improving performance. \n\n2. Exposure to Diverse Structures: Template-based question generation exposes the transformer model to a wider variety of question structures and types. This increased exposure helps the model develop a more comprehensive understanding of language and better performance on a broader range of questions. \n\n3. Model Robustness: By creating synthetic data that includes a variety of linguistic features and structures, the model becomes more robust.",
                    "score": 0.44639955441161105,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 173
                        },
                        {
                            "start": 174,
                            "end": 398
                        },
                        {
                            "start": 399,
                            "end": 569
                        },
                        {
                            "start": 570,
                            "end": 756
                        },
                        {
                            "start": 759,
                            "end": 972
                        },
                        {
                            "start": 973,
                            "end": 1117
                        },
                        {
                            "start": 1118,
                            "end": 1319
                        },
                        {
                            "start": 1322,
                            "end": 1490
                        },
                        {
                            "start": 1493,
                            "end": 1571
                        },
                        {
                            "start": 1572,
                            "end": 1795
                        },
                        {
                            "start": 1796,
                            "end": 1977
                        },
                        {
                            "start": 1980,
                            "end": 2132
                        },
                        {
                            "start": 2133,
                            "end": 2283
                        },
                        {
                            "start": 2286,
                            "end": 2427
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 150,
                            "end": 172,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8623046875
                },
                {
                    "corpus_id": "263909600",
                    "title": "Does Synthetic Data Make Large Language Models More Efficient?",
                    "text": "The approach of synthetic data generation in Natural Language Processing (NLP) has shown promise, but there's still much room for improvement and exploration. Here are some potential directions for future work: \n\n\u2022 Improving Synthetic Data Quality: One of the main challenges with synthetic data is ensuring its quality. Future work could focus on developing new techniques to generate higher-quality synthetic data that more accurately reflects real-world language patterns and distributions. \n\n\u2022 Hybrid Generation Methods: Combining template-based question generation with more flexible methods, such as machine learning or transformer-based question generation techniques, could create a more diverse set of synthetic questions and mitigate some of the limitations of template-based generation. \n\n\u2022 Evaluation Metrics for Synthetic Data: Designing metrics to evaluate the quality of synthetic data could be a valuable contribution. These metrics could help guide the generation process and provide a more objective measure of whether the synthetic data is likely to improve model performance. \n\n\u2022 Adaptive Synthetic Data Generation: Research could be directed towards adaptive synthetic data generation, where the synthetic data generation process is guided by the performance of the model, focusing on areas where the model struggles. \n\n\u2022 Investigating Optimal Ratios of Synthetic to Real Data: More extensive empirical studies could help identify the optimal ratios of synthetic to real data for various types of NLP tasks and models. \n\n\u2022 Application-Specific Synthetic Data: Different NLP tasks might benefit from different types of synthetic data. Future work could investigate how to tailor synthetic data generation to specific applications. \n\n\u2022 Addressing Biases: Future work could also focus on how synthetic data generation can be used to mitigate biases in NLP models, exploring different strategies for generating synthetic data that helps to counteract known biases in the training data. \n\n\u2022 Computational Efficiency: Reducing the computational cost of synthetic data generation is another important direction for future work. This could involve developing more efficient algorithms or making better use of hardware resources. \n\nBy pursuing these avenues of future work, the field can continue to advance the use of synthetic data in NLP and fully realize its potential for improving the performance and robustness of language models.",
                    "score": 0.47108735754461806,
                    "section_title": "Future Work",
                    "char_start_offset": 23700,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 159,
                            "end": 210
                        },
                        {
                            "start": 213,
                            "end": 320
                        },
                        {
                            "start": 321,
                            "end": 493
                        },
                        {
                            "start": 496,
                            "end": 797
                        },
                        {
                            "start": 800,
                            "end": 934
                        },
                        {
                            "start": 935,
                            "end": 1095
                        },
                        {
                            "start": 1098,
                            "end": 1338
                        },
                        {
                            "start": 1341,
                            "end": 1539
                        },
                        {
                            "start": 1542,
                            "end": 1654
                        },
                        {
                            "start": 1655,
                            "end": 1750
                        },
                        {
                            "start": 1753,
                            "end": 2002
                        },
                        {
                            "start": 2005,
                            "end": 2141
                        },
                        {
                            "start": 2142,
                            "end": 2241
                        },
                        {
                            "start": 2244,
                            "end": 2449
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8486328125
                }
            ],
            "relevance_judgement": 0.8623046875,
            "relevance_judgment_input_expanded": "# Title: Does Synthetic Data Make Large Language Models More Efficient?\n# Venue: arXiv.org\n# Authors: Sia Gholami, Marwan Omar\n## Abstract\nNatural Language Processing (NLP) has undergone transformative changes with the advent of deep learning methodologies. One challenge persistently confronting researchers is the scarcity of high-quality, annotated datasets that drive these models. This paper explores the nuances of synthetic data generation in NLP, with a focal point on template-based question generation. By assessing its advantages, including data augmentation potential and the introduction of structured variety, we juxtapose these benefits against inherent limitations, such as the risk of overfitting and the constraints posed by pre-defined templates. Drawing from empirical evaluations, we demonstrate the impact of template-based synthetic data on the performance of modern transformer models. We conclude by emphasizing the delicate balance required between synthetic and real-world data, and the future trajectories of integrating synthetic data in model training pipelines. The findings aim to guide NLP practitioners in harnessing synthetic data's potential, ensuring optimal model performance in diverse applications.\n## Introduction\nIn the burgeoning field of Natural Language Processing (NLP), acquiring substantial data for training and fine-tuning models is a continual challenge [Vaswani et al., 2017]. While real-world annotated datasets are invaluable, their availability is often constrained, making them expensive to produce, and they can sometimes carry inherent biases from their collection methods [Bowman et al., 2015]. This context underscores the potential of synthetic data generation techniques, with synthetic questionanswer pairs emerging as a notable subset [Rajpurkar et al., 2016]. Among the diverse strategies available, template-based question generation, recognized for its rule-driven approach, provides a systematic avenue for data generation [Chen et al., 2016]. \n\nHowever, as with many techniques in the realm of computational linguistics, the adoption of templatebased generation in transformer models within NLP presents a complex landscape to navigate [Devlin et al., 2018]. This paper seeks to illuminate the intricacies of this approach, offering insights into its methodologies, advantages, and inherent limitations. Through our examination, our aim is to equip readers with a nuanced understanding of the technique, its impact on transformer architectures, and the potential avenues for its evolution in NLP research. \n\nThe implementation of template-based question generation for creating synthetic question-answer pairs can significantly impact the performance of a LLM in several ways: \n\n1. Data Augmentation: The most direct impact is the increase in training data. When you create synthetic question-answer pairs from existing text, you're effectively augmenting your dataset, which can be particularly useful when dealing with tasks where the amount of available labeled data is limited. This increased data volume helps the model better understand language patterns and variations, which can enhance the model's ability to generalize, ultimately improving performance. \n\n2. Exposure to Diverse Structures: Template-based question generation exposes the transformer model to a wider variety of question structures and types. This increased exposure helps the model develop a more comprehensive understanding of language and better performance on a broader range of questions. \n\n3. Model Robustness: By creating synthetic data that includes a variety of linguistic features and structures, the model becomes more robust.\n\n## Future Work\nThe approach of synthetic data generation in Natural Language Processing (NLP) has shown promise, but there's still much room for improvement and exploration. Here are some potential directions for future work: \n\n\u2022 Improving Synthetic Data Quality: One of the main challenges with synthetic data is ensuring its quality. Future work could focus on developing new techniques to generate higher-quality synthetic data that more accurately reflects real-world language patterns and distributions. \n\n\u2022 Hybrid Generation Methods: Combining template-based question generation with more flexible methods, such as machine learning or transformer-based question generation techniques, could create a more diverse set of synthetic questions and mitigate some of the limitations of template-based generation. \n\n\u2022 Evaluation Metrics for Synthetic Data: Designing metrics to evaluate the quality of synthetic data could be a valuable contribution. These metrics could help guide the generation process and provide a more objective measure of whether the synthetic data is likely to improve model performance. \n\n\u2022 Adaptive Synthetic Data Generation: Research could be directed towards adaptive synthetic data generation, where the synthetic data generation process is guided by the performance of the model, focusing on areas where the model struggles. \n\n\u2022 Investigating Optimal Ratios of Synthetic to Real Data: More extensive empirical studies could help identify the optimal ratios of synthetic to real data for various types of NLP tasks and models. \n\n\u2022 Application-Specific Synthetic Data: Different NLP tasks might benefit from different types of synthetic data. Future work could investigate how to tailor synthetic data generation to specific applications. \n\n\u2022 Addressing Biases: Future work could also focus on how synthetic data generation can be used to mitigate biases in NLP models, exploring different strategies for generating synthetic data that helps to counteract known biases in the training data. \n\n\u2022 Computational Efficiency: Reducing the computational cost of synthetic data generation is another important direction for future work. This could involve developing more efficient algorithms or making better use of hardware resources. \n\nBy pursuing these avenues of future work, the field can continue to advance the use of synthetic data in NLP and fully realize its potential for improving the performance and robustness of language models.",
            "reference_string": "[263909600 | Gholami et al. | 2023 | Citations: 15]"
        },
        {
            "title": "Successive Prompting for Decomposing Complex Questions",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 42,
            "citation_count": 118,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2212.04092",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.04092, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "33546336",
                    "name": "Dheeru Dua"
                },
                {
                    "authorId": "1698760333",
                    "name": "Shivanshu Gupta"
                },
                {
                    "authorId": "34650964",
                    "name": "Sameer Singh"
                },
                {
                    "authorId": "40642935",
                    "name": "Matt Gardner"
                }
            ],
            "abstract": "Answering complex questions that require making latent decisions is a challenging task, especially when limited supervision is available. Recent works leverage the capabilities of large language models (LMs) to perform complex question answering in a few-shot setting by demonstrating how to output intermediate rationalizations while solving the complex question in a single pass. We introduce \u201cSuccessive Prompting\u201d where, we iteratively break down a complex task into a simple task, solve it, and then repeat the process until we get the final solution. Successive prompting decouples the supervision for decomposing complex questions from the supervision for answering simple questions, allowing us to (1) have multiple opportunities to query in-context examples at each reasoning step (2) learn question decomposition separately from question answering, including using synthetic data, and (3) use bespoke (fine-tuned) components for reasoning steps where a large LM does not perform well. The intermediate supervision is typically manually written, which can be expensive to collect. We introduce a way to generate synthetic dataset which can be used to bootstrap model\u2019s ability to decompose and answer intermediate questions. Our best model (with successive prompting) achieves an improvement in F1 of ~5% when compared with a state-of-the-art model with synthetic augmentations and few-shot version of the DROP dataset.",
            "corpus_id": 254408974,
            "sentences": [
                {
                    "corpus_id": "254408974",
                    "title": "Successive Prompting for Decomposing Complex Questions",
                    "text": "Any method that prompts LMs to produce intermediate reasoning steps to answer complex questions needs some amount of supervision for those reasoning steps. This kind of annotation can be expensive to collect and often requires expert knowledge.   Figure 3).\n\nPrior work has typically relied on a small handful of manually-written example decompositions. We find that such small collections lead to very poor performance on a dataset as varied as DROP, even for large models.\n\nTo mitigate these data issues, we propose a way to synthetically generate complex questions and their decompositions using semi-structured data which is easy to parse. We show that we can bootstrap model learning with this out-of-domain, synthetically generated data so it can adapt better when fine-tuned with limited in-domain supervision.\n\nGeneration Process: Inspired by Yoran et al.\n\n(2021), we use semi-structured data from tables in English Wikipedia which are available in plenty. We employ curated templates to convert the rows in the tables into paragraphs. We use single column headers to create first order simple questions and a combination of columns for higher order complex questions. We synthesize data for 10 simple operations: COUNT, TOP(k), BOTTOM(k), FILTER, SUM, COMPARISON, DIFFERENCE, NEGATION, GATHER, and INTERSECTION.\n\nWe generate higher order combinations of firstorder operations, wherever possible. Figure 3 shows examples of higher order combinations of the atomic operation COUNT with a few other simple operations using Table 1 Figure 3: Examples of COUNT operation and some of its higher order combinations, with natural language and symbolic decompositions of the complex question. Underneath the first instance of a symbolic operation we show its corresponding natural language version. See Table 1 for the original table used to generate context and questions.",
                    "score": 0.5922391191384221,
                    "section_title": "Synthetic Dataset",
                    "char_start_offset": 13297,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.861328125
                }
            ],
            "relevance_judgement": 0.861328125,
            "relevance_judgment_input_expanded": "# Title: Successive Prompting for Decomposing Complex Questions\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Dheeru Dua, Shivanshu Gupta, Sameer Singh, Matt Gardner\n## Abstract\nAnswering complex questions that require making latent decisions is a challenging task, especially when limited supervision is available. Recent works leverage the capabilities of large language models (LMs) to perform complex question answering in a few-shot setting by demonstrating how to output intermediate rationalizations while solving the complex question in a single pass. We introduce \u201cSuccessive Prompting\u201d where, we iteratively break down a complex task into a simple task, solve it, and then repeat the process until we get the final solution. Successive prompting decouples the supervision for decomposing complex questions from the supervision for answering simple questions, allowing us to (1) have multiple opportunities to query in-context examples at each reasoning step (2) learn question decomposition separately from question answering, including using synthetic data, and (3) use bespoke (fine-tuned) components for reasoning steps where a large LM does not perform well. The intermediate supervision is typically manually written, which can be expensive to collect. We introduce a way to generate synthetic dataset which can be used to bootstrap model\u2019s ability to decompose and answer intermediate questions. Our best model (with successive prompting) achieves an improvement in F1 of ~5% when compared with a state-of-the-art model with synthetic augmentations and few-shot version of the DROP dataset.\n## Synthetic Dataset\nAny method that prompts LMs to produce intermediate reasoning steps to answer complex questions needs some amount of supervision for those reasoning steps. This kind of annotation can be expensive to collect and often requires expert knowledge.   Figure 3).\n\nPrior work has typically relied on a small handful of manually-written example decompositions. We find that such small collections lead to very poor performance on a dataset as varied as DROP, even for large models.\n\nTo mitigate these data issues, we propose a way to synthetically generate complex questions and their decompositions using semi-structured data which is easy to parse. We show that we can bootstrap model learning with this out-of-domain, synthetically generated data so it can adapt better when fine-tuned with limited in-domain supervision.\n\nGeneration Process: Inspired by Yoran et al.\n\n(2021), we use semi-structured data from tables in English Wikipedia which are available in plenty. We employ curated templates to convert the rows in the tables into paragraphs. We use single column headers to create first order simple questions and a combination of columns for higher order complex questions. We synthesize data for 10 simple operations: COUNT, TOP(k), BOTTOM(k), FILTER, SUM, COMPARISON, DIFFERENCE, NEGATION, GATHER, and INTERSECTION.\n\nWe generate higher order combinations of firstorder operations, wherever possible. Figure 3 shows examples of higher order combinations of the atomic operation COUNT with a few other simple operations using Table 1 Figure 3: Examples of COUNT operation and some of its higher order combinations, with natural language and symbolic decompositions of the complex question. Underneath the first instance of a symbolic operation we show its corresponding natural language version. See Table 1 for the original table used to generate context and questions.",
            "reference_string": "[254408974 | Dua et al. | 2022 | Citations: 118]"
        },
        {
            "title": "Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic",
            "venue": "International Conference on Machine Learning",
            "year": 2023,
            "reference_count": 65,
            "citation_count": 25,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.07336",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.07336, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1379579811",
                    "name": "Terufumi Morishita"
                },
                {
                    "authorId": "29347584",
                    "name": "Gaku Morio"
                },
                {
                    "authorId": "145412147",
                    "name": "Atsuki Yamaguchi"
                },
                {
                    "authorId": "2106369",
                    "name": "Yasuhiro Sogawa"
                }
            ],
            "abstract": "We study a synthetic corpus based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary, limiting the generalizability of acquired reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. Then, using the proposed corpora, which we name FLD (Formal Logic Deduction), we first evaluate and analyze the logical reasoning ability of the latest LLMs. Even GPT-4 can solve only half of the problems, suggesting that pure logical reasoning isolated from knowledge is still challenging for the LLMs, and additional training specialized in logical reasoning is indeed essential. We next empirically verify that LMs trained on FLD corpora acquire more generalizable reasoning ability. Furthermore, we identify the aspects of reasoning ability on which deduction corpora can enhance LMs and those on which they cannot, and discuss future directions on each aspect. The released corpora serve both as learning resources and as challenging benchmarks.",
            "corpus_id": 260846474,
            "sentences": [
                {
                    "corpus_id": "260846474",
                    "title": "Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic",
                    "text": "A synthetic deduction corpus can be one promising approach for language models (LMs) to acquire logical deductive reasoning ability. The automatic (programmatic) generation ensures the validity of the resulting deductive proof examples. \n\nFurther, since we can bypass high-cost human annotations we can generate many examples, which should be required by LMs to learn deductive reasoning inductively. \n\nRuleTaker (Clark et al., 2021) proposed a deduction corpus composed of synthetically generated multistep deductive proofs written in natural languages. Each deductive proof (dis-)proves a hypothesis by applying deduction rules multiple times to a given set of facts. As the deduction rules, rules of the implication kind were used. They showed that Transformer (Vaswani et al., 2017) LMs can solve these problems in the sense that they can predict the final answer (i.e., \"proved\", \"disproved\", or \"unknown\") of each deductive proof given the fact set. Later studies (Saha et al., 2020;Dalvi et al., 2021;Tafjord et al., 2021;Sanyal et al., 2022) showed that generative LMs can generate even the intermediate proofs as well as the final answer. \n\nArtificial Argument Corpus (Betz et al., 2021) is another corpus composed of synthetically generated single-step deductive proofs. As the deduction rules, hand-selected rules useful for critical thinking were used. They showed that the LMs trained on this corpus not only solve the task of this corpus itself but generalize to other NLI tasks from GLUE benchmark (Wang et al., 2018). However, at the same time, they showed that such LMs do not generalize well to more challenging logical reasoning tasks such as ARC (Habernal et al., 2018) and LogiQA (Liu et al., 2020). Gontier et al. (2020) investigated the deductive reasoning ability of LMs on a corpus, which is composed of a specific type of multistep inference: kinship relationship on synthetic kinship graphs.",
                    "score": 0.5646204039265872,
                    "section_title": "A.1. Synthetic Corpus for Acquiring Deductive Reasoning Ability",
                    "char_start_offset": 34565,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 132
                        },
                        {
                            "start": 133,
                            "end": 236
                        },
                        {
                            "start": 239,
                            "end": 400
                        },
                        {
                            "start": 403,
                            "end": 554
                        },
                        {
                            "start": 555,
                            "end": 669
                        },
                        {
                            "start": 670,
                            "end": 734
                        },
                        {
                            "start": 735,
                            "end": 955
                        },
                        {
                            "start": 956,
                            "end": 1147
                        },
                        {
                            "start": 1150,
                            "end": 1280
                        },
                        {
                            "start": 1281,
                            "end": 1364
                        },
                        {
                            "start": 1365,
                            "end": 1533
                        },
                        {
                            "start": 1534,
                            "end": 1720
                        },
                        {
                            "start": 1721,
                            "end": 1918
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 413,
                            "end": 433,
                            "matchedPaperCorpusId": "211126663"
                        },
                        {
                            "start": 764,
                            "end": 786,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 970,
                            "end": 989,
                            "matchedPaperCorpusId": "222141025"
                        },
                        {
                            "start": 989,
                            "end": 1008,
                            "matchedPaperCorpusId": "233297051"
                        },
                        {
                            "start": 1008,
                            "end": 1029,
                            "matchedPaperCorpusId": "229371222"
                        },
                        {
                            "start": 1029,
                            "end": 1049,
                            "matchedPaperCorpusId": "247594506"
                        },
                        {
                            "start": 1177,
                            "end": 1196,
                            "matchedPaperCorpusId": "221702898"
                        },
                        {
                            "start": 1513,
                            "end": 1532,
                            "matchedPaperCorpusId": "5034059"
                        },
                        {
                            "start": 1666,
                            "end": 1689,
                            "matchedPaperCorpusId": "3555187"
                        },
                        {
                            "start": 1701,
                            "end": 1719,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8564453125
                }
            ],
            "relevance_judgement": 0.8564453125,
            "relevance_judgment_input_expanded": "# Title: Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic\n# Venue: International Conference on Machine Learning\n# Authors: Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, Yasuhiro Sogawa\n## Abstract\nWe study a synthetic corpus based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary, limiting the generalizability of acquired reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. Then, using the proposed corpora, which we name FLD (Formal Logic Deduction), we first evaluate and analyze the logical reasoning ability of the latest LLMs. Even GPT-4 can solve only half of the problems, suggesting that pure logical reasoning isolated from knowledge is still challenging for the LLMs, and additional training specialized in logical reasoning is indeed essential. We next empirically verify that LMs trained on FLD corpora acquire more generalizable reasoning ability. Furthermore, we identify the aspects of reasoning ability on which deduction corpora can enhance LMs and those on which they cannot, and discuss future directions on each aspect. The released corpora serve both as learning resources and as challenging benchmarks.\n## A.1. Synthetic Corpus for Acquiring Deductive Reasoning Ability\nA synthetic deduction corpus can be one promising approach for language models (LMs) to acquire logical deductive reasoning ability. The automatic (programmatic) generation ensures the validity of the resulting deductive proof examples. \n\nFurther, since we can bypass high-cost human annotations we can generate many examples, which should be required by LMs to learn deductive reasoning inductively. \n\nRuleTaker (Clark et al., 2021) proposed a deduction corpus composed of synthetically generated multistep deductive proofs written in natural languages. Each deductive proof (dis-)proves a hypothesis by applying deduction rules multiple times to a given set of facts. As the deduction rules, rules of the implication kind were used. They showed that Transformer (Vaswani et al., 2017) LMs can solve these problems in the sense that they can predict the final answer (i.e., \"proved\", \"disproved\", or \"unknown\") of each deductive proof given the fact set. Later studies (Saha et al., 2020;Dalvi et al., 2021;Tafjord et al., 2021;Sanyal et al., 2022) showed that generative LMs can generate even the intermediate proofs as well as the final answer. \n\nArtificial Argument Corpus (Betz et al., 2021) is another corpus composed of synthetically generated single-step deductive proofs. As the deduction rules, hand-selected rules useful for critical thinking were used. They showed that the LMs trained on this corpus not only solve the task of this corpus itself but generalize to other NLI tasks from GLUE benchmark (Wang et al., 2018). However, at the same time, they showed that such LMs do not generalize well to more challenging logical reasoning tasks such as ARC (Habernal et al., 2018) and LogiQA (Liu et al., 2020). Gontier et al. (2020) investigated the deductive reasoning ability of LMs on a corpus, which is composed of a specific type of multistep inference: kinship relationship on synthetic kinship graphs.",
            "reference_string": "[260846474 | Morishita et al. | 2023 | Citations: 25]"
        },
        {
            "title": "Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 67,
            "citation_count": 54,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.acl-long.416.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2107.07261, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2074100429",
                    "name": "Ori Yoran"
                },
                {
                    "authorId": "12371246",
                    "name": "Alon Talmor"
                },
                {
                    "authorId": "1750652",
                    "name": "Jonathan Berant"
                }
            ],
            "abstract": "Models pre-trained with a language modeling objective possess ample world knowledge and language skills, but are known to struggle in tasks that require reasoning. In this work, we propose to leverage semi-structured tables, and automatically generate at scale question-paragraph pairs, where answering the question requires reasoning over multiple facts in the paragraph. We add a pre-training step over this synthetic data, which includes examples that require 16 different reasoning skills such as number comparison, conjunction, and fact composition. To improve data efficiency, we sample examples from reasoning skills where the model currently errs. We evaluate our approach on three reasoning-focused reading comprehension datasets, and show that our model, PReasM, substantially outperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling examples based on model errors leads to faster training and higher performance.",
            "corpus_id": 235898880,
            "sentences": [
                {
                    "corpus_id": "235898880",
                    "title": "Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills",
                    "text": "Large pre-trained language models (LMs) Brown et al., 2020) have become the backbone of natural language processing in recent years. However, recent work has shown that they struggle in performing symbolic reasoning operations, such as composition or conjunction of facts (Talmor et al., , 2020, numerical operations (Wallace et al., 2019;Hidey et al., 2020), and quantification (Warstadt et al., 2019), without substantial amounts of additional data.\n\nPast work on improving reasoning in pre-trained models has taken two flavors: (a) adding specialized components for specific skills, like numerical and temporal reasoning (Ran et al., 2019;Gupta et al., 2020a;Khot et al., 2021;Chen et al., 2020a), or (b) generating synthetic examples at scale, for example, by using grammars or templates (Rozen \u2020 Work done while working at the Allen Institute for Artificial Intelligence. Figure 1: An example table and question-contextanswer triplets generated from the table as synthetic data. Each color corresponds to a different reasoning skill and colored cells are necessary to answer the question. The contexts shown are partial, such that the actual context contains the necessary information to answer the question and additional distractors. Answers are not necessarily extractive (e.g., date difference). Zhao et al., 2019;Andreas, 2020;Asai and Hajishirzi, 2020;Campagna et al., 2020), and question generation models (Alberti et al., 2019;Puri et al., 2020;Bartolo et al., 2021).\n\nIn this work, we take the latter approach and argue that semi-structured tables are a valuable resource for automatic generation of training data that can endow LMs with reasoning skills. Tables can be crawled from the web at scale, and cover a wide range of domains and topics. Moreover, their structured nature makes them amenable to automatic processes of data generation. Specifically, given a table, we use templates to generate reading comprehension (RC) examples, that is, questioncontext-answer triplets, where answering the question requires diverse types of reasoning over facts mentioned in the context. Fig. 1 shows an example table, and three generated question-context-answer examples",
                    "score": 0.4486043796076775,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 40,
                            "end": 59,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 317,
                            "end": 339,
                            "matchedPaperCorpusId": "202583694"
                        },
                        {
                            "start": 339,
                            "end": 358,
                            "matchedPaperCorpusId": "216553790"
                        },
                        {
                            "start": 379,
                            "end": 402,
                            "matchedPaperCorpusId": "202233787"
                        },
                        {
                            "start": 624,
                            "end": 642,
                            "matchedPaperCorpusId": "202788490"
                        },
                        {
                            "start": 662,
                            "end": 680,
                            "matchedPaperCorpusId": "221448158"
                        },
                        {
                            "start": 1305,
                            "end": 1323,
                            "matchedPaperCorpusId": "201660404"
                        },
                        {
                            "start": 1323,
                            "end": 1337,
                            "matchedPaperCorpusId": "128000127"
                        },
                        {
                            "start": 1337,
                            "end": 1363,
                            "matchedPaperCorpusId": "216035859"
                        },
                        {
                            "start": 1363,
                            "end": 1385,
                            "matchedPaperCorpusId": "214816511"
                        },
                        {
                            "start": 1418,
                            "end": 1440,
                            "matchedPaperCorpusId": "189762081"
                        },
                        {
                            "start": 1440,
                            "end": 1458,
                            "matchedPaperCorpusId": "211258652"
                        },
                        {
                            "start": 1458,
                            "end": 1479,
                            "matchedPaperCorpusId": "233296924"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85302734375
                }
            ],
            "relevance_judgement": 0.85302734375,
            "relevance_judgment_input_expanded": "# Title: Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Ori Yoran, Alon Talmor, Jonathan Berant\n## Abstract\nModels pre-trained with a language modeling objective possess ample world knowledge and language skills, but are known to struggle in tasks that require reasoning. In this work, we propose to leverage semi-structured tables, and automatically generate at scale question-paragraph pairs, where answering the question requires reasoning over multiple facts in the paragraph. We add a pre-training step over this synthetic data, which includes examples that require 16 different reasoning skills such as number comparison, conjunction, and fact composition. To improve data efficiency, we sample examples from reasoning skills where the model currently errs. We evaluate our approach on three reasoning-focused reading comprehension datasets, and show that our model, PReasM, substantially outperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling examples based on model errors leads to faster training and higher performance.\n## Introduction\nLarge pre-trained language models (LMs) Brown et al., 2020) have become the backbone of natural language processing in recent years. However, recent work has shown that they struggle in performing symbolic reasoning operations, such as composition or conjunction of facts (Talmor et al., , 2020, numerical operations (Wallace et al., 2019;Hidey et al., 2020), and quantification (Warstadt et al., 2019), without substantial amounts of additional data.\n\nPast work on improving reasoning in pre-trained models has taken two flavors: (a) adding specialized components for specific skills, like numerical and temporal reasoning (Ran et al., 2019;Gupta et al., 2020a;Khot et al., 2021;Chen et al., 2020a), or (b) generating synthetic examples at scale, for example, by using grammars or templates (Rozen \u2020 Work done while working at the Allen Institute for Artificial Intelligence. Figure 1: An example table and question-contextanswer triplets generated from the table as synthetic data. Each color corresponds to a different reasoning skill and colored cells are necessary to answer the question. The contexts shown are partial, such that the actual context contains the necessary information to answer the question and additional distractors. Answers are not necessarily extractive (e.g., date difference). Zhao et al., 2019;Andreas, 2020;Asai and Hajishirzi, 2020;Campagna et al., 2020), and question generation models (Alberti et al., 2019;Puri et al., 2020;Bartolo et al., 2021).\n\nIn this work, we take the latter approach and argue that semi-structured tables are a valuable resource for automatic generation of training data that can endow LMs with reasoning skills. Tables can be crawled from the web at scale, and cover a wide range of domains and topics. Moreover, their structured nature makes them amenable to automatic processes of data generation. Specifically, given a table, we use templates to generate reading comprehension (RC) examples, that is, questioncontext-answer triplets, where answering the question requires diverse types of reasoning over facts mentioned in the context. Fig. 1 shows an example table, and three generated question-context-answer examples",
            "reference_string": "[235898880 | Yoran et al. | 2021 | Citations: 54]"
        },
        {
            "title": "Understanding tables with intermediate pre-training",
            "venue": "Findings",
            "year": 2020,
            "reference_count": 58,
            "citation_count": 121,
            "influential_citation_count": 18,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.findings-emnlp.27.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.00571, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "117595858",
                    "name": "Julian Martin Eisenschlos"
                },
                {
                    "authorId": "48876302",
                    "name": "Syrine Krichene"
                },
                {
                    "authorId": "40150608",
                    "name": "Thomas M\u00fcller"
                }
            ],
            "abstract": "Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on textual entailment, table entailment is less well studied. We adapt TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize entailment. Motivated by the benefits of data augmentation, we create a balanced dataset of millions of automatically created training examples which are learned in an intermediate step prior to fine-tuning. This new data is not only useful for table entailment, but also for SQA (Iyyer et al., 2017), a sequential table QA task. To be able to use long examples as input of BERT models, we evaluate table pruning techniques as a pre-processing step to drastically improve the training and prediction efficiency at a moderate drop in accuracy. The different methods set the new state-of-the-art on the TabFact (Chen et al., 2020) and SQA datasets.",
            "corpus_id": 222090330,
            "sentences": [
                {
                    "corpus_id": "222090330",
                    "title": "Understanding tables with intermediate pre-training",
                    "text": "The most similar approach to ours appears in Xiong et al. (2020), replacing entities in Wikipedia by others with the same type for a MASK-LM model objective. We, on the one hand, take advantage of other rows in the table to produce plausible negatives, and also replace dates and numbers. Recently, Kaushik et al. Numeric reasoning Numeric reasoning in Natural Language processing has been recognized as an important part in entailment models (Sammons et al., 2010) and reading comprehension (Ran et al., 2019). Wallace et al. (2019) studied the capacity of different models on understanding numerical operations and show that BERT-based model still have headroom. This motivates the use of the synthetic generation approach to improve numerical reasoning in our model.\n\nSynthetic data generation Synthetic data has been used to improve learning in NLP tasks (Alberti et al., 2019;Lewis et al., 2019;Wu et al., 2016;Leonandya et al., 2019). In semantic parsing for example (Wang et al., 2015;Iyer et al., 2017;Weir et al., 2020), templates are used to bootstrap models that map text to logical forms or SQL. Salvatore et al. (2019) use synthetic data generated from logical forms to evaluate the performance of textual entailment models (e.g., BERT). Geiger et al. (2019) use synthetic data to create fair evaluation sets for natural language inference. Geva et al. (2020) show the importance of injecting numerical reasoning via generated data into the model to solve reading comprehension tasks. They propose different templates for generating synthetic numerical examples. In our work we use a method that is better suited for tables and to the entailment task, and is arguably simpler.",
                    "score": 0.5624707358635029,
                    "section_title": "Counterfactual data generation",
                    "char_start_offset": 25077,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 45,
                            "end": 64,
                            "matchedPaperCorpusId": "209439872"
                        },
                        {
                            "start": 512,
                            "end": 533,
                            "matchedPaperCorpusId": "202583694"
                        },
                        {
                            "start": 859,
                            "end": 881,
                            "matchedPaperCorpusId": "189762081"
                        },
                        {
                            "start": 900,
                            "end": 916,
                            "matchedPaperCorpusId": "3099009"
                        },
                        {
                            "start": 973,
                            "end": 992,
                            "matchedPaperCorpusId": "14472576"
                        },
                        {
                            "start": 1010,
                            "end": 1028,
                            "matchedPaperCorpusId": "218981535"
                        },
                        {
                            "start": 1251,
                            "end": 1271,
                            "matchedPaperCorpusId": "202763794"
                        },
                        {
                            "start": 1354,
                            "end": 1372,
                            "matchedPaperCorpusId": "215548225"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8525390625
                }
            ],
            "relevance_judgement": 0.8525390625,
            "relevance_judgment_input_expanded": "# Title: Understanding tables with intermediate pre-training\n# Venue: Findings\n# Authors: Julian Martin Eisenschlos, Syrine Krichene, Thomas M\u00fcller\n## Abstract\nTable entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on textual entailment, table entailment is less well studied. We adapt TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize entailment. Motivated by the benefits of data augmentation, we create a balanced dataset of millions of automatically created training examples which are learned in an intermediate step prior to fine-tuning. This new data is not only useful for table entailment, but also for SQA (Iyyer et al., 2017), a sequential table QA task. To be able to use long examples as input of BERT models, we evaluate table pruning techniques as a pre-processing step to drastically improve the training and prediction efficiency at a moderate drop in accuracy. The different methods set the new state-of-the-art on the TabFact (Chen et al., 2020) and SQA datasets.\n## Counterfactual data generation\nThe most similar approach to ours appears in Xiong et al. (2020), replacing entities in Wikipedia by others with the same type for a MASK-LM model objective. We, on the one hand, take advantage of other rows in the table to produce plausible negatives, and also replace dates and numbers. Recently, Kaushik et al. Numeric reasoning Numeric reasoning in Natural Language processing has been recognized as an important part in entailment models (Sammons et al., 2010) and reading comprehension (Ran et al., 2019). Wallace et al. (2019) studied the capacity of different models on understanding numerical operations and show that BERT-based model still have headroom. This motivates the use of the synthetic generation approach to improve numerical reasoning in our model.\n\nSynthetic data generation Synthetic data has been used to improve learning in NLP tasks (Alberti et al., 2019;Lewis et al., 2019;Wu et al., 2016;Leonandya et al., 2019). In semantic parsing for example (Wang et al., 2015;Iyer et al., 2017;Weir et al., 2020), templates are used to bootstrap models that map text to logical forms or SQL. Salvatore et al. (2019) use synthetic data generated from logical forms to evaluate the performance of textual entailment models (e.g., BERT). Geiger et al. (2019) use synthetic data to create fair evaluation sets for natural language inference. Geva et al. (2020) show the importance of injecting numerical reasoning via generated data into the model to solve reading comprehension tasks. They propose different templates for generating synthetic numerical examples. In our work we use a method that is better suited for tables and to the entailment task, and is arguably simpler.",
            "reference_string": "[222090330 | Eisenschlos et al. | 2020 | Citations: 121]"
        },
        {
            "title": "Towards Efficient and Robust VQA-NLE Data Generation with Large Vision-Language Models",
            "venue": "International Conference on Computational Linguistics",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.14785, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306258046",
                    "name": "Patrick Amadeus Irawan"
                },
                {
                    "authorId": "9162688",
                    "name": "Genta Indra Winata"
                },
                {
                    "authorId": "2220548276",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "2257345523",
                    "name": "Ayu Purwarianti"
                }
            ],
            "abstract": "Natural Language Explanation (NLE) aims to elucidate the decision-making process by providing detailed, human-friendly explanations in natural language. It helps demystify the decision-making processes of large vision-language models (LVLMs) through the use of language models. While existing methods for creating a Vision Question-Answering with Natural Language Explanation (VQA-NLE) datasets can provide explanations, they heavily rely on human annotations that are time-consuming and costly. In this study, we propose a novel approach that leverages LVLMs to efficiently generate high-quality synthetic VQA-NLE datasets. By evaluating our synthetic data, we showcase how advanced prompting techniques can lead to the production of high-quality VQA-NLE data. Our findings indicate that this proposed method achieves up to 20x faster than human annotation, with only a minimal decrease in qualitative metrics, achieving robust quality that is nearly equivalent to human-annotated data. Furthermore, we show that incorporating visual prompts significantly enhances the relevance of text generation. Our study paves the way for a more efficient and robust automated generation of multi-modal NLE data, offering a promising solution to the problem.",
            "corpus_id": 272826678,
            "sentences": [
                {
                    "corpus_id": "272826678",
                    "title": "Towards Efficient and Robust VQA-NLE Data Generation with Large Vision-Language Models",
                    "text": "Explanation Generation in VQA Several studies have emphasized the generation of explanations, either manually or automatically. Manual approaches, such as VCR (Zellers et al., 2019b) and e-SNLI-VE (Do et al., 2021), employ human annotators to derive explanations from existing VQA datasets. In contrast, automatic methods like GQA-REX (Chen and Zhao, 2022) utilize functional programming, while VQA-E (Li et al., 2018) aligns and merges constituency parse trees from QA-caption pairs. Another automatic explanation generation framework, VQA-X (Park et al., 2018), employs separate answer and explanation models for generation. In this paper, we introduce a novel approach by proposing a unified model using single LVLM, eliminating multiple architectures need. \n\nNeural Synthetic Data Generation In the realm of multi-modal learning, particularly in the visionlanguage domain, the potential of synthetic data generation has been extensively explored. Li et al. (2023) discuss the application of synthetic data across various tasks and modalities. In computer vision, GAN-based models (Karras et al., 2019) and diffusion-based approaches (Nichol et al., 2022) are utilized for image synthesis. Within natural language processing domain, several studies (Kumar et al., 2021;Chung et al., 2023;Schmidt et al., 2024)",
                    "score": 0.4680714851228165,
                    "section_title": "Related Work",
                    "char_start_offset": 19158,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 128,
                            "end": 290
                        },
                        {
                            "start": 291,
                            "end": 484
                        },
                        {
                            "start": 485,
                            "end": 626
                        },
                        {
                            "start": 627,
                            "end": 760
                        },
                        {
                            "start": 763,
                            "end": 950
                        },
                        {
                            "start": 951,
                            "end": 1046
                        },
                        {
                            "start": 1047,
                            "end": 1192
                        },
                        {
                            "start": 1193,
                            "end": 1312
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1084,
                            "end": 1105,
                            "matchedPaperCorpusId": "234338081"
                        },
                        {
                            "start": 1272,
                            "end": 1291,
                            "matchedPaperCorpusId": "259096160"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8515625
                }
            ],
            "relevance_judgement": 0.8515625,
            "relevance_judgment_input_expanded": "# Title: Towards Efficient and Robust VQA-NLE Data Generation with Large Vision-Language Models\n# Venue: International Conference on Computational Linguistics\n# Authors: Patrick Amadeus Irawan, Genta Indra Winata, Samuel Cahyawijaya, Ayu Purwarianti\n## Abstract\nNatural Language Explanation (NLE) aims to elucidate the decision-making process by providing detailed, human-friendly explanations in natural language. It helps demystify the decision-making processes of large vision-language models (LVLMs) through the use of language models. While existing methods for creating a Vision Question-Answering with Natural Language Explanation (VQA-NLE) datasets can provide explanations, they heavily rely on human annotations that are time-consuming and costly. In this study, we propose a novel approach that leverages LVLMs to efficiently generate high-quality synthetic VQA-NLE datasets. By evaluating our synthetic data, we showcase how advanced prompting techniques can lead to the production of high-quality VQA-NLE data. Our findings indicate that this proposed method achieves up to 20x faster than human annotation, with only a minimal decrease in qualitative metrics, achieving robust quality that is nearly equivalent to human-annotated data. Furthermore, we show that incorporating visual prompts significantly enhances the relevance of text generation. Our study paves the way for a more efficient and robust automated generation of multi-modal NLE data, offering a promising solution to the problem.\n## Related Work\nExplanation Generation in VQA Several studies have emphasized the generation of explanations, either manually or automatically. Manual approaches, such as VCR (Zellers et al., 2019b) and e-SNLI-VE (Do et al., 2021), employ human annotators to derive explanations from existing VQA datasets. In contrast, automatic methods like GQA-REX (Chen and Zhao, 2022) utilize functional programming, while VQA-E (Li et al., 2018) aligns and merges constituency parse trees from QA-caption pairs. Another automatic explanation generation framework, VQA-X (Park et al., 2018), employs separate answer and explanation models for generation. In this paper, we introduce a novel approach by proposing a unified model using single LVLM, eliminating multiple architectures need. \n\nNeural Synthetic Data Generation In the realm of multi-modal learning, particularly in the visionlanguage domain, the potential of synthetic data generation has been extensively explored. Li et al. (2023) discuss the application of synthetic data across various tasks and modalities. In computer vision, GAN-based models (Karras et al., 2019) and diffusion-based approaches (Nichol et al., 2022) are utilized for image synthesis. Within natural language processing domain, several studies (Kumar et al., 2021;Chung et al., 2023;Schmidt et al., 2024)",
            "reference_string": "[272826678 | Irawan et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Evaluating Inter-Column Logical Relationships in Synthetic Tabular Data Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 20,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.04055, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323315765",
                    "name": "Yunbo Long"
                },
                {
                    "authorId": "2239053851",
                    "name": "Liming Xu"
                },
                {
                    "authorId": "2244621141",
                    "name": "Alexandra Brintrup"
                }
            ],
            "abstract": "Current evaluations of synthetic tabular data mainly focus on how well joint distributions are modeled, often overlooking the assessment of their effectiveness in preserving realistic event sequences and coherent entity relationships across columns.This paper proposes three evaluation metrics designed to assess the preservation of logical relationships among columns in synthetic tabular data. We validate these metrics by assessing the performance of both classical and state-of-the-art generation methods on a real-world industrial dataset.Experimental results reveal that existing methods often fail to rigorously maintain logical consistency (e.g., hierarchical relationships in geography or organization) and dependencies (e.g., temporal sequences or mathematical relationships), which are crucial for preserving the fine-grained realism of real-world tabular data. Building on these insights, this study also discusses possible pathways to better capture logical relationships while modeling the distribution of synthetic tabular data.",
            "corpus_id": 276161201,
            "sentences": [
                {
                    "corpus_id": "276161201",
                    "title": "Evaluating Inter-Column Logical Relationships in Synthetic Tabular Data Generation",
                    "text": "We introduce three metrics-HCS, MDI, and DSI-for evaluating inter-column logical relationship in synthetic tabular data generation. Our experiments show that existing methods often fail to strictly maintain hierarchical consistency and multivariate dependencies-essential characteristics of realworld datasets. Our future work will focus on enhancing the preservation of inter-column logical relationships in synthetic tabular data generation. For LLM-based methods, the column serialization format and order are crucial for the model's ability to learn the joint distribution of logically related features. Knowledge graphs (Dong & Wang, 2024) or Bayesian networks (Ling et al., 2024) would be employed to reorder tokenization sequences or restructure the serialization of columns in natural language, leveraging prior knowledge to guide the synthetic tabular data generation. For latent space-based methods, LLM reasoning (Hegselmann et al., 2023;Dong & Wang, 2024) can be utilized to analyze column names and descriptions, identifying semantic or logical relationships without prior knowledge. Additionally, inspired by CTSyn (Lin et al., 2024), grouping data by logical relationships and embedding them into a shared latent space could potentially capture inherent structures, improving joint distribution modeling. Lastly, incorporating interpolation techniques like SMOTE may help can help balance data classes (Yang et al., 2024), particularly in learning minority logical relationships. These directions are worthy to explore for designing generative models that effectively capture inter-column logical relationships in synthetic tabular data generation.",
                    "score": 0.457061236132535,
                    "section_title": "CONCLUSION AND RESEARCH DIRECTIONS",
                    "char_start_offset": 7572,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 310
                        },
                        {
                            "start": 311,
                            "end": 443
                        },
                        {
                            "start": 444,
                            "end": 607
                        },
                        {
                            "start": 608,
                            "end": 877
                        },
                        {
                            "start": 878,
                            "end": 1096
                        },
                        {
                            "start": 1097,
                            "end": 1319
                        },
                        {
                            "start": 1320,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1663
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 625,
                            "end": 644,
                            "matchedPaperCorpusId": "271114498"
                        },
                        {
                            "start": 924,
                            "end": 949,
                            "matchedPaperCorpusId": "252992811"
                        },
                        {
                            "start": 949,
                            "end": 966,
                            "matchedPaperCorpusId": "271114498"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85107421875
                }
            ],
            "relevance_judgement": 0.85107421875,
            "relevance_judgment_input_expanded": "# Title: Evaluating Inter-Column Logical Relationships in Synthetic Tabular Data Generation\n# Venue: arXiv.org\n# Authors: Yunbo Long, Liming Xu, Alexandra Brintrup\n## Abstract\nCurrent evaluations of synthetic tabular data mainly focus on how well joint distributions are modeled, often overlooking the assessment of their effectiveness in preserving realistic event sequences and coherent entity relationships across columns.This paper proposes three evaluation metrics designed to assess the preservation of logical relationships among columns in synthetic tabular data. We validate these metrics by assessing the performance of both classical and state-of-the-art generation methods on a real-world industrial dataset.Experimental results reveal that existing methods often fail to rigorously maintain logical consistency (e.g., hierarchical relationships in geography or organization) and dependencies (e.g., temporal sequences or mathematical relationships), which are crucial for preserving the fine-grained realism of real-world tabular data. Building on these insights, this study also discusses possible pathways to better capture logical relationships while modeling the distribution of synthetic tabular data.\n## CONCLUSION AND RESEARCH DIRECTIONS\nWe introduce three metrics-HCS, MDI, and DSI-for evaluating inter-column logical relationship in synthetic tabular data generation. Our experiments show that existing methods often fail to strictly maintain hierarchical consistency and multivariate dependencies-essential characteristics of realworld datasets. Our future work will focus on enhancing the preservation of inter-column logical relationships in synthetic tabular data generation. For LLM-based methods, the column serialization format and order are crucial for the model's ability to learn the joint distribution of logically related features. Knowledge graphs (Dong & Wang, 2024) or Bayesian networks (Ling et al., 2024) would be employed to reorder tokenization sequences or restructure the serialization of columns in natural language, leveraging prior knowledge to guide the synthetic tabular data generation. For latent space-based methods, LLM reasoning (Hegselmann et al., 2023;Dong & Wang, 2024) can be utilized to analyze column names and descriptions, identifying semantic or logical relationships without prior knowledge. Additionally, inspired by CTSyn (Lin et al., 2024), grouping data by logical relationships and embedding them into a shared latent space could potentially capture inherent structures, improving joint distribution modeling. Lastly, incorporating interpolation techniques like SMOTE may help can help balance data classes (Yang et al., 2024), particularly in learning minority logical relationships. These directions are worthy to explore for designing generative models that effectively capture inter-column logical relationships in synthetic tabular data generation.",
            "reference_string": "[276161201 | Long et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Hallucination Augmented Recitations for Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 40,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.07424, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2266763487",
                    "name": "Abdullatif K\u00f6ksal"
                },
                {
                    "authorId": "2205544066",
                    "name": "Renat Aksitov"
                },
                {
                    "authorId": "2152948655",
                    "name": "Chung-Ching Chang"
                }
            ],
            "abstract": "Attribution is a key concept in large language models (LLMs) as it enables control over information sources and enhances the factuality of LLMs. While existing approaches utilize open book question answering to improve attribution, factual datasets may reward language models to recall facts that they already know from their pretraining data, not attribution. In contrast, counterfactual open book QA datasets would further improve attribution because the answer could only be grounded in the given text. We propose Hallucination Augmented Recitations (HAR) for creating counterfactual datasets by utilizing hallucination in LLMs to improve attribution. For open book QA as a case study, we demonstrate that models finetuned with our counterfactual datasets improve text grounding, leading to better open book QA performance, with up to an 8.0% increase in F1 score. Our counterfactual dataset leads to significantly better performance than using humanannotated factual datasets, even with 4x smaller datasets and 4x smaller models. We observe that improvements are consistent across various model sizes and datasets, including multi-hop, biomedical, and adversarial QA datasets.",
            "corpus_id": 265150344,
            "sentences": [
                {
                    "corpus_id": "265150344",
                    "title": "Hallucination Augmented Recitations for Language Models",
                    "text": "To the best of our knowledge, we are the first ones to utilize LLM hallucination to create counterfactual datasets. Synthetic Data Generation: Synthetic question answering dataset generation without counterfactuals has shown limited improvement in out-ofdomain generalization (Bartolo et al., 2021;Lewis et al., 2021). However, recent advancements in large language models (LLMs) have led to growing interest in synthetic data generation with LLMs, such as in more generalized instruction tuning datasets from scratch (Wang et al., 2023) or by restructuring existing corpora (K\u00f6ksal et al., 2023). Synthetic data generation with LLMs has also been applied to existing datasets for specific tasks to improve model quality, such as natural language inference (Liu et al., 2022) and sentiment analysis (Meng et al., 2023).",
                    "score": 0.6072823695384437,
                    "section_title": "Related Work",
                    "char_start_offset": 21389,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 115
                        },
                        {
                            "start": 116,
                            "end": 318
                        },
                        {
                            "start": 319,
                            "end": 597
                        },
                        {
                            "start": 598,
                            "end": 819
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 276,
                            "end": 298,
                            "matchedPaperCorpusId": "233296924"
                        },
                        {
                            "start": 298,
                            "end": 317,
                            "matchedPaperCorpusId": "231924957"
                        },
                        {
                            "start": 518,
                            "end": 537,
                            "matchedPaperCorpusId": "254877310"
                        },
                        {
                            "start": 757,
                            "end": 775,
                            "matchedPaperCorpusId": "246016339"
                        },
                        {
                            "start": 799,
                            "end": 818,
                            "matchedPaperCorpusId": "253384628"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8486328125
                }
            ],
            "relevance_judgement": 0.8486328125,
            "relevance_judgment_input_expanded": "# Title: Hallucination Augmented Recitations for Language Models\n# Venue: arXiv.org\n# Authors: Abdullatif K\u00f6ksal, Renat Aksitov, Chung-Ching Chang\n## Abstract\nAttribution is a key concept in large language models (LLMs) as it enables control over information sources and enhances the factuality of LLMs. While existing approaches utilize open book question answering to improve attribution, factual datasets may reward language models to recall facts that they already know from their pretraining data, not attribution. In contrast, counterfactual open book QA datasets would further improve attribution because the answer could only be grounded in the given text. We propose Hallucination Augmented Recitations (HAR) for creating counterfactual datasets by utilizing hallucination in LLMs to improve attribution. For open book QA as a case study, we demonstrate that models finetuned with our counterfactual datasets improve text grounding, leading to better open book QA performance, with up to an 8.0% increase in F1 score. Our counterfactual dataset leads to significantly better performance than using humanannotated factual datasets, even with 4x smaller datasets and 4x smaller models. We observe that improvements are consistent across various model sizes and datasets, including multi-hop, biomedical, and adversarial QA datasets.\n## Related Work\nTo the best of our knowledge, we are the first ones to utilize LLM hallucination to create counterfactual datasets. Synthetic Data Generation: Synthetic question answering dataset generation without counterfactuals has shown limited improvement in out-ofdomain generalization (Bartolo et al., 2021;Lewis et al., 2021). However, recent advancements in large language models (LLMs) have led to growing interest in synthetic data generation with LLMs, such as in more generalized instruction tuning datasets from scratch (Wang et al., 2023) or by restructuring existing corpora (K\u00f6ksal et al., 2023). Synthetic data generation with LLMs has also been applied to existing datasets for specific tasks to improve model quality, such as natural language inference (Liu et al., 2022) and sentiment analysis (Meng et al., 2023).",
            "reference_string": "[265150344 | Koksal et al. | 2023 | Citations: 5]"
        },
        {
            "title": "Generating Synthetic Data for Neural Keyword-to-Question Models",
            "venue": "International Conference on the Theory of Information Retrieval",
            "year": 2018,
            "reference_count": 47,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1807.05324",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.05324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2113453574",
                    "name": "Heng Ding"
                },
                {
                    "authorId": "1680484",
                    "name": "K. Balog"
                }
            ],
            "abstract": "Search typically relies on keyword queries, but these are often semantically ambiguous. We propose to overcome this by offering users natural language questions, based on their keyword queries, to disambiguate their intent. This keyword-to-question task may be addressed using neural machine translation techniques. Neural translation models, however, require massive amounts of training data (keyword-question pairs), which is unavailable for this task. The main idea of this paper is to generate large amounts of synthetic training data from a small seed set of hand-labeled keyword-question pairs. Since natural language questions are available in large quantities, we develop models to automatically generate the corresponding keyword queries. Further, we introduce various filtering mechanisms to ensure that synthetic training data is of high quality. We demonstrate the feasibility of our approach using both automatic and manual evaluation.",
            "corpus_id": 49865372,
            "sentences": [
                {
                    "corpus_id": "49865372",
                    "title": "Generating Synthetic Data for Neural Keyword-to-Question Models",
                    "text": "Sect. 2). (2) We introduce several generative models for producing synthetic keyword queries from natural language questions (Sect. 3.1). (3) We develop two filtering mechanisms, which are essential for ensuring that the synthetic training data we feed into the neural network is of high-quality (Sect. 3.2). (4) We evaluate our synthetic data generation approach on the endto-end K2Q task using both automatic and manual evaluation (Sect. 6).",
                    "score": 0.4757768092357937,
                    "section_title": "Did you mean instead ? (Diverse Questions)",
                    "char_start_offset": 5348,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8349609375
                }
            ],
            "relevance_judgement": 0.8349609375,
            "relevance_judgment_input_expanded": "# Title: Generating Synthetic Data for Neural Keyword-to-Question Models\n# Venue: International Conference on the Theory of Information Retrieval\n# Authors: Heng Ding, K. Balog\n## Abstract\nSearch typically relies on keyword queries, but these are often semantically ambiguous. We propose to overcome this by offering users natural language questions, based on their keyword queries, to disambiguate their intent. This keyword-to-question task may be addressed using neural machine translation techniques. Neural translation models, however, require massive amounts of training data (keyword-question pairs), which is unavailable for this task. The main idea of this paper is to generate large amounts of synthetic training data from a small seed set of hand-labeled keyword-question pairs. Since natural language questions are available in large quantities, we develop models to automatically generate the corresponding keyword queries. Further, we introduce various filtering mechanisms to ensure that synthetic training data is of high quality. We demonstrate the feasibility of our approach using both automatic and manual evaluation.\n## Did you mean instead ? (Diverse Questions)\nSect. 2). (2) We introduce several generative models for producing synthetic keyword queries from natural language questions (Sect. 3.1). (3) We develop two filtering mechanisms, which are essential for ensuring that the synthetic training data we feed into the neural network is of high-quality (Sect. 3.2). (4) We evaluate our synthetic data generation approach on the endto-end K2Q task using both automatic and manual evaluation (Sect. 6).",
            "reference_string": "[49865372 | Ding et al. | 2018 | Citations: 7]"
        },
        {
            "title": "LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 78,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.12929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319385003",
                    "name": "Jin Jiang"
                },
                {
                    "authorId": "2284984220",
                    "name": "Yuchen Yan"
                },
                {
                    "authorId": "2316670312",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2321879179",
                    "name": "Yonggang Jin"
                },
                {
                    "authorId": "2072715089",
                    "name": "Shuai Peng"
                },
                {
                    "authorId": "2284132141",
                    "name": "Mengdi Zhang"
                },
                {
                    "authorId": "2317059694",
                    "name": "Xunliang Cai"
                },
                {
                    "authorId": "2319459835",
                    "name": "Yixin Cao"
                },
                {
                    "authorId": "2165982472",
                    "name": "Liangcai Gao"
                },
                {
                    "authorId": "2268046262",
                    "name": "Zhi Tang"
                }
            ],
            "abstract": "In this paper, we propose a new data synthesis method called \\textbf{LogicPro}, which leverages LeetCode-style algorithm \\underline{Pro}blems and their corresponding \\underline{Pro}gram solutions to synthesize Complex \\underline{Logic}al Reasoning data in text format. First, we synthesize complex reasoning problems through source algorithm problems and test cases. Then, standard answers and intermediate variable outputs are obtained for each problem based on standard python solutions and test cases. Finally, with the guidance of code intermediate variables, we synthesize the text reasoning process for each reasoning problems. Through this method, we can synthesize data that is difficult, scalable, effective, and comes with golden standard answers and high-quality reasoning processes. As a result, with our 540K synthesized dataset constructed solely from 2,360 algorithm problems, our approach Code and data are publicly available at https://github.com/jiangjin1999/LogicPro achieves significant improvements in multiple models for the datasets \\textit{BBH$^{27}$}, \\textit{LogicBench}, \\textit{DROP}, \\textit{AR-LSAT}, and \\textit{GSM8K}, etc. outperforming a wide range of existing reasoning datasets.",
            "corpus_id": 272753174,
            "sentences": [
                {
                    "corpus_id": "272753174",
                    "title": "LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning",
                    "text": "Synthetic data has played an important role in enhancing LLMs' reasoning capabilities (Dubey et al., 2024), especially in mathematics and coding domains. \n\nFor mathematical reasoning, synthetic data generation includes problem-driven methods such as evol-instruct (Luo et al., 2023a;Zeng et al., 2024), problem restatement (Yu et al., 2023), backtranslation (Lu et al., 2024), and few-shot examples (Dalvi et al., 2021), as well as knowledge-driven methods that rely on knowledge bases (Dalvi et al., 2021) or concept graphs (Tang et al., 2024) to generate new problems by sampling key reasoning points (Huang et al., 2024). \n\nIn terms of code reasoning, from Code Alpaca's (Wang et al., 2023) use of self-instruct to generate 20K instruction data based on seed tasks, to Wiz-ardCoder's (Luo et al., 2023b) use of Code evolinstruct to generate more complex tasks, to Magicoder's use of oss-instruct (Wei et al., 2024) to extract 75K instructions from open source code. Synthesizing data continuously improves the model's code reasoning capabilities. \n\nIn contrast, there is less research on synthetic data for complex logical reasoning. Learning combines formal logic theory (Morishita et al., 2023) to synthesize data using basic reasoning rules to train language models' multi-step reasoning abilities. LogicBench (Parmar et al., 2024) not only constructs logical reasoning benchmark datasets but also provides synthetic data based on formal logic. \n\nLeetCode-style algorithm problems contain rich reasoning processes. This paper synthesizes highquality complex logical sreasoning data based on the formal logic of programming languages to enhance models' reasoning capabilities.",
                    "score": 0.5559595643864345,
                    "section_title": "A Related work A.1 Synthetic Reasoning Data",
                    "char_start_offset": 25934,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 153
                        },
                        {
                            "start": 156,
                            "end": 624
                        },
                        {
                            "start": 627,
                            "end": 968
                        },
                        {
                            "start": 969,
                            "end": 1049
                        },
                        {
                            "start": 1052,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1304
                        },
                        {
                            "start": 1305,
                            "end": 1450
                        },
                        {
                            "start": 1453,
                            "end": 1520
                        },
                        {
                            "start": 1521,
                            "end": 1681
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 399,
                            "end": 419,
                            "matchedPaperCorpusId": "233297051"
                        },
                        {
                            "start": 486,
                            "end": 506,
                            "matchedPaperCorpusId": "233297051"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8291015625
                }
            ],
            "relevance_judgement": 0.8291015625,
            "relevance_judgment_input_expanded": "# Title: LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning\n# Venue: arXiv.org\n# Authors: Jin Jiang, Yuchen Yan, Yang Liu, Yonggang Jin, Shuai Peng, Mengdi Zhang, Xunliang Cai, Yixin Cao, Liangcai Gao, Zhi Tang\n## Abstract\nIn this paper, we propose a new data synthesis method called \\textbf{LogicPro}, which leverages LeetCode-style algorithm \\underline{Pro}blems and their corresponding \\underline{Pro}gram solutions to synthesize Complex \\underline{Logic}al Reasoning data in text format. First, we synthesize complex reasoning problems through source algorithm problems and test cases. Then, standard answers and intermediate variable outputs are obtained for each problem based on standard python solutions and test cases. Finally, with the guidance of code intermediate variables, we synthesize the text reasoning process for each reasoning problems. Through this method, we can synthesize data that is difficult, scalable, effective, and comes with golden standard answers and high-quality reasoning processes. As a result, with our 540K synthesized dataset constructed solely from 2,360 algorithm problems, our approach Code and data are publicly available at https://github.com/jiangjin1999/LogicPro achieves significant improvements in multiple models for the datasets \\textit{BBH$^{27}$}, \\textit{LogicBench}, \\textit{DROP}, \\textit{AR-LSAT}, and \\textit{GSM8K}, etc. outperforming a wide range of existing reasoning datasets.\n## A Related work A.1 Synthetic Reasoning Data\nSynthetic data has played an important role in enhancing LLMs' reasoning capabilities (Dubey et al., 2024), especially in mathematics and coding domains. \n\nFor mathematical reasoning, synthetic data generation includes problem-driven methods such as evol-instruct (Luo et al., 2023a;Zeng et al., 2024), problem restatement (Yu et al., 2023), backtranslation (Lu et al., 2024), and few-shot examples (Dalvi et al., 2021), as well as knowledge-driven methods that rely on knowledge bases (Dalvi et al., 2021) or concept graphs (Tang et al., 2024) to generate new problems by sampling key reasoning points (Huang et al., 2024). \n\nIn terms of code reasoning, from Code Alpaca's (Wang et al., 2023) use of self-instruct to generate 20K instruction data based on seed tasks, to Wiz-ardCoder's (Luo et al., 2023b) use of Code evolinstruct to generate more complex tasks, to Magicoder's use of oss-instruct (Wei et al., 2024) to extract 75K instructions from open source code. Synthesizing data continuously improves the model's code reasoning capabilities. \n\nIn contrast, there is less research on synthetic data for complex logical reasoning. Learning combines formal logic theory (Morishita et al., 2023) to synthesize data using basic reasoning rules to train language models' multi-step reasoning abilities. LogicBench (Parmar et al., 2024) not only constructs logical reasoning benchmark datasets but also provides synthetic data based on formal logic. \n\nLeetCode-style algorithm problems contain rich reasoning processes. This paper synthesizes highquality complex logical sreasoning data based on the formal logic of programming languages to enhance models' reasoning capabilities.",
            "reference_string": "[272753174 | Jiang et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2024,
            "reference_count": 222,
            "citation_count": 81,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17944, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2288381545",
                    "name": "Xi Fang"
                },
                {
                    "authorId": "2110546424",
                    "name": "Weijie Xu"
                },
                {
                    "authorId": "2287992265",
                    "name": "F. Tan"
                },
                {
                    "authorId": "2258747730",
                    "name": "Jiani Zhang"
                },
                {
                    "authorId": "2287978009",
                    "name": "Ziqing Hu"
                },
                {
                    "authorId": "2288038766",
                    "name": "Yanjun Qi"
                },
                {
                    "authorId": "2287927391",
                    "name": "Scott Nickleach"
                },
                {
                    "authorId": "2287926360",
                    "name": "Diego Socolinsky"
                },
                {
                    "authorId": "2287927370",
                    "name": "Srinivasan H. Sengamedu"
                },
                {
                    "authorId": "2263543517",
                    "name": "Christos Faloutsos"
                }
            ],
            "abstract": "Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.",
            "corpus_id": 268041519,
            "sentences": [
                {
                    "corpus_id": "268041519",
                    "title": "Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
                    "text": "The ability to synthesize real and highquality data is essential for model development. Data generation is used for augmentation when the data is sparse (Onishi & Meguro, 2023), imputing missing values (Jolicoeur-Martineau et al., 2023), and class rebalancing in imbalanced data (Sauber-Cole & Khoshgoftaar, 2022). Traditional methods for synthetic data generation are mostly based on Copulas (Patki et al., 2016;Li et al., 2020b) and Bayesian networks (Zhang et al., 2017;Madl et al., 2023) while recent advancement in generative models such as Variational Autoencoders (VAEs) (Ma et al., 2020;Darabi & Elor, 2021;Vardhan & Kok, 2020;Liu et al., 2023d;Xu et al., 2023b)), generative adversarial networks (GANs) (Park et al., 2018;Choi et al., 2018;Baowaly et al., 2019;Xu et al., 2019), diffusion models (Kotelnikov et al., 2022;Xu et al., 2023a;Kim et al., 2022b;a;Lee et al., 2023;Zhang et al., 2023c), and LLMs, opened up many new opportunities. These deep learning approaches have demonstrated superior performance over classical methods such as Bayesian networks ( (Xu et al., 2019)). A comprehensive understanding of the strengths and weaknesses of different tabular data synthesis methods can be found in Du & Li (2024). \n\nTable understanding is a broad field, covering various tasks like question answering (QA), natural language inference (NLI), Text2SQL tasks, and more.",
                    "score": 0.5038857718493599,
                    "section_title": "Traditional and deep learning in tabular data",
                    "char_start_offset": 9697,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 87
                        },
                        {
                            "start": 88,
                            "end": 314
                        },
                        {
                            "start": 315,
                            "end": 949
                        },
                        {
                            "start": 950,
                            "end": 1090
                        },
                        {
                            "start": 1091,
                            "end": 1228
                        },
                        {
                            "start": 1231,
                            "end": 1381
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 279,
                            "end": 313,
                            "matchedPaperCorpusId": "251725737"
                        },
                        {
                            "start": 393,
                            "end": 413,
                            "matchedPaperCorpusId": "9027681"
                        },
                        {
                            "start": 453,
                            "end": 473,
                            "matchedPaperCorpusId": "13095103"
                        },
                        {
                            "start": 615,
                            "end": 635,
                            "matchedPaperCorpusId": "229131432"
                        },
                        {
                            "start": 635,
                            "end": 653,
                            "matchedPaperCorpusId": "259298730"
                        },
                        {
                            "start": 712,
                            "end": 731,
                            "matchedPaperCorpusId": "47017667"
                        },
                        {
                            "start": 749,
                            "end": 770,
                            "matchedPaperCorpusId": "54479855"
                        },
                        {
                            "start": 830,
                            "end": 847,
                            "matchedPaperCorpusId": "264439324"
                        },
                        {
                            "start": 847,
                            "end": 865,
                            "matchedPaperCorpusId": "249847841"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82861328125
                }
            ],
            "relevance_judgement": 0.82861328125,
            "relevance_judgment_input_expanded": "# Title: Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Xi Fang, Weijie Xu, F. Tan, Jiani Zhang, Ziqing Hu, Yanjun Qi, Scott Nickleach, Diego Socolinsky, Srinivasan H. Sengamedu, Christos Faloutsos\n## Abstract\nRecent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.\n## Traditional and deep learning in tabular data\nThe ability to synthesize real and highquality data is essential for model development. Data generation is used for augmentation when the data is sparse (Onishi & Meguro, 2023), imputing missing values (Jolicoeur-Martineau et al., 2023), and class rebalancing in imbalanced data (Sauber-Cole & Khoshgoftaar, 2022). Traditional methods for synthetic data generation are mostly based on Copulas (Patki et al., 2016;Li et al., 2020b) and Bayesian networks (Zhang et al., 2017;Madl et al., 2023) while recent advancement in generative models such as Variational Autoencoders (VAEs) (Ma et al., 2020;Darabi & Elor, 2021;Vardhan & Kok, 2020;Liu et al., 2023d;Xu et al., 2023b)), generative adversarial networks (GANs) (Park et al., 2018;Choi et al., 2018;Baowaly et al., 2019;Xu et al., 2019), diffusion models (Kotelnikov et al., 2022;Xu et al., 2023a;Kim et al., 2022b;a;Lee et al., 2023;Zhang et al., 2023c), and LLMs, opened up many new opportunities. These deep learning approaches have demonstrated superior performance over classical methods such as Bayesian networks ( (Xu et al., 2019)). A comprehensive understanding of the strengths and weaknesses of different tabular data synthesis methods can be found in Du & Li (2024). \n\nTable understanding is a broad field, covering various tasks like question answering (QA), natural language inference (NLI), Text2SQL tasks, and more.",
            "reference_string": "[268041519 | Fang et al. | 2024 | Citations: 81]"
        },
        {
            "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 62,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.01281, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2203079037",
                    "name": "Sakhinana Sagar Srinivas"
                },
                {
                    "authorId": "2139833562",
                    "name": "Venkataramana Runkana"
                }
            ],
            "abstract": "We present a comprehensive framework for enhancing Retrieval-Augmented Generation (RAG) systems through dynamic retrieval strategies and reinforcement fine-tuning. This approach significantly improves large language models on knowledge-intensive tasks, including opendomain question answering and complex reasoning. Our framework integrates two complementary techniques: Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS), which dynamically determines retrieval timing and content based on contextual needs. Together, these techniques enhance both the utilization and relevance of retrieved content, improving factual accuracy and response quality. Designed as a lightweight solution compatible with any Transformer-based LLM without requiring additional training, our framework excels in knowledge-intensive tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a novel method to selectively compress key-value caches by token importance, mitigating memory bottlenecks in long-context applications. The framework also incorporates test-time scaling techniques to dynamically balance reasoning depth and computational resources, alongside optimized decoding strategies for faster inference. Experiments on benchmark datasets show that our framework reduces hallucinations, strengthens domain-specific reasoning, and achieves significant efficiency and scalability gains over traditional RAG systems. This integrated approach advances the development of robust, efficient, and scalable RAG systems across diverse applications.",
            "corpus_id": 277501853,
            "sentences": [
                {
                    "corpus_id": "277501853",
                    "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
                    "text": "We utilize Monte Carlo Tree Search (MCTS) (Tang et al., 2024;Xie et al., 2024;Gao et al., 2024;Feng et al., 2023;Zhang et al., 2024a) for improved reasoning-driven response generation in large language models (LLMs), especially for complex, multi-step language tasks where traditional methods often fall short. MCTS offers a framework for language models to engage in structured thinking, logical inference, and multi-step problem-solving, enabling capabilities such as hypothetical and counterfactual reasoning, commonsense and causal reasoning, and multisource, multi-hop question answering with RAG. By formulating reasoning-driven response generation as a sequential decision-making problem, we demonstrate how MCTS can systematically explore the vast space of potential responses to identify optimal outputs for a given end-user query. This systematic exploration is particularly crucial when dealing with complex queries that require intricate reasoning and planning over multiple steps. Our methodology leverages the inherent uncertainty in language generation and provides a principled way to balance exploration of diverse responses with exploitation of high-quality language patterns. MCTS demonstrates significant improvements in response quality, coherence, and relevance compared to traditional sampling and beam search methods, which are often inadequate for navigating the complexities of multi-step reasoning. We formulate reasoning-driven response generation as a search problem within a state space that evolves with the generation process. Let s \u2208 S denote a state in the generation process, where S represents the set of all possible states the generation process can assume. Each state s is formally defined as: \n\nHere, p \u2208 P is the system prompt, which serves to guide and condition the language model's behavior. P represents the entire set of possible system prompts that can be used. Next, q \u2208 Q denotes the current user query, which is the latest input to the language model. Q is the set encompassing all possible queries a user might pose. Finally, h = ((r 1 , c 1 ), (r 2 , c 2 ), . . . , (r n , c n )) \u2208 H represents the generation history up to the current point.",
                    "score": 0.4529969235647129,
                    "section_title": "D.10. Monte Carlo Tree Search Algorithm",
                    "char_start_offset": 115463,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 310
                        },
                        {
                            "start": 311,
                            "end": 602
                        },
                        {
                            "start": 603,
                            "end": 840
                        },
                        {
                            "start": 841,
                            "end": 993
                        },
                        {
                            "start": 994,
                            "end": 1194
                        },
                        {
                            "start": 1195,
                            "end": 1425
                        },
                        {
                            "start": 1426,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1695
                        },
                        {
                            "start": 1696,
                            "end": 1732
                        },
                        {
                            "start": 1735,
                            "end": 1835
                        },
                        {
                            "start": 1836,
                            "end": 1908
                        },
                        {
                            "start": 1909,
                            "end": 2001
                        },
                        {
                            "start": 2002,
                            "end": 2067
                        },
                        {
                            "start": 2068,
                            "end": 2115
                        },
                        {
                            "start": 2116,
                            "end": 2194
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82275390625
                }
            ],
            "relevance_judgement": 0.82275390625,
            "relevance_judgment_input_expanded": "# Title: Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding\n# Venue: arXiv.org\n# Authors: Sakhinana Sagar Srinivas, Venkataramana Runkana\n## Abstract\nWe present a comprehensive framework for enhancing Retrieval-Augmented Generation (RAG) systems through dynamic retrieval strategies and reinforcement fine-tuning. This approach significantly improves large language models on knowledge-intensive tasks, including opendomain question answering and complex reasoning. Our framework integrates two complementary techniques: Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS), which dynamically determines retrieval timing and content based on contextual needs. Together, these techniques enhance both the utilization and relevance of retrieved content, improving factual accuracy and response quality. Designed as a lightweight solution compatible with any Transformer-based LLM without requiring additional training, our framework excels in knowledge-intensive tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a novel method to selectively compress key-value caches by token importance, mitigating memory bottlenecks in long-context applications. The framework also incorporates test-time scaling techniques to dynamically balance reasoning depth and computational resources, alongside optimized decoding strategies for faster inference. Experiments on benchmark datasets show that our framework reduces hallucinations, strengthens domain-specific reasoning, and achieves significant efficiency and scalability gains over traditional RAG systems. This integrated approach advances the development of robust, efficient, and scalable RAG systems across diverse applications.\n## D.10. Monte Carlo Tree Search Algorithm\nWe utilize Monte Carlo Tree Search (MCTS) (Tang et al., 2024;Xie et al., 2024;Gao et al., 2024;Feng et al., 2023;Zhang et al., 2024a) for improved reasoning-driven response generation in large language models (LLMs), especially for complex, multi-step language tasks where traditional methods often fall short. MCTS offers a framework for language models to engage in structured thinking, logical inference, and multi-step problem-solving, enabling capabilities such as hypothetical and counterfactual reasoning, commonsense and causal reasoning, and multisource, multi-hop question answering with RAG. By formulating reasoning-driven response generation as a sequential decision-making problem, we demonstrate how MCTS can systematically explore the vast space of potential responses to identify optimal outputs for a given end-user query. This systematic exploration is particularly crucial when dealing with complex queries that require intricate reasoning and planning over multiple steps. Our methodology leverages the inherent uncertainty in language generation and provides a principled way to balance exploration of diverse responses with exploitation of high-quality language patterns. MCTS demonstrates significant improvements in response quality, coherence, and relevance compared to traditional sampling and beam search methods, which are often inadequate for navigating the complexities of multi-step reasoning. We formulate reasoning-driven response generation as a search problem within a state space that evolves with the generation process. Let s \u2208 S denote a state in the generation process, where S represents the set of all possible states the generation process can assume. Each state s is formally defined as: \n\nHere, p \u2208 P is the system prompt, which serves to guide and condition the language model's behavior. P represents the entire set of possible system prompts that can be used. Next, q \u2208 Q denotes the current user query, which is the latest input to the language model. Q is the set encompassing all possible queries a user might pose. Finally, h = ((r 1 , c 1 ), (r 2 , c 2 ), . . . , (r n , c n )) \u2208 H represents the generation history up to the current point.",
            "reference_string": "[277501853 | Srinivas et al. | 2025 | Citations: 1]"
        },
        {
            "title": "On Memorization of Large Language Models in Logical Reasoning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 86,
            "citation_count": 41,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.23123, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308424593",
                    "name": "Chulin Xie"
                },
                {
                    "authorId": "2283305597",
                    "name": "Yangsibo Huang"
                },
                {
                    "authorId": "2309481623",
                    "name": "Chiyuan Zhang"
                },
                {
                    "authorId": "2290010050",
                    "name": "Da Yu"
                },
                {
                    "authorId": "2328442132",
                    "name": "Xinyun Chen"
                },
                {
                    "authorId": "2328296250",
                    "name": "Bill Yuchen Lin"
                },
                {
                    "authorId": "2290141407",
                    "name": "Bo Li"
                },
                {
                    "authorId": "2529354",
                    "name": "Badih Ghazi"
                },
                {
                    "authorId": "2237408905",
                    "name": "Ravi Kumar"
                }
            ],
            "abstract": "Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs' reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems. In this paper, we systematically investigate this hypothesis with a quantitative measurement of memorization in reasoning tasks, using a dynamically generated logical reasoning benchmark based on Knights and Knaves (K&K) puzzles. We find that LLMs could interpolate and memorize the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet they struggle with slight variations of these puzzles. On the other hand, we show that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. Through in-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers, we establish that LLMs develop reasoning skills on K&K puzzles alongside memorization. Finally, our analysis based on a per-sample memorization score sheds light on how LLMs switch between reasoning and memorization when solving logical puzzles. Our code and data are available at https://memkklogic.github.io.",
            "corpus_id": 273695832,
            "sentences": [
                {
                    "corpus_id": "273695832",
                    "title": "On Memorization of Large Language Models in Logical Reasoning",
                    "text": "Logical reasoning benchmarks. To evaluate logical reasoning capabilities in LLMs, synthetic benchmarks have been developed to enable scalable generation of samples with varying configurations and difficulty levels (Clark et al., 2020;Giadikiaroglou et al., 2024;Parmar et al., 2024). For instance, DyVal (Zhu et al., 2024) uses directed acyclic graphs to dynamically generate samples on reasoning tasks including deductive, Boolean, and abductive reasoning. (Chen et al., 2024) focus on propositional logical problems involving definite clauses, and synthetically generate variations with different premise orders, such as forward, backward, and shuffled. (Dziri et al., 2024) explore the limitations of LLMs in tasks requiring compositional reasoning, including multiplication, logic grid puzzles, and dynamic programming problems. ZebraLogic (Lin et al., 2024) is an extended benchmark that systematically tests logical reasoning capabilities. BoardgameQA (Kazemi et al., 2024) presents a question-answering dataset characterized by contradictory facts and rules in the questions. PRONTOQA (Saparov & He, 2023) is a synthetic question-answering dataset where each example is generated from a synthetic world model represented in first-order logic. This dataset enables parsing the generated chain of thought into symbolic proofs, facilitating formal analysis. TruthQuest (Mondorf & Plank, 2024) is the most similar task to our work, which provides evaluation samples based on K&K-type of puzzles involving 3-6 person. Our work provides more comprehensive dynamic set of K&K puzzles that support automatic generation of perturbations, solutions and detailed reasoning steps. Moreover, based on this benchmark, we define and measure memorization in reasoning tasks, revealing intricate interplay between memorization and reasoning in LLMs. \n\nImproving reasoning via fine-tuning. Prior work has explored fine-tuning LLMs on synthetic reasoning data to enhance their performance on reasoning. DyVal (Zhu et al., 2024) shows that fine-tuning Llama2-13B-chat on their synthetic reasoning benchmark improves its performance on other popular reasoning benchmarks.",
                    "score": 0.514781688258103,
                    "section_title": "Conclusion",
                    "char_start_offset": 30685,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 29
                        },
                        {
                            "start": 30,
                            "end": 283
                        },
                        {
                            "start": 284,
                            "end": 457
                        },
                        {
                            "start": 458,
                            "end": 655
                        },
                        {
                            "start": 656,
                            "end": 832
                        },
                        {
                            "start": 833,
                            "end": 945
                        },
                        {
                            "start": 946,
                            "end": 1082
                        },
                        {
                            "start": 1083,
                            "end": 1249
                        },
                        {
                            "start": 1250,
                            "end": 1361
                        },
                        {
                            "start": 1362,
                            "end": 1519
                        },
                        {
                            "start": 1520,
                            "end": 1675
                        },
                        {
                            "start": 1676,
                            "end": 1839
                        },
                        {
                            "start": 1842,
                            "end": 1878
                        },
                        {
                            "start": 1879,
                            "end": 1990
                        },
                        {
                            "start": 1991,
                            "end": 2157
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 214,
                            "end": 234,
                            "matchedPaperCorpusId": "211126663"
                        },
                        {
                            "start": 234,
                            "end": 262,
                            "matchedPaperCorpusId": "267751102"
                        },
                        {
                            "start": 262,
                            "end": 282,
                            "matchedPaperCorpusId": "269330143"
                        },
                        {
                            "start": 458,
                            "end": 477,
                            "matchedPaperCorpusId": "267657940"
                        },
                        {
                            "start": 958,
                            "end": 979,
                            "matchedPaperCorpusId": "259144942"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82080078125
                }
            ],
            "relevance_judgement": 0.82080078125,
            "relevance_judgment_input_expanded": "# Title: On Memorization of Large Language Models in Logical Reasoning\n# Venue: arXiv.org\n# Authors: Chulin Xie, Yangsibo Huang, Chiyuan Zhang, Da Yu, Xinyun Chen, Bill Yuchen Lin, Bo Li, Badih Ghazi, Ravi Kumar\n## Abstract\nLarge language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs' reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems. In this paper, we systematically investigate this hypothesis with a quantitative measurement of memorization in reasoning tasks, using a dynamically generated logical reasoning benchmark based on Knights and Knaves (K&K) puzzles. We find that LLMs could interpolate and memorize the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet they struggle with slight variations of these puzzles. On the other hand, we show that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. Through in-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers, we establish that LLMs develop reasoning skills on K&K puzzles alongside memorization. Finally, our analysis based on a per-sample memorization score sheds light on how LLMs switch between reasoning and memorization when solving logical puzzles. Our code and data are available at https://memkklogic.github.io.\n## Conclusion\nLogical reasoning benchmarks. To evaluate logical reasoning capabilities in LLMs, synthetic benchmarks have been developed to enable scalable generation of samples with varying configurations and difficulty levels (Clark et al., 2020;Giadikiaroglou et al., 2024;Parmar et al., 2024). For instance, DyVal (Zhu et al., 2024) uses directed acyclic graphs to dynamically generate samples on reasoning tasks including deductive, Boolean, and abductive reasoning. (Chen et al., 2024) focus on propositional logical problems involving definite clauses, and synthetically generate variations with different premise orders, such as forward, backward, and shuffled. (Dziri et al., 2024) explore the limitations of LLMs in tasks requiring compositional reasoning, including multiplication, logic grid puzzles, and dynamic programming problems. ZebraLogic (Lin et al., 2024) is an extended benchmark that systematically tests logical reasoning capabilities. BoardgameQA (Kazemi et al., 2024) presents a question-answering dataset characterized by contradictory facts and rules in the questions. PRONTOQA (Saparov & He, 2023) is a synthetic question-answering dataset where each example is generated from a synthetic world model represented in first-order logic. This dataset enables parsing the generated chain of thought into symbolic proofs, facilitating formal analysis. TruthQuest (Mondorf & Plank, 2024) is the most similar task to our work, which provides evaluation samples based on K&K-type of puzzles involving 3-6 person. Our work provides more comprehensive dynamic set of K&K puzzles that support automatic generation of perturbations, solutions and detailed reasoning steps. Moreover, based on this benchmark, we define and measure memorization in reasoning tasks, revealing intricate interplay between memorization and reasoning in LLMs. \n\nImproving reasoning via fine-tuning. Prior work has explored fine-tuning LLMs on synthetic reasoning data to enhance their performance on reasoning. DyVal (Zhu et al., 2024) shows that fine-tuning Llama2-13B-chat on their synthetic reasoning benchmark improves its performance on other popular reasoning benchmarks.",
            "reference_string": "[273695832 | Xie et al. | 2024 | Citations: 41]"
        },
        {
            "title": "The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2016,
            "reference_count": 52,
            "citation_count": 58,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00113",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1710.03838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2099597",
                    "name": "D. Wang"
                },
                {
                    "authorId": "145043214",
                    "name": "Jason Eisner"
                }
            ],
            "abstract": "We release Galactic Dependencies 1.0\u2014a large set of synthetic languages not found on Earth, but annotated in Universal Dependencies format. This new resource aims to provide training and development data for NLP methods that aim to adapt to unfamiliar languages. Each synthetic treebank is produced from a real treebank by stochastically permuting the dependents of nouns and/or verbs to match the word order of other real languages. We discuss the usefulness, realism, parsability, perplexity, and diversity of the synthetic languages. As a simple demonstration of the use of Galactic Dependencies, we consider single-source transfer, which attempts to parse a real target language using a parser trained on a \u201cnearby\u201d source language. We find that including synthetic source languages somewhat increases the diversity of the source pool, which significantly improves results for most target languages.",
            "corpus_id": 10817864,
            "sentences": [
                {
                    "corpus_id": "10817864",
                    "title": "The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages",
                    "text": "Synthetic data generation is a well-known trick for effectively training a large model on a small dataset. Abu-Mostafa (1995) reviews early work that provided \"hints\" to a learning system in the form of virtual training examples. While datasets have grown in recent years, so have models: e.g., neural networks have many parameters to train. Thus, it is still common to create synthetic training examples-often by adding noise to real inputs or otherwise transforming them in ways that are expected to preserve their labels. Domains where it is easy to exploit these invariances include image recognition (Simard et al., 2003;Krizhevsky et al., 2012), speech recognition (Jaitly and Hinton, 2013;Cui et al., 2015), information retrieval (Vilares et al., 2011), and grammatical error correction (Rozovskaya and Roth, 2010). \n\nSynthetic datasets have also arisen recently for semantic tasks in natural language processing. bAbI is a dataset of facts, questions, and answers, generated by random simulation, for training machines to do simple logic (Weston et al., 2016). Hermann et al. (2015) generate reading comprehension questions and their answers, based on a large set of newssummarization pairs, for training machine readers. Serban et al. (2016) used RNNs to generate 30 million factoid questions about Freebase, with answers, for training question-answering systems. Wang et al. (2015) obtain data to train semantic parsers in a new domain by first generating synthetic (utterance, logical form) pairs and then asking human annotators to paraphrase the synthetic utterances into more natural human language. \n\nIn speech recognition, morphology-based \"vocabulary expansion\" creates synthetic word forms (Rasooli et al., 2014;Varjokallio and Klakow, 2016). \n\nMachine translation researchers have often tried to automatically preprocess parse trees of a source language to more closely resemble those of the target language, using either hand-crafted or automatically extracted rules (Dorr et al., 2002;Collins et al., 2005, etc.; see review by Howlett and Dras, 2011).",
                    "score": 0.6019626798669611,
                    "section_title": "Related Work",
                    "char_start_offset": 3552,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 106
                        },
                        {
                            "start": 107,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 341
                        },
                        {
                            "start": 342,
                            "end": 524
                        },
                        {
                            "start": 525,
                            "end": 822
                        },
                        {
                            "start": 825,
                            "end": 920
                        },
                        {
                            "start": 921,
                            "end": 1229
                        },
                        {
                            "start": 1230,
                            "end": 1372
                        },
                        {
                            "start": 1373,
                            "end": 1613
                        },
                        {
                            "start": 1616,
                            "end": 1760
                        },
                        {
                            "start": 1763,
                            "end": 2072
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 605,
                            "end": 626,
                            "matchedPaperCorpusId": "4659176"
                        },
                        {
                            "start": 626,
                            "end": 650,
                            "matchedPaperCorpusId": "195908774"
                        },
                        {
                            "start": 671,
                            "end": 696,
                            "matchedPaperCorpusId": "14140670"
                        },
                        {
                            "start": 696,
                            "end": 713,
                            "matchedPaperCorpusId": "3355461"
                        },
                        {
                            "start": 794,
                            "end": 821,
                            "matchedPaperCorpusId": "16131749"
                        },
                        {
                            "start": 1046,
                            "end": 1066,
                            "matchedPaperCorpusId": "3178759"
                        },
                        {
                            "start": 1066,
                            "end": 1090,
                            "matchedPaperCorpusId": "6203757"
                        },
                        {
                            "start": 1230,
                            "end": 1250,
                            "matchedPaperCorpusId": "12241221"
                        },
                        {
                            "start": 1373,
                            "end": 1391,
                            "matchedPaperCorpusId": "14472576"
                        },
                        {
                            "start": 1708,
                            "end": 1730,
                            "matchedPaperCorpusId": "7180044"
                        },
                        {
                            "start": 1730,
                            "end": 1759,
                            "matchedPaperCorpusId": "14784553"
                        },
                        {
                            "start": 1987,
                            "end": 2006,
                            "matchedPaperCorpusId": "7766190"
                        },
                        {
                            "start": 2048,
                            "end": 2071,
                            "matchedPaperCorpusId": "9592788"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81884765625
                }
            ],
            "relevance_judgement": 0.81884765625,
            "relevance_judgment_input_expanded": "# Title: The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: D. Wang, Jason Eisner\n## Abstract\nWe release Galactic Dependencies 1.0\u2014a large set of synthetic languages not found on Earth, but annotated in Universal Dependencies format. This new resource aims to provide training and development data for NLP methods that aim to adapt to unfamiliar languages. Each synthetic treebank is produced from a real treebank by stochastically permuting the dependents of nouns and/or verbs to match the word order of other real languages. We discuss the usefulness, realism, parsability, perplexity, and diversity of the synthetic languages. As a simple demonstration of the use of Galactic Dependencies, we consider single-source transfer, which attempts to parse a real target language using a parser trained on a \u201cnearby\u201d source language. We find that including synthetic source languages somewhat increases the diversity of the source pool, which significantly improves results for most target languages.\n## Related Work\nSynthetic data generation is a well-known trick for effectively training a large model on a small dataset. Abu-Mostafa (1995) reviews early work that provided \"hints\" to a learning system in the form of virtual training examples. While datasets have grown in recent years, so have models: e.g., neural networks have many parameters to train. Thus, it is still common to create synthetic training examples-often by adding noise to real inputs or otherwise transforming them in ways that are expected to preserve their labels. Domains where it is easy to exploit these invariances include image recognition (Simard et al., 2003;Krizhevsky et al., 2012), speech recognition (Jaitly and Hinton, 2013;Cui et al., 2015), information retrieval (Vilares et al., 2011), and grammatical error correction (Rozovskaya and Roth, 2010). \n\nSynthetic datasets have also arisen recently for semantic tasks in natural language processing. bAbI is a dataset of facts, questions, and answers, generated by random simulation, for training machines to do simple logic (Weston et al., 2016). Hermann et al. (2015) generate reading comprehension questions and their answers, based on a large set of newssummarization pairs, for training machine readers. Serban et al. (2016) used RNNs to generate 30 million factoid questions about Freebase, with answers, for training question-answering systems. Wang et al. (2015) obtain data to train semantic parsers in a new domain by first generating synthetic (utterance, logical form) pairs and then asking human annotators to paraphrase the synthetic utterances into more natural human language. \n\nIn speech recognition, morphology-based \"vocabulary expansion\" creates synthetic word forms (Rasooli et al., 2014;Varjokallio and Klakow, 2016). \n\nMachine translation researchers have often tried to automatically preprocess parse trees of a source language to more closely resemble those of the target language, using either hand-crafted or automatically extracted rules (Dorr et al., 2002;Collins et al., 2005, etc.; see review by Howlett and Dras, 2011).",
            "reference_string": "[10817864 | Wang et al. | 2016 | Citations: 58]"
        },
        {
            "title": "Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 72,
            "citation_count": 14,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.07398, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2044202073",
                    "name": "Tianqing Fang"
                },
                {
                    "authorId": "2111435018",
                    "name": "Zeming Chen"
                },
                {
                    "authorId": "2241325169",
                    "name": "Yangqiu Song"
                },
                {
                    "authorId": "2691021",
                    "name": "Antoine Bosselut"
                }
            ],
            "abstract": "Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense inferences for contexts and questions involving interactions between complex events. To address this demand, we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions. Our experiments show that language models trained on COM2 exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for question answering and generative commonsense reasoning, without expensive human annotations. Code and data are available at https://github.com/tqfang/complex-commonsense-reasoning.",
            "corpus_id": 268363344,
            "sentences": [
                {
                    "corpus_id": "268363344",
                    "title": "Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs",
                    "text": "Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense inferences for contexts and questions involving interactions between complex events. To address this demand, we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions. Our experiments show that language models trained on COM2 exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for question answering and generative commonsense reasoning, without expensive human annotations. Code and data are available at https://github.com/tqfang/complex-commonsense-reasoning.",
                    "score": 0.46840855973188517,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81787109375
                }
            ],
            "relevance_judgement": 0.81787109375,
            "relevance_judgment_input_expanded": "# Title: Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Tianqing Fang, Zeming Chen, Yangqiu Song, Antoine Bosselut\n## Abstract\nEvent commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense inferences for contexts and questions involving interactions between complex events. To address this demand, we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions. Our experiments show that language models trained on COM2 exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for question answering and generative commonsense reasoning, without expensive human annotations. Code and data are available at https://github.com/tqfang/complex-commonsense-reasoning.\n",
            "reference_string": "[268363344 | Fang et al. | 2024 | Citations: 14]"
        },
        {
            "title": "A Survey on Data Augmentation in Large Model Era",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 285,
            "citation_count": 27,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.15422, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2281788432",
                    "name": "Yue Zhou"
                },
                {
                    "authorId": "2282250059",
                    "name": "Chenlu Guo"
                },
                {
                    "authorId": "2281791317",
                    "name": "Xu Wang"
                },
                {
                    "authorId": "2281762754",
                    "name": "Yi Chang"
                },
                {
                    "authorId": "2280899184",
                    "name": "Yuan Wu"
                }
            ],
            "abstract": "Large models, encompassing large language and diffusion models, have shown exceptional promise in approximating human-level intelligence, garnering significant interest from both academic and industrial spheres. However, the training of these large models necessitates vast quantities of high-quality data, and with continuous updates to these models, the existing reservoir of high-quality data may soon be depleted. This challenge has catalyzed a surge in research focused on data augmentation methods. Leveraging large models, these data augmentation techniques have outperformed traditional approaches. This paper offers an exhaustive review of large model-driven data augmentation methods, adopting a comprehensive perspective. We begin by establishing a classification of relevant studies into three main categories: image augmentation, text augmentation, and paired data augmentation. Following this, we delve into various data post-processing techniques pertinent to large model-based data augmentation. Our discussion then expands to encompass the array of applications for these data augmentation methods within natural language processing, computer vision, and audio signal processing. We proceed to evaluate the successes and limitations of large model-based data augmentation across different scenarios. Concluding our review, we highlight prospective challenges and avenues for future exploration in the field of data augmentation. Our objective is to furnish researchers with critical insights, ultimately contributing to the advancement of more sophisticated large models. We consistently maintain the related open-source materials at: https://github.com/MLGroup-JLU/LLM-data-aug-survey.",
            "corpus_id": 267311830,
            "sentences": [
                {
                    "corpus_id": "267311830",
                    "title": "A Survey on Data Augmentation in Large Model Era",
                    "text": "Based on this observation, FlipDA first generates data using word substitution based on a pre-trained T5 and uses a classifier to select label-flipped data. Guo et al. (2022a) presented GENIUS, a text generation model that operates based on conditional input in the form of sketches. GENIUS is designed to fill in the missing contexts in a given sketch. Additionally, the study demonstrates that GENIUS can serve as a powerful and readily applicable tool for data augmentation in various NLP tasks. InPars (Bonifacio et al., 2022), a method for generating synthetic training data for information retrieval tasks, utilizes LLMs in a fewshot manner. It generates one question per document by employing GPT-3's Curie model, while using the \"vanilla\" and \"guided by bad questions\" (GBQ) prompt templates. Khatri et al. (2022) illustrated that the capabilities of a substantial pre-trained transformer-based LM, such as GPT-2, can be effectively utilized to enrich limited datasets created by humans. This enhancement process is designed to preserve the original intent of the expanded utterances while also capturing various alternative expressions for the same intent. Consequently, this methodology leads to a notable improvement in the performance of chatbots driven by machine learning, enabling them to respond more accurately and diversely in conversational contexts. Quteineh et al. (2020) introduced a new method of data augmentation that leverages the guided outputs of a language generation model such as GPT-2 to enhance the performance of text classifiers through an active learning process, which aims to generate synthetic data as unlabeled data that is required by active learning algorithms. Liu et al. (2022) introduced a fresh method to create datasets by combining the efforts of human workers and AI. It began with an established dataset called MultiNLI, which is designed to deal with natural language inference. The proposed approach involves employing dataset cartography to automatically identify instances that exhibit complex reasoning patterns. These patterns serve as guidelines for instructing GPT-3 to generate new examples with similar characteristics. The generated examples are then filtered automatically and subsequently reviewed and labeled by human crowd workers.",
                    "score": 0.45956215773529474,
                    "section_title": "Generated content-based approaches",
                    "char_start_offset": 45179,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 156
                        },
                        {
                            "start": 157,
                            "end": 283
                        },
                        {
                            "start": 284,
                            "end": 353
                        },
                        {
                            "start": 354,
                            "end": 498
                        },
                        {
                            "start": 499,
                            "end": 647
                        },
                        {
                            "start": 648,
                            "end": 800
                        },
                        {
                            "start": 801,
                            "end": 995
                        },
                        {
                            "start": 996,
                            "end": 1165
                        },
                        {
                            "start": 1166,
                            "end": 1369
                        },
                        {
                            "start": 1370,
                            "end": 1703
                        },
                        {
                            "start": 1704,
                            "end": 1816
                        },
                        {
                            "start": 1817,
                            "end": 1929
                        },
                        {
                            "start": 1930,
                            "end": 2067
                        },
                        {
                            "start": 2068,
                            "end": 2179
                        },
                        {
                            "start": 2180,
                            "end": 2296
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 801,
                            "end": 821,
                            "matchedPaperCorpusId": "257959548"
                        },
                        {
                            "start": 1370,
                            "end": 1392,
                            "matchedPaperCorpusId": "226262334"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.814453125
                }
            ],
            "relevance_judgement": 0.814453125,
            "relevance_judgment_input_expanded": "# Title: A Survey on Data Augmentation in Large Model Era\n# Venue: arXiv.org\n# Authors: Yue Zhou, Chenlu Guo, Xu Wang, Yi Chang, Yuan Wu\n## Abstract\nLarge models, encompassing large language and diffusion models, have shown exceptional promise in approximating human-level intelligence, garnering significant interest from both academic and industrial spheres. However, the training of these large models necessitates vast quantities of high-quality data, and with continuous updates to these models, the existing reservoir of high-quality data may soon be depleted. This challenge has catalyzed a surge in research focused on data augmentation methods. Leveraging large models, these data augmentation techniques have outperformed traditional approaches. This paper offers an exhaustive review of large model-driven data augmentation methods, adopting a comprehensive perspective. We begin by establishing a classification of relevant studies into three main categories: image augmentation, text augmentation, and paired data augmentation. Following this, we delve into various data post-processing techniques pertinent to large model-based data augmentation. Our discussion then expands to encompass the array of applications for these data augmentation methods within natural language processing, computer vision, and audio signal processing. We proceed to evaluate the successes and limitations of large model-based data augmentation across different scenarios. Concluding our review, we highlight prospective challenges and avenues for future exploration in the field of data augmentation. Our objective is to furnish researchers with critical insights, ultimately contributing to the advancement of more sophisticated large models. We consistently maintain the related open-source materials at: https://github.com/MLGroup-JLU/LLM-data-aug-survey.\n## Generated content-based approaches\nBased on this observation, FlipDA first generates data using word substitution based on a pre-trained T5 and uses a classifier to select label-flipped data. Guo et al. (2022a) presented GENIUS, a text generation model that operates based on conditional input in the form of sketches. GENIUS is designed to fill in the missing contexts in a given sketch. Additionally, the study demonstrates that GENIUS can serve as a powerful and readily applicable tool for data augmentation in various NLP tasks. InPars (Bonifacio et al., 2022), a method for generating synthetic training data for information retrieval tasks, utilizes LLMs in a fewshot manner. It generates one question per document by employing GPT-3's Curie model, while using the \"vanilla\" and \"guided by bad questions\" (GBQ) prompt templates. Khatri et al. (2022) illustrated that the capabilities of a substantial pre-trained transformer-based LM, such as GPT-2, can be effectively utilized to enrich limited datasets created by humans. This enhancement process is designed to preserve the original intent of the expanded utterances while also capturing various alternative expressions for the same intent. Consequently, this methodology leads to a notable improvement in the performance of chatbots driven by machine learning, enabling them to respond more accurately and diversely in conversational contexts. Quteineh et al. (2020) introduced a new method of data augmentation that leverages the guided outputs of a language generation model such as GPT-2 to enhance the performance of text classifiers through an active learning process, which aims to generate synthetic data as unlabeled data that is required by active learning algorithms. Liu et al. (2022) introduced a fresh method to create datasets by combining the efforts of human workers and AI. It began with an established dataset called MultiNLI, which is designed to deal with natural language inference. The proposed approach involves employing dataset cartography to automatically identify instances that exhibit complex reasoning patterns. These patterns serve as guidelines for instructing GPT-3 to generate new examples with similar characteristics. The generated examples are then filtered automatically and subsequently reviewed and labeled by human crowd workers.",
            "reference_string": "[267311830 | Zhou et al. | 2024 | Citations: 27]"
        },
        {
            "title": "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 0,
            "citation_count": 78,
            "influential_citation_count": 12,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.16049, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2150345078",
                    "name": "Zayne Sprague"
                },
                {
                    "authorId": "50183897",
                    "name": "Xi Ye"
                },
                {
                    "authorId": "1620502132",
                    "name": "Kaj Bostrom"
                },
                {
                    "authorId": "35865989",
                    "name": "Swarat Chaudhuri"
                },
                {
                    "authorId": "1814094",
                    "name": "Greg Durrett"
                }
            ],
            "abstract": "While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our dataset instances are free text narratives corresponding to real-world domains of reasoning; this makes it simultaneously much more challenging than other synthetically-crafted benchmarks while remaining realistic and tractable for human annotators to solve with high accuracy. We evaluate a range of LLMs and prompting techniques on this dataset and characterize the gaps that remain for techniques like chain-of-thought to perform robust reasoning.",
            "corpus_id": 264439655,
            "sentences": [
                {
                    "corpus_id": "264439655",
                    "title": "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning",
                    "text": "While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our dataset instances are free text narratives corresponding to real-world domains of reasoning; this makes it simultaneously much more challenging than other synthetically-crafted benchmarks while remaining realistic and tractable for human annotators to solve with high accuracy. We evaluate a range of LLMs and prompting techniques on this dataset and characterize the gaps that remain for techniques like chain-of-thought to perform robust reasoning.",
                    "score": 0.458194625641725,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.810546875
                }
            ],
            "relevance_judgement": 0.810546875,
            "relevance_judgment_input_expanded": "# Title: MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning\n# Venue: arXiv.org\n# Authors: Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett\n## Abstract\nWhile large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our dataset instances are free text narratives corresponding to real-world domains of reasoning; this makes it simultaneously much more challenging than other synthetically-crafted benchmarks while remaining realistic and tractable for human annotators to solve with high accuracy. We evaluate a range of LLMs and prompting techniques on this dataset and characterize the gaps that remain for techniques like chain-of-thought to perform robust reasoning.\n",
            "reference_string": "[264439655 | Sprague et al. | 2023 | Citations: 78]"
        },
        {
            "title": "Think Visually: Question Answering through Virtual Imagery",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2018,
            "reference_count": 34,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/P18-1242.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.11025, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47989608",
                    "name": "Ankit Goyal"
                },
                {
                    "authorId": "2152769770",
                    "name": "Jian Wang"
                },
                {
                    "authorId": "153302678",
                    "name": "Jia Deng"
                }
            ],
            "abstract": "In this paper, we study the problem of geometric reasoning (a form of visual reasoning) in the context of question-answering. We introduce Dynamic Spatial Memory Network (DSMN), a new deep network architecture that specializes in answering questions that admit latent visual representations, and learns to generate and reason over such representations. Further, we propose two synthetic benchmarks, FloorPlanQA and ShapeIntersection, to evaluate the geometric reasoning capability of QA systems. Experimental results validate the effectiveness of our proposed DSMN for visual thinking tasks.",
            "corpus_id": 44096233,
            "sentences": [
                {
                    "corpus_id": "44096233",
                    "title": "Think Visually: Question Answering through Virtual Imagery",
                    "text": "Natural language datasets for QA: Several natural language QA datasets have been proposed to test AI systems on various reasoning abilities (Levesque et al., 2011;Richardson et al., 2013). Our work differs from them in two key aspects: first, we use synthetic data instead of natural data; and second, we specialize in geometrical reasoning instead of general language understanding. Using synthetic data helps us simplify language parsing and thereby focus on geometric reasoning. However, additional research is necessary to generalize our work to natural data. \n\nSynthetic datasets for QA: Recently, synthetic datasets for QA are also becoming crucial in AI. In particular, bAbI (Weston et al., 2015) has driven the development of several recent DNN-based QA systems (Kumar et al., 2016;Sukhbaatar et al., 2015;Henaff et al., 2016). bAbI consists of 20 tasks to evaluate different reasoning abilities. Two tasks, Positional Reasoning (PR) and Path Finding (PF), are related to geometric reasoning. However, each Positional Reasoning question contains only two sentences, and can be solved through simple logical deduction such as 'A is left of B implies B is right of A'. Similarly, Path Finding involves a search problem that requires simple spatial deductions such as 'A is east of B implies B is west of A'. In contrast, the questions in our datasets involve longer descriptions, more entities, and more relations; they are thus harder to answer with simple deductions. We also provide reference visual representation for each sample, which is not available in bAbI.",
                    "score": 0.44171240979001164,
                    "section_title": "Related Work",
                    "char_start_offset": 4464,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 188
                        },
                        {
                            "start": 189,
                            "end": 383
                        },
                        {
                            "start": 384,
                            "end": 481
                        },
                        {
                            "start": 482,
                            "end": 563
                        },
                        {
                            "start": 566,
                            "end": 661
                        },
                        {
                            "start": 662,
                            "end": 835
                        },
                        {
                            "start": 836,
                            "end": 904
                        },
                        {
                            "start": 905,
                            "end": 1000
                        },
                        {
                            "start": 1001,
                            "end": 1174
                        },
                        {
                            "start": 1175,
                            "end": 1313
                        },
                        {
                            "start": 1314,
                            "end": 1475
                        },
                        {
                            "start": 1476,
                            "end": 1572
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 140,
                            "end": 163,
                            "matchedPaperCorpusId": "15710851"
                        },
                        {
                            "start": 163,
                            "end": 187,
                            "matchedPaperCorpusId": "2100831"
                        },
                        {
                            "start": 770,
                            "end": 790,
                            "matchedPaperCorpusId": "2319779"
                        },
                        {
                            "start": 790,
                            "end": 814,
                            "matchedPaperCorpusId": "1399322"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80419921875
                }
            ],
            "relevance_judgement": 0.80419921875,
            "relevance_judgment_input_expanded": "# Title: Think Visually: Question Answering through Virtual Imagery\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Ankit Goyal, Jian Wang, Jia Deng\n## Abstract\nIn this paper, we study the problem of geometric reasoning (a form of visual reasoning) in the context of question-answering. We introduce Dynamic Spatial Memory Network (DSMN), a new deep network architecture that specializes in answering questions that admit latent visual representations, and learns to generate and reason over such representations. Further, we propose two synthetic benchmarks, FloorPlanQA and ShapeIntersection, to evaluate the geometric reasoning capability of QA systems. Experimental results validate the effectiveness of our proposed DSMN for visual thinking tasks.\n## Related Work\nNatural language datasets for QA: Several natural language QA datasets have been proposed to test AI systems on various reasoning abilities (Levesque et al., 2011;Richardson et al., 2013). Our work differs from them in two key aspects: first, we use synthetic data instead of natural data; and second, we specialize in geometrical reasoning instead of general language understanding. Using synthetic data helps us simplify language parsing and thereby focus on geometric reasoning. However, additional research is necessary to generalize our work to natural data. \n\nSynthetic datasets for QA: Recently, synthetic datasets for QA are also becoming crucial in AI. In particular, bAbI (Weston et al., 2015) has driven the development of several recent DNN-based QA systems (Kumar et al., 2016;Sukhbaatar et al., 2015;Henaff et al., 2016). bAbI consists of 20 tasks to evaluate different reasoning abilities. Two tasks, Positional Reasoning (PR) and Path Finding (PF), are related to geometric reasoning. However, each Positional Reasoning question contains only two sentences, and can be solved through simple logical deduction such as 'A is left of B implies B is right of A'. Similarly, Path Finding involves a search problem that requires simple spatial deductions such as 'A is east of B implies B is west of A'. In contrast, the questions in our datasets involve longer descriptions, more entities, and more relations; they are thus harder to answer with simple deductions. We also provide reference visual representation for each sample, which is not available in bAbI.",
            "reference_string": "[44096233 | Goyal et al. | 2018 | Citations: 2]"
        },
        {
            "title": "A Little Human Data Goes A Long Way",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 77,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.13098, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326296224",
                    "name": "Dhananjay Ashok"
                },
                {
                    "authorId": "2326300954",
                    "name": "Jonathan May"
                }
            ],
            "abstract": "Faced with an expensive human annotation process, creators of NLP systems increasingly turn to synthetic data generation. While this method shows promise, the extent to which synthetic data can replace human annotation is poorly understood. We investigate the use of synthetic data in Fact Verification (FV) and Question Answering (QA) by studying the effects of incrementally replacing human generated data with synthetic points on eight diverse datasets. Strikingly, replacing up to 90% of the training data only marginally decreases performance, but replacing the final 10% leads to severe declines. We find that models trained on purely synthetic data can be reliably improved by including as few as 125 human generated data points. We show that matching the performance gain of just a little additional human data (only 200 points) requires an order of magnitude more synthetic data and estimate price ratios at which human annotation would be a more cost-effective solution. Our results suggest that even when human annotation at scale is infeasible, there is great value to having a small proportion of the dataset being human generated.",
            "corpus_id": 273403996,
            "sentences": [
                {
                    "corpus_id": "273403996",
                    "title": "A Little Human Data Goes A Long Way",
                    "text": "From BERT (Devlin et al., 2019) to GPT-4 (Achiam et al., 2023), the explosive growth of language models (LMs) has been underpinned by exponential increases in the size of available training data. However, the more complex and specialized the task, the more expensive and challenging it is to collect human generated data at scale (Wang et al., 2021). Combined with growing concerns that LMs may soon exhaust the stock of publicly available training data (Villalobos et al., 2024), many turn to synthetic data generation, hoping to eliminate their reliance on human annotation. \n\nSynthetic data generation has long been used to increase the amount of training data available (Simard et al., 2002;Krizhevsky et al., Figure 1: Change in model performance as the proportion of synthetic points in the training data is increased. Across datasets, the performance decrease when moving from 0% to 90% synthetic data is often less than that of moving from 90% to purely synthetic data. 2012). Early NLP approaches use rule based methods (De Gispert et al., 2005;Chen et al., 2012), paraphrasing (Wang and Yang, 2015;Kobayashi, 2018), noising (Xie et al., 2017;Wang et al., 2018), and backtranslation (Sennrich et al., 2016;Yu et al., 2018), but are limited in their capability. \n\nModern LMs demonstrate the capability to solve myriad NLP tasks with minimal task specific data (Brown, 2020;Wei et al., 2022a,b;Ouyang et al., 2022;Ashok and Lipton, 2023), making them more powerful synthetic data generators.",
                    "score": 0.48051836594576036,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 195
                        },
                        {
                            "start": 196,
                            "end": 350
                        },
                        {
                            "start": 351,
                            "end": 576
                        },
                        {
                            "start": 579,
                            "end": 824
                        },
                        {
                            "start": 825,
                            "end": 977
                        },
                        {
                            "start": 978,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1269
                        },
                        {
                            "start": 1272,
                            "end": 1498
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 10,
                            "end": 31,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 330,
                            "end": 349,
                            "matchedPaperCorpusId": "237363383"
                        },
                        {
                            "start": 454,
                            "end": 479,
                            "matchedPaperCorpusId": "272330241"
                        },
                        {
                            "start": 674,
                            "end": 695,
                            "matchedPaperCorpusId": "16067356"
                        },
                        {
                            "start": 695,
                            "end": 713,
                            "matchedPaperCorpusId": "195908774"
                        },
                        {
                            "start": 1029,
                            "end": 1054,
                            "matchedPaperCorpusId": "2571493"
                        },
                        {
                            "start": 1054,
                            "end": 1072,
                            "matchedPaperCorpusId": "1326531"
                        },
                        {
                            "start": 1087,
                            "end": 1108,
                            "matchedPaperCorpusId": "3257353"
                        },
                        {
                            "start": 1108,
                            "end": 1124,
                            "matchedPaperCorpusId": "21725995"
                        },
                        {
                            "start": 1134,
                            "end": 1152,
                            "matchedPaperCorpusId": "10635893"
                        },
                        {
                            "start": 1152,
                            "end": 1170,
                            "matchedPaperCorpusId": "52078335"
                        },
                        {
                            "start": 1192,
                            "end": 1215,
                            "matchedPaperCorpusId": "15600925"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78662109375
                }
            ],
            "relevance_judgement": 0.78662109375,
            "relevance_judgment_input_expanded": "# Title: A Little Human Data Goes A Long Way\n# Venue: arXiv.org\n# Authors: Dhananjay Ashok, Jonathan May\n## Abstract\nFaced with an expensive human annotation process, creators of NLP systems increasingly turn to synthetic data generation. While this method shows promise, the extent to which synthetic data can replace human annotation is poorly understood. We investigate the use of synthetic data in Fact Verification (FV) and Question Answering (QA) by studying the effects of incrementally replacing human generated data with synthetic points on eight diverse datasets. Strikingly, replacing up to 90% of the training data only marginally decreases performance, but replacing the final 10% leads to severe declines. We find that models trained on purely synthetic data can be reliably improved by including as few as 125 human generated data points. We show that matching the performance gain of just a little additional human data (only 200 points) requires an order of magnitude more synthetic data and estimate price ratios at which human annotation would be a more cost-effective solution. Our results suggest that even when human annotation at scale is infeasible, there is great value to having a small proportion of the dataset being human generated.\n## Introduction\nFrom BERT (Devlin et al., 2019) to GPT-4 (Achiam et al., 2023), the explosive growth of language models (LMs) has been underpinned by exponential increases in the size of available training data. However, the more complex and specialized the task, the more expensive and challenging it is to collect human generated data at scale (Wang et al., 2021). Combined with growing concerns that LMs may soon exhaust the stock of publicly available training data (Villalobos et al., 2024), many turn to synthetic data generation, hoping to eliminate their reliance on human annotation. \n\nSynthetic data generation has long been used to increase the amount of training data available (Simard et al., 2002;Krizhevsky et al., Figure 1: Change in model performance as the proportion of synthetic points in the training data is increased. Across datasets, the performance decrease when moving from 0% to 90% synthetic data is often less than that of moving from 90% to purely synthetic data. 2012). Early NLP approaches use rule based methods (De Gispert et al., 2005;Chen et al., 2012), paraphrasing (Wang and Yang, 2015;Kobayashi, 2018), noising (Xie et al., 2017;Wang et al., 2018), and backtranslation (Sennrich et al., 2016;Yu et al., 2018), but are limited in their capability. \n\nModern LMs demonstrate the capability to solve myriad NLP tasks with minimal task specific data (Brown, 2020;Wei et al., 2022a,b;Ouyang et al., 2022;Ashok and Lipton, 2023), making them more powerful synthetic data generators.",
            "reference_string": "[273403996 | Ashok et al. | 2024 | Citations: 4]"
        },
        {
            "title": "A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 118,
            "citation_count": 33,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.12994, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1491705917",
                    "name": "Shubham Vatsal"
                },
                {
                    "authorId": "2311890970",
                    "name": "Harsh Dubey"
                }
            ],
            "abstract": "Large language models (LLMs) have shown remarkable performance on many different Natural Language Processing (NLP) tasks. Prompt engineering plays a key role in adding more to the already existing abilities of LLMs to achieve significant performance gains on various NLP tasks. Prompt engineering requires composing natural language instructions called prompts to elicit knowledge from LLMs in a structured way. Unlike previous state-of-the-art (SoTA) models, prompt engineering does not require extensive parameter re-training or fine-tuning based on the given NLP task and thus solely operates on the embedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently extract LLMs' knowledge through a basic natural language conversational exchange or prompt engineering, allowing more and more people even without deep mathematical machine learning background to experiment with LLMs. With prompt engineering gaining popularity in the last two years, researchers have come up with numerous engineering techniques around designing prompts to improve accuracy of information extraction from the LLMs. In this paper, we summarize different prompting techniques and club them together based on different NLP tasks that they have been used for. We further granularly highlight the performance of these prompting strategies on various datasets belonging to that NLP task, talk about the corresponding LLMs used, present a taxonomy diagram and discuss the possible SoTA for specific datasets. In total, we read and present a survey of 44 research papers which talk about 39 different prompting methods on 29 different NLP tasks of which most of them have been published in the last two years.",
            "corpus_id": 271270817,
            "sentences": [
                {
                    "corpus_id": "271270817",
                    "title": "A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks",
                    "text": "The authors of Shao et al. (2023) come up with Synthetic prompting using LLMs to generate synthetic examples which are augmented to the existing hand-crafted examples as seen in a conventional few-shot setting.This prompting method involves two steps: (1) the backward step, where the LLM synthesizes a query based on a self-generated reasoning chain; and (2) the forward step, where the LLM generates a reasoning chain for the synthesized query, making the reasoning chain to be more accurate.Finally, to choose the best examples, this work uses an in-cluster complexity and the most complex examples with the longest reasoning chains are used during inference.The results show Synthetic prompting achieving up to 15.6% absolute gains when experimented with different Mathematical Problem Solving, Commonsense Reasoning and Logical Reasoning task datasets.",
                    "score": 0.507761660667294,
                    "section_title": "SYNTHETIC PROMPTING",
                    "char_start_offset": 15478,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 210
                        },
                        {
                            "start": 210,
                            "end": 494
                        },
                        {
                            "start": 494,
                            "end": 662
                        },
                        {
                            "start": 662,
                            "end": 857
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 15,
                            "end": 33,
                            "matchedPaperCorpusId": "256459681"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78369140625
                }
            ],
            "relevance_judgement": 0.78369140625,
            "relevance_judgment_input_expanded": "# Title: A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks\n# Venue: arXiv.org\n# Authors: Shubham Vatsal, Harsh Dubey\n## Abstract\nLarge language models (LLMs) have shown remarkable performance on many different Natural Language Processing (NLP) tasks. Prompt engineering plays a key role in adding more to the already existing abilities of LLMs to achieve significant performance gains on various NLP tasks. Prompt engineering requires composing natural language instructions called prompts to elicit knowledge from LLMs in a structured way. Unlike previous state-of-the-art (SoTA) models, prompt engineering does not require extensive parameter re-training or fine-tuning based on the given NLP task and thus solely operates on the embedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently extract LLMs' knowledge through a basic natural language conversational exchange or prompt engineering, allowing more and more people even without deep mathematical machine learning background to experiment with LLMs. With prompt engineering gaining popularity in the last two years, researchers have come up with numerous engineering techniques around designing prompts to improve accuracy of information extraction from the LLMs. In this paper, we summarize different prompting techniques and club them together based on different NLP tasks that they have been used for. We further granularly highlight the performance of these prompting strategies on various datasets belonging to that NLP task, talk about the corresponding LLMs used, present a taxonomy diagram and discuss the possible SoTA for specific datasets. In total, we read and present a survey of 44 research papers which talk about 39 different prompting methods on 29 different NLP tasks of which most of them have been published in the last two years.\n## SYNTHETIC PROMPTING\nThe authors of Shao et al. (2023) come up with Synthetic prompting using LLMs to generate synthetic examples which are augmented to the existing hand-crafted examples as seen in a conventional few-shot setting.This prompting method involves two steps: (1) the backward step, where the LLM synthesizes a query based on a self-generated reasoning chain; and (2) the forward step, where the LLM generates a reasoning chain for the synthesized query, making the reasoning chain to be more accurate.Finally, to choose the best examples, this work uses an in-cluster complexity and the most complex examples with the longest reasoning chains are used during inference.The results show Synthetic prompting achieving up to 15.6% absolute gains when experimented with different Mathematical Problem Solving, Commonsense Reasoning and Logical Reasoning task datasets.",
            "reference_string": "[271270817 | Vatsal et al. | 2024 | Citations: 33]"
        },
        {
            "title": "MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via Event-Driven Text-Code Cyclic Training",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 37,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.08904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46691467",
                    "name": "Xinxin You"
                },
                {
                    "authorId": "2109080606",
                    "name": "Xien Liu"
                },
                {
                    "authorId": "2346292265",
                    "name": "Qixin Sun"
                },
                {
                    "authorId": "2345290517",
                    "name": "Huan Zhang"
                },
                {
                    "authorId": "2153575079",
                    "name": "Kaiyin Zhou"
                },
                {
                    "authorId": "2345188616",
                    "name": "Shaohui Liu"
                },
                {
                    "authorId": "2316008068",
                    "name": "Guoping Hu"
                },
                {
                    "authorId": "2345249602",
                    "name": "Shijin Wang"
                },
                {
                    "authorId": "2352812032",
                    "name": "Si Liu"
                },
                {
                    "authorId": "2302133366",
                    "name": "Ji Wu"
                }
            ],
            "abstract": "Recent methodologies utilizing synthetic datasets have aimed to address inconsistent hallucinations in large language models (LLMs); however,these approaches are primarily tailored to specific tasks, limiting their generalizability. Inspired by the strong performance of code-trained models in logic-intensive domains, we propose a novel framework that leverages event-based text to generate corresponding code and employs cyclic training to transfer the logical consistency of code to natural language effectively. Our method significantly reduces inconsistent hallucinations across three leading LLMs and two categories of natural language tasks while maintaining overall performance. This framework effectively alleviates hallucinations without necessitating adaptation to downstream tasks, demonstrating generality and providing new perspectives to tackle the challenge of inconsistent hallucinations.",
            "corpus_id": 276318012,
            "sentences": [
                {
                    "corpus_id": "276318012",
                    "title": "MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via Event-Driven Text-Code Cyclic Training",
                    "text": "Recent studies have emphasized the effectiveness of synthetic datasets in mitigating inconsistent hallucinations related to specific tasks. For instance, synthetic mathematical datasets (Toshniwal et al., 2024;Huang et al., 2024) have shown reduced contradictions in the calculation process and significant improvements in solving mathematical problems. Additionally, some research has synthesized reasoning data in the logical reasoning (Nie et al., 2020;Saeed et al., 2021) field while providing the reasoning process (Dalvi et al., 2021) during synthesis. LOGIC-LM and SymbCOT (Pan et al., 2023;Xu et al., 2024) integrate LLMs with symbolic solvers to enhance logical problem-solving. Unfortunately, the synthetic datasets that have been developed are often tailored to specific tasks or domains, which can restrict their generalizability. \n\nAdvancements in training LLMs on extensive code datasets, such as CoCoGen and CodeRL, have significantly reduced inconsistent hallucinations in code generation tasks (Madaan et al., 2022;Le et al., 2022). However, these improvements have not effectively transferred to general natural language tasks due to fundamental differences in semantics, language styles, and structures between code and natural language. The precision of code contrasts with the ambiguity of human language, posing challenges in adapting code-focused techniques for natural language understanding and generation. Recent approaches, such as program-ofthought (Chen et al., 2023) and program-assisted LMs (Gao et al., 2023), have sought to bridge this divide by interpreting natural language problems and generating corresponding code solutions. However, these methodologies fail to successfully transfer the logical consistency capabilities gained from code training to a broader spectrum of NLP tasks. \n\nInspired by code-oriented LLMs that excel in generating logically consistent code, we propose that integrating code data is vital for enhancing the ability of LLMs to maintain logical consistency across various NLP applications. Furthermore, we observe that there exists a correspondence between the structure of event-based text and that of code.",
                    "score": 0.5467878734674013,
                    "section_title": "Inconsistent Hallucinations Explanation",
                    "char_start_offset": 1803,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 139
                        },
                        {
                            "start": 140,
                            "end": 353
                        },
                        {
                            "start": 354,
                            "end": 558
                        },
                        {
                            "start": 559,
                            "end": 687
                        },
                        {
                            "start": 688,
                            "end": 842
                        },
                        {
                            "start": 845,
                            "end": 1049
                        },
                        {
                            "start": 1050,
                            "end": 1256
                        },
                        {
                            "start": 1257,
                            "end": 1431
                        },
                        {
                            "start": 1432,
                            "end": 1662
                        },
                        {
                            "start": 1663,
                            "end": 1820
                        },
                        {
                            "start": 1823,
                            "end": 2051
                        },
                        {
                            "start": 2052,
                            "end": 2170
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 438,
                            "end": 456,
                            "matchedPaperCorpusId": "207756753"
                        },
                        {
                            "start": 456,
                            "end": 475,
                            "matchedPaperCorpusId": "237940562"
                        },
                        {
                            "start": 520,
                            "end": 540,
                            "matchedPaperCorpusId": "233297051"
                        },
                        {
                            "start": 580,
                            "end": 598,
                            "matchedPaperCorpusId": "258833332"
                        },
                        {
                            "start": 1011,
                            "end": 1032,
                            "matchedPaperCorpusId": "252873120"
                        },
                        {
                            "start": 1032,
                            "end": 1048,
                            "matchedPaperCorpusId": "250280117"
                        },
                        {
                            "start": 1477,
                            "end": 1496,
                            "matchedPaperCorpusId": "253801709"
                        },
                        {
                            "start": 1522,
                            "end": 1540,
                            "matchedPaperCorpusId": "253708270"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77734375
                }
            ],
            "relevance_judgement": 0.77734375,
            "relevance_judgment_input_expanded": "# Title: MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via Event-Driven Text-Code Cyclic Training\n# Venue: arXiv.org\n# Authors: Xinxin You, Xien Liu, Qixin Sun, Huan Zhang, Kaiyin Zhou, Shaohui Liu, Guoping Hu, Shijin Wang, Si Liu, Ji Wu\n## Abstract\nRecent methodologies utilizing synthetic datasets have aimed to address inconsistent hallucinations in large language models (LLMs); however,these approaches are primarily tailored to specific tasks, limiting their generalizability. Inspired by the strong performance of code-trained models in logic-intensive domains, we propose a novel framework that leverages event-based text to generate corresponding code and employs cyclic training to transfer the logical consistency of code to natural language effectively. Our method significantly reduces inconsistent hallucinations across three leading LLMs and two categories of natural language tasks while maintaining overall performance. This framework effectively alleviates hallucinations without necessitating adaptation to downstream tasks, demonstrating generality and providing new perspectives to tackle the challenge of inconsistent hallucinations.\n## Inconsistent Hallucinations Explanation\nRecent studies have emphasized the effectiveness of synthetic datasets in mitigating inconsistent hallucinations related to specific tasks. For instance, synthetic mathematical datasets (Toshniwal et al., 2024;Huang et al., 2024) have shown reduced contradictions in the calculation process and significant improvements in solving mathematical problems. Additionally, some research has synthesized reasoning data in the logical reasoning (Nie et al., 2020;Saeed et al., 2021) field while providing the reasoning process (Dalvi et al., 2021) during synthesis. LOGIC-LM and SymbCOT (Pan et al., 2023;Xu et al., 2024) integrate LLMs with symbolic solvers to enhance logical problem-solving. Unfortunately, the synthetic datasets that have been developed are often tailored to specific tasks or domains, which can restrict their generalizability. \n\nAdvancements in training LLMs on extensive code datasets, such as CoCoGen and CodeRL, have significantly reduced inconsistent hallucinations in code generation tasks (Madaan et al., 2022;Le et al., 2022). However, these improvements have not effectively transferred to general natural language tasks due to fundamental differences in semantics, language styles, and structures between code and natural language. The precision of code contrasts with the ambiguity of human language, posing challenges in adapting code-focused techniques for natural language understanding and generation. Recent approaches, such as program-ofthought (Chen et al., 2023) and program-assisted LMs (Gao et al., 2023), have sought to bridge this divide by interpreting natural language problems and generating corresponding code solutions. However, these methodologies fail to successfully transfer the logical consistency capabilities gained from code training to a broader spectrum of NLP tasks. \n\nInspired by code-oriented LLMs that excel in generating logically consistent code, we propose that integrating code data is vital for enhancing the ability of LLMs to maintain logical consistency across various NLP applications. Furthermore, we observe that there exists a correspondence between the structure of event-based text and that of code.",
            "reference_string": "[276318012 | You et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Data Augmentation using Large Language Models: Data Perspectives, Learning Paradigms and Challenges",
            "venue": "",
            "year": 2024,
            "reference_count": 152,
            "citation_count": 20,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.02990, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2064493724",
                    "name": "Bosheng Ding"
                },
                {
                    "authorId": "2084609980",
                    "name": "Chengwei Qin"
                },
                {
                    "authorId": "2091437375",
                    "name": "Ruochen Zhao"
                },
                {
                    "authorId": "2290030532",
                    "name": "Tianze Luo"
                },
                {
                    "authorId": "2290023354",
                    "name": "Xinze Li"
                },
                {
                    "authorId": "2290026474",
                    "name": "Guizhen Chen"
                },
                {
                    "authorId": "2276610995",
                    "name": "Wenhan Xia"
                },
                {
                    "authorId": "2290145274",
                    "name": "Junjie Hu"
                },
                {
                    "authorId": "1755919",
                    "name": "A. Luu"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                }
            ],
            "abstract": "In the rapidly evolving field of large language models (LLMs), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of LLMs on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From both data and learning perspectives, we examine various strategies that utilize LLMs for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for diverse forms of further training. Additionally, this paper highlights the primary open challenges faced in this domain, ranging from controllable data augmentation to multi-modal data augmentation. This survey highlights a paradigm shift introduced by LLMs in DA, and aims to serve as a comprehensive guide for researchers and practitioners.",
            "corpus_id": 268249187,
            "sentences": [
                {
                    "corpus_id": "268249187",
                    "title": "Data Augmentation using Large Language Models: Data Perspectives, Learning Paradigms and Challenges",
                    "text": "Automate-CoT (Shum et al., 2023) proposes a fully automatic pipeline for finding better chainof-thought prompts to solve commonsense reasoning question answering tasks. This pipeline leverages powerful LLMs like GPT-3 to augment reasoning paths, prune incorrect paths, and select optimal combinations of exemplars. \n\n\u2022 Story Completing: Whitehouse et al. (2023) explore the generation of synthetic data for question answering under the multilingual setting using various LLMs. This work conducts experiments with dolly-12B, stablevicuna-13B, ChatGPT, and GPT-4 on three datasets that covers over 20 languages on commonsense reasoning and story completing tasks. \n\n\u2022 Fact Checking: SCOTT (Wang et al., 2023b) prompts a GPT-neox-20B model to generate annotated data to train a smaller student model to perform QA tasks. Experiments conducted on four QA datasets across commonsense reasoning and fact checking showcase the efficiency of this data augmentation technique. \n\n\u2022 MRC: Samuel et al. (2023) introduce a GPT-4 based data augmentation technique targeting the low source setting on machine reading comprehension task. This technique separately generates passsages, questions, and answers using in-context learning. \n\n\u2022 MedicalQA: Dr.LLaMA (Guo et al., 2023) employs ChatGPT and GPT-4 to either rewrite existing medical question-answering pairs or generate new pairs from the training dataset with zero-shot prompting. Such data augmentation helps to train a LLaMA model specialised on medical knowledge. \n\nLogical Reasoning Logical reasoning involves the ability to analyze and draw conclusions based on given information, a crucial cognitive skill that LLMs aim to master. In the context of LLMs, logical reasoning reflects the model's aptitude for understanding and manipulating symbolic representations, solving problems, and making deductions. The significance of logical reasoning lies in its application across various domains, from puzzlesolving and decision-making to complex problemsolving tasks, showcasing the model's capacity for high-level cognitive functions. \n\nWithin the logical reasoning task, Chain-of-Thought has been a popular technique that improves the reasoning process for the language models.",
                    "score": 0.504390702264951,
                    "section_title": "\u2022 Commonsense Reasoning:",
                    "char_start_offset": 39211,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 168
                        },
                        {
                            "start": 169,
                            "end": 314
                        },
                        {
                            "start": 317,
                            "end": 476
                        },
                        {
                            "start": 477,
                            "end": 661
                        },
                        {
                            "start": 664,
                            "end": 817
                        },
                        {
                            "start": 818,
                            "end": 967
                        },
                        {
                            "start": 970,
                            "end": 1121
                        },
                        {
                            "start": 1122,
                            "end": 1218
                        },
                        {
                            "start": 1221,
                            "end": 1421
                        },
                        {
                            "start": 1422,
                            "end": 1507
                        },
                        {
                            "start": 1510,
                            "end": 1677
                        },
                        {
                            "start": 1678,
                            "end": 1851
                        },
                        {
                            "start": 1852,
                            "end": 2077
                        },
                        {
                            "start": 2080,
                            "end": 2221
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77392578125
                }
            ],
            "relevance_judgement": 0.77392578125,
            "relevance_judgment_input_expanded": "# Title: Data Augmentation using Large Language Models: Data Perspectives, Learning Paradigms and Challenges\n# Venue: \n# Authors: Bosheng Ding, Chengwei Qin, Ruochen Zhao, Tianze Luo, Xinze Li, Guizhen Chen, Wenhan Xia, Junjie Hu, A. Luu, Shafiq R. Joty\n## Abstract\nIn the rapidly evolving field of large language models (LLMs), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of LLMs on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From both data and learning perspectives, we examine various strategies that utilize LLMs for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for diverse forms of further training. Additionally, this paper highlights the primary open challenges faced in this domain, ranging from controllable data augmentation to multi-modal data augmentation. This survey highlights a paradigm shift introduced by LLMs in DA, and aims to serve as a comprehensive guide for researchers and practitioners.\n## \u2022 Commonsense Reasoning:\nAutomate-CoT (Shum et al., 2023) proposes a fully automatic pipeline for finding better chainof-thought prompts to solve commonsense reasoning question answering tasks. This pipeline leverages powerful LLMs like GPT-3 to augment reasoning paths, prune incorrect paths, and select optimal combinations of exemplars. \n\n\u2022 Story Completing: Whitehouse et al. (2023) explore the generation of synthetic data for question answering under the multilingual setting using various LLMs. This work conducts experiments with dolly-12B, stablevicuna-13B, ChatGPT, and GPT-4 on three datasets that covers over 20 languages on commonsense reasoning and story completing tasks. \n\n\u2022 Fact Checking: SCOTT (Wang et al., 2023b) prompts a GPT-neox-20B model to generate annotated data to train a smaller student model to perform QA tasks. Experiments conducted on four QA datasets across commonsense reasoning and fact checking showcase the efficiency of this data augmentation technique. \n\n\u2022 MRC: Samuel et al. (2023) introduce a GPT-4 based data augmentation technique targeting the low source setting on machine reading comprehension task. This technique separately generates passsages, questions, and answers using in-context learning. \n\n\u2022 MedicalQA: Dr.LLaMA (Guo et al., 2023) employs ChatGPT and GPT-4 to either rewrite existing medical question-answering pairs or generate new pairs from the training dataset with zero-shot prompting. Such data augmentation helps to train a LLaMA model specialised on medical knowledge. \n\nLogical Reasoning Logical reasoning involves the ability to analyze and draw conclusions based on given information, a crucial cognitive skill that LLMs aim to master. In the context of LLMs, logical reasoning reflects the model's aptitude for understanding and manipulating symbolic representations, solving problems, and making deductions. The significance of logical reasoning lies in its application across various domains, from puzzlesolving and decision-making to complex problemsolving tasks, showcasing the model's capacity for high-level cognitive functions. \n\nWithin the logical reasoning task, Chain-of-Thought has been a popular technique that improves the reasoning process for the language models.",
            "reference_string": "[268249187 | Ding et al. | 2024 | Citations: 20]"
        },
        {
            "title": "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.00390, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2309218600",
                    "name": "Mingqian He"
                },
                {
                    "authorId": "1471660296",
                    "name": "Yongliang Shen"
                },
                {
                    "authorId": "2135282890",
                    "name": "Wenqi Zhang"
                },
                {
                    "authorId": "2092670555",
                    "name": "Zeqi Tan"
                },
                {
                    "authorId": "1776903",
                    "name": "Weiming Lu"
                }
            ],
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in handling complex reasoning tasks by generating step-by-step rationales. Some methods have proven effective in boosting accuracy by introducing extra verifiers to assess these paths. However, existing verifiers, typically trained on binary-labeled reasoning paths, fail to fully utilize the relative merits of intermediate steps, thereby limiting the effectiveness of the feedback provided. To overcome this limitation, we propose Tree-based Preference Learning Verifier (Tree-PLV), a novel approach that constructs reasoning trees via a best-first search algorithm and collects step-level paired data for preference training. Compared to traditional binary classification, step-level preferences more finely capture the nuances between reasoning steps, allowing for a more precise evaluation of the complete reasoning path. We empirically evaluate Tree-PLV across a range of arithmetic and commonsense reasoning tasks, where it significantly outperforms existing benchmarks. For instance, Tree-PLV achieved substantial performance gains over the Mistral-7B self-consistency baseline on GSM8K (67.55% \u2192 82.79%), MATH (17.00% \u2192 26.80%), CSQA (68.14% \u2192 72.97%), and StrategyQA (82.86% \u2192 83.25%). Additionally, our study explores the appropriate granularity for applying preference learning, revealing that step-level guidance provides feedback that better aligns with the evaluation of the reasoning process.",
            "corpus_id": 270870071,
            "sentences": [
                {
                    "corpus_id": "270870071",
                    "title": "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning",
                    "text": "Multi-step Reasoning Challenging reasoning tasks have spurred innovative research in large language models (LLMs), which are essential for handling complex queries (Kaddour et al., 2023;Lightman et al., 2023;Huang et al., 2023).A prominent strategy employed is the Chain-of-Thought (CoT) prompting technique (Wei et al., 2022), along with its derivatives (Kojima et al., 2022;Wang et al., 2022;Yao et al., 2024).These methods decompose the reasoning process into sequential steps, systematically approaching problem-solving by mimicking human-like reasoning.To further enhance the accuracy of these intermediate steps, recent studies leverage extensive synthetic datasets, which are either distilled from cutting-edge models (Yu et al., 2023b;Luo et al., 2023) or composed of selfgenerated rationales (Zelikman et al., 2022;Yuan et al., 2023;Ni et al., 2022), to fine-tune the LLMs.\n\nSuch training effectively sharpens the models' ability to produce CoT reasoning that leads to correct answers.\n\nIn addition, there is an increasing emphasis on test-time verification, a process that generates multiple solutions and ranks them through a separate verifier (Cobbe et al., 2021) to select the most accurate one.The DIVERSE framework (Li et al., 2022) demonstrates the effectiveness of employing a variety of CoT prompts in conjunction with a verifier to tackle reasoning challenges.CoRe (Zhu et al., 2022) implements a dual-process system by finetuning both the generator and the verifier, thereby enhancing the performance of LLMs on math word problems.",
                    "score": 0.4715438945992949,
                    "section_title": "A Related Work",
                    "char_start_offset": 26179,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 228
                        },
                        {
                            "start": 228,
                            "end": 412
                        },
                        {
                            "start": 412,
                            "end": 558
                        },
                        {
                            "start": 558,
                            "end": 882
                        },
                        {
                            "start": 884,
                            "end": 994
                        },
                        {
                            "start": 996,
                            "end": 1208
                        },
                        {
                            "start": 1208,
                            "end": 1379
                        },
                        {
                            "start": 1379,
                            "end": 1551
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7666015625
                }
            ],
            "relevance_judgement": 0.7666015625,
            "relevance_judgment_input_expanded": "# Title: Advancing Process Verification for Large Language Models via Tree-Based Preference Learning\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Mingqian He, Yongliang Shen, Wenqi Zhang, Zeqi Tan, Weiming Lu\n## Abstract\nLarge Language Models (LLMs) have demonstrated remarkable potential in handling complex reasoning tasks by generating step-by-step rationales. Some methods have proven effective in boosting accuracy by introducing extra verifiers to assess these paths. However, existing verifiers, typically trained on binary-labeled reasoning paths, fail to fully utilize the relative merits of intermediate steps, thereby limiting the effectiveness of the feedback provided. To overcome this limitation, we propose Tree-based Preference Learning Verifier (Tree-PLV), a novel approach that constructs reasoning trees via a best-first search algorithm and collects step-level paired data for preference training. Compared to traditional binary classification, step-level preferences more finely capture the nuances between reasoning steps, allowing for a more precise evaluation of the complete reasoning path. We empirically evaluate Tree-PLV across a range of arithmetic and commonsense reasoning tasks, where it significantly outperforms existing benchmarks. For instance, Tree-PLV achieved substantial performance gains over the Mistral-7B self-consistency baseline on GSM8K (67.55% \u2192 82.79%), MATH (17.00% \u2192 26.80%), CSQA (68.14% \u2192 72.97%), and StrategyQA (82.86% \u2192 83.25%). Additionally, our study explores the appropriate granularity for applying preference learning, revealing that step-level guidance provides feedback that better aligns with the evaluation of the reasoning process.\n## A Related Work\nMulti-step Reasoning Challenging reasoning tasks have spurred innovative research in large language models (LLMs), which are essential for handling complex queries (Kaddour et al., 2023;Lightman et al., 2023;Huang et al., 2023).A prominent strategy employed is the Chain-of-Thought (CoT) prompting technique (Wei et al., 2022), along with its derivatives (Kojima et al., 2022;Wang et al., 2022;Yao et al., 2024).These methods decompose the reasoning process into sequential steps, systematically approaching problem-solving by mimicking human-like reasoning.To further enhance the accuracy of these intermediate steps, recent studies leverage extensive synthetic datasets, which are either distilled from cutting-edge models (Yu et al., 2023b;Luo et al., 2023) or composed of selfgenerated rationales (Zelikman et al., 2022;Yuan et al., 2023;Ni et al., 2022), to fine-tune the LLMs.\n\nSuch training effectively sharpens the models' ability to produce CoT reasoning that leads to correct answers.\n\nIn addition, there is an increasing emphasis on test-time verification, a process that generates multiple solutions and ranks them through a separate verifier (Cobbe et al., 2021) to select the most accurate one.The DIVERSE framework (Li et al., 2022) demonstrates the effectiveness of employing a variety of CoT prompts in conjunction with a verifier to tackle reasoning challenges.CoRe (Zhu et al., 2022) implements a dual-process system by finetuning both the generator and the verifier, thereby enhancing the performance of LLMs on math word problems.",
            "reference_string": "[270870071 | He et al. | 2024 | Citations: 7]"
        },
        {
            "title": "OmniSQL: Synthesizing High-quality Text-to-SQL Data at Scale",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 77,
            "citation_count": 9,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.02240, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2274084217",
                    "name": "Haoyang Li"
                },
                {
                    "authorId": "2348412920",
                    "name": "Shang Wu"
                },
                {
                    "authorId": "2108046717",
                    "name": "Xiaokang Zhang"
                },
                {
                    "authorId": "2198199708",
                    "name": "Xinmei Huang"
                },
                {
                    "authorId": "2345829238",
                    "name": "Jing Zhang"
                },
                {
                    "authorId": "2295705983",
                    "name": "Fuxin Jiang"
                },
                {
                    "authorId": "2346968040",
                    "name": "Shuai Wang"
                },
                {
                    "authorId": "2297140223",
                    "name": "Tieying Zhang"
                },
                {
                    "authorId": "2225061614",
                    "name": "Jianjun Chen"
                },
                {
                    "authorId": "2349071083",
                    "name": "Rui Shi"
                },
                {
                    "authorId": "2191043357",
                    "name": "Hong Chen"
                },
                {
                    "authorId": "2287979930",
                    "name": "Cuiping Li"
                }
            ],
            "abstract": "Text-to-SQL, the task of translating natural language questions into SQL queries, plays a crucial role in enabling non-experts to interact with databases. While recent advancements in large language models (LLMs) have significantly enhanced text-to-SQL performance, existing approaches face notable limitations in real-world text-to-SQL applications. Prompting-based methods often depend on closed-source LLMs, which are expensive, raise privacy concerns, and lack customization. Fine-tuning-based methods, on the other hand, suffer from poor generalizability due to the limited coverage of publicly available training data. To overcome these challenges, we propose a novel and scalable text-to-SQL data synthesis framework for automatically synthesizing large-scale, high-quality, and diverse datasets without extensive human intervention. Using this framework, we introduce SynSQL-2.5M, the first million-scale text-to-SQL dataset, containing 2.5 million samples spanning over 16,000 synthetic databases. Each sample includes a database, SQL query, natural language question, and chain-of-thought (CoT) solution. Leveraging SynSQL-2.5M, we develop OmniSQL, a powerful open-source text-to-SQL model available in three sizes: 7B, 14B, and 32B. Extensive evaluations across nine datasets demonstrate that OmniSQL achieves state-of-the-art performance, matching or surpassing leading closed-source and open-source LLMs, including GPT-4o and DeepSeek-V3, despite its smaller size. We release all code, datasets, and models to support further research.",
            "corpus_id": 276774742,
            "sentences": [
                {
                    "corpus_id": "276774742",
                    "title": "OmniSQL: Synthesizing High-quality Text-to-SQL Data at Scale",
                    "text": "Chain-of-thought (CoT) reasoning has demonstrated remarkable success across various challenging tasks [34,71]. By decomposing complex problems into smaller, manageable steps, this approach enables LLMs to tackle intricate tasks more effectively, improving both accuracy and interpretability. Building on this, we augment the synthetic <database, question, SQL query> triplets by generating CoT solutions that explicitly outline the reasoning process behind constructing the SQL query from the question. The prompt for synthesizing CoT solutions consists of the following components: \n\n\u2022 Task Instruction: Directs the LLM to generate a step-by-step \n\nCoT solution using the provided information. \u2022 Database Schema: Includes the CREATE TABLE statements for all relational tables in the database. \u2022 NL Question and SQL Query Pair: The natural language question and its corresponding SQL query. \n\nA typical CoT solution begins by analyzing the question to identify the key information required. It then determines the relevant tables, columns, and filtering criteria needed to retrieve the desired data. Finally, it constructs the SQL query step by step, incorporating necessary joins, filters, aggregations, groupings, and other operators, culminating in the complete SQL query as the final answer. \n\nInterestingly, in our preliminary experiments, we observe that the SQL queries generated by the synthetic CoT sometimes differ  from the original ones. Upon closer examination, we find that CoTgenerated SQL queries often better align with the questions compared to the original SQL queries. This improvement arises because the original <database, question, SQL query> triplets occasionally contain minor issues, such as unnecessary column selections, and incorrect join paths. The CoT synthesis process allows the LLM to identify and correct these issues during step-by-step reasoning, resulting in more accurate and refined SQL queries. This observation also aligns with prior research showing that LLMs excel at detecting and resolving minor errors in predicted SQL queries [22,52,64]. Thus, incorporating CoT not only provides detailed solutions but also enhances the overall quality of the synthetic data. \n\nTo enhance the diversity and reliability of synthetic CoT solutions, we generate multiple candidate CoT solutions for each synthetic <database, question, SQL query> triplet. To select the most reliable CoT solution, we extract SQL queries from these candidates and perform a majority vote.",
                    "score": 0.4491453662916123,
                    "section_title": "Chain-of-Thought Solution Synthesis",
                    "char_start_offset": 25307,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 110
                        },
                        {
                            "start": 111,
                            "end": 291
                        },
                        {
                            "start": 292,
                            "end": 502
                        },
                        {
                            "start": 503,
                            "end": 582
                        },
                        {
                            "start": 585,
                            "end": 647
                        },
                        {
                            "start": 650,
                            "end": 694
                        },
                        {
                            "start": 695,
                            "end": 793
                        },
                        {
                            "start": 794,
                            "end": 890
                        },
                        {
                            "start": 893,
                            "end": 990
                        },
                        {
                            "start": 991,
                            "end": 1099
                        },
                        {
                            "start": 1100,
                            "end": 1295
                        },
                        {
                            "start": 1298,
                            "end": 1449
                        },
                        {
                            "start": 1450,
                            "end": 1588
                        },
                        {
                            "start": 1589,
                            "end": 1774
                        },
                        {
                            "start": 1775,
                            "end": 1935
                        },
                        {
                            "start": 1936,
                            "end": 2085
                        },
                        {
                            "start": 2086,
                            "end": 2207
                        },
                        {
                            "start": 2210,
                            "end": 2383
                        },
                        {
                            "start": 2384,
                            "end": 2499
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 102,
                            "end": 106,
                            "matchedPaperCorpusId": "249017743"
                        },
                        {
                            "start": 106,
                            "end": 109,
                            "matchedPaperCorpusId": "246411621"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76171875
                }
            ],
            "relevance_judgement": 0.76171875,
            "relevance_judgment_input_expanded": "# Title: OmniSQL: Synthesizing High-quality Text-to-SQL Data at Scale\n# Venue: arXiv.org\n# Authors: Haoyang Li, Shang Wu, Xiaokang Zhang, Xinmei Huang, Jing Zhang, Fuxin Jiang, Shuai Wang, Tieying Zhang, Jianjun Chen, Rui Shi, Hong Chen, Cuiping Li\n## Abstract\nText-to-SQL, the task of translating natural language questions into SQL queries, plays a crucial role in enabling non-experts to interact with databases. While recent advancements in large language models (LLMs) have significantly enhanced text-to-SQL performance, existing approaches face notable limitations in real-world text-to-SQL applications. Prompting-based methods often depend on closed-source LLMs, which are expensive, raise privacy concerns, and lack customization. Fine-tuning-based methods, on the other hand, suffer from poor generalizability due to the limited coverage of publicly available training data. To overcome these challenges, we propose a novel and scalable text-to-SQL data synthesis framework for automatically synthesizing large-scale, high-quality, and diverse datasets without extensive human intervention. Using this framework, we introduce SynSQL-2.5M, the first million-scale text-to-SQL dataset, containing 2.5 million samples spanning over 16,000 synthetic databases. Each sample includes a database, SQL query, natural language question, and chain-of-thought (CoT) solution. Leveraging SynSQL-2.5M, we develop OmniSQL, a powerful open-source text-to-SQL model available in three sizes: 7B, 14B, and 32B. Extensive evaluations across nine datasets demonstrate that OmniSQL achieves state-of-the-art performance, matching or surpassing leading closed-source and open-source LLMs, including GPT-4o and DeepSeek-V3, despite its smaller size. We release all code, datasets, and models to support further research.\n## Chain-of-Thought Solution Synthesis\nChain-of-thought (CoT) reasoning has demonstrated remarkable success across various challenging tasks [34,71]. By decomposing complex problems into smaller, manageable steps, this approach enables LLMs to tackle intricate tasks more effectively, improving both accuracy and interpretability. Building on this, we augment the synthetic <database, question, SQL query> triplets by generating CoT solutions that explicitly outline the reasoning process behind constructing the SQL query from the question. The prompt for synthesizing CoT solutions consists of the following components: \n\n\u2022 Task Instruction: Directs the LLM to generate a step-by-step \n\nCoT solution using the provided information. \u2022 Database Schema: Includes the CREATE TABLE statements for all relational tables in the database. \u2022 NL Question and SQL Query Pair: The natural language question and its corresponding SQL query. \n\nA typical CoT solution begins by analyzing the question to identify the key information required. It then determines the relevant tables, columns, and filtering criteria needed to retrieve the desired data. Finally, it constructs the SQL query step by step, incorporating necessary joins, filters, aggregations, groupings, and other operators, culminating in the complete SQL query as the final answer. \n\nInterestingly, in our preliminary experiments, we observe that the SQL queries generated by the synthetic CoT sometimes differ  from the original ones. Upon closer examination, we find that CoTgenerated SQL queries often better align with the questions compared to the original SQL queries. This improvement arises because the original <database, question, SQL query> triplets occasionally contain minor issues, such as unnecessary column selections, and incorrect join paths. The CoT synthesis process allows the LLM to identify and correct these issues during step-by-step reasoning, resulting in more accurate and refined SQL queries. This observation also aligns with prior research showing that LLMs excel at detecting and resolving minor errors in predicted SQL queries [22,52,64]. Thus, incorporating CoT not only provides detailed solutions but also enhances the overall quality of the synthetic data. \n\nTo enhance the diversity and reliability of synthetic CoT solutions, we generate multiple candidate CoT solutions for each synthetic <database, question, SQL query> triplet. To select the most reliable CoT solution, we extract SQL queries from these candidates and perform a majority vote.",
            "reference_string": "[276774742 | Li et al. | 2025 | Citations: 9]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "265150344",
            "title": "Hallucination Augmented Recitations for Language Models",
            "text": "To the best of our knowledge, we are the first ones to utilize LLM hallucination to create counterfactual datasets. Synthetic Data Generation: Synthetic question answering dataset generation without counterfactuals has shown limited improvement in out-ofdomain generalization (Bartolo et al., 2021;Lewis et al., 2021). However, recent advancements in large language models (LLMs) have led to growing interest in synthetic data generation with LLMs, such as in more generalized instruction tuning datasets from scratch (Wang et al., 2023) or by restructuring existing corpora (K\u00f6ksal et al., 2023). Synthetic data generation with LLMs has also been applied to existing datasets for specific tasks to improve model quality, such as natural language inference (Liu et al., 2022) and sentiment analysis (Meng et al., 2023).",
            "score": 0.6072823695384437,
            "section_title": "Related Work",
            "char_start_offset": 21389,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 819
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 298,
                    "matchedPaperCorpusId": "233296924"
                },
                {
                    "start": 298,
                    "end": 317,
                    "matchedPaperCorpusId": "231924957"
                },
                {
                    "start": 518,
                    "end": 537,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 757,
                    "end": 775,
                    "matchedPaperCorpusId": "246016339"
                },
                {
                    "start": 799,
                    "end": 818,
                    "matchedPaperCorpusId": "253384628"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8486328125
        },
        {
            "corpus_id": "10817864",
            "title": "The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages",
            "text": "Synthetic data generation is a well-known trick for effectively training a large model on a small dataset. Abu-Mostafa (1995) reviews early work that provided \"hints\" to a learning system in the form of virtual training examples. While datasets have grown in recent years, so have models: e.g., neural networks have many parameters to train. Thus, it is still common to create synthetic training examples-often by adding noise to real inputs or otherwise transforming them in ways that are expected to preserve their labels. Domains where it is easy to exploit these invariances include image recognition (Simard et al., 2003;Krizhevsky et al., 2012), speech recognition (Jaitly and Hinton, 2013;Cui et al., 2015), information retrieval (Vilares et al., 2011), and grammatical error correction (Rozovskaya and Roth, 2010). \n\nSynthetic datasets have also arisen recently for semantic tasks in natural language processing. bAbI is a dataset of facts, questions, and answers, generated by random simulation, for training machines to do simple logic (Weston et al., 2016). Hermann et al. (2015) generate reading comprehension questions and their answers, based on a large set of newssummarization pairs, for training machine readers. Serban et al. (2016) used RNNs to generate 30 million factoid questions about Freebase, with answers, for training question-answering systems. Wang et al. (2015) obtain data to train semantic parsers in a new domain by first generating synthetic (utterance, logical form) pairs and then asking human annotators to paraphrase the synthetic utterances into more natural human language. \n\nIn speech recognition, morphology-based \"vocabulary expansion\" creates synthetic word forms (Rasooli et al., 2014;Varjokallio and Klakow, 2016). \n\nMachine translation researchers have often tried to automatically preprocess parse trees of a source language to more closely resemble those of the target language, using either hand-crafted or automatically extracted rules (Dorr et al., 2002;Collins et al., 2005, etc.; see review by Howlett and Dras, 2011).",
            "score": 0.6019626798669611,
            "section_title": "Related Work",
            "char_start_offset": 3552,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1613
                },
                {
                    "start": 1616,
                    "end": 1760
                },
                {
                    "start": 1763,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 605,
                    "end": 626,
                    "matchedPaperCorpusId": "4659176"
                },
                {
                    "start": 626,
                    "end": 650,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 671,
                    "end": 696,
                    "matchedPaperCorpusId": "14140670"
                },
                {
                    "start": 696,
                    "end": 713,
                    "matchedPaperCorpusId": "3355461"
                },
                {
                    "start": 794,
                    "end": 821,
                    "matchedPaperCorpusId": "16131749"
                },
                {
                    "start": 1046,
                    "end": 1066,
                    "matchedPaperCorpusId": "3178759"
                },
                {
                    "start": 1066,
                    "end": 1090,
                    "matchedPaperCorpusId": "6203757"
                },
                {
                    "start": 1230,
                    "end": 1250,
                    "matchedPaperCorpusId": "12241221"
                },
                {
                    "start": 1373,
                    "end": 1391,
                    "matchedPaperCorpusId": "14472576"
                },
                {
                    "start": 1708,
                    "end": 1730,
                    "matchedPaperCorpusId": "7180044"
                },
                {
                    "start": 1730,
                    "end": 1759,
                    "matchedPaperCorpusId": "14784553"
                },
                {
                    "start": 1987,
                    "end": 2006,
                    "matchedPaperCorpusId": "7766190"
                },
                {
                    "start": 2048,
                    "end": 2071,
                    "matchedPaperCorpusId": "9592788"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81884765625
        },
        {
            "corpus_id": "254877131",
            "title": "Optimization Techniques for Unsupervised Complex Table Reasoning via Self-Training Framework",
            "text": "Unsupervised data generation has been extensively studied on various tasks like question answering and natural language inference, and has shown surprising performances [51], [53] [54]. Recently, methods for synthesizing human-like tabular reasoning samples have also been proposed [52], [56]. Chemmengath et al. [46] sample complex SQL queries and generate natural language questions in a seq2seq manner. Eisenschlos et al. [29] generate factual claims leveraging context-free grammar (CFG) and counterfactual heuristics. Unfortunately, these methods focus on a specific task or scenario. Based on the modules and predefined operators, our approach can convert different types of programs into natural language questions or claims with tabular evidence or a hybrid of tabular and textual evidence. \n\nSelf-training has been widely explored in the realm of semi-supervised learning [60], [63], [65]. For example, Li et al. [66] proposed FlexKBQA, a method that combines selftraining and synthetic data to improve the performance of few-shot knowledge based question answering. Most works employ self-training techniques in the few-shot setting, where a small number of labeled samples are available. However, this study effectively combines data generation methods with selftraining to achieve good results in an unsupervised scenario.",
            "score": 0.5937405089648865,
            "section_title": "B. Unsupervised Data Generation and Self-Training.",
            "char_start_offset": 45826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1334
                }
            ],
            "ref_mentions": [
                {
                    "start": 881,
                    "end": 885,
                    "matchedPaperCorpusId": "250451588"
                },
                {
                    "start": 887,
                    "end": 891,
                    "matchedPaperCorpusId": "7890982"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "18507866"
                },
                {
                    "start": 922,
                    "end": 926,
                    "matchedPaperCorpusId": "261076103"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88037109375
        },
        {
            "corpus_id": "263605725",
            "title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models",
            "text": "Human Annotation. Crowdsourcing is one of the major approaches to create datasets, such as for NLI tasks. The advantages of this methodology include a richer linguistic grammar and potentially increased task complexity. However, it comes with drawbacks. In addition to being a costintensive process, crowdsourced datasets tend to harbor biases (as highlighted in numerous previous studies (Yu et al., 2019)). These biases can be leveraged by neural models to artificially inflate accuracy scores. Furthermore, assembling a dataset for logical reasoning tasks demands a level of expertise that poses a significant challenge. \n\nExtraction from Academic Challenge. It is hard for crowdsourcing workers to produce questions requiring complicated logical reasoning since such reasoning tasks require extensive training and practice. Fortunately, questions in some standardized tests are aligned with the goal of logical reasoning and can be utilized to create such datasets after some preprocessing (Yu et al., 2019;Liu et al., 2020). However, the domains of these examinations are limited and the dataset size is small. Synthetic Generation. Synthetic generation is more efficient to create large data than manually created ones (Luo et al., 2022b). There are two ways, simulation based (Weston et al., 2015b) and rule-based (Clark et al., 2020;Saeed et al., 2021;Banerjee et al., 2020). In rule based methods, logic programs (either written by humans or mined from knowledge graphs) are generated, and then implications are drawn by automatic theorem prover. Last, the rules and facts in the logic programs are converted into English form using natural language patterns. Synthetic generation has issues that the rules or facts do not have real-world meaning and the language could be simple.",
            "score": 0.5927005182116336,
            "section_title": "Dataset Creation Techniques",
            "char_start_offset": 11533,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 18,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1789
                }
            ],
            "ref_mentions": [
                {
                    "start": 389,
                    "end": 406,
                    "matchedPaperCorpusId": "209485573"
                },
                {
                    "start": 994,
                    "end": 1011,
                    "matchedPaperCorpusId": "209485573"
                },
                {
                    "start": 1011,
                    "end": 1028,
                    "matchedPaperCorpusId": "220483148"
                },
                {
                    "start": 1225,
                    "end": 1244,
                    "matchedPaperCorpusId": "250311154"
                },
                {
                    "start": 1321,
                    "end": 1341,
                    "matchedPaperCorpusId": "211126663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58837890625
        },
        {
            "corpus_id": "254408974",
            "title": "Successive Prompting for Decomposing Complex Questions",
            "text": "Any method that prompts LMs to produce intermediate reasoning steps to answer complex questions needs some amount of supervision for those reasoning steps. This kind of annotation can be expensive to collect and often requires expert knowledge.   Figure 3).\n\nPrior work has typically relied on a small handful of manually-written example decompositions. We find that such small collections lead to very poor performance on a dataset as varied as DROP, even for large models.\n\nTo mitigate these data issues, we propose a way to synthetically generate complex questions and their decompositions using semi-structured data which is easy to parse. We show that we can bootstrap model learning with this out-of-domain, synthetically generated data so it can adapt better when fine-tuned with limited in-domain supervision.\n\nGeneration Process: Inspired by Yoran et al.\n\n(2021), we use semi-structured data from tables in English Wikipedia which are available in plenty. We employ curated templates to convert the rows in the tables into paragraphs. We use single column headers to create first order simple questions and a combination of columns for higher order complex questions. We synthesize data for 10 simple operations: COUNT, TOP(k), BOTTOM(k), FILTER, SUM, COMPARISON, DIFFERENCE, NEGATION, GATHER, and INTERSECTION.\n\nWe generate higher order combinations of firstorder operations, wherever possible. Figure 3 shows examples of higher order combinations of the atomic operation COUNT with a few other simple operations using Table 1 Figure 3: Examples of COUNT operation and some of its higher order combinations, with natural language and symbolic decompositions of the complex question. Underneath the first instance of a symbolic operation we show its corresponding natural language version. See Table 1 for the original table used to generate context and questions.",
            "score": 0.5922391191384221,
            "section_title": "Synthetic Dataset",
            "char_start_offset": 13297,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.861328125
        },
        {
            "corpus_id": "260846474",
            "title": "Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic",
            "text": "A synthetic deduction corpus can be one promising approach for language models (LMs) to acquire logical deductive reasoning ability. The automatic (programmatic) generation ensures the validity of the resulting deductive proof examples. \n\nFurther, since we can bypass high-cost human annotations we can generate many examples, which should be required by LMs to learn deductive reasoning inductively. \n\nRuleTaker (Clark et al., 2021) proposed a deduction corpus composed of synthetically generated multistep deductive proofs written in natural languages. Each deductive proof (dis-)proves a hypothesis by applying deduction rules multiple times to a given set of facts. As the deduction rules, rules of the implication kind were used. They showed that Transformer (Vaswani et al., 2017) LMs can solve these problems in the sense that they can predict the final answer (i.e., \"proved\", \"disproved\", or \"unknown\") of each deductive proof given the fact set. Later studies (Saha et al., 2020;Dalvi et al., 2021;Tafjord et al., 2021;Sanyal et al., 2022) showed that generative LMs can generate even the intermediate proofs as well as the final answer. \n\nArtificial Argument Corpus (Betz et al., 2021) is another corpus composed of synthetically generated single-step deductive proofs. As the deduction rules, hand-selected rules useful for critical thinking were used. They showed that the LMs trained on this corpus not only solve the task of this corpus itself but generalize to other NLI tasks from GLUE benchmark (Wang et al., 2018). However, at the same time, they showed that such LMs do not generalize well to more challenging logical reasoning tasks such as ARC (Habernal et al., 2018) and LogiQA (Liu et al., 2020). Gontier et al. (2020) investigated the deductive reasoning ability of LMs on a corpus, which is composed of a specific type of multistep inference: kinship relationship on synthetic kinship graphs.",
            "score": 0.5646204039265872,
            "section_title": "A.1. Synthetic Corpus for Acquiring Deductive Reasoning Ability",
            "char_start_offset": 34565,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 236
                },
                {
                    "start": 239,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1147
                },
                {
                    "start": 1150,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1918
                }
            ],
            "ref_mentions": [
                {
                    "start": 413,
                    "end": 433,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 764,
                    "end": 786,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 970,
                    "end": 989,
                    "matchedPaperCorpusId": "222141025"
                },
                {
                    "start": 989,
                    "end": 1008,
                    "matchedPaperCorpusId": "233297051"
                },
                {
                    "start": 1008,
                    "end": 1029,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 1029,
                    "end": 1049,
                    "matchedPaperCorpusId": "247594506"
                },
                {
                    "start": 1177,
                    "end": 1196,
                    "matchedPaperCorpusId": "221702898"
                },
                {
                    "start": 1513,
                    "end": 1532,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1666,
                    "end": 1689,
                    "matchedPaperCorpusId": "3555187"
                },
                {
                    "start": 1701,
                    "end": 1719,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8564453125
        },
        {
            "corpus_id": "222090330",
            "title": "Understanding tables with intermediate pre-training",
            "text": "The most similar approach to ours appears in Xiong et al. (2020), replacing entities in Wikipedia by others with the same type for a MASK-LM model objective. We, on the one hand, take advantage of other rows in the table to produce plausible negatives, and also replace dates and numbers. Recently, Kaushik et al. Numeric reasoning Numeric reasoning in Natural Language processing has been recognized as an important part in entailment models (Sammons et al., 2010) and reading comprehension (Ran et al., 2019). Wallace et al. (2019) studied the capacity of different models on understanding numerical operations and show that BERT-based model still have headroom. This motivates the use of the synthetic generation approach to improve numerical reasoning in our model.\n\nSynthetic data generation Synthetic data has been used to improve learning in NLP tasks (Alberti et al., 2019;Lewis et al., 2019;Wu et al., 2016;Leonandya et al., 2019). In semantic parsing for example (Wang et al., 2015;Iyer et al., 2017;Weir et al., 2020), templates are used to bootstrap models that map text to logical forms or SQL. Salvatore et al. (2019) use synthetic data generated from logical forms to evaluate the performance of textual entailment models (e.g., BERT). Geiger et al. (2019) use synthetic data to create fair evaluation sets for natural language inference. Geva et al. (2020) show the importance of injecting numerical reasoning via generated data into the model to solve reading comprehension tasks. They propose different templates for generating synthetic numerical examples. In our work we use a method that is better suited for tables and to the entailment task, and is arguably simpler.",
            "score": 0.5624707358635029,
            "section_title": "Counterfactual data generation",
            "char_start_offset": 25077,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 45,
                    "end": 64,
                    "matchedPaperCorpusId": "209439872"
                },
                {
                    "start": 512,
                    "end": 533,
                    "matchedPaperCorpusId": "202583694"
                },
                {
                    "start": 859,
                    "end": 881,
                    "matchedPaperCorpusId": "189762081"
                },
                {
                    "start": 900,
                    "end": 916,
                    "matchedPaperCorpusId": "3099009"
                },
                {
                    "start": 973,
                    "end": 992,
                    "matchedPaperCorpusId": "14472576"
                },
                {
                    "start": 1010,
                    "end": 1028,
                    "matchedPaperCorpusId": "218981535"
                },
                {
                    "start": 1251,
                    "end": 1271,
                    "matchedPaperCorpusId": "202763794"
                },
                {
                    "start": 1354,
                    "end": 1372,
                    "matchedPaperCorpusId": "215548225"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8525390625
        },
        {
            "corpus_id": "253384598",
            "title": "Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference",
            "text": "With the advancement of text generation models in recent years (Radford et al., 2019;Brown et al., 2020;Lewis et al., 2020), researchers have started to adopt synthetic text generation as a data augmentation strategy for various tasks such as commonsense reasoning (Yang et al., 2020), sentence classification (Anaby-Tavor et al., 2020) and question answering (Puri et al., 2020). More recently, synthetic text generation methods have also been employed to remove spurious correlations in human annotated data for fact verification (Lee et al., 2021) and NLI (Wu et al., 2022). However, to our knowledge, no prior work aims at tackling low-resource NLI scenarios nor do they address the unavailability of hypotheses for unlabeled premises by using synthetic data generation methods.",
            "score": 0.5585385263552101,
            "section_title": "Synthetic Data Generation",
            "char_start_offset": 28074,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 85,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 104,
                    "end": 123,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 265,
                    "end": 284,
                    "matchedPaperCorpusId": "250390686"
                },
                {
                    "start": 360,
                    "end": 379,
                    "matchedPaperCorpusId": "211258652"
                },
                {
                    "start": 532,
                    "end": 550,
                    "matchedPaperCorpusId": "238227205"
                },
                {
                    "start": 559,
                    "end": 576,
                    "matchedPaperCorpusId": "247628095"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90380859375
        },
        {
            "corpus_id": "258832686",
            "title": "LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4",
            "text": "This notion inspired researchers to use self-generated rationales for demonstrations. In particular, Zelikman et al. (2022) demonstrated the practicality of using LLMs to generate rationales. They prompted GPT-J (Wang and Komatsuzaki, 2021) to generate rationales and then selected the ones leading to the correct answer. We adopt this method for GPT-4 generation. Our approach, however, tackles complex logical reasoning scenarios utilizing questions with annotated answers. \n\nLogical reasoning. Logical reasoning is a key aspect of human cognition and a critical capability for AI systems. Researchers have been exploring various approaches to achieve this goal, including rule-based methods, symbolic systems (Mac-Cartney and Manning, 2007), fine-tuning large language models (Wang et al., 2018), and combining both neural and symbolic approaches (Li and Srikumar, 2019). Logical reasoning tasks often require multi-step, complex reasoning, which makes them an ideal target for CoT instruction tuning. By integrating logical reasoning tasks into CoT instruction tuning, we can push the boundaries of what AI systems can achieve and get closer to systems that can understand and reason about the world in a human-like way.",
            "score": 0.5584547934613179,
            "section_title": "Related Work",
            "char_start_offset": 7277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1224
                }
            ],
            "ref_mentions": [
                {
                    "start": 779,
                    "end": 798,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 850,
                    "end": 873,
                    "matchedPaperCorpusId": "189898023"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5576171875
        },
        {
            "corpus_id": "272753174",
            "title": "LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning",
            "text": "Synthetic data has played an important role in enhancing LLMs' reasoning capabilities (Dubey et al., 2024), especially in mathematics and coding domains. \n\nFor mathematical reasoning, synthetic data generation includes problem-driven methods such as evol-instruct (Luo et al., 2023a;Zeng et al., 2024), problem restatement (Yu et al., 2023), backtranslation (Lu et al., 2024), and few-shot examples (Dalvi et al., 2021), as well as knowledge-driven methods that rely on knowledge bases (Dalvi et al., 2021) or concept graphs (Tang et al., 2024) to generate new problems by sampling key reasoning points (Huang et al., 2024). \n\nIn terms of code reasoning, from Code Alpaca's (Wang et al., 2023) use of self-instruct to generate 20K instruction data based on seed tasks, to Wiz-ardCoder's (Luo et al., 2023b) use of Code evolinstruct to generate more complex tasks, to Magicoder's use of oss-instruct (Wei et al., 2024) to extract 75K instructions from open source code. Synthesizing data continuously improves the model's code reasoning capabilities. \n\nIn contrast, there is less research on synthetic data for complex logical reasoning. Learning combines formal logic theory (Morishita et al., 2023) to synthesize data using basic reasoning rules to train language models' multi-step reasoning abilities. LogicBench (Parmar et al., 2024) not only constructs logical reasoning benchmark datasets but also provides synthetic data based on formal logic. \n\nLeetCode-style algorithm problems contain rich reasoning processes. This paper synthesizes highquality complex logical sreasoning data based on the formal logic of programming languages to enhance models' reasoning capabilities.",
            "score": 0.5559595643864345,
            "section_title": "A Related work A.1 Synthetic Reasoning Data",
            "char_start_offset": 25934,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 156,
                    "end": 624
                },
                {
                    "start": 627,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1049
                },
                {
                    "start": 1052,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1681
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 419,
                    "matchedPaperCorpusId": "233297051"
                },
                {
                    "start": 486,
                    "end": 506,
                    "matchedPaperCorpusId": "233297051"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8291015625
        },
        {
            "corpus_id": "272600015",
            "title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources",
            "text": "Large Language Models (LLMs) (Devlin et al., 2019;Chowdhery et al., 2022;Brown et al., 2020;Vaswani et al., 2017) have risen to popularity due to their remarkable ability to digest and generate human-like text (Radford et al., 2018). However, LLMs still struggle with more complex tasks such as multi-step reasoning, tool use and manipulating or processing structured data. For many of these tasks there exists source data, such as existing structured data on the web, but little data of how to use it to solve a task.In principle, one can achieve performance improvements during fine-tuning by collecting human annotated data of such tasks. However, this is an expensive and time-consuming process (Touvron et al., 2023). \n\nIn this paper, we propose Source2Synth, a general approach to generate synthetic data grounded in external real-world sources. Grounding the data generation process in real-world sources steers the examples to be more realistic, diverse, and factually correct. We showcase our method on two challenging tasks: multi-hop questions based on sources from the web, and tabular question answering using SQL as a tool. In both cases it achieves improved performance without relying on human annotations, resulting in a scalable data generation method for complex tasks. \n\nSource2Synth consists of three stages: Dataset Generation and Dataset Curation, followed by Model Finetuning, see Figure 1. At the Data Generation stage, we start by selecting a data source (such as tables on the web, or related Wikipedia articles) to ground our synthetic data generation in realistic information for a specific task. Then, to generate a given example, our method first selects a seed topic to condition the generation -for example a specific entity in a Wikipedia article or a factual statement about a table. Given the seed topic, the method then generates the full example: the instruction (e.g., question), the reasoning chain to arrive at the answer (e.g., the steps of multi-hop question answering, or tool use) and the answer itself.",
            "score": 0.5541473068744649,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 722
                },
                {
                    "start": 725,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 2048
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 113,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9638671875
        },
        {
            "corpus_id": "251953302",
            "title": "Generating Intermediate Steps for NLI with Next-Step Supervision",
            "text": "Complex NLP tasks such as Natural Language Inference (NLI) and Question-Answering (QA) often requires reasoning over multiple steps using multiple facts and implicit commonsense knowledge (Trivedi et al., 2020;Sap et al., 2019;Camburu et al., 2018). It has been long argued that (Lipton, 2018), the state-of-the-art Deep Learning models should also output some sort of explanation (such as intermediate steps or a textual explanation) alongwith the final answer. The opaque performance and poor out-of-distribution generalization performance of Transformers-based models (Kaushik et al., 2019;Ribeiro et al., 2020) have refuelled this discussion. However, it is unclear how these intermediate steps can be generated for unconstrained natural language Premise-hypothesis pairs (such as in crowd-sourced NLI datasets) as it is non-trivial to collect crowd-sourced fine-grained explanations or generate them synthetically. Most importantly, it is unclear how such steps can be utilized further for the NLI task. \n\nDespite some efforts (Camburu et al., 2018) crowd-sourced collection of intermediate steps comes with complications, as human-written explanation can be subjective, and it is hard to automatically verify or utilize such explanations. Recently, researchers have explored synthetic generation of intermediate steps (or proof trees) (Clark et al., 2020;Tafjord et al., 2021;Saha et al., 2020), where the main goal was to test whether Transformers (the backbone for NLP models) can perform deductive reasoning over natural language statements. Provided their examples come from an underlying symbolic system with closed world rules and facts, it is unclear how this strategy of generating proof tree can be extended to unconstrained natural language premise-hypothesis pairs. In comparably structured domains, such as code snippet retrieval, knowledge-graph based QA and symbolic mathematics, Nye et al. (2021); He et al. (2021a); Agarwal et al. (2021) have shown how to generate such intermediate steps automatically, and how utilizing such steps can enhance end-to-end task performance. \n\nWe take inspiration from these domains.",
            "score": 0.5520075202711578,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 2095
                },
                {
                    "start": 2098,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 188,
                    "end": 210,
                    "matchedPaperCorpusId": "221749191"
                },
                {
                    "start": 210,
                    "end": 227,
                    "matchedPaperCorpusId": "53170360"
                },
                {
                    "start": 227,
                    "end": 248,
                    "matchedPaperCorpusId": "54040953"
                },
                {
                    "start": 279,
                    "end": 293,
                    "matchedPaperCorpusId": "5981909"
                },
                {
                    "start": 593,
                    "end": 614,
                    "matchedPaperCorpusId": "218551201"
                },
                {
                    "start": 1032,
                    "end": 1053,
                    "matchedPaperCorpusId": "54040953"
                },
                {
                    "start": 1341,
                    "end": 1361,
                    "matchedPaperCorpusId": "8587959"
                },
                {
                    "start": 1361,
                    "end": 1382,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 1382,
                    "end": 1400,
                    "matchedPaperCorpusId": "222141025"
                },
                {
                    "start": 1919,
                    "end": 1936,
                    "matchedPaperCorpusId": "231572861"
                },
                {
                    "start": 1938,
                    "end": 1959,
                    "matchedPaperCorpusId": "233443775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69287109375
        },
        {
            "corpus_id": "276318012",
            "title": "MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via Event-Driven Text-Code Cyclic Training",
            "text": "Recent studies have emphasized the effectiveness of synthetic datasets in mitigating inconsistent hallucinations related to specific tasks. For instance, synthetic mathematical datasets (Toshniwal et al., 2024;Huang et al., 2024) have shown reduced contradictions in the calculation process and significant improvements in solving mathematical problems. Additionally, some research has synthesized reasoning data in the logical reasoning (Nie et al., 2020;Saeed et al., 2021) field while providing the reasoning process (Dalvi et al., 2021) during synthesis. LOGIC-LM and SymbCOT (Pan et al., 2023;Xu et al., 2024) integrate LLMs with symbolic solvers to enhance logical problem-solving. Unfortunately, the synthetic datasets that have been developed are often tailored to specific tasks or domains, which can restrict their generalizability. \n\nAdvancements in training LLMs on extensive code datasets, such as CoCoGen and CodeRL, have significantly reduced inconsistent hallucinations in code generation tasks (Madaan et al., 2022;Le et al., 2022). However, these improvements have not effectively transferred to general natural language tasks due to fundamental differences in semantics, language styles, and structures between code and natural language. The precision of code contrasts with the ambiguity of human language, posing challenges in adapting code-focused techniques for natural language understanding and generation. Recent approaches, such as program-ofthought (Chen et al., 2023) and program-assisted LMs (Gao et al., 2023), have sought to bridge this divide by interpreting natural language problems and generating corresponding code solutions. However, these methodologies fail to successfully transfer the logical consistency capabilities gained from code training to a broader spectrum of NLP tasks. \n\nInspired by code-oriented LLMs that excel in generating logically consistent code, we propose that integrating code data is vital for enhancing the ability of LLMs to maintain logical consistency across various NLP applications. Furthermore, we observe that there exists a correspondence between the structure of event-based text and that of code.",
            "score": 0.5467878734674013,
            "section_title": "Inconsistent Hallucinations Explanation",
            "char_start_offset": 1803,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1820
                },
                {
                    "start": 1823,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2170
                }
            ],
            "ref_mentions": [
                {
                    "start": 438,
                    "end": 456,
                    "matchedPaperCorpusId": "207756753"
                },
                {
                    "start": 456,
                    "end": 475,
                    "matchedPaperCorpusId": "237940562"
                },
                {
                    "start": 520,
                    "end": 540,
                    "matchedPaperCorpusId": "233297051"
                },
                {
                    "start": 580,
                    "end": 598,
                    "matchedPaperCorpusId": "258833332"
                },
                {
                    "start": 1011,
                    "end": 1032,
                    "matchedPaperCorpusId": "252873120"
                },
                {
                    "start": 1032,
                    "end": 1048,
                    "matchedPaperCorpusId": "250280117"
                },
                {
                    "start": 1477,
                    "end": 1496,
                    "matchedPaperCorpusId": "253801709"
                },
                {
                    "start": 1522,
                    "end": 1540,
                    "matchedPaperCorpusId": "253708270"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77734375
        },
        {
            "corpus_id": "277104955",
            "title": "Synthetic Data Generation Using Large Language Models: Advances in Text and Code",
            "text": "A bootstrapping algorithm where a pretrained LM generates new tasks and solutions from a few seed examples. Initially applied to create Stanford Alpaca dataset (52k instructions) using text-davinci-003. This method showed that an LLM can recursively teach itself new tasks, yielding high-quality multi-turn instruction data without manual writing. Inspired many subsequent works in synthetic instruction generation for both text and code. Empirical Study [35] Text Classification (6 tasks) Systematic analysis of GPT-3.5 generated data for classification. Showed significant gains (3-26%) with synthetic augmentation at 100 real samples, diminishing returns as data increases. Compared prompting methods: finding that topic-guided generation improves diversity and performance. Provided practical recommendations on how much data to generate and which prompting strategy to use in various scenarios. AugGPT: Leveraging ChatGPT for Text Data Augmentation [9] Few-shot text classification (Amazon Reviews, PubMed20K, Symptoms Dataset) \n\nAugGPT rephrases sentences into multiple diverse yet semantically consistent versions, improving training data quality. Evaluation shows AugGPT significantly improves model accuracy compared to traditional augmentation methods (e.g., synonym replacement, back-translation). Findings suggest ChatGPT-generated synthetic data is a scalable, highquality augmentation strategy, making it a cost-effective alternative to manual data expansion in NLP. Self-Ask [47] Multi-hop Question Answering (QA), Reasoning \n\nIntroduced an LLM-based self-asking strategy where models recursively break down complex questions into simpler sub-questions. Demonstrated improved reasoning performance on multi-step inference tasks by leveraging self-generated sub-questions for enhanced accuracy. CoT [13] Scientific QA, Logical Reasoning \n\nExamined how Chain-of-Thought (CoT) prompting improves LLM reasoning capabilities. Proposed Sci-CoT, a two-stage framework where intermediate rationales are generated separately from answer inference, improving efficiency and model performance. \n\nEnd of Table 3.",
            "score": 0.5423484238828125,
            "section_title": "Various approaches for",
            "char_start_offset": 20773,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1032
                },
                {
                    "start": 1035,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1539
                },
                {
                    "start": 1542,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1850
                },
                {
                    "start": 1853,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2097
                },
                {
                    "start": 2100,
                    "end": 2115
                }
            ],
            "ref_mentions": [
                {
                    "start": 1813,
                    "end": 1817,
                    "matchedPaperCorpusId": "264288838"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94775390625
        },
        {
            "corpus_id": "270045573",
            "title": "Text Generation: A Systematic Literature Review of Tasks, Evaluation, and Challenges",
            "text": "Reasoning describes the capability of a text generation system to infer its choices and writing from the source context logically and sensibly. A model should be able to make an understandable decision and explain its reasoning to a human. \n\nSeveral works point out that, even though recent years yielded significant improvements, LMs have difficulties with such commonsense inference [109,225]. Thus, eliciting reasoning with text generation is an active research topic. The lack of reasoning capabilities comes from misunderstanding facts in the source context and is also referred to as intrinsic hallucinations [79] (more details in Section 5.3). Text generation systems require reasoning capabilities for complex tasks like story generation [108], question answering [47], or math problems [28]. \n\nOne popular approach to improve reasoning is Chain-of-Thought Prompting (CoT) [206]. Here, the model is prompted to think step by step and output its thought process in natural language. It improves performance for arithmetic, commonsense, and symbolic reasoning tasks. Exchange-of-Thought maintains the reasoning capabilities of CoT [206] while allowing for an iterative self-interaction that improves the output text [219]. The approach imitates a multi-turn conversation between experts on a task. Chen et al. [24] survey another method to improve reasoning capabilities via path-finding strategies on a knowledge graph. We also find considerable research related to testing a model's reasoning capabilities. Stolfo et al. [177] explain how to specifically test the robustness of mathematical reasoning by grounding a behavioral analysis in a causal graph. The CommonGen dataset [109] requires relational reasoning and compositional generalization. The HellaSwag dataset [225] can also be used to test a model's ability of generative commonsense reasoning.",
            "score": 0.534441569028108,
            "section_title": "Reasoning",
            "char_start_offset": 65679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 239
                },
                {
                    "start": 242,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1862
                }
            ],
            "ref_mentions": [
                {
                    "start": 385,
                    "end": 390,
                    "matchedPaperCorpusId": "218500588"
                },
                {
                    "start": 390,
                    "end": 394,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 615,
                    "end": 619,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 746,
                    "end": 751,
                    "matchedPaperCorpusId": "259370630"
                },
                {
                    "start": 772,
                    "end": 776,
                    "matchedPaperCorpusId": "196170479"
                },
                {
                    "start": 1316,
                    "end": 1320,
                    "matchedPaperCorpusId": "203207429"
                },
                {
                    "start": 1529,
                    "end": 1534,
                    "matchedPaperCorpusId": "253080612"
                },
                {
                    "start": 1685,
                    "end": 1690,
                    "matchedPaperCorpusId": "218500588"
                },
                {
                    "start": 1777,
                    "end": 1782,
                    "matchedPaperCorpusId": "159041722"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.318359375
        },
        {
            "corpus_id": "270688337",
            "title": "On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey",
            "text": "In the previous paragraphs, we have introduced some common prompting strategies, which are typically designed for a specific generation task T .However, in most cases, due to the lack of enough reasoning abilities, it is unrealistic to expect the LLMs to generate the entire desired dataset within a single reference, especially when targeting data with complex structures or semantics (Cui and Wang, 2023).In addressing this problem, a common strategy is multi-step generation, through which the overall generation process is manually decomposed into a chain of simpler sub-tasks T 1:k , to force the LLMs to produce data in a step-by-step manner as scheduled:\n\nwhere D 0 = D sup .Each intermediate output D i is generated using model M i , prompted by p i , for a sub-task T i .These outputs can then potentially be used in subsequent generations.By manually scheduling the generation procedure, we implicitly align the reasoning paths of LLMs with human prior knowledge.Specifically, there are two common strategies for task decomposition: sample-wise and dataset-wise decomposition, which mainly aim at enhancing the quality of synthetic data at different scales.\n\nSample-Wise Decomposition.A typical usecase of multi-step generation is for addressing the challenges of long-text processing and logical reasoning when dealing with multi-text data such as dialogues and entity-relation triplets.In such cases, a straightforward approach is to divide the sample into smaller chunks and generate only a portion of each sample at a time (Li et al., 2022;Ye et al., 2023;Wang et al., 2023e).In this way, D 1:k can be considered as different parts of D gen :\n\nNotably, as shown in Eq. 4, each iteration of the generation process can be conditioned on the previously generated contents.For example, Ding et al. (2023b) prompts the LLMs to alternate between acting as the assistant and the user, replying to each other based on the context, ultimately producing a complete conversation transcript.In this way, the coherence among each internal component D i can be pointedly reinforced with separated instructions, thus making it easier for the model to follow the requirements and generate more faithful data.",
            "score": 0.5339152997679943,
            "section_title": "Multi-Step Generation",
            "char_start_offset": 14929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 407
                },
                {
                    "start": 407,
                    "end": 661
                },
                {
                    "start": 663,
                    "end": 682
                },
                {
                    "start": 682,
                    "end": 780
                },
                {
                    "start": 780,
                    "end": 849
                },
                {
                    "start": 849,
                    "end": 973
                },
                {
                    "start": 973,
                    "end": 1167
                },
                {
                    "start": 1169,
                    "end": 1195
                },
                {
                    "start": 1195,
                    "end": 1398
                },
                {
                    "start": 1398,
                    "end": 1590
                },
                {
                    "start": 1590,
                    "end": 1656
                },
                {
                    "start": 1658,
                    "end": 1783
                },
                {
                    "start": 1783,
                    "end": 1993
                },
                {
                    "start": 1993,
                    "end": 2206
                }
            ],
            "ref_mentions": [
                {
                    "start": 1554,
                    "end": 1570,
                    "matchedPaperCorpusId": "258841402"
                },
                {
                    "start": 1570,
                    "end": 1589,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 1796,
                    "end": 1815,
                    "matchedPaperCorpusId": "258840897"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7607421875
        },
        {
            "corpus_id": "216914911",
            "title": "Logic2Text: High-Fidelity Natural Language Generation from Logical Forms",
            "text": "Natural language generation (NLG) from structured data has been an important research problem in many applications. Recent data-driven methods have achieved good performances on various NLG tasks (Liu et al., 2018;Freitag and Roy, 2018;Chen et al., 2019b). However most studies focus on surface descriptions of simple record sequences, for example, attribute-value pairs of fixed or very limited schema, like E2E (Novikova et al., 2017) and WikiBio (Lebret et al., 2016). In real-world cases for multi-row tables, it is often more desirable and plausible to provide descriptions involving higherlevel logical inference across data records. For example, in Figure 1, instead of plain restatements, human readers would be more favorable to abstract descriptions that can summarize or conclude information over the table records. To produce such logical-level generations of high fidelity, it is not yet appropriate to provide only the table as the input in a real-world NLG system, based on the following reasons: 1) Low Fidelity. Given only the table, it is challenging for existing neural models to produce such logically correct generations involving reasoning and symbolic calculations, e.g., max, min, counting, averaging, etc.\n\n2) Uncontrollable content selection. Given a table, the space of logically entailed descriptions is exponentially large, due to vast number of combinations of different operations and arguments from the table, e.g., count, comparison, superlative, etc. It is hard and uncontrollable for neural models to decide a valid, favorable choice of logical selections solely based on the table, due to the difficulty of imposing high-level semantic constraints in the compositional generation process.\n\nTo combat with the above problems, we argue that it is necessary to leverage intermediate meaning representations to achieve faithful and controllable logical generations. To this end, we formulate the task of logical-level NLG as a logical form to text problem. Specifically, besides the table information, the generation module is provided with a logical form representing the semantics of the target text (see Figure 1 for an example). By separating logical reasoning and language realization, the correctness of the intermediate logical form is guaranteed, and the challenge for the realization module is",
            "score": 0.5322502729901715,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.392333984375
        },
        {
            "corpus_id": "270688337",
            "title": "On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey",
            "text": "Current multi-step generation algorithms depend on the model's understanding of task requirements, requiring it to perform complex logical reasoning with limited information.However, in realworld complex scenarios, this limited information may not adequately support effective decisionmaking.For instance, the generation of mathematical problem-solution pairs entails multiple reasoning steps and may necessitate the utilization of calculator tools for validation.To date, there remains a lack of systematic investigation on how to activate the reasoning and planning capabilities of LLMs for autonomous synthetic data generation.Inspired by prevalent LLMs-based agents like HuggingGPT (Shen et al., 2023) and MetaGPT (Hong et al., 2023), we believe it would also be quite valuable to develop a data generation agent for industrial applications.",
            "score": 0.5271605483078415,
            "section_title": "Complex Task Decomposition",
            "char_start_offset": 26785,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 174,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 464
                },
                {
                    "start": 464,
                    "end": 630
                },
                {
                    "start": 630,
                    "end": 845
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52294921875
        },
        {
            "corpus_id": "276421244",
            "title": "Theorem Prover as a Judge for Synthetic Data Generation",
            "text": "Recent studies have shown that large language models (LLMs) are increasingly capable of tackling mathematical problems (Dubey et al., 2024;Jaech et al., 2024;Yang et al., 2024a;Guo et al., 2025). Techniques such as Chain of Thought (CoT) aim to enhance the reasoning process by breaking complex problems into intermediate steps (Wei et al., 2022;Kojima et al., 2022). Although these methods have achieved success on benchmarks like GSM8K (Cobbe et al., 2021), they are prone to Find the area of a circle with radius 2 1. The formula for area is 2. By performing substitution: \n\n3. The answer is 1. The formula for area is 2. By performing substitution: \n\n3. The answer is Final Answer: critical flaws. Specifically, LLM-generated CoT reasoning steps may often include errors, leading to incorrect solutions even when the final answer appears plausible (Leang et al., 2024). \n\nTo address these flaws, coding-based tools such as Python (Chen et al., 2022;Lyu et al., 2023) and SMT Solvers like the Z3 solver (De Moura and Bj\u00f8rner, 2008) have been widely used to improve reasoning capabilities, as well as theorem provers such as Lean (De Moura et al., 2015) and Isabelle (Paulson, 1994). These systems provide formal verification on the logical validity of LLM reasoning steps. Nevertheless, they are not without challenges; autoformalisation errors often lead to execution failures during theorem prover verification in complex tasks (Xu et al., 2024;Quan et al., 2024;Gou et al., 2023a;Olausson et al., 2023), including Python's struggles with Olympiad-level problems (Leang et al., 2024). \n\nParallel to these strategies, synthetic data generation emerged as a promising approach to enhance mathematical reasoning (Xin et al., 2024;Lu et al., 2024b).",
            "score": 0.5241706039840525,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1750
                }
            ],
            "ref_mentions": [
                {
                    "start": 328,
                    "end": 346,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 346,
                    "end": 366,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 1006,
                    "end": 1034,
                    "matchedPaperCorpusId": "15912959"
                },
                {
                    "start": 1132,
                    "end": 1155,
                    "matchedPaperCorpusId": "232990"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45166015625
        },
        {
            "corpus_id": "270766239",
            "title": "Compositional Structured Explanation Generation with Dynamic Modularized Reasoning",
            "text": "Other works probed the compositional abilities of models in natural language inference (Geiger et al., 2020;Goodwin et al., 2020;Yanaka et al., 2020Yanaka et al., , 2021;;Fu andFrank, 2023, 2024), focusing on specific linguistic phenomena, such as quantifiers, negation, or predicate replacements.I.e., they investigate compositionality in phenomena manifesting at the word level, in contrast to inferential processes holding between sentences.\n\nTo fill this gap, we examine compositional generalization in a multi-step entailment tree generation task, where different inference rules need to be composed.Concurrent work (Saparov et al., 2023) also concentrates on sentence-level compositionality in reasoning, but is limited in using a synthetic dataset.In comparison, we employ both natural language and synthetic data, and introduce a new model, with potential for further improvement, that can serve as a strong baseline for the task.\n\nNeural-Symbolic and Neural Methods Prior works show that symbolic approaches (Angeli and Manning, 2014;Mineshima et al., 2015;Mart\u00ednez-G\u00f3mez et al., 2017) that adopt pre-defined inference rules to establish derivations through iterative reasoning, endow models with strong compositionality.But being dependent on pre-defined rules, the models are limited to well-defined tasks.Recently, Yi et al. (2018); Yin et al. (2018); Li et al. (2020); Jiang et al. (2021) used neural networks to map raw signals to symbolic representations and subsequently performed symbolic reasoning to make predictions.As symbolic reasoning is brittle, novel works based on Neural Modular Networks (NMN) (Andreas et al., 2016;Hu et al., 2017) combine individual neural modules endowed with specialized reasoning capabilities.E.g., Jiang and Bansal (2019); Gupta et al. (2019) designed various modules in an NMN to perform unique types of reasoning in end-to-end manner.Similarly, Khot et al. (2021Khot et al. ( , 2023) ) proposed a Text Module Network for complex reasoning tasks, where each module is an existing QA system.",
            "score": 0.5225174299870414,
            "section_title": "Related Work",
            "char_start_offset": 6111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 297
                },
                {
                    "start": 297,
                    "end": 444
                },
                {
                    "start": 446,
                    "end": 605
                },
                {
                    "start": 605,
                    "end": 755
                },
                {
                    "start": 755,
                    "end": 938
                },
                {
                    "start": 940,
                    "end": 1230
                },
                {
                    "start": 1230,
                    "end": 1317
                },
                {
                    "start": 1317,
                    "end": 1536
                },
                {
                    "start": 1536,
                    "end": 1742
                },
                {
                    "start": 1742,
                    "end": 1886
                },
                {
                    "start": 1886,
                    "end": 2041
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 108,
                    "matchedPaperCorpusId": "222396695"
                },
                {
                    "start": 108,
                    "end": 129,
                    "matchedPaperCorpusId": "218581294"
                },
                {
                    "start": 129,
                    "end": 148,
                    "matchedPaperCorpusId": "216868459"
                },
                {
                    "start": 148,
                    "end": 171,
                    "matchedPaperCorpusId": "231709707"
                },
                {
                    "start": 621,
                    "end": 643,
                    "matchedPaperCorpusId": "258865898"
                },
                {
                    "start": 1017,
                    "end": 1043,
                    "matchedPaperCorpusId": "2854390"
                },
                {
                    "start": 1043,
                    "end": 1066,
                    "matchedPaperCorpusId": "17291409"
                },
                {
                    "start": 1066,
                    "end": 1094,
                    "matchedPaperCorpusId": "17619888"
                },
                {
                    "start": 1327,
                    "end": 1343,
                    "matchedPaperCorpusId": "52919654"
                },
                {
                    "start": 1345,
                    "end": 1362,
                    "matchedPaperCorpusId": "49325612"
                },
                {
                    "start": 1364,
                    "end": 1380,
                    "matchedPaperCorpusId": "219573265"
                },
                {
                    "start": 1382,
                    "end": 1401,
                    "matchedPaperCorpusId": "235485085"
                },
                {
                    "start": 1621,
                    "end": 1643,
                    "matchedPaperCorpusId": "5276660"
                },
                {
                    "start": 1643,
                    "end": 1659,
                    "matchedPaperCorpusId": "18682"
                },
                {
                    "start": 1748,
                    "end": 1771,
                    "matchedPaperCorpusId": "202565945"
                },
                {
                    "start": 1773,
                    "end": 1792,
                    "matchedPaperCorpusId": "209202200"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.417724609375
        },
        {
            "corpus_id": "267406931",
            "title": "Prover: Generating Intermediate Steps for NLI with Commonsense Knowledge Retrieval and Next-Step Prediction",
            "text": "Complex NLP tasks such as Natural Language Inference (NLI) often requires reasoning over multiple steps using multiple facts and implicit commonsense knowledge (Trivedi et al., 2020;Sap et al., 2019;Camburu et al., 2018). For such cases, it has been long argued that (Lipton, 2018), the state-ofthe-art models should also output some sort of explanation (such as intermediate steps or a textual explanation) alongwith the final answer. The opaque performance and poor out-of-distribution generalization performance of Transformers-based models (Kaushik et al., 2019;Ribeiro et al., 2020) have refuelled this discussion. However, it is unclear how these intermediate steps can be generated for unconstrained natural language Premise-hypothesis pairs (such as in crowd-sourced NLI datasets) as it A dog jumping for a frisbee in the snow. \n\nA frisbee is a plastic toy. \n\nSnow comes down in cold weather. \n\nA dog is jumping for a plastic toy in cold weather. \n\nAn animal is jumping for a plastic toy in cold weather. \n\nA dog is jumping for a plastic toy in the snow. \n\nDog is an animal. is non-trivial to collect crowd-sourced fine-grained explanations or generate them synthetically. Most importantly, it is unclear how such steps can be utilized further for the NLI task. \n\nCrowd-sourced collection of intermediate steps (or explanations) (Camburu et al., 2018) comes with complications, as human-written explanation can be subjective, and it is hard to automatically verify or utilize such explanations. Recently, researchers have explored synthetic generation of intermediate steps (or proof trees) templated from first order logic theories (Clark et al., 2020;Tafjord et al., 2021;Saha et al., 2020). Here, the objective is to test whether Transformer models can perform deductive reasoning over natural language statements. Provided their examples come from an underlying symbolic system with closed world rules and facts, it is unclear how this strategy of generating proof tree can be extended to unconstrained natural language premise-hypothesis pairs.",
            "score": 0.5222289293280032,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 954
                },
                {
                    "start": 957,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1062
                },
                {
                    "start": 1065,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1269
                },
                {
                    "start": 1272,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 2057
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 182,
                    "matchedPaperCorpusId": "221749191"
                },
                {
                    "start": 182,
                    "end": 199,
                    "matchedPaperCorpusId": "53170360"
                },
                {
                    "start": 199,
                    "end": 220,
                    "matchedPaperCorpusId": "54040953"
                },
                {
                    "start": 267,
                    "end": 281,
                    "matchedPaperCorpusId": "5981909"
                },
                {
                    "start": 566,
                    "end": 587,
                    "matchedPaperCorpusId": "218551201"
                },
                {
                    "start": 1337,
                    "end": 1359,
                    "matchedPaperCorpusId": "54040953"
                },
                {
                    "start": 1641,
                    "end": 1661,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 1661,
                    "end": 1682,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 1682,
                    "end": 1700,
                    "matchedPaperCorpusId": "222141025"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.453857421875
        },
        {
            "corpus_id": "273403575",
            "title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs",
            "text": "The ability to reason is a fundamental element of human cognition, encompassing our ability to think logically, draw conclusions, and make decisions based on available information (Gendron et al., 2024). Large Language Models (LLMs) have demonstrated remarkable performance across wide range of general reasoning and specialized knowledge tasks. In particular, the improvement of LLMs in solving complex mathematical reasoning tasks (Hendrycks et al., 2021b;Cobbe et al., 2021a) has been significant in recent years (Gemini, 2024;Nvidia et al., 2024;OpenAI, 2024). \n\nStrong mathematical reasoning ability heavily relies on the abundance of high-quality, composite, and structured pretraining corpora. An effective mathematical corpus should not only contain relevant content but also be formatted to guide models break down complex problems into smaller sub-problems and solve each part step-by-step-enhancing the model's ability to process and reason about complex problems (Wei et al., 2022). Prior studies show that structured and well-formatted corpora play a crucial role in enhancing multi-hop and logical reasoning abilities (Cobbe et al., 2021a;Li et al., 2023;Gunasekar et al., 2023), underscoring the importance of well-organized mathematical datasets in pretraining LLMs. \n\nCurating complex, high-quality structured mathematical data is costly and resource-intensive, largely due to the uneven distribution of high-quality sources. Most advanced models (OpenAI, 2024;Gemini, 2024) are not publicly accessible, and it is unclear how their approach is enhancing math reasoning. To mitigate this challenge, synthetic data generation has emerged as a scalable, and cost-effective alternative for creating a more balanced and diverse training corpus for pretraining LLMs (Maini et al., 2024;Eldan & Li, 2023;Gunasekar et al., 2023;Shah et al., 2024). However, while these techniques have shown promise in improving general reasoning tasks, their data often lack the step-by-step problem solving structure crucial for multi-hop reasoning and complex mathematical tasks (Maini et al., 2024), making them sub-optimal for such reasoning.",
            "score": 0.5165761071534467,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 564
                },
                {
                    "start": 567,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 433,
                    "end": 458,
                    "matchedPaperCorpusId": "232134851"
                },
                {
                    "start": 975,
                    "end": 993,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1169,
                    "end": 1192,
                    "matchedPaperCorpusId": "259203998"
                },
                {
                    "start": 1777,
                    "end": 1797,
                    "matchedPaperCorpusId": "267312030"
                },
                {
                    "start": 1814,
                    "end": 1837,
                    "matchedPaperCorpusId": "259203998"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57958984375
        },
        {
            "corpus_id": "273695832",
            "title": "On Memorization of Large Language Models in Logical Reasoning",
            "text": "Logical reasoning benchmarks. To evaluate logical reasoning capabilities in LLMs, synthetic benchmarks have been developed to enable scalable generation of samples with varying configurations and difficulty levels (Clark et al., 2020;Giadikiaroglou et al., 2024;Parmar et al., 2024). For instance, DyVal (Zhu et al., 2024) uses directed acyclic graphs to dynamically generate samples on reasoning tasks including deductive, Boolean, and abductive reasoning. (Chen et al., 2024) focus on propositional logical problems involving definite clauses, and synthetically generate variations with different premise orders, such as forward, backward, and shuffled. (Dziri et al., 2024) explore the limitations of LLMs in tasks requiring compositional reasoning, including multiplication, logic grid puzzles, and dynamic programming problems. ZebraLogic (Lin et al., 2024) is an extended benchmark that systematically tests logical reasoning capabilities. BoardgameQA (Kazemi et al., 2024) presents a question-answering dataset characterized by contradictory facts and rules in the questions. PRONTOQA (Saparov & He, 2023) is a synthetic question-answering dataset where each example is generated from a synthetic world model represented in first-order logic. This dataset enables parsing the generated chain of thought into symbolic proofs, facilitating formal analysis. TruthQuest (Mondorf & Plank, 2024) is the most similar task to our work, which provides evaluation samples based on K&K-type of puzzles involving 3-6 person. Our work provides more comprehensive dynamic set of K&K puzzles that support automatic generation of perturbations, solutions and detailed reasoning steps. Moreover, based on this benchmark, we define and measure memorization in reasoning tasks, revealing intricate interplay between memorization and reasoning in LLMs. \n\nImproving reasoning via fine-tuning. Prior work has explored fine-tuning LLMs on synthetic reasoning data to enhance their performance on reasoning. DyVal (Zhu et al., 2024) shows that fine-tuning Llama2-13B-chat on their synthetic reasoning benchmark improves its performance on other popular reasoning benchmarks.",
            "score": 0.514781688258103,
            "section_title": "Conclusion",
            "char_start_offset": 30685,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1839
                },
                {
                    "start": 1842,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2157
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 234,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 234,
                    "end": 262,
                    "matchedPaperCorpusId": "267751102"
                },
                {
                    "start": 262,
                    "end": 282,
                    "matchedPaperCorpusId": "269330143"
                },
                {
                    "start": 458,
                    "end": 477,
                    "matchedPaperCorpusId": "267657940"
                },
                {
                    "start": 958,
                    "end": 979,
                    "matchedPaperCorpusId": "259144942"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "266150003",
            "title": "PathFinder: Guided Search over Multi-Step Reasoning Paths",
            "text": "Recent progress in large language models (LLMs) has led to a new era in machine reasoning, particularly through the use of prompting methods. These methods, such as chain-of-thought (CoT) Wei et al. (2022), scratchpads Nye et al. (2021), least-to-most Zhou et al. (2023), and program-aided language models (PAL) Gao et al. (2023), typically break down complex tasks into reasoning chains and have shown to improve model performance on tasks such as logical Clark et al. (2020), arithmetic Cobbe et al. (2021) and commonsense Talmor et al. (2021) reasoning. \n\nAs tasks that require multiple steps of reasoning become more complex, LLMs begin to struggle with accumulating errors across multiple reasoning steps. Even more challenging is ensuring that each step in a chain is correctly evaluated and contributes positively to the overall reasoning chain and accuracy of the solution. To address these issues, recent work has implemented methods like self-consistency for majority voting Wang et al. (2023), diversifying prompts Li et al. (2023) and Python programs for more accurate reasoning generations Gao et al. (2023). Despite these improvements, the process of creating reasoning chains as a standard autoregressive process still faces challenges due to large search space, sub-optimal assessment and guidance of the reasoning process, especially in complex, multi-step tasks. \n\nIn this work, we introduce PATHFINDER, a decoding method designed for the generation and refinement of reasoning chains generated by LLMs. PATHFINDER embodies our approach of dividing the reasoning decoding into two distinct tasks: candidate generation and candidate selection. For the candidate generation process, PATHFINDER employs a tree-search-based method. It integrates a set of constraints to improve the quality of generated reasoning candidates, along with a pruning \n\nThe Japanese have a long history of using jars to store things. \n\n-0.0186 -0.0180 -0.0099 -0.0022 \n\nEnergy can be stored in a battery. \n\n-0.0028 -0.0048 -0.0044 \n\nEnergy cannot be stored. \n\n-0.0147 \n\nThus, Reiki cannot be stored in a bottle.",
            "score": 0.5110858296898182,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 556
                },
                {
                    "start": 559,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1859
                },
                {
                    "start": 1862,
                    "end": 1925
                },
                {
                    "start": 1928,
                    "end": 1959
                },
                {
                    "start": 1962,
                    "end": 1996
                },
                {
                    "start": 1999,
                    "end": 2022
                },
                {
                    "start": 2025,
                    "end": 2049
                },
                {
                    "start": 2052,
                    "end": 2059
                },
                {
                    "start": 2062,
                    "end": 2103
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.382568359375
        },
        {
            "corpus_id": "270688337",
            "title": "On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey",
            "text": "LLM-driven synthetic data generation has served as a new alternative to traditional human-dependent data collection and demonstrated great potential in various applications, including general tasks, domain-specific tasks, and multimodal tasks.Generic Tasks.With the exploding capabilities of LLMs, this generation pipeline has been adopted in a wide range of basic NLP studies, including text classification (Ye et al., 2022b;Yu et al., 2023c;Sahu et al., 2022), named entity recognition (Xiao et al., 2023), question answering (Li and Callison-Burch, 2023), relationship extraction (He et al., 2023), and natural language inference (Zhang et al., 2023b).These studies further underpin diverse applications, such as sentiment recognition (Gao et al., 2023a;Ye et al., 2022b), online translation (Oh et al., 2023), stance detection (Li et al., 2023a) and spam identification (Smith et al., 2022).Domain-specific Tasks.Some domain-specific tasks also impose significant demands on this pipeline, where human annotation can be extremely expensive and impractical, such as medical diagnosis (Tang et al., 2023), drug discovery (Xiao et al., 2023), clinical trial extraction (Xu et al., 2023b), industrial advertisement (Zhang et al., 2022) and tabular data analysis (Seedat et al., 2023).\n\nMultimodal Tasks.Stemming from the simplicity and low cost, this generation paradigm has also exhibited significant promise in multimodal tasks, including text-image retrieval (Kritharoula et al., 2023), chat understanding (Han et al., 2023), visual question answering (Han and Gardent, 2023), and multimodal instruction tuning (Liu et al., 2023).",
            "score": 0.5101744570195459,
            "section_title": "C Applications",
            "char_start_offset": 35118,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 243,
                    "end": 257
                },
                {
                    "start": 257,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 895
                },
                {
                    "start": 895,
                    "end": 917
                },
                {
                    "start": 917,
                    "end": 1284
                },
                {
                    "start": 1286,
                    "end": 1303
                },
                {
                    "start": 1303,
                    "end": 1633
                }
            ],
            "ref_mentions": [
                {
                    "start": 408,
                    "end": 426,
                    "matchedPaperCorpusId": "253098311"
                },
                {
                    "start": 426,
                    "end": 443,
                    "matchedPaperCorpusId": "258762711"
                },
                {
                    "start": 443,
                    "end": 461,
                    "matchedPaperCorpusId": "247958012"
                },
                {
                    "start": 528,
                    "end": 557,
                    "matchedPaperCorpusId": "258298856"
                },
                {
                    "start": 633,
                    "end": 654,
                    "matchedPaperCorpusId": "264814421"
                },
                {
                    "start": 757,
                    "end": 774,
                    "matchedPaperCorpusId": "253098311"
                },
                {
                    "start": 831,
                    "end": 849,
                    "matchedPaperCorpusId": "264439555"
                },
                {
                    "start": 1462,
                    "end": 1488,
                    "matchedPaperCorpusId": "264426366"
                },
                {
                    "start": 1555,
                    "end": 1578,
                    "matchedPaperCorpusId": "266166029"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7578125
        },
        {
            "corpus_id": "272753147",
            "title": "Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data",
            "text": "Reasoning tasks in natural language often involve a structured representation of facts that can be abstracted into a graph following predefined rules (Ji et al., 2022). For example, family relationships can be systematically represented in a logical graph, where nodes denote family members and edges define their relationships, all governed by logical rules. Let G=(V, E, R) represent a relational graph with a set of nodes V, a set of edges E between the nodes, and a set of relations R expressed in first-order logic. In this graph, vertices (e.g., family members) are denoted as v i \u2208V, and directed edges (e.g., familial relationships) are represented as (v i , r i,j , v j ) \u2208 E, with relations r i,j \u2208R. Our goal is to generate synthetic examples of graphstructured data from such relation graphs to adapt LLMs for targeted reasoning tasks. Existing graphbased data generation methods, such as (Sinha et al., 2019), may produce large amounts of data, but this data is frequently redundant and lacks the necessary complexity (see Figure 1). Therefore, we propose a random-walk-based algorithm (Lov\u00e1sz, 1993) that produces a manageable yet diverse set of examples by sampling sub-graphs from G. In the remainder of this section, we describe our algorithm for constructing G in \u00a7 2.1, the process of generating synthetic reasoning data in \u00a7 2.2, and their deployment to enhance LLM capabilities in natural language reasoning tasks in \u00a7 2.3.",
            "score": 0.5099859762098198,
            "section_title": "Methodology",
            "char_start_offset": 2524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1444
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 167,
                    "matchedPaperCorpusId": "211010433"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "270560130",
            "title": "Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars",
            "text": "Language models trained only on natural language show lackluster capabilities at logical reasoning (McCoy et al., 2023;Mahowald et al., 2024).As a countermeasure, we can train neural models to match the output of symbolic reasoning systems (e.g., logic theorem provers, or other algorithms) on procedurally generated problems, to sharpen their reasoning capabilities.This process improves accuracy on some human-authored problems (Wu et al., 2021;Clark et al., 2020;Wu et al., 2022;Liu et al., 2023).\n\nPrevious work on synthetic first-order logic (FOL) reasoning datasets, RuleTaker (Clark et al., 2020), LogicNLI (Goodwin et al., 2020a) and FLD (Morishita et al., 2023), write dedicated code reimplementing the FOL axioms from scratch to generate proofs, and translate the generated problems to natural language.We propose Unigram, a framework for synthetic reasoning data generation, specifically designed to generate problems jointly into multiple languages.We represent grammars with concise and expressive rules binding two languages, and constraints to prune unwanted generations.\n\nWe write the most extensive grammar of FOL semantic fragments to our knowledge.We structure the generated expressions into PREMISE, HY-POTHESIS pairs, and annotate their logical relationship (entailment/contradiction/neutral) with a FOL solver, following the natural language inference (NLI) framework (Goodwin et al., 2020b).A simplistic FOL NLI problem is: PREMISE: Everyone who is happy is rich.Mary is rich.HYPOTHESIS: Mary is happy LABEL: Neutral.\n\nWe fine-tune DeBERTa NLI models (He et al., 2021) on Unigram-FOL and compare it with previous similar datasets.The 184M parameters (basesize) beats GPT-4 augmented or not with external theorem provers, on the FOLIO (Han et al., 2022) dataset.",
            "score": 0.5078030726996459,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 142,
                    "end": 367
                },
                {
                    "start": 367,
                    "end": 500
                },
                {
                    "start": 502,
                    "end": 813
                },
                {
                    "start": 813,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1086
                },
                {
                    "start": 1088,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1414
                },
                {
                    "start": 1414,
                    "end": 1486
                },
                {
                    "start": 1486,
                    "end": 1499
                },
                {
                    "start": 1499,
                    "end": 1540
                },
                {
                    "start": 1542,
                    "end": 1653
                },
                {
                    "start": 1653,
                    "end": 1784
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 141,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 430,
                    "end": 447,
                    "matchedPaperCorpusId": "231627491"
                },
                {
                    "start": 447,
                    "end": 466,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 466,
                    "end": 482,
                    "matchedPaperCorpusId": "249890002"
                },
                {
                    "start": 583,
                    "end": 603,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 646,
                    "end": 670,
                    "matchedPaperCorpusId": "260846474"
                },
                {
                    "start": 1390,
                    "end": 1413,
                    "matchedPaperCorpusId": "218581294"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75
        },
        {
            "corpus_id": "271270817",
            "title": "A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks",
            "text": "The authors of Shao et al. (2023) come up with Synthetic prompting using LLMs to generate synthetic examples which are augmented to the existing hand-crafted examples as seen in a conventional few-shot setting.This prompting method involves two steps: (1) the backward step, where the LLM synthesizes a query based on a self-generated reasoning chain; and (2) the forward step, where the LLM generates a reasoning chain for the synthesized query, making the reasoning chain to be more accurate.Finally, to choose the best examples, this work uses an in-cluster complexity and the most complex examples with the longest reasoning chains are used during inference.The results show Synthetic prompting achieving up to 15.6% absolute gains when experimented with different Mathematical Problem Solving, Commonsense Reasoning and Logical Reasoning task datasets.",
            "score": 0.507761660667294,
            "section_title": "SYNTHETIC PROMPTING",
            "char_start_offset": 15478,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 210,
                    "end": 494
                },
                {
                    "start": 494,
                    "end": 662
                },
                {
                    "start": 662,
                    "end": 857
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 33,
                    "matchedPaperCorpusId": "256459681"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78369140625
        },
        {
            "corpus_id": "276557738",
            "title": "Empowering LLMs with Logical Reasoning: A Comprehensive Survey",
            "text": "Specifically, for logical question answering, these methods are classified into solver-based methods, prompt-based methods, pretrain and fine-tuning methods, according to whether they primarily rely on logic solvers, LLM prompts and in-context learning, or require further training. In general, solver-based approaches translate natural language problems to symbolic language expressions, and then solving them via external logical solvers [Ye et al., 2023;Olausson et al., 2023]. Prompt-based methods either explicitly model the logical chain when answering the questions [Zhang et al., 2024], or translate natural language into symbolic language by well-defined prompt, and then utilize LLMs for better reasoning [Xu et al., 2024a]. Considering the lack of high-quality reasoning samples such as logical multi-step deduction or proofs in the pre-training corpus [Morishita et al., 2024], pretrain and fine-tuning methods propose to training LLMs with augmented deductive proofs and natural language examples explicitly including the logical reasoning process. For logical consistency, we formulate most common logical consistency concepts, including implication, negation, transitivity, factuality consistency, and their composites, discuss the state-of-the-art solutions for enhancing each type of logical consistency, and review commonly used benchmark datasets and evaluation metrics. Lastly, we discuss promising research directions, such as extension to conditional and modal logic, and efficient algorithms satisfying multiple logical consistencies. \n\nTo the best of our knowledge, this is the first work to comprehensively investigate the most cutting-edge research on improving LLM logical reasoning capabilities, covering complex logical question answering and logical consistency. As our most relevant work [Zong and Lin, 2024], our survey focuses only on categorical syllogisms, as a specific logical reasoning paradigm, but ignores other complex logic inference rules. In addition, [Lam et al., 2024] only discuss about the methods based on external tools such as logical solvers.",
            "score": 0.506419287351751,
            "section_title": "Introduction",
            "char_start_offset": 4018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1557
                },
                {
                    "start": 1560,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 440,
                    "end": 457,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 457,
                    "end": 479,
                    "matchedPaperCorpusId": "264436468"
                },
                {
                    "start": 573,
                    "end": 593,
                    "matchedPaperCorpusId": "270764919"
                },
                {
                    "start": 715,
                    "end": 733,
                    "matchedPaperCorpusId": "265221038"
                },
                {
                    "start": 864,
                    "end": 888,
                    "matchedPaperCorpusId": "264436468"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.444091796875
        },
        {
            "corpus_id": "272880709",
            "title": "E-SQL: Direct Schema Linking via Question Enrichment in Text-to-SQL",
            "text": "With the emergence of both proprietary and open-source Large Language Models (LLMs) such as GPT-3, instructGPT, GPT-4, LLama family, Codex, PaLM, StarCoder CodeLlama and DeepSeek-Coder [1-4, 7, 14, 26, 30, 31, 36, 40, 41] has marked a significant shift in the field. Thanks to their advanced reasoning and comprehension  capabilities, the research community has increasingly focused on harnessing the power of these models for Text-to-SQL tasks. \n\n2.2.1 LLM Reasoning. Advanced reasoning techniques are crucial for improving LLM performance on complex tasks. While prompt design is important, methods that enhance intrinsic reasoning, such as breaking down problems or refining formulations, have led to significant advancements in LLM capabilities. \n\nA breakthrough method proposed by Wei et al. [47] improves the ability of large language models on complex and multi-step reasoning tasks significantly by leading them to generate a series of intermediate reasoning steps, chain-of-thought(CoT). Kojima et al. [19] explored a simple yet effective prompting, \"Let's think step by step\", to uncover the reasoning ability of the language models in zero-shot mode. Zhou et al. [55], Khot et al. [18] and Dua et al. [11] leverage the idea of decomposing complex questions into several simpler sub-questions and constructing a solution by generating an answer for each sub-question. Self-Consistency, proposed by Wang et al. [46], utilizes the majority voting technique to select the most consistent answer over a set of answers generated by diverse reasoning paths. Huang et al. proposed Self-Improve [17], a technique that enhances the reasoning capabilities of large language models by self-training on self-generated data, utilizing Chain-of-Thought (CoT) prompting, self-consistency, and supervised labels. Self-Refine [29] demonstrates how to generate accurate and consistent answers for complex tasks by iteratively refining the language model's responses through a self-feedback mechanism, using only a single LLM.",
            "score": 0.5050195149375727,
            "section_title": "LLMs in Text-to-SQL",
            "char_start_offset": 10024,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 445
                },
                {
                    "start": 448,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 749
                },
                {
                    "start": 752,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 2017
                }
            ],
            "ref_mentions": [
                {
                    "start": 797,
                    "end": 801,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1011,
                    "end": 1015,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 1174,
                    "end": 1178,
                    "matchedPaperCorpusId": "248986239"
                },
                {
                    "start": 1192,
                    "end": 1196,
                    "matchedPaperCorpusId": "252715485"
                },
                {
                    "start": 1212,
                    "end": 1216,
                    "matchedPaperCorpusId": "254408974"
                },
                {
                    "start": 1420,
                    "end": 1424,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1597,
                    "end": 1601,
                    "matchedPaperCorpusId": "253080328"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.340087890625
        },
        {
            "corpus_id": "268249187",
            "title": "Data Augmentation using Large Language Models: Data Perspectives, Learning Paradigms and Challenges",
            "text": "Automate-CoT (Shum et al., 2023) proposes a fully automatic pipeline for finding better chainof-thought prompts to solve commonsense reasoning question answering tasks. This pipeline leverages powerful LLMs like GPT-3 to augment reasoning paths, prune incorrect paths, and select optimal combinations of exemplars. \n\n\u2022 Story Completing: Whitehouse et al. (2023) explore the generation of synthetic data for question answering under the multilingual setting using various LLMs. This work conducts experiments with dolly-12B, stablevicuna-13B, ChatGPT, and GPT-4 on three datasets that covers over 20 languages on commonsense reasoning and story completing tasks. \n\n\u2022 Fact Checking: SCOTT (Wang et al., 2023b) prompts a GPT-neox-20B model to generate annotated data to train a smaller student model to perform QA tasks. Experiments conducted on four QA datasets across commonsense reasoning and fact checking showcase the efficiency of this data augmentation technique. \n\n\u2022 MRC: Samuel et al. (2023) introduce a GPT-4 based data augmentation technique targeting the low source setting on machine reading comprehension task. This technique separately generates passsages, questions, and answers using in-context learning. \n\n\u2022 MedicalQA: Dr.LLaMA (Guo et al., 2023) employs ChatGPT and GPT-4 to either rewrite existing medical question-answering pairs or generate new pairs from the training dataset with zero-shot prompting. Such data augmentation helps to train a LLaMA model specialised on medical knowledge. \n\nLogical Reasoning Logical reasoning involves the ability to analyze and draw conclusions based on given information, a crucial cognitive skill that LLMs aim to master. In the context of LLMs, logical reasoning reflects the model's aptitude for understanding and manipulating symbolic representations, solving problems, and making deductions. The significance of logical reasoning lies in its application across various domains, from puzzlesolving and decision-making to complex problemsolving tasks, showcasing the model's capacity for high-level cognitive functions. \n\nWithin the logical reasoning task, Chain-of-Thought has been a popular technique that improves the reasoning process for the language models.",
            "score": 0.504390702264951,
            "section_title": "\u2022 Commonsense Reasoning:",
            "char_start_offset": 39211,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 314
                },
                {
                    "start": 317,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 661
                },
                {
                    "start": 664,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 967
                },
                {
                    "start": 970,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2077
                },
                {
                    "start": 2080,
                    "end": 2221
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77392578125
        },
        {
            "corpus_id": "272753147",
            "title": "Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data",
            "text": "The reasoning capabilities of Large Language Models (LLMs) (Touvron et al., 2023;Jiang et al., 2023;Dubey et al., 2024) can be greatly enhanced by post-training techniques (Ouyang et al., 2022;Zhang et al., 2023) and prompting strategies (Wei et al., 2022b;Yao et al., 2023;Madaan et al., 2023). However, even with the aforementioned techniques, the multi-hop reasoning tasks remain challenging (Touvron et al., 2023;Jiang et al., 2023;Dubey et al., 2024): LLMs struggle to reason over steps (Agrawal et al., 2024;Zhao and Zhang, 2024), and are fragile to minor perturbations (Ullman, 2023;Chen et al., 2024) in the input prompt. \n\nRecently, several works (Xu et al., 2024;Abdin et al., 2024;Anil et al., 2023) have demonstrated the efficacy of boosting the LLMs' reasoning capacity via fine-tuning on synthetic data generated by stronger LLMs. However, how to make such synthetic data generation effective and controllable for specific applications remains an open question. \n\nExtensive prompt engineering and quality filtering are required to guide LLMs' generation, yet the quality of generated reasoning questions and their labels remains uncertain (Gudibande et al., 2023;Wang et al., 2023;Tan et al., 2024). Motivated by the fact that natural language reasoning tasks can be represented as structured data with finite nodes and edges (Jin et al., 2024), and inspired by existing works on constructing reasoning benchmarks (Fatemi et al., 2024;Kazemi et al., 2023;Agrawal et al., 2024), we propose to leverage synthetic graph-based data for task-specific posttraining adaptation to improve the correctness of the generated reasoning questions and labels. \n\nIn this paper, we carefully design a random walk sampling algorithm on graphs and introduce a new prompting strategy that first extracts a reasoning chain and then derives the answer.",
            "score": 0.5043763906136552,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1845
                }
            ],
            "ref_mentions": [
                {
                    "start": 590,
                    "end": 608,
                    "matchedPaperCorpusId": "267657940"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9267578125
        },
        {
            "corpus_id": "268041519",
            "title": "Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
            "text": "The ability to synthesize real and highquality data is essential for model development. Data generation is used for augmentation when the data is sparse (Onishi & Meguro, 2023), imputing missing values (Jolicoeur-Martineau et al., 2023), and class rebalancing in imbalanced data (Sauber-Cole & Khoshgoftaar, 2022). Traditional methods for synthetic data generation are mostly based on Copulas (Patki et al., 2016;Li et al., 2020b) and Bayesian networks (Zhang et al., 2017;Madl et al., 2023) while recent advancement in generative models such as Variational Autoencoders (VAEs) (Ma et al., 2020;Darabi & Elor, 2021;Vardhan & Kok, 2020;Liu et al., 2023d;Xu et al., 2023b)), generative adversarial networks (GANs) (Park et al., 2018;Choi et al., 2018;Baowaly et al., 2019;Xu et al., 2019), diffusion models (Kotelnikov et al., 2022;Xu et al., 2023a;Kim et al., 2022b;a;Lee et al., 2023;Zhang et al., 2023c), and LLMs, opened up many new opportunities. These deep learning approaches have demonstrated superior performance over classical methods such as Bayesian networks ( (Xu et al., 2019)). A comprehensive understanding of the strengths and weaknesses of different tabular data synthesis methods can be found in Du & Li (2024). \n\nTable understanding is a broad field, covering various tasks like question answering (QA), natural language inference (NLI), Text2SQL tasks, and more.",
            "score": 0.5038857718493599,
            "section_title": "Traditional and deep learning in tabular data",
            "char_start_offset": 9697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1381
                }
            ],
            "ref_mentions": [
                {
                    "start": 279,
                    "end": 313,
                    "matchedPaperCorpusId": "251725737"
                },
                {
                    "start": 393,
                    "end": 413,
                    "matchedPaperCorpusId": "9027681"
                },
                {
                    "start": 453,
                    "end": 473,
                    "matchedPaperCorpusId": "13095103"
                },
                {
                    "start": 615,
                    "end": 635,
                    "matchedPaperCorpusId": "229131432"
                },
                {
                    "start": 635,
                    "end": 653,
                    "matchedPaperCorpusId": "259298730"
                },
                {
                    "start": 712,
                    "end": 731,
                    "matchedPaperCorpusId": "47017667"
                },
                {
                    "start": 749,
                    "end": 770,
                    "matchedPaperCorpusId": "54479855"
                },
                {
                    "start": 830,
                    "end": 847,
                    "matchedPaperCorpusId": "264439324"
                },
                {
                    "start": 847,
                    "end": 865,
                    "matchedPaperCorpusId": "249847841"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82861328125
        },
        {
            "corpus_id": "278171321",
            "title": "RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library",
            "text": "The development of advanced reasoning Large Language Models (LLMs) (Zhao et al., 2023;OpenAI, 2024) has markedly improved their ability to address complex tasks across domains such as mathematics, science, and coding. This highlights the importance of synthesizing complex reasoning data to drive further advancements, given the limited availability of high-quality annotated instructions (Shah et al., 2024;Yuan et al., 2023). \n\nTo address this scarcity, researchers have explored various synthesizing methods, particularly in the mathematics domain. The mainstream methods involve data augmentation based on existing annotated training sets, such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), ranging from self-evolving instructions (Xu et al., 2024;Zeng et al., 2024) and question paraphrasing (Yu et al., 2024), to solution augmentation (Lu et al., 2024). However, these methods are limited by the available training data, constraining the synthesis diversity (Li et al., 2024c(Li et al., , 2023)). To enhance diversity, recent approaches enable LLMs to generate a large scale of questions from various mathematicsrelated sources, including web pages (Yue et al., 2024) and knowledge points (Tang et al., 2024) from web corpora or textbooks. \n\nHowever, as shown in Figure 1, these direct problem generation methods encounter challenges in lacking a deep understanding and simulation of the underlying reasoning processes. As a result, the synthesized problems may appear superficially plausible but actually suffer from insufficient reasoning depth and logical inconsistencies (such as internal contradictions), especially when generating complex multi-step reasoning problems. Furthermore, a long-standing challenge in synthetic data generation is the difficulty in supporting the validation of solution correctness during the synthesis process, ultimately compromising training efficiency. \n\nTo address the aforementioned challenges, we draw inspiration from how human educators create problems. Just as one cannot directly write down an Olympic-level math problem without deep consideration and careful curation of the underlying problem-solving process, the same holds true for models.",
            "score": 0.5037788418459601,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 427
                },
                {
                    "start": 430,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1264
                },
                {
                    "start": 1267,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1914
                },
                {
                    "start": 1917,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2212
                }
            ],
            "ref_mentions": [
                {
                    "start": 688,
                    "end": 712,
                    "matchedPaperCorpusId": "268247488"
                },
                {
                    "start": 771,
                    "end": 789,
                    "matchedPaperCorpusId": "270213007"
                },
                {
                    "start": 1214,
                    "end": 1233,
                    "matchedPaperCorpusId": "268247902"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87451171875
        },
        {
            "corpus_id": "258461502",
            "title": "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings",
            "text": "Recent advances in large language models elicit reasoning in a chain-of-thought that allows models to decompose problems in a human-like fashion. Though this paradigm improves multi-step reasoning ability in language models, it is limited by being unimodal and applied mainly to question-answering tasks. We claim that incorporating visual augmentation into reasoning is essential, especially for complex, imaginative tasks. Consequently, we introduce VCoT, a novel method that leverages chain-of-thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data. Our method uses visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning, as well as provide interpretability into models' multi-step reasoning. We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evaluation that VCoT offers novel and consistent synthetic data augmentation beating chain-of-thought baselines, which can be used to enhance downstream performance.",
            "score": 0.5013432754647728,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8818359375
        },
        {
            "corpus_id": "253370208",
            "title": "Teaching Broad Reasoning Skills for Multi-Step QA by Generating Hard Contexts",
            "text": "Further, context generation also enables us to balance out the distribution of reasoning patterns, e.g., by synthesizing additional contexts (and thereby examples) for questions from the long-tail of underrepresented reasoning patterns. \n\nOur use of synthetic contexts to reliably teach broad skills is inspired by three strands of recent RC QA research. One strand has shown that skills learnt over synthetic data can indeed transfer to real datasets (Geva et al., 2020;Yang et al., 2021;Yoran et al., 2022;Pi et al., 2022). A second strand has shown that perturbing the existing (natural) contexts of RC instances in a targeted fashion can reduce artifact-based reasoning (Jia and Liang, 2017;Trivedi et al., 2020). A third strand has shown that carefully constructing contexts (for synthetic questions) to have sufficient distractors can reduce artifacts (Trivedi et al., 2022;Khot et al., 2022a). \n\nBuilding upon these three strands, we introduce TEABREAC,2 a teaching dataset that includes carefully constructed synthetic contexts for a broad set of real multi-step questions sourced from six existing datasets. TEABREAC was designed with the goals of strong control over cheatability and balanced coverage of reasoning patterns. To identify the intended reasoning, we leverage question decomposition annotations, specifically Question Decomposition Meaning Representation or QDMR annotations which are widely available for a broad set of datasets (Wolfson et al., 2020). \n\nFigure 1 shows the overview of our construction process for TEABREAC. Our approach relies on treating a question decomposition as an unambiguous typed program that can be used to generate a synthetic context and can be executed to provide an answer. To this end, we first turn natural language QDMRs into a precise typed program. We then construct a synthetic context by asserting a set of facts that relate to various parts of the multi-step question. We do this by grounding the predicates of QDMR (e.g., field goals of Shayne Graham in Fig. 1) with randomly generated entities. We also add distractor statements to the context to ensure that bypassing reasoning steps results in an incorrect answer.",
            "score": 0.5010452032659285,
            "section_title": "Introduction",
            "char_start_offset": 2090,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 239,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2181
                }
            ],
            "ref_mentions": [
                {
                    "start": 452,
                    "end": 471,
                    "matchedPaperCorpusId": "215548225"
                },
                {
                    "start": 489,
                    "end": 508,
                    "matchedPaperCorpusId": "235898880"
                },
                {
                    "start": 674,
                    "end": 695,
                    "matchedPaperCorpusId": "7228830"
                },
                {
                    "start": 695,
                    "end": 716,
                    "matchedPaperCorpusId": "221749191"
                },
                {
                    "start": 858,
                    "end": 880,
                    "matchedPaperCorpusId": "236771976"
                },
                {
                    "start": 880,
                    "end": 899,
                    "matchedPaperCorpusId": "248666080"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "273345185",
            "title": "Boosting Deductive Reasoning with Step Signals In RLHF",
            "text": "Recent advancements in large language models (LLMs) (Ouyang et al., 2022;Bai et al., 2022) have yielded remarkable outcomes. Among the various capabilities of LLMs, reasoning stands out as one of crucial skills, serving as a foundational ability required for solving complex tasks. Numerous efforts (Sun et al., 2023) have been made to explore and enhance the reasoning capabilities of LLMs. \n\nIn this work, we focus on multi-step deductive reasoning tasks (Sun et al., 2023) within the realm of reasoning. Many previous works (Han et al., 2024;Saparov & He, 2023;Saparov et al., 2024) have been done of deductive reasoning data generation. However, most works concentrate on supervised fine-tuning (Sanh et al., 2022) or evaluation. Our work mainly concentrates on generation data for Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022). Deductive reasoning tasks concentrate on deriving correct conclusions from given premises through rigorous and effective reasoning. Our attention is directed towards constructing high-quality data to improve the deductive reasoning abilities of models during the alignment phase. The inherent rigor of deductive reasoning tasks dictates that the corresponding prompts should not contain contradictory information or be incapable of leading to the correct answer. This presents our first challenge: how to obtain prompts with correct answers and no contradictions. Based on the proper conditions in prompts, we expect LLMs to employ multi-step reasoning to deduce the correct answer. Assessing the correctness of such a multi-step reasoning process constitutes our second issue, since we need accurate scores of responses to construct training data. Lastly, how to efficiently acquire a substantial amount of data for training is the third problem we need to consider. \n\nTo address these issues, we propose a generation scheme for multi-step deductive reasoning data, named Multi-step Deduction (MuseD). MuseD is a scalable approach from prompt creation to final evaluation. To sum up, we base our method MuseD on the syllogistic reasoning of deductive inference (Copi et al., 2016), employing a backward generation approach to obtain the conditions required for the prompt.",
            "score": 0.5006428042535779,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 391
                },
                {
                    "start": 394,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1826
                },
                {
                    "start": 1829,
                    "end": 1961
                },
                {
                    "start": 1962,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2232
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 73,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 545,
                    "end": 564,
                    "matchedPaperCorpusId": "252693237"
                },
                {
                    "start": 564,
                    "end": 585,
                    "matchedPaperCorpusId": "258865898"
                },
                {
                    "start": 699,
                    "end": 718,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 836,
                    "end": 857,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6962890625
        },
        {
            "corpus_id": "253553681",
            "title": "Consecutive Question Generation via Dynamic Multitask Learning",
            "text": "Question Generation (QG) is an important and promising task in natural language generation (NLG). It has long served as an effective way to improve other NLP tasks. The applications of synthetic questions have expanded from QA data augmentation (Duan et al., 2017;Lewis et al., 2021) to building tutoring or dialogue systems (Lindberg et al., 2013;Bordes and Weston, 2017), self-assessing the ability of language models (Sun et al., 2019), and checking the faithfulness of an abstract summary (Durmus et al., 2020), etc. \n\nTraditionally, syntax-based methods such as semantic parsing are commonly adopted to synthe-Table 1: Example QG results using a two-step inconsecutive method based on extractive answers. size questions (Berant et al., 2013;Khullar et al., 2018). Recently, transformer-based pre-trained language models (Vaswani et al., 2017;Devlin et al., 2019) are widely used to generate questions. Most of these works are two-step QG methods (Sun et al., 2018;Rennie et al., 2020), which rely on ground-truth or pre-extracted answers (Wang et al., 2019;Jia et al., 2020) and generate questions independently (Puri et al., 2020;Bartolo et al., 2021). However, in real scenarios such as daily conversations or reading comprehension, we usually raise several questions consecutively to understand a whole story. Current QG methods are inadequate to generate such questions, as Table 1 shows. We can see that there are no logical connections between the questions (e.g., Q3 and Q1) and pre-extracted answers also lead to simplicity (e.g., Q1) and inconsistency (e.g., Q3). \n\nIn such cases, we propose the task of consecutive question generation (CQG), which automatically produces a set of well-ordered and logically related question-answer (Q-A) pairs to help understand a given passage (or story). Table 2 shows several \"ideal\" questions which are mutually connected and cover diverse information in the text.",
            "score": 0.4996125018579798,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 520
                },
                {
                    "start": 523,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1577
                },
                {
                    "start": 1580,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1916
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 264,
                    "matchedPaperCorpusId": "427742"
                },
                {
                    "start": 264,
                    "end": 283,
                    "matchedPaperCorpusId": "231924957"
                },
                {
                    "start": 325,
                    "end": 348,
                    "matchedPaperCorpusId": "14313287"
                },
                {
                    "start": 420,
                    "end": 438,
                    "matchedPaperCorpusId": "53109787"
                },
                {
                    "start": 493,
                    "end": 514,
                    "matchedPaperCorpusId": "218571335"
                },
                {
                    "start": 725,
                    "end": 746,
                    "matchedPaperCorpusId": "6401679"
                },
                {
                    "start": 746,
                    "end": 767,
                    "matchedPaperCorpusId": "51868684"
                },
                {
                    "start": 847,
                    "end": 867,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 951,
                    "end": 969,
                    "matchedPaperCorpusId": "53083677"
                },
                {
                    "start": 969,
                    "end": 989,
                    "matchedPaperCorpusId": "226262188"
                },
                {
                    "start": 1043,
                    "end": 1062,
                    "matchedPaperCorpusId": "59215159"
                },
                {
                    "start": 1062,
                    "end": 1079,
                    "matchedPaperCorpusId": "220047945"
                },
                {
                    "start": 1117,
                    "end": 1136,
                    "matchedPaperCorpusId": "211258652"
                },
                {
                    "start": 1136,
                    "end": 1157,
                    "matchedPaperCorpusId": "233296924"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67626953125
        },
        {
            "corpus_id": "277065796",
            "title": "nvBench 2.0: A Benchmark for Natural Language to Visualization under Ambiguity",
            "text": "Recently, the use of LLMs for data synthesis or data augmentation has become increasingly prevalent. Many studies leverage LLMgenerated data for training models [9,36,53,56,59], as well as for evaluating the performance of other trained models [29]. In the NLP domain, researchers have utilized LLMs to generate synthetic data for tasks like text classification [1,3,55]. These works showcase that LLM-generated data can enhance data diversity, thereby improving model generalization and robustness. Building on this, VL2NL [13] extends LLMs to nl2vis domain, generating NL descriptions (e.g., L1 and L2 captions, and user commands) from Vega-Lite specifications. Similarly, the application of LLMs for tabular data or database-related tasks has gained attraction. Common approaches for generating nl2sql or table question answering datasets often involve generating nl queries first, followed by sql generation [2,30]. ScienceBenchmark [58] takes a reverse approach by starting with seed sql queries, then generating new queries from the domain schema, and translating them into NL using fine-tuned LLMs. We follow this reverse construction philosophy in developing nvBench 2.0. Specifically, we begin by extracting vql from seed charts and then use LLMs to reverse engineer the corresponding NL descriptions. The advantage of this approach is that vql clearly defines each step and the ambiguity types involved, allowing us to better capture one-to-many (nl, vis) pairs. By leveraging LLMs to generate multi-step reasoning data, the performance of models on long-chain and complex reasoning tasks can be further improved. As demonstrated by Hunter et al. [18], process supervision via multi-step reasoning significantly enhances model reliability on tasks such as mathematical problem-solving. Similarly, Step-DPO [14] shows that generating step-wise reasoning data enables models to better capture intermediate steps, resulting in improved accuracy. Following this approach, we also generate multi-step reasoning data for tasks in the nl2vis domain, where each step of the reasoning process is explicitly defined, contributing to more accurate and interpretable model predictions.",
            "score": 0.499595526384076,
            "section_title": "LLMs Data Synthesis",
            "char_start_offset": 48619,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2183
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "247519233"
                },
                {
                    "start": 244,
                    "end": 248,
                    "matchedPaperCorpusId": "248779886"
                },
                {
                    "start": 367,
                    "end": 370,
                    "matchedPaperCorpusId": "233296100"
                },
                {
                    "start": 524,
                    "end": 528,
                    "matchedPaperCorpusId": "262054595"
                },
                {
                    "start": 912,
                    "end": 915,
                    "matchedPaperCorpusId": "264405947"
                },
                {
                    "start": 937,
                    "end": 941,
                    "matchedPaperCorpusId": "259108708"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91552734375
        },
        {
            "corpus_id": "258041354",
            "title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4",
            "text": "As a result, building language models with trillions of parameters become possible (Brown et al., 2020) (Raffel et al., 2019). The paradigm of pre-training and fine-tuning has since become the dominant solution to textual inference tasks. Researchers fine-tune language models on task-specific datasets after pre-training models on massive text corpora. Large pre-trained language models (LMs) achieve beyond-human performances on popular NLI and MRC benchmarks, urging for more sophisticated benchmarks in textual inference. \n\nNLP research on logical reasoning regains momentum with the recent releases of a handful of datasets, notably LogiQA and Reclor. The datasets are collected from logical reasoning exams such as Chinese Civil Servant Exams and Law School Admission Test (LSAT). These tests are challenging even for humans and are golden-labeled data with good quality. Logical reasoning is exploited in many probing tasks over large Pre-trained Language Models (PLMs) and downstream tasks like question-answering and dialogue systems. PLMs perform poorly compared to traditional benchmarks. Despite the progress made so far, achieving human-like logical reasoning capabilities in NLU systems remains a challenging task. \n\nGenerative Pre-trained Transformer 4 (GPT-4) (OpenAI, 2023), as well as ChatGPT, is a newlyreleased language model developed by OpenAI, designed to understand and generate multi-modal contents. GPT-4 is promoted to possess even more powerful capabilities in tasks that require logical reasoning. Logical reasoning is essential to human intelligence, enabling us to draw conclusions, make predictions, and solve problems based on given information. Incorporating logical reasoning into language models like GPT-4 can revolutionize natural language understanding (NLU) systems, making them more accurate, robust, and capable of understanding complex information in natural language. \n\nThe evaluation of ChatGPT and GPT-4 for logical reasoning tasks explores their performance on several logical reasoning benchmarks, detailing the strengths and limitations of ChatGPT and GPT-4 in this domain. Previous ChatGPT evaluation indicates that ChatGPT performs better on deductive reasoning than inductive ones (Bang et al., 2023) and that ChatGPT performs better on factual inputs (Qin et al., 2023).",
            "score": 0.49717164553489684,
            "section_title": "Introduction",
            "char_start_offset": 2085,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 525
                },
                {
                    "start": 528,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1911
                },
                {
                    "start": 1914,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2323
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33203125
        },
        {
            "corpus_id": "272753174",
            "title": "LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning",
            "text": "With the rapid development of artificial intelligence, Large Language Models (LLMs) (Bi et al., 2024;Liu et al., 2024a) demonstrate excellent performance in reasoning tasks. The success of these models is inseparable from the support of largescale and high-quality reasoning data. However, data acquisition and processing face numerous challenges in the real world. As a viable alternative, synthetic data (Wang et al., 2024) can effectively alleviate this problem and further enhance (Dubey et al., 2024;Adler et al., 2024) the model's reasoning capabilities. \n\nSynthetic data (Liu et al., 2024b) has wide applications in mathematics and code domains. For 1 Code and data are publicly available at https://github. com/jiangjin1999/LogicPro You are given an integer array cards of length 4. You have four cards, each containing a number in the range [1,9]. You should arrange the numbers on these cards in a mathematical expression using the operators ['+', '-', '*', '/'] and the parentheses '(' and ')' to get the value 24. Please give code to solve it.",
            "score": 0.49506449754752213,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 560
                },
                {
                    "start": 563,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1055
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6572265625
        },
        {
            "corpus_id": "277622127",
            "title": "Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use",
            "text": "Reinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus shifts toward more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8K (a math dataset) by a relative 16.9%.",
            "score": 0.4946349513207618,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "257632157",
            "title": "Retrieving Multimodal Information for Augmented Generation: A Survey",
            "text": "An open challenge in generative models is hallucination, where the model is likely to output false information. Thus, A potential solution is to ground generation with retrieved structured knowledge, such as knowledge graphs, tables, and databases. Question Answering (QA) A natural setting to use knowledge is QA. To augment Knowledge Base  2023) feed in the retrieved contexts as additional inputs or prompts to the PLM. (Ma et al., 2022;Fan et al., 2019) adapt the generator to accept the context representations as inputs. At model inference stage, an interesting work is Hu et al. (2022c), which inserts an interaction layer into PLMs to guide an external KG reasoning module. \n\nGeneral text generation External knowledge retrieval can improve general text generation to be more factually grounded. Liu et al. (2022a) presents a memory-augmented approach to condition an autoregressive language model on a knowledge graph (KG). During inference, Tan et al. (2022) selects knowledge entries through dense retrieval and then injects them into the input encoding and output decoding stages in pretrained language models (PLMs). For domain-specific text generation, Frisoni et al. ( 2022 2019) retrieve medical report chunks or report templates to augment input prompts. Then, they use self-devised decoders or graph transformers to generate grounded reports. To improve interpretability, RAG could be used to select facts as interpretable reasoning paths (Aggarwal et al., 2021;Jansen and Ustalov, 2019). Moreover, RAG is especially useful for low-resource generation tasks, such as question generation (Yu and Jiang, 2021;Xin et al., 2021;Gu et al., 2019), documentto-slide (Sun et al., 2021), table-to-text (Su et al., 2021), counterargument generation (Jo et al., 2021), entity description generation (Cheng et al., 2020) and text-based games (Murugesan et al., 2021). \n\nRecent research has attempted to reduce hallucinations in LLMs by leveraging external structured knowledge.",
            "score": 0.4941770158582071,
            "section_title": "Structured Knowledge",
            "char_start_offset": 18086,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1873
                },
                {
                    "start": 1876,
                    "end": 1983
                }
            ],
            "ref_mentions": [
                {
                    "start": 423,
                    "end": 440,
                    "matchedPaperCorpusId": "253098174"
                },
                {
                    "start": 576,
                    "end": 593,
                    "matchedPaperCorpusId": "253522964"
                },
                {
                    "start": 804,
                    "end": 822,
                    "matchedPaperCorpusId": "246240585"
                },
                {
                    "start": 951,
                    "end": 968,
                    "matchedPaperCorpusId": "247475975"
                },
                {
                    "start": 1457,
                    "end": 1480,
                    "matchedPaperCorpusId": "236459873"
                },
                {
                    "start": 1480,
                    "end": 1505,
                    "matchedPaperCorpusId": "208089867"
                },
                {
                    "start": 1605,
                    "end": 1625,
                    "matchedPaperCorpusId": "233189589"
                },
                {
                    "start": 1625,
                    "end": 1642,
                    "matchedPaperCorpusId": "235490277"
                },
                {
                    "start": 1642,
                    "end": 1658,
                    "matchedPaperCorpusId": "207910555"
                },
                {
                    "start": 1677,
                    "end": 1695,
                    "matchedPaperCorpusId": "234334020"
                },
                {
                    "start": 1711,
                    "end": 1728,
                    "matchedPaperCorpusId": "237353494"
                },
                {
                    "start": 1757,
                    "end": 1774,
                    "matchedPaperCorpusId": "237571419"
                },
                {
                    "start": 1848,
                    "end": 1872,
                    "matchedPaperCorpusId": "236459800"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3603515625
        },
        {
            "corpus_id": "257766470",
            "title": "Natural Language Reasoning, A Survey",
            "text": "Some datasets explicitly target classical reasoning types in philosophy and logic, e.g. deduction, abduction and induction, following the definitions in the two areas. Thus, we call them \"classical logical reasoning tasks\". A key characteristic of this topic is that tasks are mostly artificial to study reasoning. There are both deductive reasoning and defeasible reasoning. \n\n5.1.1 Deductive reasoning. Classical deductive reasoning tasks are defined formally based on formal logic, such as propositional logic and first-order logic. There are mainly three types of task: inference [6,100,160], theorem proving [5,25,49,142] and reasoning path generation [100]. The inference task is to reason the conclusion given the premises in a single step, while theorem proving is to predict whether the given proposition is true or false with the given knowledge bases, which usually requires multiple steps. Obviously, inference is the fundamental task that forms the basic capability of multi-step reasoning tasks such as theorem proving, while reasoning path generation is an interpretable task that can be complementary to multi-step reasoning. However, except FOLIO [49], all the existing explicit deductive reasoning datasets are synthesized. We list the classical deductive reasoning datasets in Table 7. \n\nDataset Size Data Source Task Remark bAbI-15 [160] synthetic inference basic deduction RuleTaker \u2020 [25]/ProofWriter \u2020 [142] 500k synthetic theorem proving the first natural language theorem proving PARARULE-Plus [5] 400k synthetic theorem proving addresses the depth imbalance issue on ParaRules AAC [6] 710k synthetic inference based on 8 syllogistic argument schemes \n\nLogicInference [100] 200k synthetic inference reasoning path generation - \n\n1.4k expert-written theorem proving more diverse patterns Table 7. Datasets of classical deductive reasoning, where bAbI-15 means \"the 15-th task in bAbI tasks\". \u2020 denotes there are ground reasoning paths. \n\nProof-finding and faithful reasoning.",
            "score": 0.49170268276374957,
            "section_title": "Classical Logical Reasoning",
            "char_start_offset": 55696,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1675
                },
                {
                    "start": 1678,
                    "end": 1751
                },
                {
                    "start": 1754,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 1959
                },
                {
                    "start": 1962,
                    "end": 1999
                }
            ],
            "ref_mentions": [
                {
                    "start": 584,
                    "end": 587,
                    "matchedPaperCorpusId": "221702898"
                },
                {
                    "start": 591,
                    "end": 595,
                    "matchedPaperCorpusId": "3178759"
                },
                {
                    "start": 613,
                    "end": 616,
                    "matchedPaperCorpusId": "251135345"
                },
                {
                    "start": 616,
                    "end": 619,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 622,
                    "end": 626,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 1352,
                    "end": 1357,
                    "matchedPaperCorpusId": "3178759"
                },
                {
                    "start": 1406,
                    "end": 1410,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 1425,
                    "end": 1430,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 1519,
                    "end": 1522,
                    "matchedPaperCorpusId": "251135345"
                },
                {
                    "start": 1607,
                    "end": 1610,
                    "matchedPaperCorpusId": "221702898"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32080078125
        },
        {
            "corpus_id": "276768281",
            "title": "ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for Reasoning Tasks",
            "text": "For each edge, we generate a textual description e jk , such as \"in this question, c k = previous answer + 3.\" Formally, it is an algorithm that constructs a string from two numbers: e ij = f (a j , c k ). f can be implemented using elementary arithmetic and text templates, ensuring that no answers or parameters in the original subtask need to be manually modified. Once the DAG is fully constructed, we refine node descriptions by removing any explicitly given numerical constants {c i } that are now dependent on the results of prior nodes. Finally, an entire graph described in natural language is a piece of synthetic data. \n\nThe proposed data synthesis framework generates structured, multi-step reasoning tasks with adjustable sizes, ensuring diverse and scalable problem structures. The synthesized dataset supports both training and testing, enabling fine-grained evaluation without human annotations.",
            "score": 0.48749243607477033,
            "section_title": "Data Synthesis",
            "char_start_offset": 15932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 911
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87646484375
        },
        {
            "corpus_id": "216056509",
            "title": "Logical Natural Language Generation from Open-Domain Tables",
            "text": "There is no back-tracking once the model makes a wrong decision like \"5\". \n\nwith generating natural language statements that can be logically entailed by the given data (i.e., the premises). The new task requires a model to jointly reason and generate sentences that are consistent both linguistically and logically. Since there are a variety of reasoning/inference tasks such as natural language inference (Bowman et al., 2015) and commonsense reasoning (Talmor et al., 2019), to avoid confusion, this paper is specifically focused on inferences involving symbolic operations over the given table (Pasupat and Liang, 2015). \n\nTo empower research in this direction, we collect a new corpus LOGICNLG based on the existing TabFact (Chen et al., 2019), which brings two major renovations to the existing NLG paradigm: 1) the text involves diversified types of logical inferences including math operations like max/min/sum/add, comparison operations like same/different, and counting operations like total/only. A more detailed description of logical inference is listed in the Appendix. 2) while existing datasets are often restricted to a specific domain such as weather (Liang et al., 2009), restaurant (Du\u0161ek et al., 2019), NBA (Wiseman et al., 2017), etc, LOGICNLG uses open-domain tables without prior knowledge about their schema. As such, existing methods based on surface-level copying (See et al., 2017;Gu et al., 2016;Puduppully et al., 2019a) becomes insufficient, so are the existing fidelity evaluation based on the surfacelevel information extraction (Wiseman et al., 2017;Rohrbach et al., 2018;Dhingra et al., 2019), which extracts surface triples in a certain pre-defined form (i.e. subj-pred-obj, n-gram) and compare them with the surface content given in the knowledge. \n\nMost neural generation models follow a monotonic generation schema from left to right with the current prediction only depending on the preceding words. Logical NLG poses unique challenges to the traditional generation scheme due to the mismatch between sequence order and logical order.",
            "score": 0.4874711872320516,
            "section_title": "Introduction",
            "char_start_offset": 1914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 76,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 624
                },
                {
                    "start": 627,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1784
                },
                {
                    "start": 1787,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2074
                }
            ],
            "ref_mentions": [
                {
                    "start": 407,
                    "end": 428,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 455,
                    "end": 476,
                    "matchedPaperCorpusId": "53296520"
                },
                {
                    "start": 598,
                    "end": 623,
                    "matchedPaperCorpusId": "9027681"
                },
                {
                    "start": 1169,
                    "end": 1189,
                    "matchedPaperCorpusId": "238873"
                },
                {
                    "start": 1228,
                    "end": 1250,
                    "matchedPaperCorpusId": "23892230"
                },
                {
                    "start": 1391,
                    "end": 1409,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1409,
                    "end": 1425,
                    "matchedPaperCorpusId": "8174613"
                },
                {
                    "start": 1425,
                    "end": 1450,
                    "matchedPaperCorpusId": "52153976"
                },
                {
                    "start": 1562,
                    "end": 1584,
                    "matchedPaperCorpusId": "23892230"
                },
                {
                    "start": 1584,
                    "end": 1606,
                    "matchedPaperCorpusId": "52176506"
                },
                {
                    "start": 1606,
                    "end": 1627,
                    "matchedPaperCorpusId": "174797747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.223876953125
        },
        {
            "corpus_id": "276617498",
            "title": "General Intelligence Requires Reward-based Pretraining",
            "text": "Retrieval-Augmented Generation (RAG) (Berges et al., 2024) provide static memory augmentation but lack dynamism. Neural Turing Machines (NTMs) (Ale\u0161, 2016) explored differentiable memory architectures but face optimization challenges due to the backpropagation through time. Recent innovations (Berges et al., 2024) embed memory as a persistent KV-cache into the model's weights, but because the memory is intricately tied to the models' representations, it becomes difficult to add new knowledge without retraining. Jin et al. (2024) proposed a novel framework using trainable \u2329memory\u232a and \u2329reason\u232a tokens to separate knowledge retrieval from reasoning, addressing key challenges in LLMs such as hallucinations and logical inconsistencies. However, this method still ties knowledge with reasoning in a single model. \n\nSynthetic Data. Most current efforts (Chen et al., 2024;HuggingFace, 2024;Microsoft, 2024) focus on generating synthetic data directly in natural language using pretrained models. In contrast, there is a less explored avenue that involves using simple, symbolic synthetic datasets for pretraining (Wu et al., 2022;2021;Krishna et al., 2021). These approaches typically depend on manually crafted rules (e.g., Set, Identity) to generate data. On top of this lack of scalability, they often lack the complexity required for emergent behavior, limiting their ability to support more complex structures or patterns. The simplicity of these rules can restrict the diversity and depth of the learned priors. \n\nIn other various domains, synthetic data has also shown promise. Studies in vision tasks (Wang et al., 2023b;Baradad et al., 2022) and RL (Baradad et al., 2023;Wang et al., 2024b) demonstrate the effectiveness of synthetic data derived from random Markov chains and noise-based images.",
            "score": 0.4871990998304635,
            "section_title": "Memory Architectures. Current memory approaches like",
            "char_start_offset": 40446,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1808
                }
            ],
            "ref_mentions": [
                {
                    "start": 1116,
                    "end": 1133,
                    "matchedPaperCorpusId": "249890002"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65185546875
        },
        {
            "corpus_id": "273346218",
            "title": "P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains",
            "text": "proof generation ProofWriter (Tafjord et al., 2021) and FLD (Morishita et al., 2023) are logical reasoning datasets equipped with natural language proofs, however both of them are synthetically generated dataset which neither contains abundant natural language variation nor encompasses challenging logical reasoning patters. Previous studies on proof generation focus on ProofWriter (Morishita et al., 2023;Saha et al., 2020Saha et al., , 2021;;Yang et al., 2022) and ProntoQA (Saparov et al., 2023). LogicBench is a synthetically generated natural language QA dataset and is used for evaluating the logical reasoning ability of LLMs (Parmar et al., 2024). While FOLIO covers first-order logic and one or more inference rules are used in each example, LogicBench focuses on reasoning patterns covering propositional logic, first-order logic, and non-monotonic logic and focuses on the use of a single inference rule for each example. \n\nWe collect proofs for FOLIO (Han et al., 2022) instead, a realistic expert-written logical reasoning dataset. Such proofs need to be written from scratch and are hard and time-consuming to write because humans need to manage both the language and reasoning complexity in the proof-writing process and manually construct many steps of reasoning. The resulting proofs contain more diverse types of inference rules and reasoning patterns in addition to containing more natural language variation and ensured semantic richness.",
            "score": 0.4869829581059859,
            "section_title": "Natural language",
            "char_start_offset": 6112,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 934
                },
                {
                    "start": 937,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1460
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 51,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 60,
                    "end": 84,
                    "matchedPaperCorpusId": "260846474"
                },
                {
                    "start": 384,
                    "end": 408,
                    "matchedPaperCorpusId": "260846474"
                },
                {
                    "start": 408,
                    "end": 425,
                    "matchedPaperCorpusId": "222141025"
                },
                {
                    "start": 446,
                    "end": 464,
                    "matchedPaperCorpusId": "249062748"
                },
                {
                    "start": 635,
                    "end": 656,
                    "matchedPaperCorpusId": "269330143"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42236328125
        },
        {
            "corpus_id": "269137153",
            "title": "Distilling Algorithmic Reasoning from LLMs via Explaining Solution Programs",
            "text": "Recent Large Language Models (LLMs) have shown impressive capabilities for various reasoning tasks, including multi-hop question answering (Wang et al., 2022;Lyu et al., 2023), symbolic reasoning (Hua & Zhang, 2022), and math word problem-solving (Chen et al., 2022;Zhou et al., 2023).Chain-of-thought (CoT) prompting (Wei et al., 2022) addresses limitations of previous LLMs by instructing them to generate intermediate steps towards the final answer, thereby decomposing complex problems step-by-step.\n\nHowever, challenges remain, particularly in complex reasoning tasks like algorithmic programming.For example, the majority of human competitors still outperform advanced models like GPT-4 in Codeforces contests (OpenAI, 2023b).Complex programming problems have stringent time and space complexity constraints, where straightforward implementation methods like (Chen et al., 2021;Yin et al., 2018;Hendrycks et al., 2021), often yield time-consuming brute-force solutions.\n\nA number of efforts have been made to tackle this challenging task (Li et al., 2022;Zhang et al., 2023;Olausson et al., 2023;Ridnik et al., 2024) by adding extra clustering or verification steps to filter or iteratively refine generated programs.While those methods focus on flow engineering for code generation, there have been limited attempts to explicitly enhance models' intrinsic reasoning abilities in this context.\n\nHuman-written rationales for solving algorithmic reasoning problems, known as editorials, are hard to collect as they are often posted on personal blogs or as tutorial videos.An alternative is to distill such natural-language-described problem-solving strategies from larger models.Distilling explicit chain-of-thoughts (CoT) reasoning processes has been shown as an effective method to learn multi-step reasoning from larger models (Hsieh et al., 2023;Yue et al., 2023).Usually, a teacher model is required to solve a set of problems while giving CoT reasoning paths at the same time, 1 arXiv:2404.08148v1[cs.CL] 11 Apr 2024 as illustrated in Figure 1.",
            "score": 0.48684242364611674,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 285
                },
                {
                    "start": 285,
                    "end": 503
                },
                {
                    "start": 505,
                    "end": 602
                },
                {
                    "start": 602,
                    "end": 732
                },
                {
                    "start": 732,
                    "end": 975
                },
                {
                    "start": 977,
                    "end": 1223
                },
                {
                    "start": 1223,
                    "end": 1399
                },
                {
                    "start": 1401,
                    "end": 1576
                },
                {
                    "start": 1576,
                    "end": 1683
                },
                {
                    "start": 1683,
                    "end": 1872
                },
                {
                    "start": 1872,
                    "end": 2007
                },
                {
                    "start": 2007,
                    "end": 2054
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 158,
                    "matchedPaperCorpusId": "253098851"
                },
                {
                    "start": 196,
                    "end": 215,
                    "matchedPaperCorpusId": "256631030"
                },
                {
                    "start": 1044,
                    "end": 1061,
                    "matchedPaperCorpusId": "246527904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.290771484375
        },
        {
            "corpus_id": "253237103",
            "title": "RLET: A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees",
            "text": "2022) propose ParaPattern by creating synthetic logical deduction data from Wikipedia, which serves as a good supplement to train the generative model. Different from existing approaches, RLET models the cumulative signals across the whole tree in training, benefiting from our designed reward function under reinforcement learning framework.\n\nPath Reasoning using Reinforcement Learning Our work also aligns well with multiple automated reasoning tasks built with RL (Xian et al., 2019;Liu et al., 2021;Poesia et al., 2021). Especially, reinforcement learning has exhibited its attractiveness in knowledge graph reasoning (Xiong et al., 2017;Das et al., 2018;Lin et al., 2018), where the multi-hop path can be represented as sequential decision problems. Similar to the above RL methods, we formulate the entailment tree generation task as a trajectory of reasoning steps. In contrast to KG based multi-hop reasoning path, however, RLET generates intermediate conclusions of the reasoning steps, providing fluent natural language explanations in detail.",
            "score": 0.4853459848820555,
            "section_title": "Related Work",
            "char_start_offset": 27747,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 468,
                    "end": 487,
                    "matchedPaperCorpusId": "186206810"
                },
                {
                    "start": 487,
                    "end": 504,
                    "matchedPaperCorpusId": "233024725"
                },
                {
                    "start": 623,
                    "end": 643,
                    "matchedPaperCorpusId": "20667722"
                },
                {
                    "start": 643,
                    "end": 660,
                    "matchedPaperCorpusId": "13206339"
                },
                {
                    "start": 660,
                    "end": 677,
                    "matchedPaperCorpusId": "52143467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70361328125
        },
        {
            "corpus_id": "257921623",
            "title": "REFINER: Reasoning Feedback on Intermediate Representations",
            "text": "Problem Formulation. In this paper, we view natural language reasoning (NLR) as an autoregressive generation task where, given input context x, a model needs to generate y, such that y satisfies the constraints of the task. Usually, to generate correct or plausible y, the model needs to make the correct inference z as intermediate steps. 4 We decompose NLR tasks as follows: p(y|x) = p(y|x, z)p(z|x). In practice, one can compute each conditional using an LM that includes its conditioning variables as a part of its input. \n\nBefore continuing with the model description, we describe three NLR tasks where we conduct our study and their respective intermediate representation z. We deliberately chose these three tasks since they broadly cover two types of reasoning: (i) logical reasoning and (ii) normative reasoning. They are exemplified in Appx Fig. 6 and detailed below. \n\nMath word problem (MWP), where given a word problem x consisting of a context and question, the goal is to map x to a valid mathematical expression z (the intermediate representation) and then to a solution y. This task requires the model to perform deduction using mathematical reasoning. Synthetic natural language reasoning (sNLR), where given a reasoning scenario x consisting of 5 synthetic rules and a fact, the model needs to deduce a conclusion y. This task requires the model to perform deductive reasoning and generate intermediate steps z and the conclusion y using closedworld rules and facts. Moral norm and action generation for moral stories (MS), where given a context x consisting of a situation, an intention, and an immoral action, the model needs to generate the moral norm z and the moral action y. Moral actions are encouraged by the moral norm. This task requires the model to perform abductive reasoning to generate moral norms and deductive reasoning for moral action. \n\nWe propose to solve these tasks by forcing the model to generate intermediate hypotheses (z) and improving them via structured feedback.",
            "score": 0.48461203984630713,
            "section_title": "REFINER",
            "char_start_offset": 7970,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 525
                },
                {
                    "start": 528,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1873
                },
                {
                    "start": 1876,
                    "end": 2012
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.329833984375
        },
        {
            "corpus_id": "273346003",
            "title": "Transformer-based Language Models for Reasoning in the Description Logic ALCQ",
            "text": "Multiple surveys (Yang et al., 2023;Huang and Chang, 2023;Yu, Zhang, and Wang, 2023) in the literature describe the most recent research developments on the use of transformers for reasoning tasks. One of the first datasets generated for this purpose was from Clark, Tafjord, and Richardson (2020) with RuleTaker, demonstrating the potential of transformers to perform logical question answering under CWA by training TLMs on synthetic datasets. However, their approach was limited to short expressions of simple conjunctive subsumption axioms. Tafjord, Dalvi, and Clark (2021), generated the ProofWriter datasets (under CWA and OWA) and with a T5 (Raffel et al., 2020)based model fined-tuned on ProofWriter showed that TLMs can generate proofs with high accuracy (94.8% for depth 5). We generated DELTA D based on the approach for the generation of the datasets RuleTaker and ProofWriter, i.e., using PCFGs. However, DELTA D is different from these datasets as i) ALCQ is a much more expressive logic language hence we produced new PCFGs; ii) we have defined different PCFGs for each linguistic complexity level (which has not been done for any other dataset in the literature); iii) it is balanced regarding the aspects discussed in Section 3.5. \n\nIn more expressive contexts, Onta\u00f1\u00f3n et al. (2022) showed that TLMs perform well (up to 90.5%) over contexts generated by propositional logic and a small subset of FOL. Han et al. (2022), with the FOLIO dataset (1.4K), generated from FOL sentences -but without number restrictions-tested the ability of various TLMs for the same reasoning task and concluded that RoBERTa (Liu et al., 2019) performed best among all tested models (including GPT-3 and Codex) but still, the performance was low.",
            "score": 0.48390870269740477,
            "section_title": "Related Work",
            "char_start_offset": 29345,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1247
                },
                {
                    "start": 1250,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1742
                }
            ],
            "ref_mentions": [
                {
                    "start": 36,
                    "end": 58,
                    "matchedPaperCorpusId": "254877753"
                },
                {
                    "start": 260,
                    "end": 297,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 545,
                    "end": 577,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 648,
                    "end": 669,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69921875
        },
        {
            "corpus_id": "273820969",
            "title": "Unlocking Temporal Question Answering for Large Language Models with Tailor-Made Reasoning Logic",
            "text": "The ongoing advances in natural language processing (NLP) have paved the way for the emergence of large language models (LLMs), which are now being utilized extensively in various applications. Models such as GPT-4 (OpenAI, 2023), Vicuna (Chiang et al., 2023), and Alpaca (Taori et al., 2023) have demonstrated impressive language understanding and generation capabilities and are being used from automated content creation to chatbots (OpenAI, 2023;Chiang et al., 2023;Taori et al., 2023). In utilizing these LLMs, recent work such as chain-of-thought (CoT) (Wei et al., 2023) has been found to further improve performance for tasks that require complex reasoning, such as math problems and symbolic question-answering tasks. \n\nHowever, there is a continuing challenge that LLMs face when it comes to temporal reasoning -the capability to understand and process information that involves time-based concepts and sequences (Wei et al., 2023;Zhao et al., 2023;Chowdhery et al., 2022). Though CoT leverages intermediate reasoning steps to guide the generation of the final answer, our investigation reveals that these approaches often fail on the temporal question-answering tasks. Figure 1 provides an example of such a failure. The second reasoning step in the CoT method states that \"Nov, 1992 is before 1983.\", which is incorrect. Consequently, this faulty reasoning in the second step leads to an erroneous answer. \n\nProgram-aided language models (PAL) (Gao et al., 2023) proposed a novel approach for addressing temporal-related tasks, wherein it transforms a provided question into Python code and then executes the code to obtain the final answer. However, our experimental Under review results indicate that PAL is limited in two ways. First, PAL lacks control over the generation of the code. This means that the generated script may have flawed or even non-executable logic, especially when dealing with complex tasks that require multi-hop reasoning. Second, when utilizing program code to solve temporal problems, it is necessary to extract temporalrelated information from the question and context into structured data as the program input.",
            "score": 0.48389954494616866,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 726
                },
                {
                    "start": 729,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 941,
                    "end": 959,
                    "matchedPaperCorpusId": "258547173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.331787109375
        },
        {
            "corpus_id": "267069444",
            "title": "Synergizing Machine Learning & Symbolic Methods: A Survey on Hybrid Approaches to Natural Language Processing",
            "text": "Natural Language Generation (NLG) is a fundamental task in Natural Language Processing, whose aim is to produce meaningful text in natural language, generally by giving it a prompt as input. This requires semantic and syntactic understanding of the language to generate text which makes it challenging while also ensuring its applicability across a broad spectrum of NLG tasks such as dialogue systems, summarization, machine translation, and question answering. NLG solutions generally follow the encoder-decoder architecture (Vaswani et al., 2017), where the encoder understands the input text or prompt, and generates hidden states interpreted by the decoder to generate meaningful text (Yu et al., 2022b). However, the text generated by those powerful models often fails to match human responses due to the limited knowledge available in the training data and the lack of generalization capabilities. This demands rapid embracement of hybrid techniques to generate knowledge-enhanced text. This section presents state-of-the-art hybrid approaches used across a range of prominent NLG tasks.",
            "score": 0.48373009106049636,
            "section_title": "Hybrid Approaches to Natural Language Generation",
            "char_start_offset": 27322,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1094
                }
            ],
            "ref_mentions": [
                {
                    "start": 690,
                    "end": 708,
                    "matchedPaperCorpusId": "222272210"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5107421875
        },
        {
            "corpus_id": "258841402",
            "title": "Generating Data for Symbolic Language with Large Language Models",
            "text": "In the natural language processing (NLP) literature, the march of scaling language models has been an unending yet predictable trend, with new models constantly surpassing previous ones in not only performance but also complexity (Radford et al., 2019;Brown et al., 2020;Chowdhery et al., 2022). Such large language models (LLMs), however, incur a large computational cost in practice, especially when deployed in resource-restricted systems and inference in low-latency applications (Bommasani Input: List the type of bed and name of all traditional rooms. Output: SELECT roomName,bedType FROM Rooms WHERE decor = \"traditional\" \n\nInput: How many large metallic items are there? Output: 1#) return items 2#) return #1 that are large 3#) return #2 that are metallic 4#) return number of #3 \n\nFigure 1: Sample symbolic language datasets with complex structured outputs. The names of the symbolic languages are shown in square brackets. et al., 2021). Instead of treating LLMs as edge task inferencers, a recent line of work leverage LLMs as data generators, with the generated data being used to train more affordable task-specific models for efficient deployment and inference (Schick and Sch\u00fctze, 2021;Meng et al., 2022;Ye et al., 2022b, inter alia). With only a few or even without demonstration examples, the LLMs can generate highquality data via in-context learning (Brown et al., 2020) or prompting (Radford et al., 2019). The task models trained on these generated data can achieve comparable or even better performance than the LLMs and enjoy a low inference cost at the same time. \n\nHowever, previous work mainly focuses on generating natural language data. To what extent this approach works for complex structured data, such as meaning representation and codes (Figure 1), remains an open question.",
            "score": 0.48365801337655107,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 628
                },
                {
                    "start": 631,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1588
                },
                {
                    "start": 1591,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1808
                }
            ],
            "ref_mentions": [
                {
                    "start": 230,
                    "end": 252,
                    "matchedPaperCorpusId": "249017865"
                },
                {
                    "start": 1404,
                    "end": 1426,
                    "matchedPaperCorpusId": "249017865"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54052734375
        },
        {
            "corpus_id": "254877131",
            "title": "Optimization Techniques for Unsupervised Complex Table Reasoning via Self-Training Framework",
            "text": "In this section, we briefly summarize the related works from these two aspects: the development of tabular reasoning models and unsupervised data generation methods. \n\nA. Tabular Reasoning Models. \n\nMany tabular reasoning models tackle the question answering and fact verification tasks in a semantic parsing manner [48], [49], converting a natural language sentence into a program. Zhong et al. [6] translate users' questions to corresponding SQL queries, and Yang et al. [47] generate semantic consistent logical forms with tree structures and execute them to judge the claims. However, the search space for programs is very large, and the model may generate spurious programs which have wrong structures but return the correct answers. Recent works demonstrate that pre-trained language models achieve better reasoning performances on various tasks by pretraining or leveraging auxiliary knowledge [50], [58], [62], [64]. Specifically, for the tabular reasoning task, TAPAS [13] is a BERT-extended model pre-trained on a large corpus of texts and tables from Wikipedia. It answers questions by applying operations on predicted table cells in an end-to-end  way. Neeraja et al. [59] boost the reasoning ability of pretrained models on the tabular NLI task by introducing external knowledge. And it is a promising direction to explore how to obtain better representations of tables. GraPPa [19] introduces a text-schema linking objective to make the model better understand the grammatical role of table elements. However, the main drawback of these methods is that they require a large amount of training data, limiting their performance when transferring to a new domain.",
            "score": 0.48299167816390454,
            "section_title": "VI. RELATED WORK",
            "char_start_offset": 44097,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 168,
                    "end": 196
                },
                {
                    "start": 199,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1674
                }
            ],
            "ref_mentions": [
                {
                    "start": 907,
                    "end": 911,
                    "matchedPaperCorpusId": "245935213"
                },
                {
                    "start": 919,
                    "end": 923,
                    "matchedPaperCorpusId": "245934689"
                },
                {
                    "start": 1180,
                    "end": 1184,
                    "matchedPaperCorpusId": "233204526"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4833984375
        },
        {
            "corpus_id": "277780928",
            "title": "Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability",
            "text": "The field of Natural Language Processing has experienced remarkable advancements with the development of large language models (LLMs) (Min et al., 2021;Kaplan et al., 2020). A significant trend in enhancing the capabilities of these models is test-time scaling (Yang et al., 2025;Wu et al., 2025), where increasing computational resources allocated during inference leads to notable performance improvements. Models such as OpenAI's o1 series (OpenAI, 2024) and DeepSeek-R1 (DeepSeek-AI, 2025) have demonstrated the effectiveness of this approach across various tasks and benchmarks (Lightman et al., 2023;Huang et al., 2024). The capability of these models to achieve superior results by allocating additional computational resources during inference indicates an important shift in optimizing performance for LLMs. Specifically, dedicating more computation to the answer-generation process, rather than solely relying on scaling training data and model parameters, can lead to significant improvements, particularly in tasks that require complex reasoning (Snell et al., 2024). The success of test-time scaling thus emphasizes the crucial role of computation during the answer-generation phase. \n\nA key factor in the success of advanced models lies in their inherent ability to perform explicit reasoning before arriving at a final answer (Wei et al., 2023;Snell et al., 2024;Wu et al., 2025). This deliberate \"think\" step enables models to evaluate multiple potential solutions, resulting in more accurate and nuanced answers. Techniques like test-time scaling have facilitated the generation of these intermediate reasoning steps, significantly boosting performance on challenging tasks (Huang and Chang, 2022). Given the enhanced quality of answers produced by such reasoning models, a relevant question emerges: Can these high-quality answers be effectively utilized to enhance the performance of less computationally intensive, non-reasoning models? \n\nThis paper investigates this question by exploring strategies for leveraging the outputs of reasoning models to enhance the capabilities of non-reasoning models. Our central hypothesis is that training non-reasoning models using improved answers derived from reasoning models can lead to superior performance. To validate (2) we systematically investigate various methods for integrating outputs generated by reasoning models to improve the capabilities of non-reasoning models.",
            "score": 0.4824055482287497,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1196
                },
                {
                    "start": 1199,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1956
                },
                {
                    "start": 1959,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2268
                },
                {
                    "start": 2269,
                    "end": 2437
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.341796875
        },
        {
            "corpus_id": "269042851",
            "title": "Best Practices and Lessons Learned on Synthetic Data for Language Models",
            "text": "Math. Recent advancements in mathematical reasoning for language models (LMs) have led to the development of various approaches to improve performance on mathrelated tasks. One approach is to train on math-targeted pre-training data, such as Minerva (Lewkowycz et al., 2022), Llemma (Azerbayev et al., 2023), and DeepSeek-Math (Shao et al., 2024). Another mainstream method is to generate synthetic questions and answers to imitate the training or validation set of target benchmarks. For instance, WizardMath (Luo et al., 2023a) leverages a series of operations to increase the complexity of questions and answers using GPT-3.5, while MetaMath (Yu et al., 2023) bootstraps the questions in MATH and GSM8K by rewriting them in different ways, such as semantic rephrasing, self-verification, and backward reasoning. GAIR-Abel (Chern et al., 2023) found that the format of the augmented answers is crucial to final performance, with answers that begin with a paraphrasing of the question followed by a step-by-step solution showing better performance than those in vanilla format. Xwin-Math (Li et al., 2024) further scaled up synthetic SFT data to one million examples and found that the LLaMA-2 7B model (Touvron et al., 2023) can still benefit from data scaling. MMIQC (Liu & Yao, 2024) composed a bundle of datasets that infuse SFT style data (via question-answer rephrasing or directly taken from MetaMath) with a subset of high-quality mathematical pre-training data, such as OpenWebMath (Paster et al., 2023). \n\nOther reasoning tasks. Synthetic data also leads to impressive performance in other reasoning tasks. For instance, Wei et al. (2023a) augmented existing natural language datasets by replacing natural language labels with arbitrary symbols, generating over 500k synthetic examples. Using these synthetic data for supervised finetuning significantly improved model performance on unseen in-context learning and algorithmic-reasoning tasks.",
            "score": 0.4821610507811901,
            "section_title": "Reasoning",
            "char_start_offset": 4189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 5
                },
                {
                    "start": 6,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1514
                },
                {
                    "start": 1517,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1954
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89013671875
        },
        {
            "corpus_id": "273638735",
            "title": "LanFL: Differentially Private Federated Learning with Large Language Models using Synthetic Samples",
            "text": "For synthetic sample generation, Figure 2 shows an example output of M syn using the CommonsenseQA data set [32]. The sentences marked gray are x i and y i , sentences marked green are chainof-thought reasoning, and the remaining sentences are meta prompt and formatting added by M syn . It is important to note that by incorporating logical reasoning and facts into the examples within the prompt, the resulting synthetic samples encompass not only the questions and their answers but also the associated logical reasoning steps. This approach is more efficient, as it allows for the simultaneous generation of synthetic samples and their logical reasoning, rather than producing the synthetic samples first and subsequently generating the reasoning as a separate step. \n\nFor numerical features, we need one pre-processing step to convert the features into tokens that LLMs can understand better. Instead of the simple textual encoder in GReaT where we add 'is' between feature name and the feature value [18], we require the participant to connect the features into a cohesive paragraph. It may require domain knowledge for specific tasks. As an example, for features concerning a credit card holder's information, instead of using 'Gender is male, education is high school, age is 30, payment is 400, usage is 600,' we use 'A high-school educated 30 year-old male has used $600 of his credit and has made a $400 payment.' as x i . \n\nAnother consideration for a participant is that of all the synthetic samples created, the participant is recommended to discard samples that are similar to those in its local data set. For example, for each of the synthetic sample x syn , the participant should compute n BLEU scores b 1 , b 2 , . . . b n for each of the local sample x 1 , x 2 , . . . x n [33]. Then the participant can set a threshold t depending on the application and discard any synthetic samples that have b max > t where b max = max(b 1 , b 2 , . . . b n ). For numerical features, the participant can utilize the L2 distance between the feature vectors to determine whether or not to discard any of the synthetic samples.",
            "score": 0.4807961231554312,
            "section_title": "B. Synthetic Samples",
            "char_start_offset": 15990,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1433
                },
                {
                    "start": 1436,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2132
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 112,
                    "matchedPaperCorpusId": "53296520"
                },
                {
                    "start": 1793,
                    "end": 1797,
                    "matchedPaperCorpusId": "11080756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.708984375
        },
        {
            "corpus_id": "273345185",
            "title": "Boosting Deductive Reasoning with Step Signals In RLHF",
            "text": "To sum up, we base our method MuseD on the syllogistic reasoning of deductive inference (Copi et al., 2016), employing a backward generation approach to obtain the conditions required for the prompt. This ensures that the conditions in the prompt can lead to the correct conclusion without any contradictory conditions. Moreover, by controlling the number of generated conditions, we regulate the number of inference steps required for the prompt. Based on prompts generated by this method, we score the responses of LLMs step by step. That is, we provide an evaluation method that can assess whether an answer was correctly obtained through multi-step reasoning rather than merely being the correct answer. The data generation process is shown in Fig. 1. \n\nUsing MuseD, we synthesize partially ordered data for multi-step deductive reasoning and use this data for Reinforcement Learning from Human Feedback (RLHF) training. We achieve significant performance enhancements on both in-domain and out-of-domain reasoning datasets, validating the effectiveness of our synthesized data. Further experiments demonstrate that step scoring is crucial for improving the model's performance in RLHF and that positive rewards for correct answers are the primary motivator for model learning. \n\nAdditionally, we utilize our method to create a multi-step deductive reasoning evaluation set, also named MuseD1 . Compared to previous evaluation sets, it can provide insights into the model's performance changes under tasks with different numbers of reasoning steps and offer more granular process evaluations. In summary, our main contributions include: (1) proposing a data synthesis method MuseD based on multi-step deductive reasoning, (2) achieving improvements in deductive reasoning performance on RLHF with this method, and (3) presenting a multi-step deductive reasoning evaluation set that allows for multi-dimensional automatic scoring. \n\nWe will briefly introduce the logical concepts involved in our method in Sec. 3 and describe our data generation approach in Sec. 4. In Sec. 5, we will present the experimental results, and in Sec. 6, we will show the performances of some LLMs on our evaluation set.",
            "score": 0.48068659326225704,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 755
                },
                {
                    "start": 758,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1281
                },
                {
                    "start": 1284,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1933
                },
                {
                    "start": 1936,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2202
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73828125
        },
        {
            "corpus_id": "273403996",
            "title": "A Little Human Data Goes A Long Way",
            "text": "From BERT (Devlin et al., 2019) to GPT-4 (Achiam et al., 2023), the explosive growth of language models (LMs) has been underpinned by exponential increases in the size of available training data. However, the more complex and specialized the task, the more expensive and challenging it is to collect human generated data at scale (Wang et al., 2021). Combined with growing concerns that LMs may soon exhaust the stock of publicly available training data (Villalobos et al., 2024), many turn to synthetic data generation, hoping to eliminate their reliance on human annotation. \n\nSynthetic data generation has long been used to increase the amount of training data available (Simard et al., 2002;Krizhevsky et al., Figure 1: Change in model performance as the proportion of synthetic points in the training data is increased. Across datasets, the performance decrease when moving from 0% to 90% synthetic data is often less than that of moving from 90% to purely synthetic data. 2012). Early NLP approaches use rule based methods (De Gispert et al., 2005;Chen et al., 2012), paraphrasing (Wang and Yang, 2015;Kobayashi, 2018), noising (Xie et al., 2017;Wang et al., 2018), and backtranslation (Sennrich et al., 2016;Yu et al., 2018), but are limited in their capability. \n\nModern LMs demonstrate the capability to solve myriad NLP tasks with minimal task specific data (Brown, 2020;Wei et al., 2022a,b;Ouyang et al., 2022;Ashok and Lipton, 2023), making them more powerful synthetic data generators.",
            "score": 0.48051836594576036,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1269
                },
                {
                    "start": 1272,
                    "end": 1498
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 31,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 330,
                    "end": 349,
                    "matchedPaperCorpusId": "237363383"
                },
                {
                    "start": 454,
                    "end": 479,
                    "matchedPaperCorpusId": "272330241"
                },
                {
                    "start": 674,
                    "end": 695,
                    "matchedPaperCorpusId": "16067356"
                },
                {
                    "start": 695,
                    "end": 713,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 1029,
                    "end": 1054,
                    "matchedPaperCorpusId": "2571493"
                },
                {
                    "start": 1054,
                    "end": 1072,
                    "matchedPaperCorpusId": "1326531"
                },
                {
                    "start": 1087,
                    "end": 1108,
                    "matchedPaperCorpusId": "3257353"
                },
                {
                    "start": 1108,
                    "end": 1124,
                    "matchedPaperCorpusId": "21725995"
                },
                {
                    "start": 1134,
                    "end": 1152,
                    "matchedPaperCorpusId": "10635893"
                },
                {
                    "start": 1152,
                    "end": 1170,
                    "matchedPaperCorpusId": "52078335"
                },
                {
                    "start": 1192,
                    "end": 1215,
                    "matchedPaperCorpusId": "15600925"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78662109375
        },
        {
            "corpus_id": "265498895",
            "title": "CLOMO: Counterfactual Logical Modification with Large Language Models",
            "text": "Despite large language models (Arkoudas, 2023;OpenAI, 2022) perform strikingly in plenty of reasoning benchmarks (Cobbe et al., 2021;Hendrycks et al., 2021a), late studies observe an internal inconsistency in their reasoning processes (Saparov and He, 2023;Arkoudas, 2023). The inconsistency is attributed to misunderstanding and misapplication of logical relations. However, logical relations in complex language reasoning are not yet properly quantified and evaluated. \n\nCurrent studies on evaluating model reasoning are limited in both form and content. On the one hand, benchmarking complex reasoning is generally applying discrimination tasks such as multiplechoice questions (Huang et al., 2023b;Hendrycks et al., 2021a;Chen et al., 2023;Suzgun et al., 2023), where accuracy and pass rate serve as the main evaluation metric. However, such evaluations oversimplify the goal of uncovering essential and subtle pitfalls in complex reasoning. For example, the reasoning processes could contain misconceptions in logical relations but give correct answers due to the data distribution (Elazar et al., 2021;Saparov and He, 2023). Therefore, evaluating the generated content would provide a more realistic measurement of model reasoning. On the other hand, unlike widely studied reasoning tasks such as math reasoning (Cobbe et al., 2021;Hendrycks et al., 2021b) and standard exams (OpenAI, 2023;Huang et al., 2023b), counterfactual reasoning (Starr, 2022) as a fundamental evaluation of logical relations is less explored in the context of large language models. Previous literature studies counterfactual reasoning either in a multiple-choice manner (Tandon et al., 2019;Qin et al., 2021) or applying labored human study to evaluate counterfactual generation (Qin et al., 2019), leaving an effective evaluation of counterfactual generation unexplored. \n\nIn our study, we delve into the realm of evaluating large language models' (LLMs) ability to generate counterfactually coherent thoughts. Figure 1 demonstrates the paradigm.",
            "score": 0.47978599214189993,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 470
                },
                {
                    "start": 473,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1853
                },
                {
                    "start": 1856,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2029
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 157,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 235,
                    "end": 257,
                    "matchedPaperCorpusId": "252693237"
                },
                {
                    "start": 702,
                    "end": 726,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 744,
                    "end": 764,
                    "matchedPaperCorpusId": "252917648"
                },
                {
                    "start": 1087,
                    "end": 1108,
                    "matchedPaperCorpusId": "233289738"
                },
                {
                    "start": 1108,
                    "end": 1129,
                    "matchedPaperCorpusId": "252693237"
                },
                {
                    "start": 1338,
                    "end": 1362,
                    "matchedPaperCorpusId": "232134851"
                },
                {
                    "start": 1673,
                    "end": 1690,
                    "matchedPaperCorpusId": "235368000"
                },
                {
                    "start": 1761,
                    "end": 1779,
                    "matchedPaperCorpusId": "202542404"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2646484375
        },
        {
            "corpus_id": "268691395",
            "title": "Towards a Zero-Data, Controllable, Adaptive Dialog System",
            "text": "Common data augmentation approaches include lexical substitution (Wei and Zou, 2019), where tokens are inserted, deleted or substituted with semantically similar replacements, as well as backtranslation (Sennrich et al., 2016) where data is automatically translated into other languages before being translated back to the source language.While such approaches can help to expand an existing dataset, they still require seed data, which may not exist for new domains.\n\nTo address this, research in, e.g., the field of low-resource Question Answering (QA) has started exploring the role of LLMs in data generation (Puri et al., 2020;Chen et al., 2023).Given a text, LLMs can be prompted to generate questions about it, e.g., by asking the model to generate a question for which a given named entity is the answer (Li et al., 2023).\n\nHowever, LLMs are black-box algorithms and suffer from hallucination (Azaria and Mitchell, 2023;Peng et al., 2023;Manakul et al., 2023).As such, it is difficult to guarantee that the generated questions are logical, natural, or answerable by the original text.Moreover, commonly used automatic evaluation metrics for text generation do not necessarily correlate with human judgment (Nema and Khapra, 2018).In light of this, we explore different generation strategies and techniques for analyzing the artificial data quality, rather than trusting a single metric.\n\nA recent approach in the dialog community trains a model for generating synthetic dialog acts and user utterances for flowchart-grounded troubleshooting dialogs (Zhan et al., 2023).While this method also relies on the domain representation in form of a structured graph, our generation approach does not require any model training, nor any training data besides the domain graph itself.Additionally, CTS is not limited to the specific task format of trouble-shooting dialogs.",
            "score": 0.47897796897663975,
            "section_title": "Data Generation and Augmentation",
            "char_start_offset": 7164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 467
                },
                {
                    "start": 469,
                    "end": 651
                },
                {
                    "start": 651,
                    "end": 830
                },
                {
                    "start": 832,
                    "end": 968
                },
                {
                    "start": 968,
                    "end": 1092
                },
                {
                    "start": 1092,
                    "end": 1238
                },
                {
                    "start": 1238,
                    "end": 1394
                },
                {
                    "start": 1396,
                    "end": 1577
                },
                {
                    "start": 1577,
                    "end": 1782
                },
                {
                    "start": 1782,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 65,
                    "end": 84,
                    "matchedPaperCorpusId": "59523656"
                },
                {
                    "start": 203,
                    "end": 226,
                    "matchedPaperCorpusId": "15600925"
                },
                {
                    "start": 613,
                    "end": 632,
                    "matchedPaperCorpusId": "211258652"
                },
                {
                    "start": 632,
                    "end": 650,
                    "matchedPaperCorpusId": "259096138"
                },
                {
                    "start": 1214,
                    "end": 1237,
                    "matchedPaperCorpusId": "52134966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "273993914",
            "title": "Building Trustworthy AI: Transparent AI Systems via Large Language Models, Ontologies, and Logical Reasoning (TranspNet)",
            "text": "Large language models (LLMs) have revolutionized natural language processing, achieving breakthrough performance on tasks like translation, summarization, and question-answering through in-context learning, a new paradigm that enables few-shot learning without modifying model parameters [24,26,29]. These models have demonstrated exceptional proficiency in what Kahneman [17] de-scribes as \"System 1\" tasks -automatic, intuitive operations -but have faced challenges in \"System 2\" reasoning tasks, which require conscious, logical steps, such as solving math word problems [4]. \n\nRecent developments in prompt-based reasoning, such as Chain-of-Thought (CoT) prompting, have been instrumental in addressing these challenges. By guiding LLMs to generate intermediate reasoning steps, CoT has significantly improved performance on reasoning-intensive benchmarks, such as the GSM8K dataset for math word problems [29]. This approach, along with advances in self-consistency and prompt-engineering techniques, has helped bridge the gap between LLMs' ability to perform associative tasks and their capacity for multistep reasoning [18]. \n\nDespite these advancements, challenges remain in ensuring the faithfulness and interpretability of reasoning processes in LLMs. Techniques like self-verification and reinforcement learning have been employed to minimize errors and improve reliability, but issues such as hallucination and error accumulation persist, particularly in complex, multi-step reasoning tasks [28]. Additionally, research continues to explore how reasoning capabilities can be transferred to smaller models or embodied agents, as computational efficiency becomes an increasingly important factor in the deployment of LLMs [20]. \n\nThe proposed pipeline integrates domain expert knowledge, retrieval-augmented generation, and formal reasoning frameworks, enabling the system to verify and refine its outputs through external data and structured reasoning. This multilayered approach mitigates some of the inherent issues in LLMs, such as hallucination and over-reliance on probabilistic reasoning. The inclusion of ASP [13] in the pipeline ensures that the reasoning process is grounded in logical consistency, providing verifiable and interpretable outputs. By incorporating multimodal data processing, the pipeline allows the system to handle diverse types of input, further improving its reliability and decision-making capabilities across different domains.",
            "score": 0.47887029329100644,
            "section_title": "State-of-the-Art in LLM Reasoning",
            "char_start_offset": 3498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 578
                },
                {
                    "start": 581,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1737
                },
                {
                    "start": 1740,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2266
                },
                {
                    "start": 2267,
                    "end": 2469
                }
            ],
            "ref_mentions": [
                {
                    "start": 292,
                    "end": 295,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 295,
                    "end": 298,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 910,
                    "end": 914,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1126,
                    "end": 1130,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 2127,
                    "end": 2131,
                    "matchedPaperCorpusId": "261517573"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31103515625
        },
        {
            "corpus_id": "271855673",
            "title": "Document-Level Event Extraction with Definition-Driven ICL",
            "text": "Zero-shot CoT [15], with simple prompts like \"Let's think step by step,\" enhances the transparency and accuracy of the reasoning process. Effective prompts for reasoning should possess the following characteristics: direct relevance to tasks, diverse expressions, guidance for model problem decomposition, integration of known facts for reasoning, and step-by-step refinement of processes. \n\nResearch indicates that explicit prompting methods for LLMs to decompose problems, such as Least-to-Most [28]and zero-shot CoT, improve the reliability of reasoning. Furthermore, the application of various prompting techniques [14,15,29]has confirmed the effectiveness of decomposition strategies, enabling models to systematically handle complex issues. \n\nRecently, Jin et al. [30] proposed the Exploration of Thought (EoT) prompting method, using evolutionary algorithms to dynamically generate diverse prompts, significantly enhancing LLMs' performance in arithmetic, common sense, and symbolic reasoning tasks. Wang et al.'s [31] Plan-and-Solve (PS) prompting method guides models to formulate and execute plans to solve complex problems, improving performance in multi-step reasoning tasks. Zhao et al.'s [32]Logical Thoughts (LoT) framework utilizes principles of symbolic logic to systematically verify and correct reasoning steps, enhancing LLMs' reasoning capabilities across diverse domains. Kim et al.'s [33] fine-tuning dataset COT COLLECTION enhances the generalization ability of small-scale language models on multi-task unseen problems. Wang et al.'s [34] Cue-CoT method introduces intermediate reasoning steps before generating answers, improving LLMs' performance in handling in-depth dialogue issues. \n\nIn summary, innovative chain-of-thought methods such as CoT, EoT, PS prompts, and LoT frameworks significantly enhance the performance of large language models in handling complex logical reasoning tasks within the field of natural language processing. These advancements demonstrate the continual progress and wide-ranging application potential of natural language understanding and reasoning capabilities.",
            "score": 0.4788592623831669,
            "section_title": "Background",
            "char_start_offset": 12879,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 389
                },
                {
                    "start": 392,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 746
                },
                {
                    "start": 749,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1711
                },
                {
                    "start": 1714,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 18,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 619,
                    "end": 623,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 623,
                    "end": 626,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 626,
                    "end": 629,
                    "matchedPaperCorpusId": "258762525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63671875
        },
        {
            "corpus_id": "256615816",
            "title": "LoFT: Enhancing Faithfulness and Diversity for Table-to-Text Generation via Logic Form Control",
            "text": "to-Text (T2T) generation aims to produce natural language descriptions from structured tables. A statement generated from tabular data can be inferred based on different levels of information (e.g., value of a specific cell, logical operation result across multiple cells). Although current T2T models (Lebret et al., 2016;Wiseman et al., 2017;Puduppully et al., 2019;Parikh et al., 2020) have shown remarkable progress in fluency and coherence, they mainly focus on surface-level realizations without much logical inference.\n\nRecently, Chen et al. (2020a) proposed LOG-ICNLG, which is tasked with generating textual descriptions that require logical reasoning over tabular data (i.e., LT2T generation). LT2T generation is challenging as it requires a model to learn the logical inference knowledge from table-text pairs and generate multiple factually correct sentences. Another challenge for LT2T generation is the diversity of generated text. Natural Language Generation (NLG) encourages the diverse output of statements over a single input, as it provides",
            "score": 0.4759634185476735,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 302,
                    "end": 323,
                    "matchedPaperCorpusId": "1238927"
                },
                {
                    "start": 323,
                    "end": 344,
                    "matchedPaperCorpusId": "23892230"
                },
                {
                    "start": 344,
                    "end": 368,
                    "matchedPaperCorpusId": "174801388"
                },
                {
                    "start": 368,
                    "end": 388,
                    "matchedPaperCorpusId": "216641852"
                },
                {
                    "start": 537,
                    "end": 556,
                    "matchedPaperCorpusId": "216056509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5498046875
        },
        {
            "corpus_id": "273229503",
            "title": "Improving LLM Reasoning through Scaling Inference Computation with Collaborative Verification",
            "text": "Large language models (Brown et al., 2020;Achiam et al., 2023;Touvron et al., 2023a;b;Jiang et al., 2023;Team et al., 2024) have demonstrated exceptional performance across various natural language tasks. Notably, the reasoning tasks such as math problem solving (Cobbe et al., 2021;Hendrycks et al., 2021), code completion (Austin et al., 2021;Chen et al., 2021), multi-modal reasoning (Yue et al., 2024a;Liang et al., 2024a) have attracted significant attention from AI researchers. Since reasoning is a critical component of many important high-level tasks, such as scientific discovery (Liang et al., 2024a;Miret & Krishnan, 2024), world model (Hao et al., 2023), embodied agents (Song et al., 2023), etc. However, even the most advanced LLMs still face challenges in complex multi-step reasoning problems (Zhang et al., 2024a;Shi et al., 2024;Trinh et al., 2024). To improve the performance of LLMs on reasoning, recent studies (Yu et al., 2024b;Yue et al., 2024b;Gou et al., 2024;Luo et al., 2023;Wei et al., 2024;Tang et al., 2024;Yue et al., 2024c) have mainly focused on generating synthetic question-answering pairs from stronger LLMs like GPT-4 (Achiam et al., 2023) or utilizing human-annotated rationales (Toshniwal et al., 2024) for supervised fine-tuning. These approaches have achieved outstanding performance on reasoning benchmarks like GSM8k (Cobbe et al., 2021), MATH (Hendrycks et al., 2021;Lightman et al., 2023), MBPP (Austin et al., 2021), etc. While these straightforward data generation methods have proven effective, these LLMs are primarily trained to produce outputs that align with the correct reasoning steps they encountered during training.",
            "score": 0.4758014503469229,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1673
                }
            ],
            "ref_mentions": [
                {
                    "start": 22,
                    "end": 42,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 387,
                    "end": 406,
                    "matchedPaperCorpusId": "265466525"
                },
                {
                    "start": 648,
                    "end": 666,
                    "matchedPaperCorpusId": "258865812"
                },
                {
                    "start": 684,
                    "end": 703,
                    "matchedPaperCorpusId": "254408960"
                },
                {
                    "start": 848,
                    "end": 867,
                    "matchedPaperCorpusId": "267032902"
                },
                {
                    "start": 933,
                    "end": 951,
                    "matchedPaperCorpusId": "262084051"
                },
                {
                    "start": 951,
                    "end": 969,
                    "matchedPaperCorpusId": "261696697"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52783203125
        },
        {
            "corpus_id": "49865372",
            "title": "Generating Synthetic Data for Neural Keyword-to-Question Models",
            "text": "Sect. 2). (2) We introduce several generative models for producing synthetic keyword queries from natural language questions (Sect. 3.1). (3) We develop two filtering mechanisms, which are essential for ensuring that the synthetic training data we feed into the neural network is of high-quality (Sect. 3.2). (4) We evaluate our synthetic data generation approach on the endto-end K2Q task using both automatic and manual evaluation (Sect. 6).",
            "score": 0.4757768092357937,
            "section_title": "Did you mean instead ? (Diverse Questions)",
            "char_start_offset": 5348,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8349609375
        },
        {
            "corpus_id": "263909600",
            "title": "Does Synthetic Data Make Large Language Models More Efficient?",
            "text": "By artificially creating data that closely mimics genuine datasets, the potential to enrich training sets and address data scarcity becomes tangible. Yet, as with all innovations, its efficacy is contingent on context and application. \n\nFor language modeling tasks, synthetic data generation might appear as a beacon of promise on the surface. Here we have a chance to artificially bolster the data pool, potentially leading to better-trained models capable of understanding and predicting linguistic structures. However, the reality reveals a different narrative. The inherent nature of language modeling, where the task revolves around predicting subsequent words in sentences or deciphering intricate linguistic patterns, demands a nuanced and authentic representation of the language. Synthetic data, even when finely crafted, may not capture the intricate unpredictability and vastness of natural language. Consequently, its inclusion often results in minimal to negligible improvements in model accuracy and fluency. This could be attributed to various factors, including the potential for synthetic data to introduce noise or fail to capture the linguistic variances found in genuine, human-generated text. \n\nOn the contrary, when examining question generation tasks, synthetic data generation has shown to be of greater relevance. Unlike the broad scope of language modeling, question generation is more constrained, relying on structured formats and specific linguistic cues. Given its rule-based nature, synthetic data can be tailored to this task more effectively, providing models with a plethora of varied question formats and structures. Our investigations indicate that, while the improvements might not be groundbreaking, there is a discernible enhancement in the model's ability to generate coherent and relevant questions when trained with a blend of real and synthetic data. It's possible that the structured nature of questions allows synthetic generation techniques to produce data that is more aligned with the inherent patterns of question formulation, hence the observed performance boost.",
            "score": 0.4757768092357937,
            "section_title": "Experiments",
            "char_start_offset": 13303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1213
                },
                {
                    "start": 1216,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2113
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51806640625
        },
        {
            "corpus_id": "276617996",
            "title": "Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs",
            "text": "In this section, we provide additional technical insights into how code-generation strategies serve as a scaffolding mechanism for complex reasoning. By interleaving textual explanations with executable or pseudo-executable code, these methods leverage the language model's ability to decompose tasks while offloading precise computations to interpreters or simulators. Below, we outline four representative approaches. \n\nProgram-Aided Language Models (PaL) PaL (Gao et al., 2023) interleaves natural language reasoning and programmatic statements by prompting large language models to emit both text (e.g., comments) and code (e.g., Python snippets). Any arithmetic or logical operations are delegated to a code interpreter, allowing the model to focus on higher-level step-by-step reasoning rather than raw calculation. This reduces errors in multi-step tasks, as correctness is grounded in the verified outputs from executing the code. \n\nProgram of Thoughts (PoT) PoT (Chen et al., 2022) frames the solution process as the generation of a \"program of thoughts,\" where each sub-step is encoded in semantically meaningful variables and partial code. Once generated, the code is executed externally to reliably produce numerical results. By breaking down complex computations into a series of small, interpretable code snippets, PoT enables more transparent and robust multi-step reasoning. \n\nMathCoder MathCoder (Lu et al., 2024) provides a dynamic interplay between reasoning and real-time code execution. The model switches between producing language-based rationales and code blocks, executing each snippet as it is generated. The output of each block is then folded back into the ongoing chain of thought, resulting in an iterative loop of code-based calculation and textual reasoning that can tackle intricate math problems more reliably. \n\nChain of Code (CoC) CoC (Li et al., 2023a) mixes semantic reasoning and code-like structures, but allows certain segments of generated code to be \"emulated\" by the language model itself if they are not executable in a standard interpreter. Whenever actual code execution is possible, it is performed directly (e.g., for arithmetic).",
            "score": 0.4756820673045348,
            "section_title": "A.1 Code-enhanced Reasoning",
            "char_start_offset": 37418,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 419
                },
                {
                    "start": 422,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1390
                },
                {
                    "start": 1393,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1844
                },
                {
                    "start": 1847,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2179
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.375244140625
        },
        {
            "corpus_id": "270710993",
            "title": "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models",
            "text": "In contrast, our work systematically evaluates multi-hop logical reasoning over various inference rules and their combinations. \n\nTo this end, we propose Multi-LogiEval, a systematically created Question-Answering (QA) dataset covering multi-step logical reasoning across three different logic types: Propositional Logic (PL), First-Order Logic (FOL), and Non-Monotonic (NM) reasoning. Our objective is to present a preliminary analysis of the LLMs' ability to perform multi-step logical reasoning and demonstrate their failures even when performing simple reasoning. We believe that, regardless of whether such reasoning is available in some existing natural data (e.g., examinations), LLMs should do proper logical reasoning. Thus, we systematically compiled data using various inference rules and varying numbers of reasoning depths. In particular, our proposed dataset provides \u223c 1.6k high-quality instances that cover 33 inference rules and reasoning patterns and more than 60 complex combinations of these inference rules with a different number of reasoning steps (1 \u223c 5). Our choice of inference rules is further explained in section 3.1. To evaluate LLMs, we formulate a binary classification task in Multi-LogiEval where the context represents a natural language story consisting of logical statements, and the models have to determine whether the story logically entails a conclusion given in the question. Examples of instances are presented in Table 4. To develop Multi-LogiEval, we propose a two-stage procedure: (i) creating meaningful combinations of inference rules to generate data instances with different reasoning depths, and (ii) prompt LLMs to generate <context, question, answer> triplets consisting of different 'ontologies' (i.e., a collection of concepts such as car, person, and animals). In the end, we perform human validation of each generated instance to ensure the quality.",
            "score": 0.47556226464659046,
            "section_title": "Introduction",
            "char_start_offset": 1650,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 130,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1906
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61279296875
        },
        {
            "corpus_id": "251929296",
            "title": "Faithful Reasoning Using Large Language Models",
            "text": "Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.",
            "score": 0.47517244156916494,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46826171875
        },
        {
            "corpus_id": "215548225",
            "title": "Injecting Numerical Reasoning Skills into Language Models",
            "text": "A2: Art Euphoric (span) In this work, we propose that reasoning skills, such as numerical reasoning, are amenable to automatic data generation. Hence, one can inject that skill directly into the model by adding additional pre-training steps, allowing the model to learn the skill in an end-to-end fashion. This results in a fully-differentiable training procedure over a standard and general-purpose architecture, where the output space can be easily controlled through the data generation procedure. \n\nSpecifically (Figure 1), we add to a large pre-trained LM two pre-training steps over automatically-generated synthetic data. First, we generate numerical data of the form 3 + 4 + 11 = 18. Training on this data teaches the model to compute the value of numbers from their tokens and to perform numerical operations. Second, we automatically generate question-passage pairs that require numerical reasoning using a compact grammar (textual data). Training on this data endows the model with the ability to understand computations expressed in pseudo-natural language. \n\nIn both pre-training steps, the model, GEN-BERT, generates output numbers token-by-token. Thus, the model has a standard architecture, where an answer can either be extracted from the input question and passage or generated from a decoder. Pre-training is done in a multi-task setup with a standard LM objective, in order to avoid \"catastrophic forgetting\" (Kirkpatrick et al., 2017) of the linguistic information in the original LM. After pre-training, the model has sufficient language and numerical skills to be directly fine-tuned on a target numerical reasoning dataset, without resorting to specialized architectures. Augmenting more numerical skills does not require changing the model, only generating additional data. \n\nWe demonstrate the validity of our approach by a series of experiments showing that: (a) GENBERT is able to solve pre-training tasks for numerical reasoning.",
            "score": 0.4741089322458395,
            "section_title": "Introduction",
            "char_start_offset": 1964,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1069
                },
                {
                    "start": 1072,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1798
                },
                {
                    "start": 1801,
                    "end": 1958
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.689453125
        },
        {
            "corpus_id": "276557738",
            "title": "Empowering LLMs with Logical Reasoning: A Comprehensive Survey",
            "text": "Large language models (LLMs) have achieved remarkable successes on various natural language tasks. However, recent studies have found that there are still significant challenges to the logical reasoning abilities of LLMs. This paper summarizes and categorizes the main challenges into two aspects: (1) Logical question answering, LLMs often fail to generate the correct answer within complex logical problem which requires sophisticated deductive, inductive or abductive reasoning given a collection of premises and constrains. (2) Logical consistency, LLMs are prone to producing responses contradicting themselves across different questions. For example, a state-of-the-art Macaw question-answering LLM answers Yes to both questions Is a magpie a bird? and Does a bird have wings? but answers No to Does a magpie have wings?. To facilitate this research direction, we comprehensively investigate the most cutting-edge methods and propose detailed taxonomies of these methods. Specifically, to accurately answer complex logic questions, previous methods can be categorized based on reliance on external solvers, prompts, pretraining, and fine-tuning. To avoid logical contradictions, we discuss concepts and solutions of various logical consistencies, including implication, negation, transitivity, factuality consistency, and their composites. In addition, we review commonly used benchmark datasets and evaluation metrics, and discuss promising research directions, such as extensions to modal logic to account for uncertainty, and efficient algorithms satisfying multiple logical consistencies simultaneously.",
            "score": 0.4740474425305491,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2724609375
        },
        {
            "corpus_id": "249062748",
            "title": "Generating Natural Language Proofs with Verifier-Guided Search",
            "text": "A fundamental goal of AI since its inception is automated reasoning (McCarthy et al., 1960): given explicitly provided knowledge as assumptions, we want the system to draw logically valid conclusions. Research in automated reasoning has traditionally focused on structured domains such as formal logic (Robinson and Voronkov, 2001). On the other hand, recent work suggests that free-form natural language can be a suitable vehicle for reasoning (Clark et al., 2020;Dalvi et al., 2021), because natural language represents knowledge without requiring labour-intensive formalization. However, reasoning in natural language is challenging, as it requires compositional generalization to novel examples (Ruis et al., 2020)-a capability that state-of-the-art large language models struggle with (Rae et al., 2021). \n\nIn this work, we focus on proof generation in natural language (Fig. 1): given a hypothesis and a set of supporting facts in natural language, the model generates a proof tree indicating how the hypothesis is derived from a subset of the supporting facts. The proof tree may contain intermediate conclusions, which need to be generated by the model. Existing methods generate the proof either in a single shot or step by step. \n\nStepwise methods leverage the compositionality of proofs, making it easier for the model to learn and generalize to longer proofs (Tafjord et al., 2021). \n\nHowever, existing stepwise methods suffer from a trade-off between generating valid steps and relevant steps. Prior works (Sanyal et al., 2022;Bostrom et al., 2022) have observed that, given the hypothesis, the model often learns to hallucinate invalid proof steps leading to the hypothesis, instead of performing valid logical inference (see examples in Table 3). To mitigate this issue, previous attempts have restricted the model from accessing the hypothesis, forcing it to generate conclusions based solely on known premises. However, without the hypothesis, the model tends to generate many valid but irrelevant steps. This problem is especially prominent for real-world natural language proofs. Due to the inherent ambiguity of natural language, the search space for each proof step is much larger than that of simple synthetic tasks.",
            "score": 0.4731043045993679,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1238
                },
                {
                    "start": 1241,
                    "end": 1394
                },
                {
                    "start": 1397,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2238
                }
            ],
            "ref_mentions": [
                {
                    "start": 445,
                    "end": 465,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 699,
                    "end": 718,
                    "matchedPaperCorpusId": "212658007"
                },
                {
                    "start": 1371,
                    "end": 1393,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 1519,
                    "end": 1540,
                    "matchedPaperCorpusId": "247594506"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25439453125
        },
        {
            "corpus_id": "257636789",
            "title": "Language Model Behavior: A Comprehensive Survey",
            "text": "We next consider logical reasoning tasks, tasks that include symbols and rules, along with algorithms for solving examples when the rules are known (Fujisawa and Kanai 2022). When provided with explicit instructions or examples, language models can perform basic step-by-step logical reasoning (Section 6.1) and numerical reasoning (Section 6.2), but they struggle with complex reasoning, and they are dependent on specific numerical inputs. Language models' numerical and logical reasoning abilities can be improved by connecting the models to external APIs and logical reasoning modules such as calculators and code execution environments (Karpas et al. 2022;Schick et al. 2023;Krawczyk and Subramanya 2023).\n\n6.1 Large language models can perform basic logical reasoning when prompted, but they still struggle with complex reasoning.\n\nIf prompted with examples of reasoning for question-answer pairs (using few-shot prompting; Section 2.3), autoregressive models with at least 8B parameters can perform well on mathematical word problems, formal logic puzzles, and other logical reasoning tasks (Wei et al. 2022c;Suzgun et al. 2022). Their reasoning abilities do not appear to rely solely on surface cues such as word overlap; randomly shuffled example explanations do not provide significant benefits (Lampinen et al. 2022). Given examples, GPT-3 is able to solve fill-in-the-blank puzzles for arbitrary letter patterns and numerical matrix patterns (Webb, Holyoak, and Lu 2022). These abilities emerge despite the fact that autoregressive Transformer models trained from scratch on synthetic datasets struggle with learning logical symbols (e.g. the distinction between \"and\" and \"or\"; Traylor, Feiman, and Pavlick 2021). In some studies, only autoregressive models with at least 20B parameters can solve logic puzzles above chance, even when provided with examples ). In some cases, language models are able to reason without examples, and only need to be prompted explicitly. Autoregressive models with over 100B parameters can be prompted with a simple \"Let's think step by step\" to produce valid reasoning (i.e. \"chain-of-thought prompting\"; Kojima et",
            "score": 0.4729947721900832,
            "section_title": "Logical and Numerical Reasoning",
            "char_start_offset": 53797,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1098,
                    "end": 1116,
                    "matchedPaperCorpusId": "245692342"
                },
                {
                    "start": 1116,
                    "end": 1135,
                    "matchedPaperCorpusId": "252917648"
                },
                {
                    "start": 1305,
                    "end": 1327,
                    "matchedPaperCorpusId": "250526626"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2626953125
        },
        {
            "corpus_id": "248887351",
            "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
            "text": "Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.",
            "score": 0.4729586618537382,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50341796875
        },
        {
            "corpus_id": "259858981",
            "title": "SORTIE: Dependency-Aware Symbolic Reasoning for Logical Data-to-text Generation",
            "text": "Early data-to-text generation mainly focuses on surface-level descriptions of the table contents (Lebret et al., 2016;Liu et al., 2018;Ma et al., 2019;Wang et al., 2020). However, in spite of generation fluency, neural-based generation models struggle to perform rich inference based on the facts in table (Chen et al., 2020a,c). To make up for that, logical table-to-text generation is proposed as a new task with the aim of generating logically-consistent descriptions from open-domain tables (Chen et al., 2020a,c). \n\nIn recent years, to endow neural models with complex reasoning ability, DCVED (Chen et al., 2021) applies causal intervention methods to reduce the spurious correlation in entities. PLOG (Liu et al., 2022) and TABT5 (Andrejczuk et al., 2022) introduce table-to-logical-form or table denoising as self-supervision tasks in the pretraining stage. Similarly, REASTAP (Zhao et al., 2022) introduces 7 pre-training tasks to mimic the 7 types of reasoning skills of humans. It is worth noting that this line of research is orthogonal to ours since they primarily concentrate on developing training instances that reflect the desired reasoning skills. Similar to the programming language in our proposal, Saha et al. (2022) introduce logic string as an intermediate step to guide generation. However, the surface realization from the logic string to the final description is very prone to hallucinations as it is done purely by neural language models.",
            "score": 0.4728850313107794,
            "section_title": "Data-to-text Generation",
            "char_start_offset": 3520,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1465
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 118,
                    "matchedPaperCorpusId": "1238927"
                },
                {
                    "start": 118,
                    "end": 135,
                    "matchedPaperCorpusId": "7672408"
                },
                {
                    "start": 135,
                    "end": 151,
                    "matchedPaperCorpusId": "196189186"
                },
                {
                    "start": 151,
                    "end": 169,
                    "matchedPaperCorpusId": "218487237"
                },
                {
                    "start": 599,
                    "end": 618,
                    "matchedPaperCorpusId": "236459782"
                },
                {
                    "start": 708,
                    "end": 726,
                    "matchedPaperCorpusId": "249062588"
                },
                {
                    "start": 885,
                    "end": 904,
                    "matchedPaperCorpusId": "253097905"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.392578125
        },
        {
            "corpus_id": "267069400",
            "title": "Visualization Generation with Large Language Models: An Evaluation",
            "text": "LLMs have recently showcased remarkable capabilities across a spectrum of natural language processing tasks. However, the quantitative performance of LLMs in different specific tasks is unclear, which may be unbeneficial for users to take advantage of LLMs. Assessing these models' actual quality, capabilities, and limitations is necessary to assist users in utilizing LLMs more effectively. Previous studies have systematically evaluated the capabilities of LLMs from different perspectives, such as code generation, reasoning, and mathematics, but there are still gaps to be filled in the visualization task. \n\nCode generation. Converting natural language to code usually includes automatic code generation, translation, code completion. The generated code needs to meet multiple standards, such as functional requirements, grammatical correctness, and coding style, making the evaluation a complex task. Hendrycks et al. [15] introduce APPS, a new code generation benchmark to systematically evaluate the capability of LLMs to generate Python code directly from natural language specifications. Moreover, Cassano et al. [40] build the first multilingual code generation benchmark, MultiPL-E, providing significant experience for understanding and improving multilingual language model models in code generation tasks. Furthermore, Liu et al. [41] propose a framework to expand the evaluation dataset. The framework uses a generator to obtain a considerable test set to cover different code paths and edge cases, thereby rigorously evaluating the accuracy of LLM-generated code. Based on the benchmark, Ding et al. [42] present a static code-completion evaluation framework. It analyzes the errors made by pre-trained LLMs on a real-world Python evaluation set and identifies common static errors and the trends of their occurrence frequency. \n\nReasoning. The reasoning task is an essential challenge for LLMs. This type of task requires the model to not only understand the meaning of the question but also infer logically and causally to generate responses with a logical structure and correct answers. Liu et al. [16] evaluate the logical reasoning capabilities of two language models developed by OpenAI, ChatGPT and GPT-4, finding that GPT-4 performs better than ChatGPT on most benchmarks, but its performance drops on logical reasoning natural language inference tasks. Fu et al. [43] introduce an open-source evaluation for measuring LLMs' inference performance.",
            "score": 0.47279093146798534,
            "section_title": "Large Language Model Capability Evaluation",
            "char_start_offset": 12301,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1845
                },
                {
                    "start": 1848,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2379
                },
                {
                    "start": 2380,
                    "end": 2473
                }
            ],
            "ref_mentions": [
                {
                    "start": 1618,
                    "end": 1622,
                    "matchedPaperCorpusId": "259088657"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33154296875
        },
        {
            "corpus_id": "264146296",
            "title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
            "text": "Research on the generalisation and robustness of large language models for logical reasoning mainly focuses on synthetic natural language reasoning. Initial findings indicate that transformers can be trained on multi-hop reasoning tasks and substantially generalise to deeper unseen reasoning depths, although it can be challenging on paraphrased synthetic test sets Clark et al. (2021). Efforts to enhance generalisation to deeper multi-step reasoning include the introduction of PARARULE-Plus, providing data augmentation on reasoning depths between 2 to 5 Bao et al. (2022). AbductionRules incorporates abductive reasoning to understand and answer the multi-step reasoning task Young et al. (2022). ROBUSTLR is a challenging dataset considering conjunction, negation, and utilising logical equivalence for paraphrasing Sanyal et al. (2022). \n\nThe existing synthetic multi-step reasoning datasets often lack the complexity and diversity found in real-world data. They might be generated using a limited set of rules or scenarios, which can lead to a narrower scope of logical reasoning challenges. ReClor Yu et al. (2020) and LogiQA Liu et al. (2020a) are challenging reading comprehension datasets derived from real-world examinations such as the GMAT, LSAT, and national civil servant exams Yu et al. (2020); Liu et al. (2020a;2023b). An enhanced version, LogiQAv2, incorporates additional data from Chinese civil servant examinations Liu et al. (2023b). Evaluations reveal that the implementation of discourse order-oriented loss functions, specifically Sentence Order Prediction (SOP) and Next Sentence Prediction (NSP), enhances the performance of models like ALBERT and BERT on reasoning tasks Lan et al. (2020); Kenton & Toutanova (2019); Li et al. (2022). Further assessments of generative large language models, including GPT-3.5 and GPT-4, on ReClor, LogiQA, and LogiQAv2, have demonstrated their commendable performance. However, it is unclear how robust large language models are in real-world logical reasoning tasks Liu et al. (2023b).",
            "score": 0.4723902304446113,
            "section_title": "Related Work",
            "char_start_offset": 4727,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2051
                }
            ],
            "ref_mentions": [
                {
                    "start": 367,
                    "end": 386,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 559,
                    "end": 576,
                    "matchedPaperCorpusId": "251135345"
                },
                {
                    "start": 681,
                    "end": 700,
                    "matchedPaperCorpusId": "247619052"
                },
                {
                    "start": 822,
                    "end": 842,
                    "matchedPaperCorpusId": "249062828"
                },
                {
                    "start": 1107,
                    "end": 1123,
                    "matchedPaperCorpusId": "209485573"
                },
                {
                    "start": 1295,
                    "end": 1311,
                    "matchedPaperCorpusId": "209485573"
                },
                {
                    "start": 1702,
                    "end": 1719,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 1748,
                    "end": 1764,
                    "matchedPaperCorpusId": "247613500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7080078125
        },
        {
            "corpus_id": "271064536",
            "title": "Prompting Techniques for Secure Code Generation: A Systematic Investigation",
            "text": "This example is directly taken from the original paper itself.Chain-of-Thought (CoT): In this prompting approach [103], the LLM is compelled to produce a sequence of intermediary logical reasoning steps in natural language, culminating in the solution to the presented problem.The goal of this approach is to replicate how humans solve a complex problem following a chain of reasoning or justification steps.In this method, the model is initially given a set of few-shot examples, consisting of <input, chain of thought, output> triplets, to guide its understanding before it tackles the actual task.This technique has been evaluated on various benchmarks including arithmetic, common sense, and symbolic reasoning.However, one can assume that CoT can also be applied to code generation tasks.Figure 7 demonstrates the CoT prompting technique for code generation.\n\nAn approach similar to this was proposed in 2017 by Ling et al. [59] where they train an attention-based sequence-tosequence model to solve complex mathematical problems using a dataset containing problems with answer rationales and the final correct answers.However, this approach focused on training rather than explicitly prompting a model, and it did not involve an LLM.Hence we identify CoT as a novel prompting technique.\n\nManuscript submitted to ACM  the most consistent response among the various final answers generated from these diverse reasoning chains.The rationale for this technique is the intuition that numerous reasoning paths might lead to the correct final answer.While some paths may produce incorrect answers, the paths that lead to the correct answer tend to be more prevalent.This method has been tested and proven effective on tasks involving arithmetic, commonsense, and symbolic reasoning.\n\nComplexity-based prompting also adopts a similar approach but posits that chains involving more reasoning steps yield better performance.Consequently, this technique emphasizes using chain-of-thought few-shot examples comprising a greater number of reasoning steps (i.e., more complexity).Furthermore, the final answer is chosen based on the consistency among responses with a larger number of reasoning steps, while responses with fewer reasoning steps are discarded.\n\nBoth techniques are particularly well-suited for tasks that have a definitive final answer, as opposed to more creative tasks like code generation.However, they can still be applied to code generation tasks.A demonstration of adapting self-consistency for code generation is included in Figure 8.",
            "score": 0.47229093567693825,
            "section_title": "Decomposition-based Techniques.",
            "char_start_offset": 30209,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 62,
                    "end": 277
                },
                {
                    "start": 277,
                    "end": 408
                },
                {
                    "start": 408,
                    "end": 600
                },
                {
                    "start": 600,
                    "end": 715
                },
                {
                    "start": 715,
                    "end": 793
                },
                {
                    "start": 793,
                    "end": 863
                },
                {
                    "start": 865,
                    "end": 1124
                },
                {
                    "start": 1124,
                    "end": 1239
                },
                {
                    "start": 1239,
                    "end": 1292
                },
                {
                    "start": 1294,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1549
                },
                {
                    "start": 1549,
                    "end": 1665
                },
                {
                    "start": 1665,
                    "end": 1781
                },
                {
                    "start": 1783,
                    "end": 1920
                },
                {
                    "start": 1920,
                    "end": 2072
                },
                {
                    "start": 2072,
                    "end": 2251
                },
                {
                    "start": 2253,
                    "end": 2400
                },
                {
                    "start": 2400,
                    "end": 2460
                },
                {
                    "start": 2460,
                    "end": 2549
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 118,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 929,
                    "end": 933,
                    "matchedPaperCorpusId": "12777818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1588134765625
        },
        {
            "corpus_id": "237091328",
            "title": "SAPPHIRE: Approaches for Enhanced Concept-to-Text Generation",
            "text": "There has been increasing interest in constrained text generation tasks which involve constructing natural language outputs under certain preconditions, such as particular words that must appear in the output sentences. A related area of work is data-to-text natural language generation (NLG), which requires generating natural language descriptions of structured or semi-structured data inputs. Many constrained text generation and NLG tasks share commonalities, one of which is their task formulation: a set of inputs must be converted into natural language sentences. This set of inputs can be, in many cases, thought of as concepts, e.g. higher-level words or structures that play an important role in the generated text.\n\nWith the increased popularity of Transformerbased models and their application to many NLP tasks, performance on many text generation tasks has improved considerably. Much progress in recent years has been from the investigation of model improvements, such as larger and more effectively pretrained language generation models. However, are there simple and effective approaches to improving performance on these tasks that can come from the data itself? Further, can we potentially use the outputs of these models themselves to further improve their task performance -a \"selfintrospection\" of sorts?\n\nIn this paper, we show that the answer is yes. We propose a suite of simple but effective improvements for concept-to-text generation called SAP-PHIRE: Set Augmentation and Post-hoc PHrase Infilling and REcombination. Specifically, SAP-PHIRE is composed of two major approaches: 1) the augmentation of input concept sets ( \u00a74.1), 2) the recombination of phrases extracted from baseline generations into more fluent and logical text ( \u00a74.2). These are mainly model-agnostic improvements that rely on the data itself and the model's own initial generations, respectively. 1 We focus on generative commonsense reasoning, or CommonGen (Lin et al., 2020), which involves generating logical sentences describing an everyday scenario from a set of concepts, which in this case are individual words that must be represented in the output text in some form. CommonGen is a challenging instance of constrained text generation that assesses 1) relational reasoning abilities using commonsense knowledge, and 2) compositional generalization capabilities to piece together concept combinations. Further, CommonGen's",
            "score": 0.4719438663362621,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38916015625
        },
        {
            "corpus_id": "270870071",
            "title": "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning",
            "text": "Multi-step Reasoning Challenging reasoning tasks have spurred innovative research in large language models (LLMs), which are essential for handling complex queries (Kaddour et al., 2023;Lightman et al., 2023;Huang et al., 2023).A prominent strategy employed is the Chain-of-Thought (CoT) prompting technique (Wei et al., 2022), along with its derivatives (Kojima et al., 2022;Wang et al., 2022;Yao et al., 2024).These methods decompose the reasoning process into sequential steps, systematically approaching problem-solving by mimicking human-like reasoning.To further enhance the accuracy of these intermediate steps, recent studies leverage extensive synthetic datasets, which are either distilled from cutting-edge models (Yu et al., 2023b;Luo et al., 2023) or composed of selfgenerated rationales (Zelikman et al., 2022;Yuan et al., 2023;Ni et al., 2022), to fine-tune the LLMs.\n\nSuch training effectively sharpens the models' ability to produce CoT reasoning that leads to correct answers.\n\nIn addition, there is an increasing emphasis on test-time verification, a process that generates multiple solutions and ranks them through a separate verifier (Cobbe et al., 2021) to select the most accurate one.The DIVERSE framework (Li et al., 2022) demonstrates the effectiveness of employing a variety of CoT prompts in conjunction with a verifier to tackle reasoning challenges.CoRe (Zhu et al., 2022) implements a dual-process system by finetuning both the generator and the verifier, thereby enhancing the performance of LLMs on math word problems.",
            "score": 0.4715438945992949,
            "section_title": "A Related Work",
            "char_start_offset": 26179,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 558
                },
                {
                    "start": 558,
                    "end": 882
                },
                {
                    "start": 884,
                    "end": 994
                },
                {
                    "start": 996,
                    "end": 1208
                },
                {
                    "start": 1208,
                    "end": 1379
                },
                {
                    "start": 1379,
                    "end": 1551
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7666015625
        },
        {
            "corpus_id": "276885355",
            "title": "Optimizing Multi-Hop Document Retrieval Through Intermediate Representations",
            "text": "Large Language Models (LLMs) have made significant advancements in natural language processing, demonstrating exceptional performance across a variety of tasks (Muennighoff et al., 2023;Hendrycks et al., 2021), including complex multi-hop tasks. However, despite their success, LLMs often generate factually incorrect answers as they rely solely on their internal parametric memory, which struggles with tasks requiring external knowledge (Lewis et al., 2020). To address this limitation, Retrieval-Augmented Generation (RAG) frameworks (Gao et al., 2023) have been developed, utilizing semantic similarity metrics to retrieve relevant external documents, which are subsequently integrated into the generation context. Nevertheless, RAG still faces challenges, particularly in handling complex queries that require multi-hop document retrieval. \n\nMulti-step methods, such as Self-ask (Press et al., 2023a) and ITER-RETGEN (Shao et al., 2023a), address the challenges of multi-hop reasoning and document retrieval by decomposing complex queries into manageable steps and iteratively refining the retrieval process (Jiang et al., 2023;Shao et al., 2023b;Ma et al., 2023). Over multiple iterations, the response is refined until a final answer is produced. As illustrated in Figure 1, while multi-step methods significantly improve retrieval and reasoning capabilities, they often result in higher computational overhead due to their iterative nature. Several variants aim to address this issue. Adaptive-RAG (Jeong et al., 2024) simplify the reasoning process for less complex queries. However, for more complex queries, multiple full inference iterations are still required. Efficient RAG (Zhuang et al., 2024) get rid of relying on LLM calls at each step, using the smaller model to generate the special token for information processing. Despite these advancements, such methods still tend to address the problem in generating human understandable query, overlooking the nexthop information contained in intermediate representations during reasoning. There is a question we try to find out that how can we leverage the information contain to achieve performance comparable to multi-step approaches while maintaining a low inference overhead similar to vanilla RAG?",
            "score": 0.47130510343741844,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2264
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 186,
                    "matchedPaperCorpusId": "252907685"
                },
                {
                    "start": 186,
                    "end": 209,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 439,
                    "end": 459,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 884,
                    "end": 905,
                    "matchedPaperCorpusId": "252762102"
                },
                {
                    "start": 922,
                    "end": 942,
                    "matchedPaperCorpusId": "258866037"
                },
                {
                    "start": 1113,
                    "end": 1133,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 1133,
                    "end": 1152,
                    "matchedPaperCorpusId": "258866037"
                },
                {
                    "start": 1506,
                    "end": 1526,
                    "matchedPaperCorpusId": "268553748"
                },
                {
                    "start": 1688,
                    "end": 1709,
                    "matchedPaperCorpusId": "271769059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.316650390625
        },
        {
            "corpus_id": "269449073",
            "title": "Logic Agent: Enhancing Validity with Logic Rule Invocation",
            "text": "Addressing formal logical reasoning, LINC (Olausson et al., 2023) leverages LLMs as FOL language translators to attain formal representations of contextual information, complemented by traditional theorem provers for validation.LINC's approach, which employs a voting strategy to resolve inconsistencies in FOL language generation, contrasts with our method which adopts a more flexible propositional logic to distill the abstract essence of context while meticulously controlling the validity of generative reasoning.Furthermore, the exploration of language models as theorem provers has introduced systems like Lang-Pro (Abzianidze, 2017), a natural language theorem prover that harnesses higher-order logic to assess linguistic expressions' consistency.LangPro's reliance on CCG parsing and a dedicated knowledge base for generating Lambda Logical Forms (LLFs) presents a contrast to our work, which utilizes propositional logic, thereby circumventing the need for a theorem-proving knowledge base.In parallel, semantic-constrained decoding techniques, as exemplified by NEUROLOGIC DE-CODING (Lu et al., 2020), enable language models to generate contextually coherent text while adhering to complex lexical constraints.Our approach resonates with this paradigm, albeit with a distinct focus on employing constrained generation paired with guided deduction rules, thereby carving a unique niche in the landscape of formal reasoning and logical inference.\n\nDistinctively, we encapsulate the logical reasoning process into callable function forms, packaging logic rules as tools for LLM agents.This strategic shift in leveraging LLMs as autonomous decisionmakers, equipped with a toolkit of generalized logic reasoning functions, marks a significant departure from existing models.\n\nFigure 2 presents the Logic Agent (LA) framework's architecture.Initially, natural language inputs undergo logic parsing on the left, resulting in structured logic forms (see Section 3.1).The center highlights the application of deduction rules for logical inference (see Section 3.2).Finally, on the right, the constrained generation process employs these inferences to produce contextually relevant and logically coherent outputs, illustrating the LA's systematic approach to enhancing reasoning in large language models (see Section 3.3).",
            "score": 0.47129970721088393,
            "section_title": "Parsed Logic",
            "char_start_offset": 9534,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 518
                },
                {
                    "start": 518,
                    "end": 756
                },
                {
                    "start": 756,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1222
                },
                {
                    "start": 1222,
                    "end": 1456
                },
                {
                    "start": 1458,
                    "end": 1594
                },
                {
                    "start": 1594,
                    "end": 1781
                },
                {
                    "start": 1783,
                    "end": 1847
                },
                {
                    "start": 1847,
                    "end": 1971
                },
                {
                    "start": 1971,
                    "end": 2068
                },
                {
                    "start": 2068,
                    "end": 2324
                }
            ],
            "ref_mentions": [
                {
                    "start": 42,
                    "end": 65,
                    "matchedPaperCorpusId": "264436468"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.343994140625
        },
        {
            "corpus_id": "263909600",
            "title": "Does Synthetic Data Make Large Language Models More Efficient?",
            "text": "The approach of synthetic data generation in Natural Language Processing (NLP) has shown promise, but there's still much room for improvement and exploration. Here are some potential directions for future work: \n\n\u2022 Improving Synthetic Data Quality: One of the main challenges with synthetic data is ensuring its quality. Future work could focus on developing new techniques to generate higher-quality synthetic data that more accurately reflects real-world language patterns and distributions. \n\n\u2022 Hybrid Generation Methods: Combining template-based question generation with more flexible methods, such as machine learning or transformer-based question generation techniques, could create a more diverse set of synthetic questions and mitigate some of the limitations of template-based generation. \n\n\u2022 Evaluation Metrics for Synthetic Data: Designing metrics to evaluate the quality of synthetic data could be a valuable contribution. These metrics could help guide the generation process and provide a more objective measure of whether the synthetic data is likely to improve model performance. \n\n\u2022 Adaptive Synthetic Data Generation: Research could be directed towards adaptive synthetic data generation, where the synthetic data generation process is guided by the performance of the model, focusing on areas where the model struggles. \n\n\u2022 Investigating Optimal Ratios of Synthetic to Real Data: More extensive empirical studies could help identify the optimal ratios of synthetic to real data for various types of NLP tasks and models. \n\n\u2022 Application-Specific Synthetic Data: Different NLP tasks might benefit from different types of synthetic data. Future work could investigate how to tailor synthetic data generation to specific applications. \n\n\u2022 Addressing Biases: Future work could also focus on how synthetic data generation can be used to mitigate biases in NLP models, exploring different strategies for generating synthetic data that helps to counteract known biases in the training data. \n\n\u2022 Computational Efficiency: Reducing the computational cost of synthetic data generation is another important direction for future work. This could involve developing more efficient algorithms or making better use of hardware resources. \n\nBy pursuing these avenues of future work, the field can continue to advance the use of synthetic data in NLP and fully realize its potential for improving the performance and robustness of language models.",
            "score": 0.47108735754461806,
            "section_title": "Future Work",
            "char_start_offset": 23700,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 210
                },
                {
                    "start": 213,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 493
                },
                {
                    "start": 496,
                    "end": 797
                },
                {
                    "start": 800,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1338
                },
                {
                    "start": 1341,
                    "end": 1539
                },
                {
                    "start": 1542,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1750
                },
                {
                    "start": 1753,
                    "end": 2002
                },
                {
                    "start": 2005,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2241
                },
                {
                    "start": 2244,
                    "end": 2449
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8486328125
        },
        {
            "corpus_id": "256846551",
            "title": "STREET: A Multi-Task Structured Reasoning and Explanation Benchmark",
            "text": "Complex Reasoning in Question-Answering Modeling complex reasoning is an important challenge and a crucial component of natural language understanding. In the context of questionanswering (QA), initial efforts to emulate complex reasoning used symbolic representations and problem solvers (Forbus & De Kleer, 1993;Platonov et al., 2020). With recent advances in pre-trained language models, reasoning over natural language has became more tractable as these models can more easily handle ambiguity and learn the reasoning rules implicitly (Clark et al., 2018;Tafjord et al., 2021). As one of the simpler forms of reasoning, textual entailment was extensively studied, and many datasets and tasks have been proposed (Bowman et al., 2015;Williams et al., 2018;Zellers et al., 2018). To address multi-step reasoning over language, many multi-hop QA (MHQA) datasets and methods were proposed (Yang et al., 2018;Dua et al., 2019;Xiong et al., 2021;Chen et al., 2021). \n\nCommon limitations of MHQA that we try to address in this paper include (1) a small number of reasoning steps (usually up to three) and ( 2) simplistic evaluation, allowing models to correctly predict answers by exploiting spurious correlations (Tang et al., 2021). Datasets such as CLUTRR (Sinha et al., 2019) and RuleTaker D* (Clark et al., 2020) better addressed the multi-step and structured aspect reasoning with explanations. However, they contain mostly synthetic data and tasks that are relatively easy to solve with current language models. \n\nExplainable Question-Answering Due to the black-box nature of neural networks (Danilevsky et al., 2020), many approaches were proposed to improve the explainability of QA systems. They include explanation graphs (Jansen et al., 2018), generating a free-form natural language explanations (Camburu et al., 2018;Rajani et al., 2019b;Ayyubi et al., 2020), and chain of reasoning explanations Jhamtani & Clark (2020).",
            "score": 0.47089446377601396,
            "section_title": "RELATED WORK",
            "char_start_offset": 23239,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1514
                },
                {
                    "start": 1517,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1930
                }
            ],
            "ref_mentions": [
                {
                    "start": 314,
                    "end": 336,
                    "matchedPaperCorpusId": "207880532"
                },
                {
                    "start": 559,
                    "end": 580,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 715,
                    "end": 736,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 736,
                    "end": 758,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 758,
                    "end": 779,
                    "matchedPaperCorpusId": "52019251"
                },
                {
                    "start": 907,
                    "end": 924,
                    "matchedPaperCorpusId": "67855846"
                },
                {
                    "start": 924,
                    "end": 943,
                    "matchedPaperCorpusId": "221970302"
                },
                {
                    "start": 943,
                    "end": 961,
                    "matchedPaperCorpusId": "224803601"
                },
                {
                    "start": 1210,
                    "end": 1229,
                    "matchedPaperCorpusId": "211258744"
                },
                {
                    "start": 1255,
                    "end": 1275,
                    "matchedPaperCorpusId": "198184456"
                },
                {
                    "start": 1595,
                    "end": 1620,
                    "matchedPaperCorpusId": "222125099"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63671875
        },
        {
            "corpus_id": "264426432",
            "title": "Reasoning about Ambiguous Definite Descriptions",
            "text": "Reasoning is a broad term and the tasks related to it are varied. We focus on the type of reasoning that is explicit and is done with natural language, which has recently been surveyed extensively (Qiao et al., 2023;Huang and Chang, 2023;Yu et al., 2023). This paradigm became possible with the advent of large language models (LLMs) such as Chat-GPT and GPT4 (OpenAI, 2023). These models can be prompted to perform 'chain-of-thought' reasoning (Wei et al., 2022); even in a zero-shot setting (Suzgun et al., 2022). This has significantly improved performance for many tasks (Suzgun et al., 2022). Using a new dataset of ambiguous definite descriptions, our experiments evaluate to what extent LLMs can use (chain-of-thought) reasoning to resolve ambiguity in language. \n\nRecent work on ambiguity includes the construction of both curated (Liu et al., 2023;Min et al., 2020) and synthetic datasets (Yuan et al., 2023). Such datasets investigate ambiguity in a variety of tasks such as: natural language inference, opendomain question answering, etc. Our dataset is synthetically generated (but factual), and focuses specifically on de dicto / de re ambiguities, forming a binary classification task. \n\nThe term 'logical reasoning over natural language' (LRNL) was coined by Yang et al. (2023) to talk about a new trend where natural language is used to represent knowledge and LLMs are used to reason over that knowledge. One challenge this paradigm could have to overcome is the potential for ambiguity inherent in natural language. Our work intentionally introduces ambiguity, and evaluates how well LLMs can resolve it by reasoning. \n\nPreviously, Winograd schemas (Levesque et al., 2012) have been used in various benchmarks to test for commonsense reasoning. These schemas consist of sentences with ambiguous pronouns which must be resolved in one of two ways. However, existing schemas focus on ambiguities that humans resolve intuitively (without the need for explicit reasoning).",
            "score": 0.47087324224741534,
            "section_title": "Related Work",
            "char_start_offset": 3025,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1199
                },
                {
                    "start": 1202,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1635
                },
                {
                    "start": 1638,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1986
                }
            ],
            "ref_mentions": [
                {
                    "start": 857,
                    "end": 874,
                    "matchedPaperCorpusId": "216056269"
                },
                {
                    "start": 898,
                    "end": 917,
                    "matchedPaperCorpusId": "256504027"
                },
                {
                    "start": 1667,
                    "end": 1690,
                    "matchedPaperCorpusId": "15710851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4921875
        },
        {
            "corpus_id": "272827168",
            "title": "Contrastive Learning for Knowledge-Based Question Generation in Large Language Models",
            "text": "The goal of question generation is to enable computers to simulate the human questioning process based on understanding specific text or knowledge base. In the field of question generation, knowledge base-based question generation is a key branch, which generates corresponding natural language questions by utilizing query subgraphs and answers in knowledge graphs [1]. \n\nQuestion generation tasks provide data support for question-answering systems. Question-answering system research requires high-quality question-answering data as training and test corpora [2]. As the scale of question-answering systems grows, the question-answering datasets that were previously only manually annotated can no longer meet the requirements of training and testing. With the accelerated development of the digital age, the application of artificial intelligence technology, especially in the field of questionanswering systems, has become more and more extensive. Professional question-answering systems can provide accurate answers to complex queries and support decision-making and learning processes in multiple fields such as education, medical care, and law. To achieve this goal, the system needs to rely on a large amount of high-quality question-answering data as training and test corpora. However, as the scale of questionanswering systems expands and domain knowledge deepens, the traditional method of manually annotating questionanswering datasets can no longer meet current needs in terms of cost, efficiency, and coverage. With the in-depth application of question-answering systems in various fields, how to build high-quality question-answering datasets at low cost has become a research focus to meet the growing demand for data in these systems [3]. \n\nThe emergence of large language models (LLMs), such as OpenAI's ChatGPT, marks a major shift in the field of artificial intelligence [4]. These models have demonstrated unprecedented capabilities in reasoning and generation tasks and have become the focus of the current artificial intelligence field. LLMs have mastered a wealth of language knowledge and world knowledge through pre-training on large-scale corpora. Researchers have found that the use of LLMs to synthesize data can already achieve the quality of manually annotated data [5].",
            "score": 0.47068156578277265,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1757
                },
                {
                    "start": 1760,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2176
                },
                {
                    "start": 2177,
                    "end": 2303
                }
            ],
            "ref_mentions": [
                {
                    "start": 562,
                    "end": 565,
                    "matchedPaperCorpusId": "258108262"
                },
                {
                    "start": 1753,
                    "end": 1756,
                    "matchedPaperCorpusId": "266378240"
                },
                {
                    "start": 1893,
                    "end": 1896,
                    "matchedPaperCorpusId": "260440590"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53173828125
        },
        {
            "corpus_id": "237562927",
            "title": "RNG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering",
            "text": "Having a ranked list of candidates, we introduce a generation model to compose the final logical form conditioned on the question and the top-k logical forms returned by our ranker. Our generator is a transformer-based seq-to-seq model (Vaswani et al., 2017) instantiated from Raffel et al. (2020), as it demonstrates strong performance in generationrelated tasks. As shown in Figure 3, we construct the inputs by concatenating the question and the top-k candidates returned by the ranker separated by semi-colon (i.e., [x; c t 1 ; ...; c t k ]). We train the model to generate the ground truth logical form token by token with cross-entropy objective using teacher forcing. In the inference, we use beamsearch to autoregressively decode top-k target logical forms. To construct the top-k logical form candidates needed for training the generator we first train the ranker, and then use the rankings it produces on the training data.\n\nSince the generation model can now leverage both the question and KB schema information (contained in the candidates), the context is much more specified as compared to only conditioning on the question. This enables our generator to leverage the training data more efficiently by focusing only on correcting or supplementing existing logical forms instead of learning both the generation rule and correctness of logical forms.\n\nExecution-Augmented Inference We use a vanilla T5 generation model without syntactic constraints, which does not guarantee the syntactic correctness nor executability of the produced logical forms. Therefore, we use an execution-augmented inference procedure, which is commonly used in prior semantic parsing related work (Devlin et al., 2017;Ye et al., 2020b). We first decode top-k logical forms using beam search and then execute each logical form until we find one that yields a valid (non-empty) answer. In case that none of the topk logical forms is valid, we return the top-ranked candidate obtained using the ranker as the final logical form, which is guaranteed to be executable. This inference schema can ensure finding one valid logical form for each problem. It is possible to incorporate a more complex mechanism to control the syntactic correct",
            "score": 0.4697697069114096,
            "section_title": "Target Logical Form Generation",
            "char_start_offset": 9341,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 236,
                    "end": 258,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 277,
                    "end": 297,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1686,
                    "end": 1707,
                    "matchedPaperCorpusId": "6933074"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4853515625
        },
        {
            "corpus_id": "236772032",
            "title": "From LSAT: The Progress and Challenges of Complex Reasoning",
            "text": "To encourage the progress of artificial intelligence systems towards deeper human-like comprehension and reasoning, there has been a surge in complex reasoning research in recent \n\nThe overall illustration of our investigation from LSAT towards complex reasoning. years. We first investigate existing works on several major aspects of complex reasoning. a) Logical Reasoning: An increasing number of tasks and datasets have been introduced targeting logical reasoning. Natural Language Inference [22,23,24] aims to determine the entailment relationship between a hypothesis and a premise, which requires relatively simple logical reasoning ability at the sentence level. Several question answering datasets have been proposed for promoting logical reasoning ability, i.e., LogiQA [25], ReClor [14], which are sourced from public standardized exams. However, previous methods usually fail to model the discrete logical inference process explicitly. This work also dives into logical reasoning, and considers understanding the elementary logical structure and perform explicit logical inference to draw a logical conclusion. \n\nb) Commonsense Reasoning: Commonsense reasoning requires utilizing commonsense knowledge to reason out the answer, which attracts great concern of the research communities. Recently many benchmarks have been introduced to assess reasoning capabilities over different commonsense knowledge, such as domain-specific knowledge [26,27,28], general semantic knowledge [29] and inferential knowledge [30]. Current methods, however, are still not robust enough to be deployed in the open domain and ignore directly modeling commonsense through symbolic integration [31]. How to incorporate symbolic commonsense and reason over relevant knowledge will be substantially explored [32,33]. c) Multi-Hop Reasoning: Multi-hop reasoning is another widely studied complex reasoning task recently [34,35,36,37], which requires reasoning across multiple pieces of sentences or documents to model multi-hop relationships to reach the answer. Conversational question answering tasks [38,39] also involve multi-hop reasoning over multi-turn utterances. \n\nConstructing correct multi-hop reasoning chains (conversation flows) has been a key challenge for these tasks, and it is an essential research direction to effectively model multi-hop reasoning paths over multiple passages [40,41]. \n\nd) Numerical Reasoning: Numerical reasoning involves performing discrete arithmetic reasoning over quantities to solve mathematical word problems [42,43,44,45], which is a fundamental and challenging task.",
            "score": 0.46967437456399114,
            "section_title": "A. Taxonomy of Complex Reasoning",
            "char_start_offset": 7046,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 181,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1122
                },
                {
                    "start": 1125,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2157
                },
                {
                    "start": 2160,
                    "end": 2391
                },
                {
                    "start": 2394,
                    "end": 2599
                }
            ],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 500,
                    "matchedPaperCorpusId": "8587959"
                },
                {
                    "start": 500,
                    "end": 503,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 780,
                    "end": 784,
                    "matchedPaperCorpusId": "220483148"
                },
                {
                    "start": 793,
                    "end": 797,
                    "matchedPaperCorpusId": "209485573"
                },
                {
                    "start": 1456,
                    "end": 1459,
                    "matchedPaperCorpusId": "202541184"
                },
                {
                    "start": 1488,
                    "end": 1492,
                    "matchedPaperCorpusId": "15206880"
                },
                {
                    "start": 1519,
                    "end": 1523,
                    "matchedPaperCorpusId": "53170360"
                },
                {
                    "start": 1683,
                    "end": 1687,
                    "matchedPaperCorpusId": "220060314"
                },
                {
                    "start": 1906,
                    "end": 1910,
                    "matchedPaperCorpusId": "5112038"
                },
                {
                    "start": 1910,
                    "end": 1913,
                    "matchedPaperCorpusId": "9192723"
                },
                {
                    "start": 2093,
                    "end": 2096,
                    "matchedPaperCorpusId": "52055325"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.580078125
        },
        {
            "corpus_id": "273662478",
            "title": "Leveraging LLMs for Hypothetical Deduction in Logical Inference: A Neuro-Symbolic Approach",
            "text": "While LINA demonstrates strong accuracy across various datasets, logical reasoning based on large language models (LLMs) remains a significant research problem. One key limitation of this work is the expressive capacity of first-order logic (FOL). Although our strategy of retaining partial natural language information prevents information loss during the extraction process, the restricted expressive power of FOL imposes constraints on the reasoning process. In problems with more complex logical structures, problem information cannot always be effectively translated into FOL, which hinders LINA from fully utilizing FOL reasoning rules to aid the inference process. Addressing this issue may require formal methods such as higher-order logics Miller & Nadathur (1986) Higginbotham (1998) or nonclassical logics Priest (2008) Burgess ( 2009) that can better capture the underlying logical structures of such problems. Another limitation arises from the lack of comprehensiveness in the deductive reasoning steps. Specifically, the method of selecting premises from the available conditions for each deductive step remains unresolved. This often leads to prolonged deduction processes, thereby increasing the likelihood of errors. In addition, our approach necessitates additional costs to ensure the accuracy and reliability of the reasoning process, leaving room for future enhancements in reasoning efficiency. \n\nIn future work, we will explore alternative formal methods to replace FOL, aiming to improve performance on more complex problems. We will also continue to refine the hypothetical-deductive method or investigate other systematic approaches to enhance the reliability of LLM-based reasoning. Our goal is to enable LLMs not only to retain their inherent generalizability advantages over external solvers but also to improve their accuracy in logical reasoning tasks. \n\nPrompt for Question-Option Fusion Given a multiple-choice question with both a question and an option, transform them into a single proposition (a declarative statement). The proposition should combine the context provided by the question with the content of the chosen option. Use the following approach: \n\n1.Treat the question as the background or context of the statement, removing any interrogative form or question marks. 2.Incorporate the option into the background as the key point or focus of the statement.",
            "score": 0.46944041925843605,
            "section_title": "DISCUSSION",
            "char_start_offset": 33117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1884
                },
                {
                    "start": 1887,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2192
                },
                {
                    "start": 2195,
                    "end": 2313
                },
                {
                    "start": 2314,
                    "end": 2402
                }
            ],
            "ref_mentions": [
                {
                    "start": 817,
                    "end": 830,
                    "matchedPaperCorpusId": "117580307"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20263671875
        },
        {
            "corpus_id": "276259394",
            "title": "Does Training on Synthetic Data Make Models Less Robust?",
            "text": "Constructing a dataset for a specific task in natural language processing can be costly in terms of time and labor. An increasingly common approach to solve this problem is to take advantage of large language models (LLMs) to generate training data. It's simple to fine-tune an LLM or just use in-context learning to generate huge amounts of training data with a relatively small number of demonstrations. However, how effective the model-written datasets are for different tasks is still an open question. \n\nModel-generated training data is widely used in different domains like image classification (Besnier et al., 2020;Gowal et al., 2021), visual language concepts understanding (Cascante-Bonilla et al., 2023) and medical image understanding (Fernandez et al., 2022). In many NLP tasks, such as commonsense reasoning (Yang et al., 2020), questionanswering (Bartolo et al., 2021;Paranjape et al., 2022), sycophancy reduction (Wei et al., 2024), cultural debiasing (Li et al., 2024a,b) and general instruction alignment (Wang et al., 2023), synthetic data created with generative models are utilized in model training. In cases where there are limited sources for model training, synthetic data would greatly benefit the performance of finetuned model. High-quality model-written datasets may also be used for evaluations. Perez et al. (2023) created 154 evaluation datasets and discovered inverse scaling of language models in some scenarios. \n\nHowever, synthetic data may also be harmful. Shumailov et al. (2024) found that language models may collapse if recursively finetuned with generated text. Such degradation has also been discovered in image generation tasks (Alemohammad et al., 2024). The use of synthetic data is also criticized from the perspective of ethics and social impact (Susser and Seeman, 2024). There's a series of research about what bias is manifested in synthetic data and how the performance in specific tasks is affected.",
            "score": 0.4689470326762965,
            "section_title": "Introduction and Related Work",
            "char_start_offset": 32,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1446
                },
                {
                    "start": 1449,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1952
                }
            ],
            "ref_mentions": [
                {
                    "start": 601,
                    "end": 623,
                    "matchedPaperCorpusId": "207847307"
                },
                {
                    "start": 623,
                    "end": 642,
                    "matchedPaperCorpusId": "239016201"
                },
                {
                    "start": 683,
                    "end": 714,
                    "matchedPaperCorpusId": "257833837"
                },
                {
                    "start": 747,
                    "end": 771,
                    "matchedPaperCorpusId": "252368079"
                },
                {
                    "start": 822,
                    "end": 841,
                    "matchedPaperCorpusId": "250390686"
                },
                {
                    "start": 861,
                    "end": 883,
                    "matchedPaperCorpusId": "233296924"
                },
                {
                    "start": 883,
                    "end": 906,
                    "matchedPaperCorpusId": "238856938"
                },
                {
                    "start": 1023,
                    "end": 1042,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 1326,
                    "end": 1345,
                    "matchedPaperCorpusId": "254854519"
                },
                {
                    "start": 1494,
                    "end": 1517,
                    "matchedPaperCorpusId": "271448069"
                },
                {
                    "start": 1672,
                    "end": 1698,
                    "matchedPaperCorpusId": "259341801"
                },
                {
                    "start": 1794,
                    "end": 1818,
                    "matchedPaperCorpusId": "274589037"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70068359375
        },
        {
            "corpus_id": "258833579",
            "title": "Abstract Meaning Representation-Based Logic-Driven Data Augmentation for Logical Reasoning",
            "text": "ReClor (Yu et al., 2020) and LogiQA (Liu et al., 2021) are two challenging logical reasoning datasets. ReClor is collected from the Graduate Management Admission Test (GMAT) and Law School Admission Test (LSAT). LogiQA is collected from the National Civil Service Examination (Liu et al., 2021). Additionally, we performed evaluation on five datasets for natural language inference and textual entailment tasks: MNLI (Williams et al., 2018), RTE (Wang et al., 2018), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), and QQP (Wang et al., 2018). MNLI, RTE, and MRPC assess the relationship between two sentences, while QNLI focuses on the relationship between a question and a sentence, and QQP evaluates the relationship between two questions. \n\nSynthetic Data for Contrastive Learning In this paper, we performed contrastive learning for discriminative large language models on sentences augmented from a synthetic dataset. This dataset contains 14,962 sentences with different combinations of 23 entities, 2 relations and 40 attributes. Synthetic data was used to generate more controllable logical sentences. More details about the synthetic dataset can be found in the Appendix Section E.",
            "score": 0.46842754747492155,
            "section_title": "Datasets",
            "char_start_offset": 16281,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1207
                }
            ],
            "ref_mentions": [
                {
                    "start": 7,
                    "end": 24,
                    "matchedPaperCorpusId": "209485573"
                },
                {
                    "start": 36,
                    "end": 54,
                    "matchedPaperCorpusId": "220483148"
                },
                {
                    "start": 276,
                    "end": 294,
                    "matchedPaperCorpusId": "220483148"
                },
                {
                    "start": 417,
                    "end": 440,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 446,
                    "end": 465,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 472,
                    "end": 498,
                    "matchedPaperCorpusId": "16639476"
                },
                {
                    "start": 505,
                    "end": 529,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 539,
                    "end": 558,
                    "matchedPaperCorpusId": "5034059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "268363344",
            "title": "Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs",
            "text": "Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense inferences for contexts and questions involving interactions between complex events. To address this demand, we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions. Our experiments show that language models trained on COM2 exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for question answering and generative commonsense reasoning, without expensive human annotations. Code and data are available at https://github.com/tqfang/complex-commonsense-reasoning.",
            "score": 0.46840855973188517,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81787109375
        },
        {
            "corpus_id": "211171653",
            "title": "VQA-LOL: Visual Question Answering under the Lens of Logic",
            "text": "Logic in Human Expression Is logical thinking a natural feature of human expression and thought? Evidence in psychological studies [8,13] suggests that children and even infants are capable of logical reasoning by using intuitive theories, abduction, and by understanding objecthood and causes. More recently [9] have shown that children understand logical operations such as negation, disjunction, and conjunction in natural language and are able to compositionally compute meanings even in complex sentences containing multiple logical operators. Children are also able to use these meanings to assign truth values to complex experimental tasks. Given this, question-answering systems also need to answer compositional questions, and be robust to the manifestation of logical operators in natural language. \n\nLogic in Natural Language Understanding In questionanswering, the task of understanding compositionality in questions can also be interpreted as understanding logical connectives in text. While question compositionality is largely unstudied as far as we know, traditional approaches in natural language understanding seek to transform sentences into symbolic formats such as first-order logic [44], relational tables [25], or a combination of semantic and symbolic logic [22] for reasoning. While such methods benefit from interpretability they suffer from practical limitations like intractability, reliance on logical background knowledge, difficulty of scaling up inference, and failure to process noise and uncertainty. [6,32,35] suggest that better generalization can be achieved by learning embeddings in order to reason about semantic relations, and to simulate the behavior of first-order logic [33]. \n\nIt has been shown that recursive neural networks can learn logical semantics on synthetic English-like sentences [7] to reason about conjunctions of relations nonatomically by using embeddings [27]. Models that learn logical rules for reasoning in a differentiable manner have been introduced in [41,12]. Detection of negation in text has been studied for information extraction and sentiment analysis [26]. Recently [19] have shown that BERT-based models are incapable of differentiating between negated and un-negated sentences. Since our work deals with both vision and language inputs, it comes with uncertainty and ambiguity, thus calling for question-answering system that is robust to logical constraints or transformations. \n\nVisual Question Answering Question answering has been regarded as a strong demonstration of natural language understanding and text comprehension [21].",
            "score": 0.4683558740774916,
            "section_title": "Related Work",
            "char_start_offset": 5319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 808
                },
                {
                    "start": 811,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1719
                },
                {
                    "start": 1722,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2129
                },
                {
                    "start": 2130,
                    "end": 2252
                },
                {
                    "start": 2253,
                    "end": 2453
                },
                {
                    "start": 2456,
                    "end": 2607
                }
            ],
            "ref_mentions": [
                {
                    "start": 309,
                    "end": 312,
                    "matchedPaperCorpusId": "4556163"
                },
                {
                    "start": 1228,
                    "end": 1232,
                    "matchedPaperCorpusId": "10910955"
                },
                {
                    "start": 1282,
                    "end": 1286,
                    "matchedPaperCorpusId": "2816192"
                },
                {
                    "start": 1535,
                    "end": 1538,
                    "matchedPaperCorpusId": "14941970"
                },
                {
                    "start": 1538,
                    "end": 1541,
                    "matchedPaperCorpusId": "2687019"
                },
                {
                    "start": 1541,
                    "end": 1544,
                    "matchedPaperCorpusId": "8429835"
                },
                {
                    "start": 2018,
                    "end": 2022,
                    "matchedPaperCorpusId": "4502993"
                },
                {
                    "start": 2022,
                    "end": 2025,
                    "matchedPaperCorpusId": "9885405"
                },
                {
                    "start": 2124,
                    "end": 2128,
                    "matchedPaperCorpusId": "10539461"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3359375
        },
        {
            "corpus_id": "270523129",
            "title": "ChatLogic: Integrating Logic Programming with Large Language Models for Multi-Step Reasoning",
            "text": "Our work introduces ChatLogic, a framework that augments LLMs with a logical reasoning engine to enhance their inferential capabilities.We have innovatively implemented a 'Mixshot Chain of Thought' technique in this framework.This approach significantly enhances the performance of LLMs by combining various prompt engineering methods.Mix-shot CoT efficiently guides the model through logical reasoning steps, achieving improved problem-solving with minimal resource consumption.ChatLogic is designed to be compatible with existing LLMs, significantly increasing their accuracy, especially in high-precision scenarios.The framework orchestrates the functioning of an LLM, enabling it to efficiently generate responses across various tasks.\n\nAt the heart of ChatLogic is the transformation of natural language into logical symbols, a process executed through pyDatalog.The primary objective of ChatLogic is to reinforce the stability of the reasoning process, ensuring that LLMs can handle intricate reasoning tasks with enhanced reliability and precision.The main characteristics of our framework are summarized below:\n\n\u2022 ChatLogic, by combining LLMs with pyDatalog, translates natural language queries into logic programs, enhancing inference accuracy.This improvement is notably evident in multi-step reasoning tasks, as demonstrated on datasets such as PARARULE-Plus",
            "score": 0.4681640811860445,
            "section_title": "arXiv:2407.10162v1 [cs.AI] 14 Jul 2024",
            "char_start_offset": 2300,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 226
                },
                {
                    "start": 226,
                    "end": 335
                },
                {
                    "start": 335,
                    "end": 479
                },
                {
                    "start": 479,
                    "end": 618
                },
                {
                    "start": 618,
                    "end": 739
                },
                {
                    "start": 741,
                    "end": 868
                },
                {
                    "start": 868,
                    "end": 1055
                },
                {
                    "start": 1055,
                    "end": 1118
                },
                {
                    "start": 1120,
                    "end": 1253
                },
                {
                    "start": 1253,
                    "end": 1369
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32470703125
        },
        {
            "corpus_id": "264128006",
            "title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
            "text": "Large Language Models (LLMs) [50,67], especially reasoning language models [18,51] demonstrate advanced capabilities in complex reasoning tasks and show significant adaptability and versatility across various applications, from simple everyday tasks to specialized domains such as coding, mathematics, law, medicine, and finance [11,22,34,37,76]. Quantitative evaluation of LLM reasoning has thus become a very important task. To this end, existing work has considered mathematical reasoning [15,26], algorithmic problem solving [9,58], and knowledge-driven reasoning [25,73]. \n\nLogical reasoning is a cornerstone of human intelligence and has been a central focus in artificial intelligence research since its inception [16,29,33]. However, evaluating verbal reasoning turned out to be too difficult in the 1950s due to insufficient natural language understanding (NLU) technologies, and thus AI researchers focused on formal logical reasoning instead [29,48,49]. Since the 2010s, NLU has witnessed huge advances, where reading comprehension [8,21] and natural language inference [7,74] tasks were solved with high accuracies, which made verbal reasoning evaluation feasible [43,80]. Figure 1 illustrates an example of logical reasoning in reading comprehension. To address such questions, LLMs must engage in multi-step, algorithmic, symbolic reasoning. This makes logical reasoning an ideal testbed for evaluating LLMs' ability to process complex natural language information accurately, robustly, and logically. \n\nTo this end, we introduce the General Logical Reasoning Evaluation (GLoRE) benchmark, designed to assess instruction-tuned LLMs on various logical reasoning tasks. GLoRE evaluates the strengths and limitations of LLMs in this domain, similar to how GLUE [71] and Super-GLUE [70] benchmark natural language understanding. GLoRE includes three types of logical reasoning tasks: Multi-choice Reading Comprehension [35], Natural Language Inference (NLI) [17], and True-or-False (Yes-or-No) Questions [13]. These tasks encompass a wide range of logical reasoning phenomena, with high-quality datasets that remain challenging for pre-trained language models [13,27,32]. In total, GLoRE covers 12 datasets with 72,848 instances.",
            "score": 0.46809927794147926,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1515
                },
                {
                    "start": 1518,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2239
                }
            ],
            "ref_mentions": [
                {
                    "start": 329,
                    "end": 333,
                    "matchedPaperCorpusId": "256409866"
                },
                {
                    "start": 568,
                    "end": 572,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 572,
                    "end": 575,
                    "matchedPaperCorpusId": "270210486"
                },
                {
                    "start": 957,
                    "end": 960,
                    "matchedPaperCorpusId": "2823033"
                },
                {
                    "start": 960,
                    "end": 963,
                    "matchedPaperCorpusId": "19454866"
                },
                {
                    "start": 1043,
                    "end": 1046,
                    "matchedPaperCorpusId": "6360322"
                },
                {
                    "start": 1046,
                    "end": 1049,
                    "matchedPaperCorpusId": "67855846"
                },
                {
                    "start": 1081,
                    "end": 1084,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 1084,
                    "end": 1087,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 1180,
                    "end": 1183,
                    "matchedPaperCorpusId": "209485573"
                },
                {
                    "start": 1929,
                    "end": 1933,
                    "matchedPaperCorpusId": "6826032"
                },
                {
                    "start": 2177,
                    "end": 2180,
                    "matchedPaperCorpusId": "238354171"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.488037109375
        },
        {
            "corpus_id": "272826678",
            "title": "Towards Efficient and Robust VQA-NLE Data Generation with Large Vision-Language Models",
            "text": "Explanation Generation in VQA Several studies have emphasized the generation of explanations, either manually or automatically. Manual approaches, such as VCR (Zellers et al., 2019b) and e-SNLI-VE (Do et al., 2021), employ human annotators to derive explanations from existing VQA datasets. In contrast, automatic methods like GQA-REX (Chen and Zhao, 2022) utilize functional programming, while VQA-E (Li et al., 2018) aligns and merges constituency parse trees from QA-caption pairs. Another automatic explanation generation framework, VQA-X (Park et al., 2018), employs separate answer and explanation models for generation. In this paper, we introduce a novel approach by proposing a unified model using single LVLM, eliminating multiple architectures need. \n\nNeural Synthetic Data Generation In the realm of multi-modal learning, particularly in the visionlanguage domain, the potential of synthetic data generation has been extensively explored. Li et al. (2023) discuss the application of synthetic data across various tasks and modalities. In computer vision, GAN-based models (Karras et al., 2019) and diffusion-based approaches (Nichol et al., 2022) are utilized for image synthesis. Within natural language processing domain, several studies (Kumar et al., 2021;Chung et al., 2023;Schmidt et al., 2024)",
            "score": 0.4680714851228165,
            "section_title": "Related Work",
            "char_start_offset": 19158,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1312
                }
            ],
            "ref_mentions": [
                {
                    "start": 1084,
                    "end": 1105,
                    "matchedPaperCorpusId": "234338081"
                },
                {
                    "start": 1272,
                    "end": 1291,
                    "matchedPaperCorpusId": "259096160"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8515625
        },
        {
            "corpus_id": "269282799",
            "title": "Look Before You Decide: Prompting Active Deduction of MLLMs for Assumptive Reasoning",
            "text": "To effectively train QwenAD-Series with Active Deduction capability, we construct a comprehensive training corpus that covers both complex reasoning tasks requiring step-by-step deduction and direct visual questions solvable with minimal reasoning. The training data comprises two major components: assumptive reasoning samples from MARS-Bench and supplementary data from existing vision-language datasets. Assumptive Reasoning Data. We select 704 assumptive questions from MARS-Bench as core samples of complex reasoning. GPT-4V is used to generate structured annotations that decompose each question into sequential reasoning steps. These structured annotations are then reformulated into natural language reasoning paths using an ensemble of state-of-the-art language models  Our method consists of AD-SFT and AD-RFT processes. Both of them adopt a divide-and-conquer strategy to process simple and complex questions independently. We also provide illustrations for both format and accuracy rewards in our AD method for intuition. \n\nA conversation between User and Assistant. \n\nThe user asks a question, and the assistant solves it. If the assistant determines that the question requires multi-step reasoning or extra thinking steps, the assistant generates a <think> tag, followed by the reasoning process enclosed within <think> </think> tags, and then provides the answer within <answer> </answer> tags, i.e., <think> reasoning process here </think> <answer> answer here </answer>. If the question is simple and does not require additional reasoning, the assistant directly provides the answer within <answer> </answer> tags, i.e., <answer> answer here </answer>. User: [prompt]. Assistant: \n\nTable 1: Template of the system prompt for executing our Active Deduction method. \n\n[prompt] will be replaced with the specific reasoning question during training. \n\n(GPT-4.5, GPT-4o, Claude-3.5-sonnet, and DeepSeek-V3). This process yields 2,816 diverse reasoning samples with validated reasoning paths enclosed in <think> tags and corresponding answers in <answer> tags. \n\nSupplementary Training Data.",
            "score": 0.4680600198523589,
            "section_title": "Training Data Construction",
            "char_start_offset": 14558,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1033
                },
                {
                    "start": 1036,
                    "end": 1078
                },
                {
                    "start": 1081,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1696
                },
                {
                    "start": 1699,
                    "end": 1780
                },
                {
                    "start": 1783,
                    "end": 1862
                },
                {
                    "start": 1865,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2071
                },
                {
                    "start": 2074,
                    "end": 2102
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3427734375
        },
        {
            "corpus_id": "256416127",
            "title": "Faithful Chain-of-Thought Reasoning",
            "text": "., 2022), and \"Let's think step by step\" (Kojima et al., 2022), are all examples of this kind. Ensemble-based prompting is designed to overcome the local optimality issue of the one-shot generation in previous methods by sampling multiple (C, A) pairs and choosing the best answer via strategies like majority voting. Examples include Self-Consistent CoT (Wang et al., 2022), Minerva (Lewkowycz et al., 2022), and DIVERSE , which differ mainly in the voting granularity and the underlying LM. Modularized methods break down Q into subproblems and then conquer them individually (Jung et al., 2022;Qian et al., 2022, i.a.). In particular, Least-2 Note that this differs from the notion of faithfulness in the Natural Language Generation (NLG) literature, primarily in what constitutes the ground truth. In interpretability, we talk about the faithfulness of an explanation w.r.t. the model's underlying reasoning mechanism -the ground truth is usually opaque. In NLG, we talk about the faithfulness of the generated text (e.g., a translated sentence, or a summary) w.r.t. some explicit source (e.g., the source sentence, or the full document) -the ground truth is transparent.  to-Most prompting (Zhou et al., 2022) uses an LM to first reduce the question to subquestions and then sequentially answer them. However, the reasoning chain is entirely in NL, so there is still no faithfulness guarantee that the answer follows from it.\n\nWe note that our work is concurrent with Chen et al. (2022) and Gao et al. (2022), both generating the reasoning chain in Python code and calling a Python interpreter to derive the answer. While we do not compare with them empirically since they are not yet published, we do want to highlight the following differences: (a) We demonstrate the generalizability of our approach to multiple symbolic languages beyond Python and multiple domains beyond arithmetic reasoning and simple symbolic reasoning. (b) In particular, we innovatively recast a diverse set of realistic tasks (Planning, Multi-hop QA, and Logical Inference)",
            "score": 0.46767486270255376,
            "section_title": "Related Work",
            "char_start_offset": 6145,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43115234375
        },
        {
            "corpus_id": "267759981",
            "title": "Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models",
            "text": "We propose GLAN, a general and scalable method for synthesizing instruction data. Experiments show GLAN can help large language models improve their capabilities in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following. Currently, our synthetic data is based on the taxonomy of human knowledge and capabilities and there are other types of useful data not been covered. We are interested to design methods with boarder coverage. Our current instruction data are mostly question answer pairs and in the next step, we plan to generate synthetic data of multi-turn conversations and long documents.",
            "score": 0.4675020670149115,
            "section_title": "Conclusions",
            "char_start_offset": 28410,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 665
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94140625
        },
        {
            "corpus_id": "273345869",
            "title": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning",
            "text": "Logical Reasoning. Logical reasoning tasks require models to handle complex logical structures (Cummins et al., 1991). Traditional methods include rule-based (Robinson, 1965) and neural-based approaches (Amayuelas et al., 2022;Gerasimova et al., 2023) for interpreting symbolic representations. Recent advancements integrate LLMs into symbolic reasoning, introducing frameworks like Logic-LM (Pan et al., 2023), SAT-LM (Ye et al., 2024) and LeanReasoner (Jiang et al., 2024a). These frameworks use LLMs to convert natural language into symbolic syntax processed by external reasoning tools, enhancing performance through self-consistency and non-linear reasoning (Wang et al., 2023;Zhang et al., 2022). However, they often rely on external tools, assuming LLMs cannot parse symbolic expressions as reliably as rule-based reasoners. \n\nSymbolic Chain-of-Thought Prompting. Symbolic CoT prompting (Lyu et al., 2023) combines natural language (NL) and symbolic language (SL) in the reasoning chain. NL decomposes complex queries into subproblems, and SL programs (e.g., Python, LEAN) handle each subproblem. A deterministic solver executes the SL to derive the final answer, ensuring faithfulness while NL aids interpretability. Recent efforts aim to reduce reliance on SL programs by leveraging LLMs (Xu et al., 2024), but these methods focus primarily on logical reasoning rather than complex mathematical tasks and still require verifiers to ensure accuracy.",
            "score": 0.4671618403370854,
            "section_title": "Related Work",
            "char_start_offset": 19894,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1457
                }
            ],
            "ref_mentions": [
                {
                    "start": 95,
                    "end": 117,
                    "matchedPaperCorpusId": "23590418"
                },
                {
                    "start": 158,
                    "end": 174,
                    "matchedPaperCorpusId": "14389185"
                },
                {
                    "start": 203,
                    "end": 227,
                    "matchedPaperCorpusId": "251649222"
                },
                {
                    "start": 227,
                    "end": 251,
                    "matchedPaperCorpusId": "260915000"
                },
                {
                    "start": 419,
                    "end": 436,
                    "matchedPaperCorpusId": "258715073"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1702880859375
        },
        {
            "corpus_id": "276250232",
            "title": "VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data",
            "text": "Reasoning Data with Labels \n\nIn order to obtain step-wise reasoning data for non-Math domains, we devise a pipeline, as outlined in Figure 2, to generate synthetic reasoning CoTs from existing questionanswering data. These CoTs are then given step-wise labels based on reasoning correctness. We detail the synthetic data generation process in Sections 5.1 and 5.2, including methods to create and annotate reasoning steps. We also provide additional analysis on the quality of the generation pipeline in Section 5.3.",
            "score": 0.46644915669159726,
            "section_title": "Automatic Generation of Multi-Domain",
            "char_start_offset": 14358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 29,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 516
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90185546875
        },
        {
            "corpus_id": "222141025",
            "title": "PRover: Proof Generation for Interpretable Reasoning over Rules",
            "text": "The rule reasoning task is related to reasoning tasks that have been proposed recently. These include tasks in the bAbI dataset (Weston et al., 2015), synthetically generated probe tasks (Richardson et al., 2020) or reading comprehension tasks in datasets such as QuaRTz (Tafjord et al., 2019) and ROPES (Lin et al., 2019). Unlike our task, most of these require reasoning over implicit rules, the focus being on language understanding and one step of rule application. Multi-hop QA datasets like HotpotQA (Yang et al., 2018) require multiple reasoning steps, but the inference rules needed are again implicitly inferred, rather than explicitly provided. Our task also bears similarity with Natural Language Inference (MacCartney and Manning, 2014), but NLI also allows unsupported inferences by filling gaps in explicitly stated knowledge (Dagan et al., 2013). \n\nFormal Reasoning and Neural Theorem Proving: Semantic parsing (Zettlemoyer and Collins, 2005;Berant et al., 2013;Berant and Liang, 2014) of multi-sentence texts into logical forms has proved to be challenging, restricting the application of semantic parsers to formal reasoning systems (Kamath and Das, 2019). PROVER bypasses this expensive and error-prone process and attempts to solve the problem in an end-to-end manner, without any intermediate logical representations. \n\nOur approach is conceptually similar to a body of work on Neural Theorem Proving (Weber et al., 2019) that has focused on developing theorem provers by combining reasoning from symbolic techniques with the possibility of differentiable learning from neural networks. These include neuro-symbolic methods for table comprehension (Neelakantan et al., 2016), executing basic compositional programs (Reed and de Freitas, 2016), SAT solving (Selsam et al., 2019), formula embedding (Abdelaziz et al., 2020), approximate (DNF) model counting (Abboud et al., 2020), etc. However, PROVER diverges from these in working with free-form natural language input to generate proofs similar to formal reasoners.",
            "score": 0.46552831481017015,
            "section_title": "QA and NLI:",
            "char_start_offset": 3578,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1337
                },
                {
                    "start": 1340,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2036
                }
            ],
            "ref_mentions": [
                {
                    "start": 187,
                    "end": 212,
                    "matchedPaperCorpusId": "202583828"
                },
                {
                    "start": 271,
                    "end": 293,
                    "matchedPaperCorpusId": "202539540"
                },
                {
                    "start": 304,
                    "end": 322,
                    "matchedPaperCorpusId": "201058633"
                },
                {
                    "start": 506,
                    "end": 525,
                    "matchedPaperCorpusId": "52822214"
                },
                {
                    "start": 926,
                    "end": 957,
                    "matchedPaperCorpusId": "449252"
                },
                {
                    "start": 957,
                    "end": 977,
                    "matchedPaperCorpusId": "6401679"
                },
                {
                    "start": 977,
                    "end": 1000,
                    "matchedPaperCorpusId": "1336493"
                },
                {
                    "start": 1150,
                    "end": 1172,
                    "matchedPaperCorpusId": "54181011"
                },
                {
                    "start": 1421,
                    "end": 1441,
                    "matchedPaperCorpusId": "189898046"
                },
                {
                    "start": 1668,
                    "end": 1694,
                    "matchedPaperCorpusId": "6715185"
                },
                {
                    "start": 1735,
                    "end": 1762,
                    "matchedPaperCorpusId": "7034786"
                },
                {
                    "start": 1776,
                    "end": 1797,
                    "matchedPaperCorpusId": "3632319"
                },
                {
                    "start": 1876,
                    "end": 1897,
                    "matchedPaperCorpusId": "102354917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.343994140625
        },
        {
            "corpus_id": "268554107",
            "title": "ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting",
            "text": "Large Language Models (LLMs) (Zhao et al., 2023) have recently made great progress on natural language understanding and generation, and have also shown great potential as general-purpose task solvers by following commands or prompts (Brown et al., 2020;Nakano et al., 2021;OpenAI, 2023).However, it is well known that LLMs often fall short in complex reasoning tasks, e.g., mathematical reasoning and symbolic reasoning.Thus concerns about improving the reasoning capabilities of LLMs have drawn significant attention from the research community (Luo et al., 2023;Pan et al., 2023).\n\nAmidst this backdrop, Chain-of-Thought (CoT) prompting has been proposed and emerged as an effective solution for complex reasoning (Wei et al., 2022), where LLMs incorporate a series of intermediate reasoning steps before inferring the final output.Kojima et al. (2022) simply add a phrase \"Let's think step by step\" in prompts, enabling LLMs to conduct zero-shot CoT reasoning without any additional exemplars.However, the same benefits may not necessarily extend to relatively smaller LLMs.Although there have been some attempts towards fine-tuning LLMs with multi-step CoT prompting data to stimulate the step-by-step reasoning capacity in smaller models (Shridhar et al., 2022;Fu et al., 2023a;Kim et al., 2023), these approaches are *Corresponding author mainly focused on single reasoning tasks or rely on simple strategies to synthesize CoT prompting data.Fu et al. (2023b) found that existing datasets mostly focus on simple tasks involving fewer reasoning steps (i.e., only 2 or 3 steps) or even omit certain intermediate steps, making the reasoning process incomplete and the model struggles in complex reasoning tasks.In addition, these automatic methods ignore considering the consistency between the reasoning process and the final answer (Wang et al., 2023b), resulting in low-quality and spurious CoT prompting data.Considering these issues, we aim to improve upon existing CoT prompting data for enhancing the complex reasoning capacities of LLMs.",
            "score": 0.4655136515080948,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 288
                },
                {
                    "start": 288,
                    "end": 421
                },
                {
                    "start": 421,
                    "end": 583
                },
                {
                    "start": 585,
                    "end": 835
                },
                {
                    "start": 835,
                    "end": 997
                },
                {
                    "start": 997,
                    "end": 1078
                },
                {
                    "start": 1078,
                    "end": 1449
                },
                {
                    "start": 1449,
                    "end": 1715
                },
                {
                    "start": 1715,
                    "end": 1917
                },
                {
                    "start": 1917,
                    "end": 2049
                }
            ],
            "ref_mentions": [
                {
                    "start": 717,
                    "end": 735,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 835,
                    "end": 855,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 1267,
                    "end": 1284,
                    "matchedPaperCorpusId": "256390607"
                },
                {
                    "start": 1449,
                    "end": 1466,
                    "matchedPaperCorpusId": "252683303"
                },
                {
                    "start": 1838,
                    "end": 1858,
                    "matchedPaperCorpusId": "247595263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.373291015625
        },
        {
            "corpus_id": "270067866",
            "title": "Arithmetic Reasoning with LLM: Prolog Generation & Permutation",
            "text": "Arithmetic Reasoning The Chain-of-Thought (CoT) prompting approach (Wei et al., 2022b) first proposes to prompt the model to generate the reasoning chain step-by-step to reach the final answer.Afterwards, advancements have been made in LLMs' reasoning capacity via step-by-step methods (Zhou et al., 2023;Zhu et al., 2023;Huang et al., 2022;Liang et al., 2023).However, the natural language generation still performs poorly on complex or multi-step reasoning.Therefore, one trajectory of efforts has been made to leverage reasoning structures like trees (Yao et al., 2023;Long, 2023) and graphs (Besta et al., 2023;Zhang et al., 2023).Another trajectory is to render the reasoning task based on external tools (Cobbe et al., 2021;Mishra et al., 2023;Gou et al., 2023;Gao et al., 2023;Shao et al., 2023;Chen et al., 2023), which is the one that we are following.Besides, Yuan et al.'s (2023) RFT method shares the idea of dataset augmentation, but they compile rejection samples from multiple models to form an augmented training set, which is different from PROPER's automatic permutation.\n\nNeural Symbolic Reasoning Neural symbolic reasoning (Andreas et al., 2016;Neelakantan et al., 2017;Hudson and Manning, 2019;Gupta et al., 2020;Nye et al., 2021) aims to leverage both neural networks and symbolic reasoning to obtain better reasoning abilities and transparency.Those methods suffer from low scalability of learning and reasoning components.LLMs are hence adopted to generate symbolic representations from natural language (Lyu et al., 2023;Pan et al., 2023;Yang et al., 2023), where deterministic symbolic solvers will process the query and symbolic representations generated by LLMs to conduct reasoning or proofs.Prolog has been a popular candidate for the format of symbolic representations.We are posited on this trajectory and in the specific field of arithmetic reasoning.",
            "score": 0.4651673328374557,
            "section_title": "Related Work",
            "char_start_offset": 11982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 361
                },
                {
                    "start": 361,
                    "end": 459
                },
                {
                    "start": 459,
                    "end": 635
                },
                {
                    "start": 635,
                    "end": 861
                },
                {
                    "start": 861,
                    "end": 1089
                },
                {
                    "start": 1091,
                    "end": 1367
                },
                {
                    "start": 1367,
                    "end": 1446
                },
                {
                    "start": 1446,
                    "end": 1721
                },
                {
                    "start": 1721,
                    "end": 1800
                },
                {
                    "start": 1800,
                    "end": 1884
                }
            ],
            "ref_mentions": [
                {
                    "start": 67,
                    "end": 86,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 305,
                    "end": 322,
                    "matchedPaperCorpusId": "253224009"
                },
                {
                    "start": 784,
                    "end": 802,
                    "matchedPaperCorpusId": "256459681"
                },
                {
                    "start": 1143,
                    "end": 1165,
                    "matchedPaperCorpusId": "5276660"
                },
                {
                    "start": 1165,
                    "end": 1190,
                    "matchedPaperCorpusId": "9864100"
                },
                {
                    "start": 1190,
                    "end": 1215,
                    "matchedPaperCorpusId": "195847902"
                },
                {
                    "start": 1215,
                    "end": 1234,
                    "matchedPaperCorpusId": "209202200"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1181640625
        },
        {
            "corpus_id": "255125039",
            "title": "Improving Complex Knowledge Base Question Answering via Question-to-Action and Question-to-Question Alignment",
            "text": "Complex knowledge base question answering (CQA) aims to answer various natural language questions with a large-scale knowledge graph. Compared to simple questions with single or multihop of relations, complex questions have more kinds of answer types such as numeric or boolean types and require more kinds of aggregation operations like min/max or intersection/union to yield answers. Semantic parsing approaches typically map questions to intermediate logical forms such as query graphs (Yih et al., 2015;Bao et al., 2016;Bhutani et al., 2019;Maheshwari et al., 2019;Lan and Jiang, 2020;Qin et al., 2021), and further transform them into queries like SPARQL query language. Recently, many works (Liang et al., 2017;Saha et al., 2019;Ansari et al., 2019;Hua et al., 2020a,b,c) predefine a collection of functions with constrained argument types and represent the intermediate logical form as a sequence of actions that can be generated using a seq2seq model. Sequencebased methods are natural to accomplish more complex operations by simply expanding the function set, thus making some logically complex questions answerable while they're difficult to answer using query graphs. \n\nThe seq2seq model has been widely used and achieved good results on many text generation tasks, such as machine translation, text summarization and style transfer. In these tasks, the source and the target sequence are both natural language texts and thus share some low-level features. However, semantic parsing aims to transform unstructured texts into structured logical forms, which requires a difficult alignment between them. This problem becomes more serious when the complexity of the question rises. Some works propose to solve this problem by modelling the hierarchical structure of logical forms. Dong and Lapata (2016) introduces a sequence-to-tree model with an attention mechanism. Dong and Lapata (2018) proposes to decode a sketch of the logical forms which contain a set of functions at first and then decode low-level details like arguments. Guo et al. (2021) iteratively segments a span from the question by a segmentation model and parses it using a base parser until the whole query is parsed.",
            "score": 0.46514918285930007,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 507,
                    "end": 524,
                    "matchedPaperCorpusId": "18549358"
                },
                {
                    "start": 524,
                    "end": 545,
                    "matchedPaperCorpusId": "207757236"
                },
                {
                    "start": 545,
                    "end": 569,
                    "matchedPaperCorpusId": "53220882"
                },
                {
                    "start": 589,
                    "end": 606,
                    "matchedPaperCorpusId": "243865277"
                },
                {
                    "start": 697,
                    "end": 717,
                    "matchedPaperCorpusId": "2742513"
                },
                {
                    "start": 717,
                    "end": 735,
                    "matchedPaperCorpusId": "139100939"
                },
                {
                    "start": 1790,
                    "end": 1812,
                    "matchedPaperCorpusId": "15412473"
                },
                {
                    "start": 1878,
                    "end": 1900,
                    "matchedPaperCorpusId": "44167998"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1650390625
        },
        {
            "corpus_id": "249062828",
            "title": "RobustLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners",
            "text": "Reasoning in natural language has been a prevalent problem in NLP. There are multiple reasoning datasets, studying different aspects of reasoning over textual inputs. Natural Language Inference (NLI) (Dagan et al., 2006) is a prominent dataset that requires reasoning over text to answer if a statement is entailed, contradicted, or neutral given a hypothesis. HotpotQA (Yang et al., 2018b) tests multi-hop reasoning abilities that require comparisons and inferring missing bridge between sentences. CLUTRR (Sinha et al., 2019) tests whether models can infer biological relationships between entities in a context. RICA (Zhou et al., 2021) requires the model to employ commonsense reasoning to answer questions based on a context. Recently, there has been an increasing focus on evaluating the logical reasoning abilities of LMs. ReClor  and LogiQA (Liu et al., 2021) are logical reasoning datasets derived from examinations. RuleTaker (Clark et al., 2020) proposes synthetic deductive reasoning datasets that uses only the knowledge in the context. There are very limited works that probe the logical reasoning abilities of language models (LMs). FaiRR (Sanyal et al., 2022) tests the robustness of logical reasoning models when the subjects and attributes in the context are altered to out-of-distribution terms. In a contemporary work, Zhang et al. (2022) show that language models can learn to use statistical features that can be present in deductive reasoning datasets. To the best of our knowledge, ROBUSTLR is the first dataset that tests how robust these LMs are to different logical perturbations.",
            "score": 0.4646424261064696,
            "section_title": "Related Works",
            "char_start_offset": 28328,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 220,
                    "matchedPaperCorpusId": "8587959"
                },
                {
                    "start": 370,
                    "end": 390,
                    "matchedPaperCorpusId": "52822214"
                },
                {
                    "start": 620,
                    "end": 639,
                    "matchedPaperCorpusId": "221865854"
                },
                {
                    "start": 849,
                    "end": 867,
                    "matchedPaperCorpusId": "220483148"
                },
                {
                    "start": 936,
                    "end": 956,
                    "matchedPaperCorpusId": "211126663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.619140625
        },
        {
            "corpus_id": "270214666",
            "title": "EffiQA: Efficient Question-Answering with Strategic Multi-Model Collaboration on Knowledge Graphs",
            "text": "Large language models (LLMs) (Radford et al., 2019;Ouyang et al., 2022;Touvron et al., 2023) have shown impressive capabilities across various natural language processing tasks, generating coherent and context-sensitive responses that demonstrate deep linguistic insights (Wei et al., 2022). However, they struggle with complex, multistep reasoning tasks, including complex arithmetic (Wang et al., 2022), commonsense (Zhao et al., 2024), symbolic reasoning (Pan et al., 2023b), and multi-hop question answering (Yasunaga et al., 2021;Pan et al., 2024a). \n\nTo enhance the reasoning abilities of LLMs, techniques like chain-of-thought (CoT) prompting (Wei et al., 2022) have been developed, enabling stepby-step rationale generation before arriving at the final answer. Despite improved performance on various reasoning tasks (Yao et al., 2024;Besta et al., 2024;Turpin et al., 2024), CoT prompting sometimes fails to generate sufficient sub-questions to gather all necessary information, leading to issues like hallucinations (Lyu et al., 2023;Lin et al., 2021), opaque reasoning (Suzgun et al., 2022), and reliance on outdated data (Borgeaud et al., 2022;Izacard et al., 2023). \n\nTo address these challenges, researchers have explored integrating external knowledge sources, such as knowledge graphs (KGs) (Sun et al., 2021;Yasunaga et al., 2021;Zhang et al., 2022a;Li et al., 2023b;Pan et al., 2024a), into the reasoning process. These methods typically involve retrieving information from KGs, augmenting the prompt, and feeding it into LLMs. However, these loosecoupling paradigms often rely on simple data retrieval, failing to harness the full reasoning potential of LLMs. Consequently, their success hinges on the completeness and quality of the KGs.",
            "score": 0.46458689611813997,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 1178
                },
                {
                    "start": 1181,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1757
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 51,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 51,
                    "end": 71,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 272,
                    "end": 290,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 458,
                    "end": 477,
                    "matchedPaperCorpusId": "258833332"
                },
                {
                    "start": 535,
                    "end": 553,
                    "matchedPaperCorpusId": "258833332"
                },
                {
                    "start": 650,
                    "end": 668,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 825,
                    "end": 843,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 843,
                    "end": 862,
                    "matchedPaperCorpusId": "261030303"
                },
                {
                    "start": 862,
                    "end": 882,
                    "matchedPaperCorpusId": "258556812"
                },
                {
                    "start": 1133,
                    "end": 1156,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1156,
                    "end": 1177,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1384,
                    "end": 1402,
                    "matchedPaperCorpusId": "258833332"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2646484375
        },
        {
            "corpus_id": "270258505",
            "title": "Automating Turkish Educational Quiz Generation Using Large Language Models",
            "text": "Question generation (QG) is a crucial task in natural language processing that aims to automatically create a question from a given sentence or paragraph, often with the assistance of answer information when available.The challenge in QG lies in identifying the key statement within the context and generating a question based on that statement.This process can be categorized into answeraware QG, where the question is generated with knowledge of the answer, or answer-agnostic QG, where the question is generated without knowledge of the answer [4].QG and question answering (QA) are interconnected tasks [5] that require reasoning between questions and answers.As a result, datasets originally designed for QA tasks, such as SciQ, RACE, and FairytaleQA [6,7,8], are also utilized in QG research [9,10,11,12].Additionally, there are specific datasets created for QG, including LearningQ, KHANQ, and EduQG, which cover a variety of subjects and education levels [13,14,15].LearningQ contains questions crafted by instructors that demand reasoning but lack answers, limiting its application to answer-aware QG.KHANQ provides triples consisting of context, prompt, and question collected from questions about online courses, with the prompt offering background knowledge or presuppositions.EduQG is a high-quality multiplechoice question dataset linked to cognitive complexity, enhancing its realism.Early QG methods relied on rule matching, but advancements led to the adoption of Seq2Seq models with attention, linguistic feature integration, multi-modal models, multi-task learning, reinforcement learning, and the application of language models like BERT and GPT-3 [16,17,18,19,20,21,22,23,24,25].To incorporate answer information, researchers proposed encoding answers with context, utilizing answer positions, or using text summaries for answer-aware or answeragnostic QG [26,27,28].In educational contexts, certain considerations must be taken when applying QG.Controlling question difficulty is crucial for effective education, with methods proposed to assess difficulty based on answerability, inference steps needed, or learners' abilities [29,31].",
            "score": 0.46456890339395396,
            "section_title": "Related Work",
            "char_start_offset": 3312,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 218,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 551
                },
                {
                    "start": 551,
                    "end": 664
                },
                {
                    "start": 664,
                    "end": 811
                },
                {
                    "start": 811,
                    "end": 974
                },
                {
                    "start": 974,
                    "end": 1110
                },
                {
                    "start": 1110,
                    "end": 1289
                },
                {
                    "start": 1289,
                    "end": 1399
                },
                {
                    "start": 1399,
                    "end": 1700
                },
                {
                    "start": 1700,
                    "end": 1888
                },
                {
                    "start": 1888,
                    "end": 1967
                },
                {
                    "start": 1967,
                    "end": 2157
                }
            ],
            "ref_mentions": [
                {
                    "start": 801,
                    "end": 804,
                    "matchedPaperCorpusId": "228375814"
                },
                {
                    "start": 963,
                    "end": 967,
                    "matchedPaperCorpusId": "263881936"
                },
                {
                    "start": 967,
                    "end": 970,
                    "matchedPaperCorpusId": "252819220"
                },
                {
                    "start": 1668,
                    "end": 1672,
                    "matchedPaperCorpusId": "1536529"
                },
                {
                    "start": 1672,
                    "end": 1675,
                    "matchedPaperCorpusId": "20969045"
                },
                {
                    "start": 1678,
                    "end": 1681,
                    "matchedPaperCorpusId": "9745861"
                },
                {
                    "start": 1696,
                    "end": 1699,
                    "matchedPaperCorpusId": "251137828"
                },
                {
                    "start": 1877,
                    "end": 1881,
                    "matchedPaperCorpusId": "53083677"
                },
                {
                    "start": 1884,
                    "end": 1887,
                    "matchedPaperCorpusId": "208527480"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45947265625
        },
        {
            "corpus_id": "276249784",
            "title": "Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation",
            "text": "Logical Reasoning Datasets Logical reasoning is a fundamental ability of intelligent systems and researchers have proposed a number of datasets to evaluate logical reasoning abilities of language models. These datasets can be divided into two groups: manually crafted ones and automatically generated ones. Human-crafted datasets are typically sourced from existing examinations (Yu et al., 2019;Liu et al., 2021;2023;Zhong et al., 2021) or directly written by domain experts (Hu et al., 2023;Han et al., 2022). Although they often exhibit higher quality and complexity, they also suffer from limited quantity and high creation costs. Automatically generated datasets are often created using rules and predefined natural language templates (Sinha et al., 2019;Hahn et al., 2021;Clark et al., 2021;Tafjord et al., 2021;Tian et al., 2021;Saparov & He, 2022;Saparov et al., 2024;Sun et al., 2021). While these datasets enable automatic problem generation, they often lack linguistic diversity and are constrained by rigid, monotonous rules. None of them cover all aspects of firstorder logic, and they cannot be used to generate problems with more complex rules. \n\nSymbolic Prover Augmented LLMs Symbolic provers are computer programs used to generate proofs and solutions of mathematical theorems automatically. In logical reasoning problems, symbolic provers, such as SymPy (Meurer et al., 2017), Z3 (De Moura & Bj\u00f8rner, 2008), Lean (De Moura et al., 2015), Pyke (Frederiksen, 2008) andProver9 (McCune, 2005), stand out with faithful solutions and traceable intermediate processes. Recent works (Pan et al., 2023;Olausson et al., 2023;Ye et al., 2024;Zhang et al., 2024) combine LLMs and symbolic provers to solve logic reasoning problems. They use LLMs to translate problems from natural language into symbolic representations and then solve that problem with a symbolic prover.",
            "score": 0.46449337308132377,
            "section_title": "RELATED WORKS",
            "char_start_offset": 5284,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1878
                }
            ],
            "ref_mentions": [
                {
                    "start": 379,
                    "end": 396,
                    "matchedPaperCorpusId": "209485573"
                },
                {
                    "start": 396,
                    "end": 413,
                    "matchedPaperCorpusId": "220483148"
                },
                {
                    "start": 418,
                    "end": 437,
                    "matchedPaperCorpusId": "271545014"
                },
                {
                    "start": 740,
                    "end": 760,
                    "matchedPaperCorpusId": "198184456"
                },
                {
                    "start": 760,
                    "end": 778,
                    "matchedPaperCorpusId": "212633677"
                },
                {
                    "start": 778,
                    "end": 797,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 797,
                    "end": 818,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 818,
                    "end": 836,
                    "matchedPaperCorpusId": "243865235"
                },
                {
                    "start": 836,
                    "end": 855,
                    "matchedPaperCorpusId": "252693237"
                },
                {
                    "start": 855,
                    "end": 876,
                    "matchedPaperCorpusId": "258865898"
                },
                {
                    "start": 1373,
                    "end": 1394,
                    "matchedPaperCorpusId": "2272000"
                },
                {
                    "start": 1399,
                    "end": 1425,
                    "matchedPaperCorpusId": "15912959"
                },
                {
                    "start": 1432,
                    "end": 1455,
                    "matchedPaperCorpusId": "232990"
                },
                {
                    "start": 1612,
                    "end": 1634,
                    "matchedPaperCorpusId": "264436468"
                },
                {
                    "start": 1634,
                    "end": 1650,
                    "matchedPaperCorpusId": "258715073"
                },
                {
                    "start": 1650,
                    "end": 1669,
                    "matchedPaperCorpusId": "271545014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.482666015625
        },
        {
            "corpus_id": "254877131",
            "title": "Optimization Techniques for Unsupervised Complex Table Reasoning via Self-Training Framework",
            "text": "(2) Methods based on synthesizing human-like data through heuristics or data-to-text models. Eisenschlos et al. [29] generate claims using context-free grammar (CFG) templates and counterfactual heuristics. Recently, Pan et al. [30] propose an unsupervised learning framework named MQA-QG. MQA-QG generates multi-hop questions for both the tabular and textual data, which is the most relevant work in this tendency. \n\nNevertheless, several critical issues remain unsolved in the realm of unsupervised complex tabular reasoning: (1) Existing methods for data generation mainly use heuristics or shallow data-to-text methods (e.g., converting a row to a sentence). Thus, they can merely generate relatively simple instances shown in Figure 2, limiting the model's effectiveness on complex reasoning samples, which require a deep understanding of the semantics and logic relationships between multiple table cells. (2) Previous works only focus on a single scenario but cannot be expanded to other tabular reasoning tasks. This is because they design heuristics based on specific data characteristics or the form of the task, and these methods cannot be transferred to other tasks flexibly. Therefore, these models struggle to handle the complex and diverse scenarios encountered in the real world. \n\nTo address these issues, we introduce UCTR-ST (Unsupervised Complex Tabular Reasoning using Self-Training), an advanced self-training framework designed explicitly for unsupervised complex tabular reasoning. More specifically, UCTR-ST primarily leverages a random sampling strategy to collect different types of programs. These programs consist of sequences of symbols that can be executed on tables, including SQL queries, logical forms, and arithmetic expressions, encompassing a wide range of reasoning types. Subsequently, we design a \"Program-Management\" module that generates program-answer pairs by leveraging numerous tables within the domain. To bridge the gap between the programs and natural language sentences, we develop a powerful \"Program-Transformation\" module based on generative language models that turns the programs into human-like natural questions or claims with complex logic.",
            "score": 0.46369215152959614,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4256,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1295
                },
                {
                    "start": 1298,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2198
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80908203125
        },
        {
            "corpus_id": "272423721",
            "title": "Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation",
            "text": "Current methods for enhancing the quality of modelgenerated content are diverse and sophisticated. Some approaches utilize a voting-based mechanism. For example, Wang et al. (2022) introduced the Self-Consistency method, which improves reasoning accuracy by first generating more than 20 CoT paths and then voting for the most consistent answer. Other methods incorporate external sources. Zheng et al. (2023) introduced Step Back, which prompts models to generate an abstract of the question to capture deeper logical structures, thereby enhancing retrieval-augmented generation (RAG) capabilities. Similarly, Yang et al. (2024b) developed another RAG-based method, Buffer of Thoughts, which uses knowledge extracted from external sources and predefined knowledge categories for each task. These elements are integrated into a predefined task prompt template, enabling the model to generate more accurate answers. Additionally, some methods introduce external tools to aid problem-solving. Gao et al. (2023) proposed PAL, which leverages large language models to parse problems and generate programs as intermediate reasoning steps, delegating the solution to a runtime environment like a Python interpreter. This neural-symbolic collaboration has demonstrated improved accuracy across various tasks. Suzgun and Kalai (2024) introduced metaprompting, which integrates existing prompt-based frameworks, enabling dynamic selection of the most effective reasoning strategy. These strategies, with their complex templates and multi-stage prompting, provide models with sophisticated tools for advancing CoT generation in LLMs. \n\nThese methods are inherently complex, with some being task-sensitive and others involving multi-turn prompting; however, they have demonstrated substantial efficacy in enhancing the reasoning capabilities of LLMs, thereby advancing the frontiers of CoT generation in machine learning.",
            "score": 0.46347973648245266,
            "section_title": "Enhancement of CoT Path",
            "char_start_offset": 6446,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1623
                },
                {
                    "start": 1626,
                    "end": 1910
                }
            ],
            "ref_mentions": [
                {
                    "start": 991,
                    "end": 1008,
                    "matchedPaperCorpusId": "253708270"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4990234375
        },
        {
            "corpus_id": "266362535",
            "title": "A Survey of Reasoning with Foundation Models: Concepts, Methodologies, and Outlook",
            "text": "Commonsense reasoning refers to the human-like capacity to make assumptions and inferences about the nature and characteristics of everyday situations that humans encounter on a regular basis \u00a7 . \n\nRecent research indicates that language models are capable of acquiring certain aspects of common sense knowledge [352]. In the domain of structured commonsense reasoning, Madaan et al. [176] tackle the task by generating a graph based on natural language input. They formalize this problem as a code generation challenge, utilizing large language models that are prompted with code to construct the graph representation. Berglund et al. [18] also point out that language models often demonstrate a fundamental lapse in logical deduction, failing to generalize a common pattern in their training set, speciically, the likelihood of \"B is A\" occurring if \"A is B\" is present. Li et al. [149] take a systematic approach to evaluate the performance of large pre-trained language models on various commonsense benchmarks. They conduct zero-shot and few-shot commonsense evaluations across four diferent benchmarks, considering six diferent model sizes. Notably, their evaluation includes a remarkably large language model with 280 billion parameters. Multiple evaluation settings, such as diferent score functions and prompt formats, are explored to comprehensively assess the models' ability to capture and reason about commonsense knowledge. \n\nAnother direction in the ield of commonsense reasoning involves combining pre-trained language models with commonsense-speciic ine-tuning techniques. Chang et al. [28] propose several architectural variations, leverage external commonsense corpora, and employ commonsense-speciic ine-tuning techniques for the Social IQA task [240]. Through their work, they demonstrate that these optimizations can enhance the model's performance in tasks related to social intelligence. Furthermore, Yang et al. [317] introduce a two-stage framework designed to connect pre-training and ine-tuning in the task of commonsense generation. \n\nIn addition to the above-mentioned works, there are other aspects of commonsense reasoning that have been explored. These include commonsense Question Answering (QA), physical reasoning, spatial reasoning, and the corresponding benchmarks, as shown in Figure 5.",
            "score": 0.4631485420141397,
            "section_title": "Commonsense Reasoning",
            "char_start_offset": 38694,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 198,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2061
                },
                {
                    "start": 2064,
                    "end": 2179
                },
                {
                    "start": 2180,
                    "end": 2325
                }
            ],
            "ref_mentions": [
                {
                    "start": 312,
                    "end": 317,
                    "matchedPaperCorpusId": "258841057"
                },
                {
                    "start": 636,
                    "end": 640,
                    "matchedPaperCorpusId": "262083829"
                },
                {
                    "start": 1603,
                    "end": 1607,
                    "matchedPaperCorpusId": "230521972"
                },
                {
                    "start": 1766,
                    "end": 1771,
                    "matchedPaperCorpusId": "128296356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1898193359375
        },
        {
            "corpus_id": "268264141",
            "title": "Advancing Chinese biomedical text mining with community challenges",
            "text": "Text generation refers to the process of generate natural language text, while knowledge reasoning refers to the process of using existing knowledge and information to make inferences. Both of them are complex and challenging NLP tasks. In 2020, CHIP organized a question generation task focused on Chinese traditional medical literatures and related texts from internet forum. Participants were required to develop algorithms that process these texts and generate questions. In 2021, CCKS organized a dialogue generation task, which was based on the medical dialogue dataset MedDG [80] related to 12 types of common gastrointestinal diseases. The task aimed to generate question-answer pairs containing 160 related entities from five categories: diseases, symptoms, attributes, examinations, and medications. At same year, CCKS organized another task focused on the reading comprehension of Chinese popular medical knowledge. Given texts and questions, the task aims to extract corresponding text spans as answers. For medical knowledge reasoning, in 2021, CCL released the medical dialogue based intelligent diagnosis evaluation task, which explored the identification of medical entities and symptom information from doctor-patient dialogue texts, the automatic generation of medical reports, and the simulation of dialogue process to determine specific diseases. In 2022, CHIP organized a task, which aims to automatic clinical diagnostic coding by given relevant diagnostic information (e.g., admission diagnosis, preoperative diagnosis, postoperative diagnosis, and discharge diagnosis), surgery, medication, and medical advice [52].",
            "score": 0.46251451978815605,
            "section_title": "Text generation and knowledge reasoning",
            "char_start_offset": 22464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1639
                }
            ],
            "ref_mentions": [
                {
                    "start": 582,
                    "end": 586,
                    "matchedPaperCorpusId": "251223928"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11859130859375
        },
        {
            "corpus_id": "256826996",
            "title": "Can GPT-3 Perform Statutory Reasoning?",
            "text": "Exploring GPT-3's ability to reason has yielded improvements on multiple benchmark datasets. Lu et al. [14] collect a dataset of common sense and science questions, pairing each question with background information and an explanation of the answer. The additional context, serving as a chain-of-thought, marginally improves GPT-3's performance. Zhou et al. [27] explicitly decompose reasoning problems, forcing GPT-3 to solve a complex problem in multiple increments. Khot et al. [10] show GPT-3 can be prompted to decompose problems into simpler steps, on both artificial tasks and open-domain question-answering. Zelikman et al. [24] exploit a language model's ability to generate chains of reasoning: GPT-J [21] is made to generate chains of reasoning for a set of questions and answers, and is further fine-tuned on those chains that lead to the correct answer. Adding structure to the process of natural language generation, Zhang et al. [25] run a symbolic reasoning engine on the output of a language model, yielding improvements on semi-synthetic benchmarks meant to test reasoning abilities. Thorburn and Kruger [20] evaluate GPT Neo's argumentative abilities, by making it suggest claims or reasons to support a claim, with no particular emphasis on logical reasoning. Data comes from Kialo, a collection of arguments curated by online users. Evaluations show that reasons and claims generated by GPT Neo are less coherent than those generated by humans. Saparov and He [17] systematically study GPT-3's ability to reason, using synthetic, controlled data, and metrics that measure the internal coherence of chains of reasoning. The authors show that GPT-3 has the ability to perform individual steps of reasoning, but tends to lose track when producing a proof with multiple steps. Results further show that GPT-3 leverages its background knowledge about the world to draw conclusions, so that GPT-3's ability to reason decreases as the overlap between the target domain and its real-world knowledge decreases. \n\nGPT-3 on legal tasks. Hendrycks et al. [7] use GPT-3 to solve legal multiple-choice questions, with data collected from online sources.",
            "score": 0.46206682513191866,
            "section_title": "RELATED WORK",
            "char_start_offset": 4193,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 2021
                },
                {
                    "start": 2024,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2159
                }
            ],
            "ref_mentions": [
                {
                    "start": 943,
                    "end": 947,
                    "matchedPaperCorpusId": "252760744"
                },
                {
                    "start": 1121,
                    "end": 1125,
                    "matchedPaperCorpusId": "253161659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3525390625
        },
        {
            "corpus_id": "259858981",
            "title": "SORTIE: Dependency-Aware Symbolic Reasoning for Logical Data-to-text Generation",
            "text": "Generating logical-consistent sentences is an integral part of human intelligence and has attracted broad research interests in the field of natural language processing recently (Chen et al., 2020a,c;Wei et al., 2022;Creswell et al., 2022;Kazemi et al., 2022). One of the most prominent attempts to investigate this capability in neural models is logical data-to-text generation (Chen et al., 2020a), which requires conducting intricate reasoning on rary deep learning methods and symbolic AI, we get inspiration from recent neural symbolic literature (Gao et al., 2022) that decouples complex reasoning with language generation. In specific, we delegate the inference of entities mentioned in the table description to a programme interpreter. The interpreter executes our generated python-like programme, thus working out the entities correctly and alleviating hallucination. \n\nHowever, synthesizing such a programme to infer entities is not a trivial task due to two major challenges: First, though there are some domain-specific programming languages on natural text (Chen et al., 2020b;Gupta et al., 2019), we need to design a table-compatible and easyto-execute programming language to support the reasoning over entities in the table. Second, the entities to infer are not independent but have a complex dependency relationship and the inference of one might rely on the others. For instance, as is shown in Figure 1, we can not count the appearance of 16150 unless we work out the 16150 first. Thus, figuring out the synthesis order of the entities is fundamental to the reasoning process. To make it worse, there is no human annotation of programmes or synthesis order for the entities. \n\nTo mitigate the aforementioned problems, we propose SORTIE (SymbOlic Reasoning with enTIty schEduling), a framework that reasons out each named entities in the table description with dependency-aware symbolic reasoning.",
            "score": 0.4619134498720516,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 876
                },
                {
                    "start": 879,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1694
                },
                {
                    "start": 1697,
                    "end": 1916
                }
            ],
            "ref_mentions": [
                {
                    "start": 379,
                    "end": 399,
                    "matchedPaperCorpusId": "216056509"
                },
                {
                    "start": 1070,
                    "end": 1090,
                    "matchedPaperCorpusId": "212814759"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4697265625
        },
        {
            "corpus_id": "273662200",
            "title": "Synthetic Data Generation with Large Language Models for Personalized Community Question Answering",
            "text": "Recent advancements in Generative Artificial Intelligence have motivated researchers to explore the potential of generative models to create synthetic data for training machine learning models, especially in Computer Vision and NLP tasks. In the context of this paper, we focus on the generation of textual data. Researchers have explored the ability of LLMs to generate data for various text classification tasks, with mixed results regarding their effectiveness [30]- [38]. Puri et al. [39] explore question and answer generation as a data augmentation method to enhance QA models, addressing the challenge of limited human-labelled data. The authors point out that the synthetic data alone overcome the performance of the original training set in the Exact Match (EM) and F1 scores. Li et al. [40] investigate the effectiveness of synthetic data generated by LLMs in training text classification models across tasks with varying levels of subjectivity. They compare models trained on real-world data with those trained on LLMgenerated data under zero-shot and few-shot settings. Their findings reveal that while models trained on synthetic data perform comparably on low-subjectivity tasks, e.g. news and topic classification, there is a notable performance drop on high-subjectivity tasks, such as sarcasm detection. Regarding the generation of synthetic data for IR tasks, LLMs can be used to generate synthetic queries [16] or documents [17]. In [41] the authors propose to generate documents by few-shot prompting LLMs and concatenate them with the original query to form a new query. \n\nWe focus our work on personalization tasks, which aim to tailor a model's outcomes for a specific user (or group of users) based on the knowledge of her/his interests and behaviour. For example, as the use of LLMs in real-world applications evolves, recent work has highlighted the impact and concerns associated with personalizing LLMs [42]. Salemi et al. [12] introduce the LaMP benchmark, a synthetic dataset designed to evaluate the ability of LLMs to produce personalized outcome. LaMP encompasses seven distinct tasks, divided into three Text Classification tasks (Citation Identification, Movie Tagging and Product Rating) and four Text Generation tasks (News Headline, Scholarly Title, Email Subject and Tweet Paraphrasing).",
            "score": 0.46186137106523967,
            "section_title": "B. Synthetic Data Generation",
            "char_start_offset": 6148,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1591
                },
                {
                    "start": 1594,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2079
                },
                {
                    "start": 2080,
                    "end": 2326
                }
            ],
            "ref_mentions": [
                {
                    "start": 470,
                    "end": 474,
                    "matchedPaperCorpusId": "259096160"
                },
                {
                    "start": 1425,
                    "end": 1429,
                    "matchedPaperCorpusId": "250340449"
                },
                {
                    "start": 1452,
                    "end": 1456,
                    "matchedPaperCorpusId": "257505063"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90771484375
        },
        {
            "corpus_id": "265213146",
            "title": "Transformers in the Service of Description Logic-Based Contexts",
            "text": "We generated the only large dataset (384K) in the literature that targets expressive DLs (namely, ALCQ), that enjoys both high expressivity and high linguistic complexity and is publicly available for further understanding of the functionality of TLMs. We showed that our DeBERTa-based model, DELTAM , can carry out entailment checking over expressive synthetic datasets with very high accuracy, regardless of the linguistic complexity of the context. Differently from recent results in the literature, we showed that our model has learned to generalize on unseen reasoning depths, smaller or greater. Zeroshot tests showed that DELTAM is mostly robust to other distributions. Tests with the GPT family showed that GPT-4 can have significant performance with only a few shots. The high accuracy of zero-shot testings in a real-world scenario demonstrates the potential of TLMs for performing reasoning tasks bypassing the necessity for domain experts to be familiar with formal representations. \n\nOur qualitative tests revealed the need for the development of systematic evaluation techniques of synthetically generated datasets. Hence, this will be our next step in future work. Furthermore, we plan to explore the upper limit of the expressivity of the logic language so that a transformer-based model will be able to perform reasoning tasks with high accuracy. \n\nFinally, we will expand our evaluation section with other state-ofthe-art generative models. \n\nTo generate the KBs, we employ a random sampling technique to select a subset of individuals, roles, and atomic Concepts from the pools mentioned above. An item from each pool has the same probability of being chosen. \n\nThe probabilistic context-free grammar for the linguistic complexity level L = 0 is shown in Table 12, the rest can be found in the supplementary material files. The PCFG shown is for Pool B. The grammars for the Pool A are similar. The probabilities in the PCFGs were determined experimentally to generate appropriate KBs that would yield the desired inferences in the minimum amount of time.",
            "score": 0.46186137106523967,
            "section_title": "Conclusions and Future Work",
            "char_start_offset": 33856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 994
                },
                {
                    "start": 997,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1678
                },
                {
                    "start": 1681,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2074
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72509765625
        },
        {
            "corpus_id": "272753174",
            "title": "LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning",
            "text": "Imagine you are a contestant on a new game show called \"The Magic 24 Points\", where your task is to ... (Game Rules) ... you are given the cards with numbers 8, 7, 6, 5. Your challenge is to determine whether it's possible to arrange these numbers and operations to achieve exactly 24. Can you find a valid expression, or prove that reaching 24 is impossible with these cards? mathematical data, synthetic data can be generated through problem-driven methods such as evolinstruct (Luo et al., 2023a;Zeng et al., 2024), problem restatement (Yu et al., 2023), back-translation (Lu et al., 2024), and few-shot examples (Dalvi et al., 2021), or knowledge-driven methods based on knowledge bases (Dalvi et al., 2021) and concept graphs (Tang et al., 2024). For code data, it generates diverse and high-quality instruction data with self-evolutionary methods (Wang et al., 2023;Luo et al., 2023b) or combined with open-source references such as OSS-Instruct (Wei et al., 2024). \n\nIn the complex logical reasoning domain, there is relatively less research on data synthesis. Previously, some work uses logical proposition-based soft reasoning methods (Tafjord et al., 2021;Clark et al., 2021) to synthesize training data, aiming to improve models' reasoning ability and interpretability (Saeed et al., 2021;Dalvi et al., 2021) Previous methods primarily used propositional logic or formal languages as the source of logic. Instead, we find that algorithmic questions and programming languages like LeetCode provide a different source of logic. Algorithmic problems typically involve explicit input-output relationships, recursive and iterative structures, and operations on data structures. All of these constitute a unique pattern of reasoning. At the same time, algorithmic problems are naturally related to real-world task contexts, such as path-planning, data sorting, and resource allocation problems, which can often be mapped to real-world application scenarios.",
            "score": 0.46181710975171497,
            "section_title": "LeetCode 679. 24 Game",
            "char_start_offset": 1096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 971
                },
                {
                    "start": 974,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1962
                }
            ],
            "ref_mentions": [
                {
                    "start": 616,
                    "end": 636,
                    "matchedPaperCorpusId": "233297051"
                },
                {
                    "start": 691,
                    "end": 711,
                    "matchedPaperCorpusId": "233297051"
                },
                {
                    "start": 853,
                    "end": 872,
                    "matchedPaperCorpusId": "258187235"
                },
                {
                    "start": 1166,
                    "end": 1185,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 1300,
                    "end": 1319,
                    "matchedPaperCorpusId": "233297051"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69921875
        },
        {
            "corpus_id": "235742855",
            "title": "Probabilistic Graph Reasoning for Natural Proof Generation",
            "text": "Text Reasoning over Formal Representation Early work employs a pipeline of methods that converts free text into logic form first (semantic parsing), and then uses formal logical reasoning (Musen and Van der Lei, 1988). Due to the serious error propagation caused by semantic parsing (Zettlemoyer and Collins, 2005;Berant et al., 2013;Berant and Liang, 2014), researchers focus on developing theorem provers by combining the symbolic techniques with the differentiable learning from neural networks (Reed and de Freitas, 2016;Abdelaziz et al., 2020;Abboud et al., 2020), such as NLProlog (Weber et al., 2019), SAT solving (Selsam et al., 2019) and Neural programme (Neelakantan et al., 2016). To bypass this expensive and error-prone intermediate logical representation, reasoning over natural language statements in an end-to-end manner is promising.\n\nText Reasoning over Natural Language Natural logic (MacCartney and Manning, 2009) focuses on semantic containment and monotonicity by incorporating semantic exclusion and implicativity. Subsequently, Clark et al. (2020) proposes to use a Transformer-based model to emulate deductive reasoning and achieves high accuracy on synthetically generated data. PROVER (Saha et al., 2020) points out that a reasoning system should not only answer queries but also generate a proof. However, PROVER adopts the multi-task learning framework in the training stage and cannot effectively capture the interactions between question answering and proof generation. Along this line, we explore more powerful joint models to achieve deep reasoning. QA and NLI There are bAbI (Weston et al., 2016), QuaRTz , ROPES  and Hotpot QA (Yang et al., 2018) (QA datasets) involved in rule reasoning. However, for those datasets, implicit rules (i.e., which multihop chains are valid) need to be inferred from the training data. In our task, the rules of reasoning are given in advance. Compared with the Natural Language Inference (MacCartney and Manning, 2014), our task can be regarded as its deductive subset. In particular, NLI allows for unsupported inferences (Da",
            "score": 0.4612952992538317,
            "section_title": "Related Work",
            "char_start_offset": 23954,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 217,
                    "matchedPaperCorpusId": "61565144"
                },
                {
                    "start": 283,
                    "end": 314,
                    "matchedPaperCorpusId": "449252"
                },
                {
                    "start": 314,
                    "end": 334,
                    "matchedPaperCorpusId": "6401679"
                },
                {
                    "start": 334,
                    "end": 357,
                    "matchedPaperCorpusId": "1336493"
                },
                {
                    "start": 498,
                    "end": 525,
                    "matchedPaperCorpusId": "7034786"
                },
                {
                    "start": 548,
                    "end": 568,
                    "matchedPaperCorpusId": "102354917"
                },
                {
                    "start": 587,
                    "end": 607,
                    "matchedPaperCorpusId": "189898046"
                },
                {
                    "start": 621,
                    "end": 642,
                    "matchedPaperCorpusId": "3632319"
                },
                {
                    "start": 664,
                    "end": 690,
                    "matchedPaperCorpusId": "6715185"
                },
                {
                    "start": 1052,
                    "end": 1071,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 1212,
                    "end": 1231,
                    "matchedPaperCorpusId": "222141025"
                },
                {
                    "start": 1609,
                    "end": 1630,
                    "matchedPaperCorpusId": "3178759"
                },
                {
                    "start": 1662,
                    "end": 1681,
                    "matchedPaperCorpusId": "52822214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2578125
        },
        {
            "corpus_id": "264590754",
            "title": "In-Context Ability Transfer for Question Decomposition in Complex QA",
            "text": "Answering complex questions is an active research topic that has widespread applications in finance, medical diagnosis, technical support, and more, enabling in-depth information access and advanced problem-solving [41]. Complexity could arise from multiple factors, such as the need to parse heterogeneous sources [7] or multiple documents [48] to answer the question. It could also arise from the nature of the questions, such as compositional questions [19,49], which require reasoning through question decomposition to arrive at the final answer [14] or numerical reasoning [7,27,42]. An example of a compositional natural language question is \"Who is the spouse of the person who voices Jarvis in Iron Man?\". To answer this question, it must be first decomposed into relevant sub-questions, such as \"Who voices Jarvis in Iron Man ?\" and \"Who is the spouse of Paul Bettany?\". To arrive at the final answer, a system must answer the sub-questions and aggregate their answers. The decomposition leads to interpretable reasoning apart from improved performance on the answering task. In numerical reasoning-based complex questions, stepwise decomposition of the reasoning process, usually called rationales, helps induce arithmetic abilities to arrive at the correct solution [27,42]. Alternate approaches also involve generating arithmetic programs from the given natural language problem statement. [2,8]. \n\nExisting methods for compositional complex QA adopt supervised [11,14,20,23,33,60] or unsupervised approaches [37] to decompose a complex question into simpler questions and provide the final answer by aggregating answers to the sub-questions. Similarly, numerical reasoning questions require supervised stepwise rationales along with the solution for fine-tuning generative models [9,16,27]. More recently, a new paradigm for solving complex questions using general purpose Large Language Models (LLMs) has emerged using in-context learning [28] and chain-of-thought reasoning [15,52]. Instead of fine-tuning task-specific models, these approaches focus on choosing carefully constructed natural language prompts with demonstrations and rationales. A distinct advantage of prompting vs fine-tuning is that the LLM is a generalpurpose machine that can be used to solve multiple complex QA tasks.",
            "score": 0.4612734238422682,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1408
                },
                {
                    "start": 1411,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2306
                }
            ],
            "ref_mentions": [
                {
                    "start": 341,
                    "end": 345,
                    "matchedPaperCorpusId": "211258744"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "226236740"
                },
                {
                    "start": 460,
                    "end": 463,
                    "matchedPaperCorpusId": "236771976"
                },
                {
                    "start": 550,
                    "end": 554,
                    "matchedPaperCorpusId": "239885904"
                },
                {
                    "start": 581,
                    "end": 584,
                    "matchedPaperCorpusId": "12777818"
                },
                {
                    "start": 1277,
                    "end": 1281,
                    "matchedPaperCorpusId": "12777818"
                },
                {
                    "start": 1402,
                    "end": 1405,
                    "matchedPaperCorpusId": "173188048"
                },
                {
                    "start": 1405,
                    "end": 1407,
                    "matchedPaperCorpusId": "53298878"
                },
                {
                    "start": 1478,
                    "end": 1481,
                    "matchedPaperCorpusId": "239885904"
                },
                {
                    "start": 1481,
                    "end": 1484,
                    "matchedPaperCorpusId": "2623009"
                },
                {
                    "start": 1487,
                    "end": 1490,
                    "matchedPaperCorpusId": "174801080"
                },
                {
                    "start": 1490,
                    "end": 1493,
                    "matchedPaperCorpusId": "253237669"
                },
                {
                    "start": 1521,
                    "end": 1525,
                    "matchedPaperCorpusId": "211258645"
                },
                {
                    "start": 1796,
                    "end": 1799,
                    "matchedPaperCorpusId": "215548225"
                },
                {
                    "start": 1799,
                    "end": 1802,
                    "matchedPaperCorpusId": "12777818"
                },
                {
                    "start": 1953,
                    "end": 1957,
                    "matchedPaperCorpusId": "231632658"
                },
                {
                    "start": 1989,
                    "end": 1993,
                    "matchedPaperCorpusId": "252683303"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32373046875
        },
        {
            "corpus_id": "268358481",
            "title": "A Logical Pattern Memory Pre-trained Model for Entailment Tree Generation",
            "text": "Generating coherent and credible explanations poses a significant challenge in AI, and addressing it represents a giant leap towards the goal of building reliable reasoning systems (Barredo Arrieta and D\u00edaz-Rodr\u00edguez, 2020).The reasoning chain has traditionally served as the primary representation for constructing reasonable explanations (DeYoung et al., 2020;Tafjord et al., 2021).However, in recent years, researchers have explored the utilization of entailment trees for explanation generation (Dalvi et al., 2021;Yang et al., 2022;Hong et al., 2022).As shown in Figure 1, the task of entailment tree generation can be defined as follows: given a hypothesis (summarizing from a question+answer pair) and a set of supporting facts, the goal is to derive an entailment tree where each non-leaf node is an intermediate conclusion generated from its children.By providing valid entailment trees, users can develop a better understanding of the reasoning process and acquire reliable information for effective decision.\n\nThe early work by Dalvi et al. (2021) proposes a single-step method for generating entailment trees.However, subsequent research conducted by Hong et al. (2022) has highlighted that such an approach often yields unreliable explanations for the hypothesis.To tackle this limitation, several recent studies employing multi-step generation methods have been introduced (Yang et et al., 2022;Hong et al., 2022;Liu et al., 2022).These methods have demonstrated impressive performance by iteratively selecting premises and generating intermediate conclusions to construct the entire tree.Nonetheless, a significant drawback of many existing iterative methods lies in their limited attention to the logical patterns between premises and conclusions.As a result, they might generate arXiv:2403.06410v1[cs.CL] 11 Mar 2024\n\nintermediate conclusions that lack logical consistency and conflict with the premises.\n\nAmong the iterative methods, METGEN (Hong et al., 2022) stands out for its attempt to integrate logical patterns into the tree generation process.However, it primarily focuses on the module responsible for premise selection, with relatively limited attention to effectively capturing and learning logical patterns.METGEN's strategy for integrating logical patterns involves training the language model on a synthetic dataset constructed from Wikipedia.",
            "score": 0.46105004768791136,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 224,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 556
                },
                {
                    "start": 556,
                    "end": 860
                },
                {
                    "start": 860,
                    "end": 1019
                },
                {
                    "start": 1021,
                    "end": 1121
                },
                {
                    "start": 1121,
                    "end": 1276
                },
                {
                    "start": 1276,
                    "end": 1445
                },
                {
                    "start": 1445,
                    "end": 1603
                },
                {
                    "start": 1603,
                    "end": 1763
                },
                {
                    "start": 1763,
                    "end": 1814
                },
                {
                    "start": 1814,
                    "end": 1833
                },
                {
                    "start": 1835,
                    "end": 1921
                },
                {
                    "start": 1923,
                    "end": 2069
                },
                {
                    "start": 2069,
                    "end": 2237
                },
                {
                    "start": 2237,
                    "end": 2375
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 223,
                    "matchedPaperCorpusId": "204824113"
                },
                {
                    "start": 362,
                    "end": 383,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 499,
                    "end": 519,
                    "matchedPaperCorpusId": "233297051"
                },
                {
                    "start": 519,
                    "end": 537,
                    "matchedPaperCorpusId": "249062748"
                },
                {
                    "start": 537,
                    "end": 555,
                    "matchedPaperCorpusId": "248524641"
                },
                {
                    "start": 1039,
                    "end": 1058,
                    "matchedPaperCorpusId": "233297051"
                },
                {
                    "start": 1163,
                    "end": 1181,
                    "matchedPaperCorpusId": "248524641"
                },
                {
                    "start": 1409,
                    "end": 1427,
                    "matchedPaperCorpusId": "248524641"
                },
                {
                    "start": 1427,
                    "end": 1444,
                    "matchedPaperCorpusId": "253237103"
                },
                {
                    "start": 1959,
                    "end": 1978,
                    "matchedPaperCorpusId": "248524641"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.419677734375
        },
        {
            "corpus_id": "253107930",
            "title": "Leveraging QA Datasets to Improve Generative Data Augmentation",
            "text": "Finally, we generate synthetic training data for the target task by generating contexts for questions and answers pertaining to the respective dataset. Then, we add the generated samples to the few-shot supervision and train a target task model on the augmented data. \n\nWe perform extensive experiments on multiple sentiment analysis and topic classification datasets with several abstractive, extractive, and commonsense reasoning QA datasets. Through rigorous experiments and thorough analysis, we observe that QA datasets that require high-level reasoning abilities such as abstractive and common-sense QA datasets suit the best for generating high-quality data. \n\nOur contributions are summarized as follows: \n\n\u2022 We propose to use QA datasets for training generative language models to be context generators for a given question and answer. \u2022 We formulate various classification tasks into a QA format and model synthetic training data generation for these tasks as context generation. \u2022 We perform experiments on multiple sentiment analysis and topic classification datasets to demonstrate the effectiveness of our method in zero-and few-shot settings. \n\n\u2022 We release the code on Github1 .",
            "score": 0.4610242289102009,
            "section_title": "General text classification format",
            "char_start_offset": 2771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 267
                },
                {
                    "start": 270,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 665
                },
                {
                    "start": 668,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1157
                },
                {
                    "start": 1160,
                    "end": 1194
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "254854559",
            "title": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning",
            "text": "Reasoning in natural language has been a prevalent problem in NLP. In recent years, logical reasoning in textual data has seen an increasing focus.\n\nReClor  and LogiQA (Liu et al., 2021) are reading comprehension-style datasets focused on questions that require reasoning using information from a given context. Wang et al. (2022) proposed LRReasoner, which parses symbolic logical structures from the training data of ReClor for data augmentation using logical context extensions. Ouyang et al. (2022) constructed logical graphs using the chain of facts present in a task instance and used GNNs to reason on the graph. Jiao et al. (2022) proposed MERIt, that used Wikipedia to generate sentence pairs for contrastive learning that are logically related, and trained the PLM using contrastive loss. Both LRReasoner and FOCAL REASONER use data augmentation that is specific to the task being solved, making the pretraining process specific to the downstream dataset, and thus not generalizable across tasks. While MERIt addresses this issue by using Wikipedia to generate logical graphs, their contrastive loss formulation requires counterfactual data augmentation, which potentially distorts the factual knowledge present in the pretrained model. Additionally, their approach is restricted to using Wikipedia as the data source since they heavily rely on forming entity graphs from Wikipedia texts. In contrast, we propose a simple continued pretraining strategy by modifying the masked language modeling loss (Devlin et al., 2019) and sentence classification loss to improve the logical reasoning ability of language models. Our approach is simple to integrate during pretraining, is not dependent on any data processing, and generalizes well across different datasets. Along a related line, Clark et al. (2020) used synthetically generated rulebases to show that PLMs can perform complex deductive reasoning to predict the entailment of a given hypothesis. This led to some recent developments in trying to build systems that can generate step-by-step reasoning chains to prove the model's entailment prediction (Saha et al., 2020;Tafjord et al., 2021;Sanyal et al., 2022b). While progress on these datasets is encouraging, the use of synthetic data for training the models",
            "score": 0.46083814085395525,
            "section_title": "Related Works",
            "char_start_offset": 21559,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 186,
                    "matchedPaperCorpusId": "220483148"
                },
                {
                    "start": 312,
                    "end": 330,
                    "matchedPaperCorpusId": "234335834"
                },
                {
                    "start": 620,
                    "end": 638,
                    "matchedPaperCorpusId": "247187518"
                },
                {
                    "start": 1510,
                    "end": 1531,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1793,
                    "end": 1812,
                    "matchedPaperCorpusId": "211126663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75
        },
        {
            "corpus_id": "273374930",
            "title": "PRefLexOR: Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning and Agentic Thinking",
            "text": "These processes are crucial to refining and validating the answer. Reflection, reasoning, and hypothesis generation are integrated to ensure that the answers are derived thoughtfully and are not merely surface-level extractions from the text. The thinking and reflection phases add depth to the question-answer generation, making the dataset richer and more valuable for subsequent learning tasks. \n\nAdditionally, the flexibility required to handle new tasks in real time, without relying on pre-constructed datasets, is often absent from these approaches. This necessitates the development of a model that can both learn autonomously and reflect on its own reasoning to improve continuously, a capability that can be framed in RL terms, where feedback loops and recursive processing drive learning improvements. \n\nRecently proposed methods such as STaR and QuietSTaR frameworks [29,30] introduce an innovative approach to enhancing the reasoning capabilities of language models through recursive thinking, reflection, and iterative refinement. Unlike traditional single-pass models that generate outputs in one step, Quiet-STaR emphasizes a multi-step process where models are encouraged to revisit, refine, and improve their reasoning before arriving at a final answer. This is achieved by integrating several key concepts that foster deeper cognitive engagement and reflection during the decision-making process. At the core of Quiet-STaR is the idea of recursive reasoning, where the model does not simply generate an output in a linear manner, but instead iteratively processes and refines its thoughts. This recursive process mirrors human thinking, where conclusions are often revisited, reassessed, and adjusted before a final decision is made. Quiet-STaR formalizes this by introducing intermediate steps that guide the model through this recursive process, allowing it to build upon its own reasoning in multiple stages. In Quiet-STaR, the model engages in multi-step reasoning cycles, where each iteration produces a more refined version of the previous reasoning. These cycles enable the model to consider various aspects of a problem, explore different reasoning paths, and improve the coherence and depth of its output. This layered reasoning process leads to outputs that are more robust, structured, and better aligned with complex tasks that require detailed thought.",
            "score": 0.46052922153463827,
            "section_title": "Motivation and Challenges",
            "char_start_offset": 9274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2233
                },
                {
                    "start": 2234,
                    "end": 2384
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.343505859375
        },
        {
            "corpus_id": "257766470",
            "title": "Natural Language Reasoning, A Survey",
            "text": "Natural Language Processing (NLP) has shown significant advancements in recent years, particularly with the introduction of transformers and pre-trained language models (PLMs). However, their abilities 2 2 to perform natural language reasoning (NLR) are still far from satisfactory. Reasoning, the process of making inferences based on existing knowledge, is a fundamental aspect of human intelligence and is essential for complex tasks such as decision-making. Building an artificial intelligence system capable of reasoning is both the ultimate goal of the research community and the necessary way to improve the performance of complex applications. Compared to reason with formal language, reasoning with natural language expressions provides a more natural human-computer interaction interface and opens the door to research on defeasible reasoning, such as abduction and induction, which are incapable of formal-based symbolic methods. \n\nPLMs such as BERT [34] and GPT [115] have been the essential components in NLP research since they occurred. Pre-trained on large-scale text corpora, PLMs are capable of natural language understanding. Recent progresses suggest that PLMs also have the potential to solve reasoning problems [25,141,145,158]. Specifically, PLMs can perform soft deductive reasoning over natural language statements [25], reason with implicit knowledge memorized in their parameters [145], and perform multi-step reasoning step-by-step just with a few demonstrations or instructions when the model size is large enough via chain-of-thought prompting [77,158]. Recently, ChatGPT and GPT4 also made impressive reasoning capabilities to the community [4,16]. \n\nHowever, while reasoning has attracted increasing attention recently [25,27,28,77,107,143,158], there still lacks a distinct definition of reasoning and the term \"reasoning\" is sometimes of mistaken usage, which may affect the communication and development towards reasoning in the NLP community. For example, while it belongs to \"commonsense reasoning\", few people might deem that telling about a shared lived experiences [10], e.g. \"name something that you might forget in a hotel room\", is reasoning. Another example is that sometimes \"natural language inference\" is introduced as a task of natural language understanding [12], but other times of reasoning [25].",
            "score": 0.46048479463396214,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 940
                },
                {
                    "start": 943,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1679
                },
                {
                    "start": 1682,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2185
                },
                {
                    "start": 2186,
                    "end": 2347
                }
            ],
            "ref_mentions": [
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1233,
                    "end": 1237,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 1340,
                    "end": 1344,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 1751,
                    "end": 1755,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 2105,
                    "end": 2109,
                    "matchedPaperCorpusId": "218487363"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31884765625
        },
        {
            "corpus_id": "259858981",
            "title": "SORTIE: Dependency-Aware Symbolic Reasoning for Logical Data-to-text Generation",
            "text": "The idea of symbolic reasoning has garnered considerable attention in numerous natural language processing and computer vision tasks. Andreas et al. (2016)   parsing questions into linguistic substructures and constructing question-specific deep networks from smaller modules that each tackle one subtask. Following this work, numerous efforts have been made to directly predict the instance-specific network layouts in an end-to-end manner (Hu et al., 2017), to alleviate the requirement for mediate supervision on semantic parsers (Hu et al., 2018;Mao et al., 2019), to infer the answer with a purely symbolic executor (Yi et al., 2018), and to conduct visual co-reference resolution (Kottur et al., 2018). Very recently, Gupta et al. (2019) and Chen et al. (2020b) concurrently proposed using neural symbolic approaches to answer questions in machine reading comprehension, which demonstrates advantages in numerical reasoning and interpretability. Compared to the previous tasks, which only need to derive a single entity or value, the task of logical table-to-text generation requires the generation of a complete natural language sentence containing multiple entities or logical types.",
            "score": 0.4603822698054748,
            "section_title": "Symbolic Reasoning",
            "char_start_offset": 5008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1191
                }
            ],
            "ref_mentions": [
                {
                    "start": 134,
                    "end": 155,
                    "matchedPaperCorpusId": "5276660"
                },
                {
                    "start": 441,
                    "end": 458,
                    "matchedPaperCorpusId": "18682"
                },
                {
                    "start": 533,
                    "end": 550,
                    "matchedPaperCorpusId": "49908459"
                },
                {
                    "start": 621,
                    "end": 638,
                    "matchedPaperCorpusId": "52919654"
                },
                {
                    "start": 686,
                    "end": 707,
                    "matchedPaperCorpusId": "52167938"
                },
                {
                    "start": 748,
                    "end": 767,
                    "matchedPaperCorpusId": "212814759"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.388916015625
        },
        {
            "corpus_id": "271534245",
            "title": "Effective Large Language Model Debugging with Best-first Tree Search",
            "text": "Code generation belongs to the category of logical reasoning tasks, which includes mathematical reasoning [23,24,25]. Existing research works have developed many useful inference techniques to boost a model's reasoning ability, such as chain-of-thought prompting [26], tree-of-thoughts [27], and code interpreter integration [28]. Least-to-most prompting [29] takes a divide-and-conquer approach to helping LLMs tackle complex problems. Tree search is another popular algorithmic framework to iteratively build a correct reasoning path [19,25]. However, these techniques are fundamentally limited by a model's capacity to identify logical errors [30]. Collecting more training data on correct reasoning paths to fine-tune base models is a fruitful approach [31,32,33,24]. A related direction leverages LLMs to generate synthetic data based on existing logical reasoning datasets [34,35] and fine-tune base models with those data to self-improve the models. Combining LLMs with classical symbolic reasoning aims to benefit from the strong language ability of LLMs and the rigor made available by symbolic solvers [36,37]. In this work, we evaluate LLMs' ability to identify bugs in programs and present a detailed analysis on the LLMs' ability to reason about their own mistakes in code generation.",
            "score": 0.460199168843621,
            "section_title": "Logical Reasoning in LLMs",
            "char_start_offset": 5594,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1297
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 116,
                    "matchedPaperCorpusId": "267032902"
                },
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 286,
                    "end": 290,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 540,
                    "end": 543,
                    "matchedPaperCorpusId": "267032902"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56689453125
        },
        {
            "corpus_id": "267770192",
            "title": "Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text",
            "text": "Natural Language Processing (NLP) has witnessed significant advancements in recent years with the emergence of Large Language Models (LLMs) such as GPT-3 [2] and ChatGPT [15]. These models have achieved remarkable results in tasks such as language generation, translation, and summarization [34]. However, studies have indicated that while LLMs can effectively handle straightforward reasoning problems, they often encounter challenges when faced with more complex reasoning, such as scenarios demanding multi-step reasoning [17]. \n\nMulti-step reasoning typically involves making inferences or answering questions that require multiple steps of logical reasoning. Here's an illustration of multi-step reasoning: \"Marian went shoe shopping with her sister Michelle. Darnell's grandfather, Stanley, taught her how to make a paper airplane while her mother, Marian, prepared dinner. What is the family relationship between Michelle and Stanley?\" Various methods, such as chain-of-thought (CoT) [21,28] and Zero-Shot-CoT [11], have been proposed to improve multistep reasoning in LLMs. These approaches involve step-by-step reasoning, either by providing examples with detailed intermediate steps leading to a conclusion or by prompting the model with \"Let's think step by step\" in a zero-shot setting. Despite their effectiveness, LLMs still face challenges in effectively addressing complex multistep reasoning questions. The first challenge involves accurately comprehending relationships conveyed through natural language, as evident in the given example where Marian has a sister named Michelle, and Darnell has a grandfather named Stanley. Identifying these relationships accurately is crucial, but the inherent ambiguity in natural language makes this difficult. For instance, consider the sentence \"Darnell's grandfather, Stanley, taught her how to make a paper airplane while her mother, Marian, prepared dinner,\" correctly inferring that Marian, not Stanley, is Darnell's mother requires understanding the gender implications and the contextual relational information. Second, LLMs must identify relevant information while",
            "score": 0.460167937488131,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 2074
                },
                {
                    "start": 2075,
                    "end": 2128
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 995,
                    "end": 998,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1017,
                    "end": 1021,
                    "matchedPaperCorpusId": "249017743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51318359375
        },
        {
            "corpus_id": "266998884",
            "title": "A Study on Training and Developing Large Language Models for Behavior Tree Generation",
            "text": "Complex reasoning encompasses the capacity to comprehend and effectively employ supporting evidence and logical frameworks to deduce conclusions or facilitate decision-making. By analyzing the reasoning abilities required for BT generation, we believe that the three most crucial reasoning abilities currently needed for BT generation are commonsense reasoning, logical reasoning, and planning. Commonsense reasoning is a fundamental ingredient of human cognition, encompassing the capacity to comprehend the world and make decisions. This cognitive Understand the meaning of actions, entities, and intentions of the BT generation tasks. \n\nNatural language generation Generate human-readable texts. Achieve better human-computer interaction and return natural language texts to explain the process in the BT generation for humans. \n\nIn-context learning Learn and perform new tasks by observing a few examples. \n\nLearn novel knowledge from fewshots demonstrations, especially in unseen scenarios of the BT generation tasks. \n\nInstruction following Understand and carry out instructions or commands accurately and effectively. \n\nAlign with the human intentions in the BT generation.",
            "score": 0.4600096913330885,
            "section_title": "Reasoning-Related Abilities",
            "char_start_offset": 21440,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 830
                },
                {
                    "start": 833,
                    "end": 909
                },
                {
                    "start": 912,
                    "end": 1022
                },
                {
                    "start": 1025,
                    "end": 1124
                },
                {
                    "start": 1127,
                    "end": 1180
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1663818359375
        },
        {
            "corpus_id": "267311830",
            "title": "A Survey on Data Augmentation in Large Model Era",
            "text": "Based on this observation, FlipDA first generates data using word substitution based on a pre-trained T5 and uses a classifier to select label-flipped data. Guo et al. (2022a) presented GENIUS, a text generation model that operates based on conditional input in the form of sketches. GENIUS is designed to fill in the missing contexts in a given sketch. Additionally, the study demonstrates that GENIUS can serve as a powerful and readily applicable tool for data augmentation in various NLP tasks. InPars (Bonifacio et al., 2022), a method for generating synthetic training data for information retrieval tasks, utilizes LLMs in a fewshot manner. It generates one question per document by employing GPT-3's Curie model, while using the \"vanilla\" and \"guided by bad questions\" (GBQ) prompt templates. Khatri et al. (2022) illustrated that the capabilities of a substantial pre-trained transformer-based LM, such as GPT-2, can be effectively utilized to enrich limited datasets created by humans. This enhancement process is designed to preserve the original intent of the expanded utterances while also capturing various alternative expressions for the same intent. Consequently, this methodology leads to a notable improvement in the performance of chatbots driven by machine learning, enabling them to respond more accurately and diversely in conversational contexts. Quteineh et al. (2020) introduced a new method of data augmentation that leverages the guided outputs of a language generation model such as GPT-2 to enhance the performance of text classifiers through an active learning process, which aims to generate synthetic data as unlabeled data that is required by active learning algorithms. Liu et al. (2022) introduced a fresh method to create datasets by combining the efforts of human workers and AI. It began with an established dataset called MultiNLI, which is designed to deal with natural language inference. The proposed approach involves employing dataset cartography to automatically identify instances that exhibit complex reasoning patterns. These patterns serve as guidelines for instructing GPT-3 to generate new examples with similar characteristics. The generated examples are then filtered automatically and subsequently reviewed and labeled by human crowd workers.",
            "score": 0.45956215773529474,
            "section_title": "Generated content-based approaches",
            "char_start_offset": 45179,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2179
                },
                {
                    "start": 2180,
                    "end": 2296
                }
            ],
            "ref_mentions": [
                {
                    "start": 801,
                    "end": 821,
                    "matchedPaperCorpusId": "257959548"
                },
                {
                    "start": 1370,
                    "end": 1392,
                    "matchedPaperCorpusId": "226262334"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.814453125
        },
        {
            "corpus_id": "268667163",
            "title": "MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts",
            "text": "In recent years, LLMs have achieved impressive performance on several question answering and language understanding tasks when provided with appropriate prompting (Brown et al., 2020).However, complex reasoning abilities often present a challenge for these models.SemEval-2024 Task 9 (Jiang et al., 2024b) introduces a novel dataset called BrainTeaser (Jiang et al., 2023) which includes a set of complex puzzles and brainteasers.Such tasks involve solving word and sentence puzzles, which require multi-step inference and deduction.The dataset covers a diverse range of puzzle types including sequences, analogies, classification, mathematical reasoning, inferences about implicit relationships, and more.Solutions frequently demand a chained application of knowledge and logic across multiple steps to uncover insights or concepts not directly stated in the problem description.\n\nSolving these elaborate reasoning problems is a challenging scenario for NLP systems.We explore whether and how LLMs can succeed on this task.We employ proprietary models such as GPT-4 (OpenAI, 2023) and Claude 2.1 (Anthropic, 2023) through APIs.These models have shown promising few-shot reasoning ability.We also use Mixtral (Jiang et al., 2024a), an open-source LLM that shows state-of-the-results in several language reasoning tasks.The prompting paradigm involves providing models with natural language descriptions that encode the reasoning process step-bystep (Liu et al., 2021).We implement various prompting approaches for mapping puzzles to conditional text and systematically transforming reasoning into explanation chains.Our core method, chain-of-thought prompting (Wei et al., 2022), iteratively breaks down the deduction into simplified logical steps.\n\nExperiments reveal that while zero-shot performance lags due to a lack of grounding, multi-step prompts can unlock substantial reasoning ability in models.Performance improves with more steps and more specificity in the prompt.While introducing few-shot prompting generates good results, we observed that models do significantly better with chain-of-thought prompting.We experiment with several chains of thought and achieve mostly similar results with each attempt.To make a more empirically confident guess towards solving the puzzles we adopt an ensemble of these chains based on majority voting.",
            "score": 0.45910352423050405,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 264
                },
                {
                    "start": 264,
                    "end": 430
                },
                {
                    "start": 430,
                    "end": 533
                },
                {
                    "start": 533,
                    "end": 706
                },
                {
                    "start": 706,
                    "end": 880
                },
                {
                    "start": 882,
                    "end": 967
                },
                {
                    "start": 967,
                    "end": 1024
                },
                {
                    "start": 1024,
                    "end": 1128
                },
                {
                    "start": 1128,
                    "end": 1189
                },
                {
                    "start": 1189,
                    "end": 1319
                },
                {
                    "start": 1319,
                    "end": 1468
                },
                {
                    "start": 1468,
                    "end": 1616
                },
                {
                    "start": 1616,
                    "end": 1748
                },
                {
                    "start": 1750,
                    "end": 1905
                },
                {
                    "start": 1905,
                    "end": 1977
                },
                {
                    "start": 1977,
                    "end": 2118
                },
                {
                    "start": 2118,
                    "end": 2216
                },
                {
                    "start": 2216,
                    "end": 2349
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 183,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 284,
                    "end": 305,
                    "matchedPaperCorpusId": "269362111"
                },
                {
                    "start": 352,
                    "end": 372,
                    "matchedPaperCorpusId": "263830212"
                },
                {
                    "start": 1449,
                    "end": 1467,
                    "matchedPaperCorpusId": "231632658"
                },
                {
                    "start": 1660,
                    "end": 1678,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50146484375
        },
        {
            "corpus_id": "270766882",
            "title": "MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thought Prompts",
            "text": "In recent years, LLMs have achieved impressive performance on several question answering and language understanding tasks when provided with appropriate prompting (Brown et al., 2020).However, complex reasoning abilities often present a challenge for these models.SemEval-2024 Task 9 (Jiang et al., 2024b) introduces a novel dataset called BrainTeaser (Jiang et al., 2023) which includes a set of complex puzzles and brainteasers.Such tasks involve solving word and sentence puzzles, which require multi-step inference and deduction.The dataset covers a diverse range of puzzle types including sequences, analogies, classification, mathematical reasoning, inferences about implicit relationships, and more.Solutions frequently demand a chained application of knowledge and logic across multiple steps to uncover insights or concepts not directly stated in the problem description.\n\nSolving these elaborate reasoning problems is a challenging scenario for NLP systems.We explore whether and how LLMs can succeed on this task.We employ proprietary models such as GPT-4 (OpenAI, 2023) and Claude 2.1 (Anthropic, 2023) through APIs.These models have shown promising few-shot reasoning ability.We also use Mixtral (Jiang et al., 2024a), an open-source LLM that shows state-of-the-results in several language reasoning tasks.The prompting paradigm involves providing models with natural language descriptions that encode the reasoning process step-bystep (Liu et al., 2021).We implement various prompting approaches for mapping puzzles to conditional text and systematically transforming reasoning into explanation chains.Our core method, chain-of-thought prompting (Wei et al., 2022), iteratively breaks down the deduction into simplified logical steps.\n\nExperiments reveal that while zero-shot performance lags due to a lack of grounding, multi-step prompts can unlock substantial reasoning ability in models.Performance improves with more steps and more specificity in the prompt.While introducing few-shot prompting generates good results, we observed that models do significantly better with chain-of-thought prompting.We experiment with several chains of thought and achieve mostly similar results with each attempt.To make a more empirically confident guess towards solving the puzzles we adopt an ensemble of these chains based on majority voting.",
            "score": 0.45904015904978734,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 264
                },
                {
                    "start": 264,
                    "end": 430
                },
                {
                    "start": 430,
                    "end": 533
                },
                {
                    "start": 533,
                    "end": 706
                },
                {
                    "start": 706,
                    "end": 880
                },
                {
                    "start": 882,
                    "end": 967
                },
                {
                    "start": 967,
                    "end": 1024
                },
                {
                    "start": 1024,
                    "end": 1128
                },
                {
                    "start": 1128,
                    "end": 1189
                },
                {
                    "start": 1189,
                    "end": 1319
                },
                {
                    "start": 1319,
                    "end": 1468
                },
                {
                    "start": 1468,
                    "end": 1616
                },
                {
                    "start": 1616,
                    "end": 1748
                },
                {
                    "start": 1750,
                    "end": 1905
                },
                {
                    "start": 1905,
                    "end": 1977
                },
                {
                    "start": 1977,
                    "end": 2118
                },
                {
                    "start": 2118,
                    "end": 2216
                },
                {
                    "start": 2216,
                    "end": 2349
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 183,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 284,
                    "end": 305,
                    "matchedPaperCorpusId": "269362111"
                },
                {
                    "start": 352,
                    "end": 372,
                    "matchedPaperCorpusId": "263830212"
                },
                {
                    "start": 1449,
                    "end": 1467,
                    "matchedPaperCorpusId": "231632658"
                },
                {
                    "start": 1660,
                    "end": 1678,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.498046875
        },
        {
            "corpus_id": "267770192",
            "title": "Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text",
            "text": "Multi-step reasoning is a challenging NLP task that requires a system to make multiple inference steps to answer a question. While LLMs exhibit strong capabilities in one-hop inference, they struggle to perform effectively in multi-step reasoning. Numerous strategies have been suggested to enhance the multi-step reasoning capabilities of LLMs, such as implementing step-by-step reasoning using few-shot examples. Unlike \"naive\" prompting, which expects that the input should be immediately followed by the output or answer, eliciting prompts direct LLMs to tackle tasks by guiding them through intermediate steps before making predictions for the final output or answer. This method, known as chain-of-thought (CoT) [21,28], has demonstrated that elicitive prompting equips LMs with superior reasoning abilities in a few-shot setting. Later, Zero-Shot-CoT [11] presented similar capabilities in a zero-shot setting. They simply prepended the input question with the phrase \"Let's think step by step\" before querying the model, and showed that large LMs performed well in zero-shot-CoT on reasoning tasks like GSM8K, though not as proficiently as in few-shot-CoT. Least to Most prompting (LtM) [35] takes CoT prompting a step further by first breaking a problem into sub problems and then proceeds to solve each one independently. These sub-question answers are then synthesized to obtain the final response. Additionally, Tree of Thoughts (ToT) [32] and Graph of Thoughts (GoT) [1] use complex structures like trees and graphs to organize thoughts. These systems combine the way LLMs generate thoughts with search algorithms for systematic exploration, further enhancing their multi-step reasoning capabilities. In contrast to all these approaches, our work introduces a distinct three-step prompting framework. This framework emulates the problem-solving approach employed by humans when dealing with data rich in relationships. It enables users to transform a natural language paragraph into a graph, and subsequently, based on the query type, navigate this graph for the purpose of answering questions.",
            "score": 0.45891036169013155,
            "section_title": "RELATED WORKS 2.1 Multi-step Reasoning with LLMs",
            "char_start_offset": 6818,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2107
                }
            ],
            "ref_mentions": [
                {
                    "start": 722,
                    "end": 725,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 858,
                    "end": 862,
                    "matchedPaperCorpusId": "249017743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55712890625
        },
        {
            "corpus_id": "253370208",
            "title": "Teaching Broad Reasoning Skills for Multi-Step QA by Generating Hard Contexts",
            "text": "Despite large LMs' impressive reading abilities and the availability of large scale multi-step QA datasets requiring a rich set of reasoning skills, LMbased QA models do not reliably learn to use such skills for answering complex questions. In this work, we show that the greater control that synthetic contexts offer can be leveraged to create a teaching dataset where models can learn a broad range of reasoning skills in a reliable manner, especially for more complex questions. \n\nOur transfer results from synthetic data to ac-  tual QA datasets add to the growing line of work that shows synthetic datasets can in fact be used to inject useful skills are valuable for real, natural language tasks. Given the artifact issues in real datasets (specifically, in their contexts) and the difficulty in controlling for them via perturbations, we present a viable alternative: leveraging existing multi-step questions for their broad reasoning patterns but using synthetic contexts for carefully constructing teaching datasets, where models can learn the right way to reason.",
            "score": 0.45874844972740453,
            "section_title": "Conclusions",
            "char_start_offset": 29310,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 1073
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93310546875
        },
        {
            "corpus_id": "277501853",
            "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
            "text": "The R * (Qi et al., 2024) algorithm is a principled approach to improving language model response generation through Monte Carlo Tree Search (MCTS). When presented with a user query, R * systematically explores diverse reasoning pathways to generate high-quality, well-reasoned responses by leveraging specialized reasoning strategies. This framework empowers language models to engage in structured thinking, logical inference, and multi-step problem-solving, enhancing capabilities such as counterfactual and causal reasoning, and multi-step question answering within RAG settings. We formulate response generation as a search process through a tree of reasoning states. In this formulation, let Q be the set of all possible user queries (input questions), S be the set of intermediate reasoning states (natural language reasoning steps), A be the finite set of predefined reasoning actions {A 1 , A 2 , A 3 , A 4 , A 5 } (reasoning strategies), and N be the set of nodes in the MCTS tree, where each node n \u2208 N corresponds to a state s \u2208 S. Given a user query q \u2208 Q, R * generates a response by performing multiple rollouts through a dynamically constructed reasoning tree. The process begins with a selection phase where, at each decision point, actions are selected using the Upper Confidence bound for Trees (UCT) to balance exploration and exploitation:",
            "score": 0.45857092549645817,
            "section_title": "D.11. R * Algorithm",
            "char_start_offset": 126174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1360
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4912109375
        },
        {
            "corpus_id": "258558899",
            "title": "Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing",
            "text": "Data synthesis plays an important role in semantic parsing, especially in dataset construction and model training. Many datasets are constructed by first generating synthetic data by rules and then manually rewriting the questions for diverse expressions (Wang et al., 2015;Cao et al., 2022;Gu et al., 2021). Since the generation of synthetic data is controllable, researchers can restrict the distribution of logical forms in order to examine or improve various generalization capabilities of models, such as compositional generalization and scalability (Lake and Baroni, 2018;Gu et al., 2021;Oren et al., 2021). In many scenarios, where manually rewriting is laborious or unavailable, training on synthetic data can also alleviate the problems of cold-start and privacy (Yang et al., 2022b;Yu et al., 2021). Limited by stereotyped expressions, semantic parsers trained with only synthetic data are difficult to generalize to diverse natural questions. Wu et al. (2021a) and Xu et al. (2020c) automatically rewrite synthetic questions with paraphrasing models, which introduces various phrase-level expressions but change little in sentence structures. Instead of rule-based synthesis, Zhong et al. (2020) and Yang et al. (2021) train a neural data synthesizer with natural data from other domains, and then generate training data for the target domain in a way of domain transfer. Such methods heavily rely on multi-domain resources, and thus their applicability is limited. There are some other studies attempting to bridge the data gap through mapping synthetic and natural questions into a common embedding space. Berant and Liang (2014) and Marzoev et al. (2020) leverage sentence embeddings or other paraphrase detection technologies to measure the semantic similarity between the input question and candidate logical forms. However, existing sentence embedding methods are poor at accurately capturing the semantic meanings of complex sentences, making them unsuitable for some challenging semantic parsing tasks (e.g., KQA [Cao et al., 2022]).",
            "score": 0.4582782095995556,
            "section_title": "Data Synthesis for Semantic Parsing",
            "char_start_offset": 4488,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 255,
                    "end": 274,
                    "matchedPaperCorpusId": "14472576"
                },
                {
                    "start": 274,
                    "end": 291,
                    "matchedPaperCorpusId": "247362971"
                },
                {
                    "start": 291,
                    "end": 307,
                    "matchedPaperCorpusId": "226965153"
                },
                {
                    "start": 565,
                    "end": 578,
                    "matchedPaperCorpusId": "46761158"
                },
                {
                    "start": 578,
                    "end": 594,
                    "matchedPaperCorpusId": "226965153"
                },
                {
                    "start": 594,
                    "end": 612,
                    "matchedPaperCorpusId": "237420426"
                },
                {
                    "start": 772,
                    "end": 792,
                    "matchedPaperCorpusId": "248779994"
                },
                {
                    "start": 792,
                    "end": 808,
                    "matchedPaperCorpusId": "226965153"
                },
                {
                    "start": 954,
                    "end": 971,
                    "matchedPaperCorpusId": "235417352"
                },
                {
                    "start": 976,
                    "end": 993,
                    "matchedPaperCorpusId": "221819674"
                },
                {
                    "start": 1187,
                    "end": 1206,
                    "matchedPaperCorpusId": "221739233"
                },
                {
                    "start": 1619,
                    "end": 1642,
                    "matchedPaperCorpusId": "1336493"
                },
                {
                    "start": 2032,
                    "end": 2050,
                    "matchedPaperCorpusId": "247362971"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89404296875
        },
        {
            "corpus_id": "264439655",
            "title": "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning",
            "text": "While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our dataset instances are free text narratives corresponding to real-world domains of reasoning; this makes it simultaneously much more challenging than other synthetically-crafted benchmarks while remaining realistic and tractable for human annotators to solve with high accuracy. We evaluate a range of LLMs and prompting techniques on this dataset and characterize the gaps that remain for techniques like chain-of-thought to perform robust reasoning.",
            "score": 0.458194625641725,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "277043899",
            "title": "Empowering Time Series Analysis with Synthetic Data: A Survey and Outlook in the Era of Foundation Models",
            "text": "Synthetic data generation can supply vast amounts of artificial data for model training and evaluation, playing an increasingly vital role in the foundation model era. The generation of synthetic time series can be broadly classified into three categories [3,75]. Statistical and simulator-based approaches generate time series through predefined rules or simulation environments, respectively, allowing for controlled and interpretable data creation. In contrast, data-driven methods leverage historical data and generative models, including GANs [21,102], VAEs [17,49], diffusion models [73,85] or LLMs [1,101], to learn complex temporal patterns and produce more realistic time series. Earlier methods to text generation primarily used statistical language models to model the conditional probabilities of words based on an n-gram context. However, data-driven methods, particularly LLMs, have since become the dominant approach [50].",
            "score": 0.45732318312132936,
            "section_title": "Generation of Synthetic Data",
            "char_start_offset": 6876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 937
                }
            ],
            "ref_mentions": [
                {
                    "start": 259,
                    "end": 262,
                    "matchedPaperCorpusId": "258822880"
                },
                {
                    "start": 552,
                    "end": 556,
                    "matchedPaperCorpusId": "202774781"
                },
                {
                    "start": 589,
                    "end": 593,
                    "matchedPaperCorpusId": "268249121"
                },
                {
                    "start": 593,
                    "end": 596,
                    "matchedPaperCorpusId": "235765577"
                },
                {
                    "start": 932,
                    "end": 936,
                    "matchedPaperCorpusId": "260435365"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6865234375
        },
        {
            "corpus_id": "263605725",
            "title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models",
            "text": "Language models (LMs) have been actively studied these days for logical reasoning tasks. Dasgupta et al. (2022) demonstrates that large language mdoels (LLMs) show human-level abstract reasoning skill. Creswell et al. (2022) proposes a selection-inference pipeline that given a context and question, the model can firstly select which facts or rules given in the context are important to answer the question and decompose the question into step by step reasoning. Wei et al. (2022) demonstrates that language models have the capacity to engage in chain-of-thoughts (CoT) reasoning. This approach facilitates a step-by-step reasoning process that enhances the performance of the model in downstream tasks such as mathematical reasoning. In following section, we summarize the five prevalent trends in utilizing language models for logical reasoning over language.",
            "score": 0.45729342873928325,
            "section_title": "Language Models for Logical Reasoning over Natural Language",
            "char_start_offset": 13386,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 862
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 224,
                    "matchedPaperCorpusId": "248887351"
                },
                {
                    "start": 464,
                    "end": 481,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5380859375
        },
        {
            "corpus_id": "276161201",
            "title": "Evaluating Inter-Column Logical Relationships in Synthetic Tabular Data Generation",
            "text": "We introduce three metrics-HCS, MDI, and DSI-for evaluating inter-column logical relationship in synthetic tabular data generation. Our experiments show that existing methods often fail to strictly maintain hierarchical consistency and multivariate dependencies-essential characteristics of realworld datasets. Our future work will focus on enhancing the preservation of inter-column logical relationships in synthetic tabular data generation. For LLM-based methods, the column serialization format and order are crucial for the model's ability to learn the joint distribution of logically related features. Knowledge graphs (Dong & Wang, 2024) or Bayesian networks (Ling et al., 2024) would be employed to reorder tokenization sequences or restructure the serialization of columns in natural language, leveraging prior knowledge to guide the synthetic tabular data generation. For latent space-based methods, LLM reasoning (Hegselmann et al., 2023;Dong & Wang, 2024) can be utilized to analyze column names and descriptions, identifying semantic or logical relationships without prior knowledge. Additionally, inspired by CTSyn (Lin et al., 2024), grouping data by logical relationships and embedding them into a shared latent space could potentially capture inherent structures, improving joint distribution modeling. Lastly, incorporating interpolation techniques like SMOTE may help can help balance data classes (Yang et al., 2024), particularly in learning minority logical relationships. These directions are worthy to explore for designing generative models that effectively capture inter-column logical relationships in synthetic tabular data generation.",
            "score": 0.457061236132535,
            "section_title": "CONCLUSION AND RESEARCH DIRECTIONS",
            "char_start_offset": 7572,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1663
                }
            ],
            "ref_mentions": [
                {
                    "start": 625,
                    "end": 644,
                    "matchedPaperCorpusId": "271114498"
                },
                {
                    "start": 924,
                    "end": 949,
                    "matchedPaperCorpusId": "252992811"
                },
                {
                    "start": 949,
                    "end": 966,
                    "matchedPaperCorpusId": "271114498"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85107421875
        },
        {
            "corpus_id": "274638079",
            "title": "TinyThinker: Distilling Reasoning through Coarse-to-Fine Knowledge Internalization with Self-Reflection",
            "text": "Knowledge distillation transfers knowledge from a larger teacher model to a smaller student model (Gou et al., 2021), enabling the student to leverage the features learned by the teacher (Hinton, 2015). As LLMs advance in generating highquality data, this process has been extended to transfer reasoning capabilities via teacher-generated data. Recent research in this area can be categorized into five main approaches: supervised finetuning, decomposer-solver framework, feedbackbased framework, retriever-augmented framework, and self-improvement framework. \n\nSupervised fine-tuning This method trains the student model exclusively on teacher-generated reasoning data, typically employing a language modeling objective. It has become a standard approach for reasoning distillation (Ho et al., 2023;Magister et al., 2023;Fu et al., 2023;Wang et al., 2023a;Li et al., 2023). Alternatively, reasoning generation is combined with direct answer prediction within a multi-task learning framework, which has been shown to yield further performance improvements (Hsieh et al., 2023;Chen et al., 2024;Li et al., 2024a). However, these methods rely solely on teacher-generated data and lack an explicit focus on the integration of reasoning with underlying knowledge. \n\nDecomposer-solver framework This framework trains two student models: one responsible for decomposing complex questions into simpler subquestions, and another tasked with solving these sub-questions. The approach can be implemented in a single-step process, where both decomposition and solution occur in one iteration (Shridhar et al., 2023), or as a turn-based approach, where decomposition and solving are repeated iteratively until a complete solution is reached (Han et al., 2023). While this method effectively simplifies complex reasoning questions, it primarily focuses on semantic decomposition. \n\nFeedback-based framework In this framework, the student model is initially trained to generate reasoning processes. When the student produces an incorrect answer, the associated reasoning process is sent back to the teacher, which provides feedback on the errors.",
            "score": 0.45692438797301727,
            "section_title": "Related Work",
            "char_start_offset": 4798,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1259
                },
                {
                    "start": 1262,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1866
                },
                {
                    "start": 1869,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2132
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 116,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 783,
                    "end": 800,
                    "matchedPaperCorpusId": "254877399"
                },
                {
                    "start": 800,
                    "end": 822,
                    "matchedPaperCorpusId": "254823156"
                },
                {
                    "start": 822,
                    "end": 838,
                    "matchedPaperCorpusId": "256390607"
                },
                {
                    "start": 838,
                    "end": 857,
                    "matchedPaperCorpusId": "258461058"
                },
                {
                    "start": 857,
                    "end": 873,
                    "matchedPaperCorpusId": "259251773"
                },
                {
                    "start": 1056,
                    "end": 1076,
                    "matchedPaperCorpusId": "258461606"
                },
                {
                    "start": 1076,
                    "end": 1094,
                    "matchedPaperCorpusId": "268253497"
                },
                {
                    "start": 1581,
                    "end": 1604,
                    "matchedPaperCorpusId": "258762841"
                },
                {
                    "start": 1729,
                    "end": 1747,
                    "matchedPaperCorpusId": "263829391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2486572265625
        },
        {
            "corpus_id": "271544102",
            "title": "CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large Language Models over Factual Knowledge",
            "text": "The goal of this work is to comprehensively evaluate the complex logical reasoning capabilities of large language models when combining and reasoning over factual knowledge from both general domains and specific domains. While previous work has focused on assessing LLMs' memorization of simple facts, there has been less exploration into how well LLMs can synthesize and reason with those facts in complex ways involving logical operations like intersections, unions, negations, and multi-hop reasoning. \n\nFormally, we aim to construct a benchmark covering a diverse set of complex logical reasoning questions over knowledge graphs. Given a knowledge graph G containing factual triplets (head, relation, tail), the task is to generate natural language questions q involving multi-step logical operations and constraints over the entities and relations in G. Then, for a given LLM M , we query M with q and evaluate whether M can provide a set of correct answers A, which is the resulting entity set after applying the specified logical operations to the relevant subsets of G's entities and relations mentioned in q. This process is demonstrated in Figure 1.",
            "score": 0.4565251000452089,
            "section_title": "CLR-Fact Evaluation Framework",
            "char_start_offset": 11078,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1159
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5400390625
        },
        {
            "corpus_id": "253097865",
            "title": "Entailer: Answering Questions with Faithful and Truthful Chains of Reasoning",
            "text": "Systematic Reasoning: Several recent systems have demonstrated the ability to perform systematic reasoning directly over natural language (Natural Language Inference (Manning and MacCartney, 2009)), namely deriving conclusions from known facts via step-wise application of well-defined inference operations. One approach is to retrain a blackbox model end-to-end (Clark et al., 2020), but has been limited to small rulebases. An alternative approach, which we follow here, is to have an outside loop around a model, where the model generates individual inference steps (i.e., rules), and a con-troller chains them together. SCSearch (Bostrom et al., 2022), NLProofS (Yang et al., 2022), IRGR (Ribeiro et al., 2022), ProofWriter (iterative version) (Tafjord et al., 2020), and Selection-Inference (Creswell et al., 2022) do this in a forward-chaining fashion, MetGen (Hong et al., 2022) does this bidirectionally, while Braid (Kalyanpur et al., 2020) (like us) does this in a backward-chaining fashion. In all these systems, the required facts were expected to be provided explicitly to the model. In contrast, Entailer's reasoning uses its own internal, latent knowledge, as well as (optionally) externally provided facts. LeapOfThought (Talmor et al., 2020) demonstrated that reasoning with a combination of implicit and explicit knowledge was possible for simple 1-step inferences. We expand this for multi-step inference, and (unlike LeapOfThought) have the system also explicitly articulate the implicit knowledge it uses, and its chain of reasoning. \n\nRecent work has shown that generating a freeform explanation (\"chain of thought\") before an answer can also improve performance on a variety of tasks (Wei et al., 2022;Cobbe et al., 2021;Nye et al., 2021). In these works, however, the explanations are unstructured, and there are no claims of faithfulness that the answer follows from the generation, nor that the explanations themselves represent model beliefs.",
            "score": 0.456513114152974,
            "section_title": "Related Work",
            "char_start_offset": 4711,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1554
                },
                {
                    "start": 1557,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1969
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 383,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 1237,
                    "end": 1258,
                    "matchedPaperCorpusId": "263869790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.283203125
        },
        {
            "corpus_id": "277622127",
            "title": "Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use",
            "text": "This granular approach enables us to apply direct feedback after each step of the trajectory, and to do so in a manner that is contextually aware. Unlike prior RL finetuning approaches used in frontier open-source models like DeepSeek-R1 (DeepSeek-AI and others, 2025) and Llama-3 (Grattafiori et al., 2024), we do not solely optimize for final performance, and use no golden labels; however, by optimizing for the reasonableness of each step given prior steps, SWiRL does in fact improve final performance. \n\nIn addition to evaluating SWiRL on challenging multi-hop question-answering and mathematical problem-solving tasks, we also study the generalization properties of this methodology. This is of key interest because there is an explosion of agentic applications for language models, and methods that generalize across datasets and tasks will be easier, cheaper and faster to adapt to new environments. We also measure the effectiveness of different synthetic data filtering strategies, study SWiRL's ability to generalize across datasets and tasks, measure the impact of model size and dataset size, and explore the mechanism driving these performance improvements. \n\nOur contributions are as follows: \n\n\u2022 We propose Step-Wise Reinforcement Learning (SWiRL), an approach to synthetic data generation and offline RL that advances multi-step reasoning and tool use.",
            "score": 0.4562921990812321,
            "section_title": "Introduction",
            "char_start_offset": 3505,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1208
                },
                {
                    "start": 1211,
                    "end": 1370
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69091796875
        },
        {
            "corpus_id": "273850363",
            "title": "Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge Reasoning and Text Generation",
            "text": "In this experiment, we used a generative model based on ChatGPT-4 as the core part of the experiment. ChatGPT-4 is a large-scale pre-trained language model developed by OpenAI. It is based on the Transformer architecture and can generate natural and coherent text [22]. ChatGPT-4 has strong language understanding and generation capabilities and can handle a wide range of text-generation tasks. However, in order to further improve the performance in knowledge-intensive and reasoning tasks, we combined the design of the RAG (Retrieval Augmented Generation) model to dynamically obtain relevant information from external knowledge bases through the retrieval module to generate more knowledge-accurate and relevant answers. This architecture can effectively make up for the problem that the generative model relies solely on training data, enabling it to handle knowledge reasoning and complex tasks in more fields. \n\nIn our experiments, we used the Natural Questions (NQ) dataset, which is widely used for retrieval and generation tasks. \n\nNQ is an open-domain question-answering dataset that contains real question queries from users, accompanied by relevant document snippets automatically retrieved from Internet resources such as Wikipedia. Each data sample consists of a query, a retrieved document, and an answer, covering a wide range of content in multiple fields such as history, science, and culture. The diversity of the NQ dataset makes it an ideal choice for evaluating the performance and reasoning ability of generative models on complex domain problems. This experiment uses this dataset to test the retrieval and generation capabilities of our model to evaluate the accuracy and knowledge completeness of its generated text. This experiment is run in a high-performance computing environment. The experimental hardware configuration includes NVIDIA A100 GPU, 128GB memory and 64-core CPU, which can support efficient training and reasoning of large-scale models. The experimental software is mainly based on the PyTorch framework for model training and evaluation, and combines Hugging Face's Transformers library to implement the retrieval and generation functions of the RAG model.",
            "score": 0.4554298923238729,
            "section_title": "A. Introduction to the dataset and the LLM used",
            "char_start_offset": 9711,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2203
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 268,
                    "matchedPaperCorpusId": "267703756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6923828125
        },
        {
            "corpus_id": "257766470",
            "title": "Natural Language Reasoning, A Survey",
            "text": "Recent progresses also show the potential to leverage PLMs on natural language reasoning, which exhibits their learning and generalization abilities of reasoning skills with both explicit and implicit knowledge. \n\nBy finetuning on the specific dataset, [25] first demonstrated that PLMs can perform deductive reasoning over explicitly provided natural language statements, which can zero-shot transfer to different domains. Moreover, [145] showed that PLMs can combine memorized implicit taxonomic and world knowledge with explicitly provided knowledge for the deduction. In addition to deduction, PLMs can also learn to perform defeasible reasoning [122,167,174]. \n\nWhile LLMs with in-context learning were once thought to be incapable of multi-step reasoning, it has been found that their capabilities of reasoning can be unlocked by generating forward reasoning paths before the final answer [158], which is called Chain-of-Thought (CoT) prompting. With this prompting, the performance of many multi-step reasoning tasks in Big-Bench Hard can surpass the average human rater. Furthermore, LLMs can perform multi-step reasoning not only with few-shot exemplars, [77] also found that they can automatically produce intermediate steps with a simple \"Let's think step by step\" prompting in a zero-shot manner. Surprisingly, LLMs can even learn from their self-generated reasoning paths [59,178]. Moreover, GP4 outperformed a majority of people on several realistic examinations such as Uniform Bar Exam which also require some reasoning. \n\nIn addition, to forward reasoning paths, question decomposition, a backward reasoning method, is also effective in multi-hop question answering, which is beneficial to both medium-size PLMs [97,102] and LLMs [102,107]. \n\nMoreover, while neural-based methods are blamed for black box prediction, [27,130] demonstrated that PLMs can produce faithful reasoning paths and make predictions based on them. \n\nIn conclusion, PLMs can learn to perform multi-step reasoning from supervised data or few-shot demonstrations. Their capabilities of natural language understanding, generalization, and leveraging implicit knowledge make them promising to deal with arbitrary natural language, commonsense knowledge and defeasible reasoning.",
            "score": 0.4552486631695707,
            "section_title": "Empirical Development",
            "char_start_offset": 23729,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 214,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1536
                },
                {
                    "start": 1539,
                    "end": 1757
                },
                {
                    "start": 1760,
                    "end": 1938
                },
                {
                    "start": 1941,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2264
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 257,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 650,
                    "end": 655,
                    "matchedPaperCorpusId": "226283602"
                },
                {
                    "start": 659,
                    "end": 663,
                    "matchedPaperCorpusId": "247619052"
                },
                {
                    "start": 1729,
                    "end": 1733,
                    "matchedPaperCorpusId": "174801080"
                },
                {
                    "start": 1838,
                    "end": 1842,
                    "matchedPaperCorpusId": "247594506"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.219970703125
        },
        {
            "corpus_id": "276422476",
            "title": "RM-PoT: Reformulating Mathematical Problems and Solving via Program of Thoughts",
            "text": "Program of Thoughts (PoT) is a method aimed at enhancing the reasoning capabilities of large language models (LLMs) Chen et al. (2022). It works by decomposing complex tasks into a series of intermediate steps, or \"thoughts\", which guide the model through a structured reasoning process. Each thought represents a logical step that incrementally leads to the final answer, enabling the model to tackle intricate problems using a step-by-step approach. \n\nThis method improves the model's ability to solve tasks that require multi-step reasoning, such as mathematical or logical problems, by fostering transparency in the reasoning process and increasing accuracy in the final result. PoT has proven effective in scenarios where direct answers are difficult, allowing LLMs to perform more reliably in problem-solving tasks. \n\nIn our proposed RM-PoT, we aim to instruct the LLM to generate intermediate Python code to solve mathematical problems. This approach can clearly show the reasoning process of LLM and improve accuracy.",
            "score": 0.4551954885389423,
            "section_title": "PROGRAM OF THOUGHTS(POT)",
            "char_start_offset": 8730,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 451
                },
                {
                    "start": 454,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 821
                },
                {
                    "start": 824,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1025
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34375
        },
        {
            "corpus_id": "259858789",
            "title": "Quantifying Train-Evaluation Overlap with Nearest Neighbors",
            "text": "NLP datasets are collected through diverse methods for multiple purposes -some datasets mirror the user-facing applications closely (e.g., question answering datasets and machine translation datasets), while other datasets are carefully designed for diagnostic purposes. With the rise of harder to interpret, high capacity models (Brown et al., 2020;Chowdhery et al., 2022), many datasets are designed to probe model qualities. Would different data collection method yield different level of train evaluation overlap? To investigate this, we first categorize the data collection method of datasets below. We propose a discrete scale of naturalness, from purely synthetic to user-generated, as follows: \n\n\u2022 Synthetic (SYN): template-generated or highly-constrained crowd-sourced text. Here, both inputs and outputs are synthetically generated. We note that our definition of synthetic data includes highly-constrained crowd-sourced text, by which we mean that the annotators have limited freedom in the content of their annotations. For example, for the WinoGrande dataset workers are instructed to choose an anchor word to use in the twin sentences, they are given a range for sentence length, and they are asked to maintain 70% overlap between sentences. This is less natural than what the human might have generated on their own. \n\nWe provide examples of the datasets of each type we study here, approximately ordered from the least to most natural datasets. \n\nWinoGrande A crowd-sourced, commonsense reasoning benchmark inspired by the Winograd Schema Challenge, in which twin sentences with a small edit distance each have a missing word and two possible options (Sakaguchi et al., 2021). CSQA 2.0 (Commonsense Question Answering 2.0) A corpus of crowdsourced yes/no commonsense reasoning questions (e.g., \"a playing card is capable of cutting soft cheese?\") (Talmor et al., 2021). ANLI (Adversarial NLI) A natural language inference corpus with data collected \"adversarially\" in three rounds using a human-in-the-loop approach (Nie et al., 2020).",
            "score": 0.45497578797562455,
            "section_title": "Categorizing Dataset Collection Method",
            "char_start_offset": 6053,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 701
                },
                {
                    "start": 704,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1331
                },
                {
                    "start": 1334,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2051
                }
            ],
            "ref_mentions": [
                {
                    "start": 330,
                    "end": 350,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1667,
                    "end": 1691,
                    "matchedPaperCorpusId": "199370376"
                },
                {
                    "start": 1863,
                    "end": 1884,
                    "matchedPaperCorpusId": "237263476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.402099609375
        },
        {
            "corpus_id": "271088560",
            "title": "Nash CoT: Multi-Path Inference with Preference Equilibrium",
            "text": "Large Language Models (LLMs) have profoundly revolutionized the field of Natural Language Processing (NLP) (Ouyang et al., 2022;etc., 2023;Jiang et al., 2023;Brown et al., 2020b;OpenAI, 2024). Specifically, leveraging human-designed instructions as input, LLMs demonstrate superior inference performance across various types of simple reasoning tasks (Radford et al., 2019;Brown et al., 2020a). \n\nHowever, in complex tasks, direct reasoning LLMs do not yield good results (Rae et al., 2022). To improve LLMs' inference performance on complex inference tasks, it is popular to adopt a stepby-step reasoning framework known as Chain-of-Thought (CoT) prompting (Wei et al., 2023). For example, when appending templates that can guide LLMs to perform step-by-step reasoning, such as \"Let's think step by step\", to the end of given question. We can lead the LLMs to output questionrelated rationale and further obtain the answer, enabling LLMs to achieve better performance than zero-shot reasoning. Subsequently, among various CoT-related improvements, modified by utilizing multi-path inference which is represented by Wang et al. (2023) is one of the most effective reasoning approaches. However, there is no theoretical proof identifying an optimal number of paths for multipath reasoning. To ensure better inference performance, it is imperative to augment the inference paths. However, this might burden the inference budgets. \n\nIn order to reduce the dependency of multi-path inference on the number of inference paths while further enhancing its effectiveness, we can cast LLMs into a relevant role for a specific question. For instance, we can utilize a template such as \"You're a mathematician, and \u2022 \u2022 \u2022 \" to cast the LLM in the role of a mathematician. This approach can improve the accuracy of each path's answer generated by the LLM, thereby reducing the multi-path CoT's reliance on the number of paths, potentially improving the LLM's reasoning. \n\nHowever, excessively casting the LLM into a specific role using templates may compromise its robustness (diversity), thereby adversely affecting",
            "score": 0.4548995959575045,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 394
                },
                {
                    "start": 397,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1427
                },
                {
                    "start": 1430,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1956
                },
                {
                    "start": 1959,
                    "end": 2103
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 178,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 373,
                    "end": 393,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32861328125
        },
        {
            "corpus_id": "274656414",
            "title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning",
            "text": "Large Language Models (LLMs) have revolutionized natural language processing by demonstrating remarkable capabilities across a wide range of language tasks. By leveraging vast datasets and complex architectures, LLMs such as ChatGPT (Kojima et al., 2022;Achiam et al., 2023) and LLaMA (Touvron et al., 2023) can generate coherent essays, answer complex questions, and engage in multi-turn dialogues with human-like fluency. These models excel at tasks requiring not only linguistic understanding but also basic reasoning, such as translating text, summarizing lengthy documents, and generating code from plain language instructions. The versatility and adaptability of LLMs have made them invaluable tools in both industry and research, opening up new avenues for solving generalpurpose problems. \n\nEnabling LLMs to successfully solve complex reasoning problems remains a challenge. A series of works have been proposed to introduce more inference during testing based on a well-trained LLM (Wei et al., 2022;Yao et al., 2024;Snell et al., 2024;OpenAI, 2024). Chainof-Thought (CoT) (Wei et al., 2022) provides a few chainof-thought demonstrations in prompting as exemplars to enhance the reasoning abilities of LLMs. Tree-of-Thought (ToT) (Yao et al., 2024) allows language models to explore multiple reasoning paths and self-evaluate to make more globally informed decisions. Graph-of-Thought (GoT) (Besta et al., 2024b) advances LLM prompting by structuring information as a graph of interconnected \"thoughts\", enabling synergistic reasoning and feedback loops. \n\nThese methods perform reasoning by using richer prompts or by decomposing a complex problem into several simpler sub-problems. However, they typically perform only a single, complete reasoning pass on the problem, which does not guarantee that the problem will be solved correctly. For example, in a complex mathematical word problem, a Treeof-Thought approach might decompose the problem into smaller steps, such as isolating terms or simplifying expressions. However, while breaking down the problem, it may still overlook critical details or make errors in intermediate steps, leading to an incorrect final answer.",
            "score": 0.45465708198576915,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1563
                },
                {
                    "start": 1566,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2183
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 254,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 991,
                    "end": 1009,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1009,
                    "end": 1026,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 1026,
                    "end": 1045,
                    "matchedPaperCorpusId": "59031327"
                },
                {
                    "start": 1082,
                    "end": 1100,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1239,
                    "end": 1257,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 1400,
                    "end": 1421,
                    "matchedPaperCorpusId": "261030303"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69189453125
        },
        {
            "corpus_id": "250390686",
            "title": "GraDA: Graph Generative Data Augmentation for Commonsense Reasoning",
            "text": "Recent work has seen the emergence of several datasets for improving commonsense reasoning of language models through tasks like question answering (QA) (Sap et al., 2019b;Talmor et al., 2019;Bisk et al., 2020) and natural language inference (Bhagavatula et al., 2020;Zellers et al., 2019;Sakaguchi et al., 2020). Some of these datasets are based on existing knowledge graphs that represent different aspects of commonsense through entities and relations. For example, annotators for SocialIQA (Sap et al., 2019b) were shown an event from the inferential knowledge graph ATOMIC (Sap et al., 2019a) and instructed to turn it into a sentence by adding names, filling placeholders and adding context, etc. For multiple-choice QA datasets, annotators are also instructed to write distractor choices for each question. These useful datasets are collected through a time-taking and money-intensive crowdsourcing process which is hard to scale. Large pretrained models like GPT2 (Radford et al., 2018) can be finetuned to generate sentences from narrow data distributions, and it has recently been leveraged to augment datasets for text classification (Anaby-Tavor et al., 2020) and question answering (Puri et al., 2020;Yang et al., 2020). However, it is challenging to generate augmentation data for commonsense reasoning because the generated questions and answers (referred to as \"synthetic\" in rest of the paper) need to depict plausible real-world scenarios accurately. Hence, we develop GRADA, a graph-based generative data augmentation framework to generate synthetic samples from existing knowledge graphs that encode information about the real world. We focus on generating synthetic samples for models that perform discriminative and generative commonsense question answering.\n\nEach sample in commonsense reasoning datasets comprises a question which describes a real-world scenario and can be mapped to a set of predefined entities and relations from knowledge bases like ConceptNet and ATOMIC. For instance, the question \"Besides a mattress, name something people sleep on.\" from the ProtoQA dataset (Boratko et al",
            "score": 0.45400762385614113,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91162109375
        },
        {
            "corpus_id": "264128006",
            "title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
            "text": "Logical reasoning with natural language. Tapping into logical reasoning capabilities represents a holistic endeavour in natural language understanding (NLU). A variety of methods have been explored to realize this objective, including symbolic systems [45,47,55], fine-tuning of language models [28,41,71,78], and hybrid approaches combining neural and symbolic elements [36,59,60]. \n\nThe recent introduction of evaluation datasets, notably LogiQA [43] and Reclor [80], has reinvigorated the focus on logical reasoning in NLP research. Logical reasoning is now leveraged in numerous probing tasks over large Pretrained Language Models (PLMs) and applied to downstream tasks such as question-answering and dialogue systems [6,63]. Despite these advancements, the aspiration to emulate human-like logical reasoning capabilities within NLU systems remains a significant challenge for traditional models [27,43]. In this study, our goal is not only to quantitatively evaluate the capability of Large Language Models (LLMs) in addressing the previously mentioned challenge but also to underscore the significance of our work in providing a validated platform for enhancing various reasoning methods with our data. \n\nLLM reasoning evaluation. Despite progress in evaluating LLMs for specific reasoning tasks like arithmetic [57] and commonsense [4], a yawning gap exists in comprehensively assessing their logical reasoning. While LLMs excel at specific tasks like arithmetic reasoning [57], they face challenges in complex areas like multi-step reasoning [23] and abstract scenarios [24]. ChatGPT exhibits strengths in chat-specific reasoning and some commonsense domains [4,53], but struggles with tasks requiring longer chains of inference [4]. Other LLMs like FLAN-T5 [12], LLaMA [67], and PaLM [2] show potential in general deductive reasoning [61], while InstructGPT and Codex excel in specialized domains like medical reasoning [38]. Despite these advances, limitations in data bias [52], and complex reasoning tasks necessitate further research and optimization to fully unlock the reasoning potential of LLMs [77]. \n\nBig-Bench Hard (BBH) [64] isolates 23 most challenging tasks from BIG-Bench [3].",
            "score": 0.453586653076387,
            "section_title": "Related Work",
            "char_start_offset": 3618,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1208
                },
                {
                    "start": 1211,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2117
                },
                {
                    "start": 2120,
                    "end": 2200
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 256,
                    "matchedPaperCorpusId": "9925526"
                },
                {
                    "start": 371,
                    "end": 375,
                    "matchedPaperCorpusId": "189898023"
                },
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "209485573"
                },
                {
                    "start": 725,
                    "end": 728,
                    "matchedPaperCorpusId": "243865561"
                },
                {
                    "start": 1984,
                    "end": 1988,
                    "matchedPaperCorpusId": "258846153"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61328125
        },
        {
            "corpus_id": "247762790",
            "title": "STaR: Bootstrapping Reasoning With Reasoning",
            "text": "Human decision-making is often the result of extended chains of thought [1,2]. Recent work has shown that explicit intermediate reasoning (\"rationales\") can improve large language model (LLM) performance as well [3][4][5][6][7][8]. For example, [5] demonstrated that LLMs explicitly trained to use \"scratchpads\" for intermediate steps can attain perfect in-distribution performance on arithmetic, and strong out-of-distribution generalization, while models trained to predict answers directly fail to do either. These works suggest that generating explicit rationales before giving a final answer (\"rationale generation\") is valuable for LLMs across diverse tasks including mathematical reasoning, commonsense reasoning, code evaluation, social bias inference, and natural language inference. However, the two primary methods for inducing rationale generation both have serious drawbacks. \n\nOne approach to rationale generation is the construction of a fine-tuning dataset of rationales, either manually by human annotators or automatically with hand-crafted templates [3][4][5]9]. Manual methods are expensive, and it is infeasible to construct such a dataset for each interesting problem [3]. Meanwhile, template-based methods rely on automatically-generated rationales but only work when a general solution is already known [5] or reasonable hard-coded heuristics can be made [4]. \n\nAn alternative is to leverage in-context learning by including only a few rationale examples in the language model prompt. This has been shown to improve accuracy on mathematical and symbolic reasoning tasks relative to prompting without rationales (\"direct\" prompting) [5,6]. Yet, while fewshot techniques with rationales tend to outperform their non-reasoning counterparts, they generally substantially underperform models fine-tuned to directly predict answers using larger datasets [5,6].",
            "score": 0.45349249472064135,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 888
                },
                {
                    "start": 891,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1383
                },
                {
                    "start": 1386,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1878
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1392822265625
        },
        {
            "corpus_id": "277501853",
            "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
            "text": "We utilize Monte Carlo Tree Search (MCTS) (Tang et al., 2024;Xie et al., 2024;Gao et al., 2024;Feng et al., 2023;Zhang et al., 2024a) for improved reasoning-driven response generation in large language models (LLMs), especially for complex, multi-step language tasks where traditional methods often fall short. MCTS offers a framework for language models to engage in structured thinking, logical inference, and multi-step problem-solving, enabling capabilities such as hypothetical and counterfactual reasoning, commonsense and causal reasoning, and multisource, multi-hop question answering with RAG. By formulating reasoning-driven response generation as a sequential decision-making problem, we demonstrate how MCTS can systematically explore the vast space of potential responses to identify optimal outputs for a given end-user query. This systematic exploration is particularly crucial when dealing with complex queries that require intricate reasoning and planning over multiple steps. Our methodology leverages the inherent uncertainty in language generation and provides a principled way to balance exploration of diverse responses with exploitation of high-quality language patterns. MCTS demonstrates significant improvements in response quality, coherence, and relevance compared to traditional sampling and beam search methods, which are often inadequate for navigating the complexities of multi-step reasoning. We formulate reasoning-driven response generation as a search problem within a state space that evolves with the generation process. Let s \u2208 S denote a state in the generation process, where S represents the set of all possible states the generation process can assume. Each state s is formally defined as: \n\nHere, p \u2208 P is the system prompt, which serves to guide and condition the language model's behavior. P represents the entire set of possible system prompts that can be used. Next, q \u2208 Q denotes the current user query, which is the latest input to the language model. Q is the set encompassing all possible queries a user might pose. Finally, h = ((r 1 , c 1 ), (r 2 , c 2 ), . . . , (r n , c n )) \u2208 H represents the generation history up to the current point.",
            "score": 0.4529969235647129,
            "section_title": "D.10. Monte Carlo Tree Search Algorithm",
            "char_start_offset": 115463,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2194
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82275390625
        },
        {
            "corpus_id": "267751049",
            "title": "Do Large Language Models Understand Logic or Just Mimick Context?",
            "text": "As our experiments require intermediate reasoning steps, we utilized the dataset released by Liu et al., 2023, known as LogicalCOT. 5 The specific tasks include the following three types: Folio (Language to Logic): This process involves translating natural language into a more formal logical notation, a fundamental task that requires comprehending and interpreting logical statements articulated in natural language and transforming them into a formalized logical framework. \n\nEntailment Bank (Inference Chains): This instructional approach advances logical reasoning by requiring the model to ascertain the probability of a potential inference from a given set of premises. Subsequently, the model must delineate the sequence of logical deductions leading to the conclusion. Such an approach fosters deeper logical analysis and the capability to formulate cogent arguments. The examples provided for practice are formulated either in a symbolic language or articulated in natural language for greater accessibility and comprehension. MRC: Machine Reading Comprehension (MRC) serves as the primary task for evaluating the reasoning capabilities of LLMs, wherein a model is provided with a passage and a corresponding question and is tasked with identifying the correct answer. This domain encompasses tasks that necessitate a deep comprehension of the provided text, often requiring the model to recognize, extract, or deduce information from the text. Models may be tasked with resolving scenarios depicted in the text, identifying fallacies within an argument, or determining information that could bolster or undermine a presented argument. Data Source for Replacement: We utilize other samples as the in-domain data. For out-of-domain data, we use the English Wikipedia (2020/03/01)6 as the out-of-domain data source. We randomly selected a paragraph from one of the 2.6 billion documents to replace the content of the text and reasoning chain.",
            "score": 0.4529855926451811,
            "section_title": "Datasets",
            "char_start_offset": 10468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 476
                },
                {
                    "start": 479,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1950
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 109,
                    "matchedPaperCorpusId": "258832686"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.232666015625
        },
        {
            "corpus_id": "277313659",
            "title": "Scaling Laws of Synthetic Data for Language Models",
            "text": "In this section, we present details of SYNTHLLM for synthetic data generation. For specific target domain (e.g., mathematical reasoning), we begin by filtering high-quality and domain-specific reference documents from Fineweb-Edu [40], an open repository of web data (Section 4.1). Next, using these filtered reference documents, we construct large-scale, diverse questions (or prompts) by prompting open-source LLMs through three distinct methods, each designed to progressively increase question diversity (Section 4.2). Finally, we produce corresponding answers (or responses) for the generated questions, again employing open-source LLMs (Section 4.3).",
            "score": 0.45283379289724596,
            "section_title": "SYNTHLLM: Web-Scale Synthetic Data Generation",
            "char_start_offset": 16484,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 656
                }
            ],
            "ref_mentions": [
                {
                    "start": 230,
                    "end": 234,
                    "matchedPaperCorpusId": "270711474"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6533203125
        },
        {
            "corpus_id": "267411737",
            "title": "Distilling LLMs' Decomposition Abilities into Compact Language Models",
            "text": "A pivotal area for future exploration involves advancing offline RL or other suitable methodologies for distilling reasoning abilities from static datasets. This extension could contribute to more effective utilization of language models in reasoning tasks. \n\nCreation of a Larger Benchmark: Expanding our methodology, future work could focus on generating a more extensive benchmark as it requires only the access to ground truth-answers in the datset which usualy holds. This benchmark might incorporate a diverse set of reasoning datasets, such as MATH (Hendrycks et al., 2021) or AQuA (Ling et al., 2017), providing a broader assessment of reasoning capabilities. \n\nConcentration on Sub-Question Answering: Delving deeper into the sub-question answering aspect of the reasoning process presents a promising direction. While our dataset includes ChatGPT responses for sub-questions, their scoring and utilization remain unexplored. Future studies could investigate this component to enhance understanding and performance. \n\nUtilization of Open-Source Models: Exploring the application of open-source models, such as LLaMA, for sub-question generation emerges as a cost-effective alternative. Accessible without financial constraints, these models present an opportunity for researchers to delve into sub-question generation without monetary limitations. We were not able to run such kind of experiments ourselves due to the computational limitations.",
            "score": 0.4520278161602136,
            "section_title": "Development of Offline RL Approaches:",
            "char_start_offset": 23949,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 257
                },
                {
                    "start": 260,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1453
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14794921875
        },
        {
            "corpus_id": "268308114",
            "title": "Generative Models for Complex Logical Reasoning over Knowledge Graphs",
            "text": "In fact, different relations may contain different semantic distributions.For instance, the semantic distribution of the relation Nationality is obviously closer than Graduate when inferring the query (Geoffrey Hinton, Citizen, ?). Accordingly, a fixed embedding may have limited capability in capturing the uncertainty of the EPFO query with multiple relational semantics.\n\nSecond, most approaches encode EPFO queries using discriminative representation frameworks that just retrieve answers without interpreting the entire query.In contrast, we argue that generative models can learn the joint distribution of queries and answers, which has great potential to model complex interactions and produce coherent answers.Recently, generative models have shown impressive results in various fields, such as image generation, video prediction, and language comprehension.However, little attention has been paid to exploring the generative process (cf. Figure 1 (b)) to capture underlying compound distributions for complex queries on KGs with generative models.\n\nTo address these issues, we innovatively regard the complex logical reasoning task as a new generative question answering to approximate the joint distribution of queries and answers.Inspired by the diffusion model [20], a new promising paradigm for generative models, we try to incorporate the diffusion model into KG reasoning.The main motivations are: (1) It provides an elegant way to inject uncertainty into the model through Gaussian distribution;\n\n(2) It utilizes the multi-step generative process to improve distribution representation; and (3) It ensures the intermediate process of the model is controllable and explainable.\n\nIn this paper, we present a Diffusion model for Complex Logical Reasoning, named DiffCLR.Specifically, our DiffCLR first proposes a query transformation to convert EPFO queries into input sequences by dynamically sampling contextual subgraphs, where relations are regarded as normal nodes (like entities) to alleviate the heterogeneity of KGs.Then, we introduce a diffusion model to perform a multi-step generative process in order to better learn the joint distribution of queries and answers under multi-grained control.For the forward diffusion phase, we corrupt the target query embedding into a Gaussian distribution via noise injection.",
            "score": 0.4519889743100895,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 74,
                    "end": 373
                },
                {
                    "start": 375,
                    "end": 531
                },
                {
                    "start": 531,
                    "end": 718
                },
                {
                    "start": 718,
                    "end": 866
                },
                {
                    "start": 866,
                    "end": 1056
                },
                {
                    "start": 1058,
                    "end": 1241
                },
                {
                    "start": 1241,
                    "end": 1387
                },
                {
                    "start": 1387,
                    "end": 1511
                },
                {
                    "start": 1513,
                    "end": 1692
                },
                {
                    "start": 1694,
                    "end": 1783
                },
                {
                    "start": 1783,
                    "end": 2037
                },
                {
                    "start": 2037,
                    "end": 2216
                },
                {
                    "start": 2216,
                    "end": 2336
                }
            ],
            "ref_mentions": [
                {
                    "start": 1273,
                    "end": 1277,
                    "matchedPaperCorpusId": "249192356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.486572265625
        },
        {
            "corpus_id": "249626098",
            "title": "CoSe-Co: Text Conditioned Generative CommonSense Contextualizer",
            "text": "Wang et al., 2020b). Hence, applying such methods is sub-optimal since most NLP tasks comprise of sentences. Further, being trained on entities, applying them directly on sentences is infeasible and lead to train-inference input type mismatch.\n\nTo address these limitations, we propose CommonSense Contextualizer -CoSe-Co, a generative framework which generates relevant commonsense knowledge given natural language sentence as input. We condition it on sentences to make it learn to incorporate overall text context and enable it to dynamically select entities/phrases from an input sentence as well as output novel yet relevant entities as part of commonsense inferences generated. We consider commonsense knowledge in the form of paths, i.e., sequence of entities connected through relations. We first create sentencepath paired dataset by -1) sampling paths from an underlying KG; 2) sampling a subset of entities from a path; and 3) retrieving & filtering sentences (from a sentence corpus) that are semantically similar to the path. The paired data is then used to train a generative language model to generate a path given a sentence as input.\n\nTo analyse the usefulness of generated commonsense, we augment it in various downstream tasks. The reasoning ability of NLP systems is commonly analysed using QA. Hence, we choose two such tasks: 1) Multi-Choice QA, where given a question and set of choices, the model has to identify the most appropriate answer choice. However, often more than one choice is a suitable answer. To mitigate this, 2) OpenCSR (Open-ended Common-Sense Reasoning) (Lin et al., 2021a) was proposed, where each question is labeled with a set of answers which have to be generated without choices. We also show applicability of CoSe-Co in improving performance on paraphrase generation task ( \u00a74.5).\n\nOur contributions can be summarised as: 1. We propose a CommonSense Contextualizer (CoSe-Co) to generate knowledge relevant to overall context of given natural language text. CoSe-Co is conditioned on sentence as input to make it generically usable in tasks without relying on entity extraction. 2. We devise a method to extract sentencerelevant commonsense knowledge paths and create the",
            "score": 0.4515782219240209,
            "section_title": "Introduction",
            "char_start_offset": 1912,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2093505859375
        },
        {
            "corpus_id": "258841402",
            "title": "Generating Data for Symbolic Language with Large Language Models",
            "text": "To what extent this approach works for complex structured data, such as meaning representation and codes (Figure 1), remains an open question. The investigation of data generation via LLMs in the context of such symbolic language tasks is also extremely intrigu-ing for two reasons: 1) the human annotation procedure for these tasks requires expensive domain expert efforts (Clarke et al., 2010) and carefullydesigned strategies (Wang et al., 2015;Iyer et al., 2017;Herzig and Berant, 2019, inter alia); 2) conventional data augmentation methods aiming at enriching datasets for these tasks require handcrafted rules, a considerable number of expert demonstration examples, and are mostly task-specific (Jia and Liang, 2016;Yu et al., 2018a;Andreas, 2020, inter alia). \n\nTo address these issues, we propose Symbolic data Generation (SYMGEN) for various annotationexpensive symbolic language tasks. SYMGEN works with an LLM trained on code (i.e., Codex-175B;Chen et al. 2021) and optional task-specific structure knowledge (e.g., database for SQL; Iyer et al. 2017) through prompting or in-context learning. SYMGEN also comprises an agreement-based verification module, in which the outputs are verified by execution (e.g., programs, logical forms) or formatting (e.g., pseudo-logical forms), to ensure high-quality generations. With the generated data, we train efficient task models with around 1% size of Codex for task inference (e.g., T5 with size 770M and 3B; Raffel et al. 2020). \n\nWe experiment on six symbolic languages which are SQL, Bash, Python, Prolog, Task Oriented Parsing representation (TOP-representation; Gupta et al. 2018) and Question Decomposition Meaning Representation (QDMR; Wolfson et al. 2020). We consider generating data in zero-shot, few-shot, and full data settings. Our key research findings include the following:",
            "score": 0.45144889056692605,
            "section_title": "Introduction",
            "char_start_offset": 1681,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1485
                },
                {
                    "start": 1488,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1845
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 395,
                    "matchedPaperCorpusId": "5667590"
                },
                {
                    "start": 429,
                    "end": 448,
                    "matchedPaperCorpusId": "14472576"
                },
                {
                    "start": 448,
                    "end": 466,
                    "matchedPaperCorpusId": "497108"
                },
                {
                    "start": 703,
                    "end": 724,
                    "matchedPaperCorpusId": "7218315"
                },
                {
                    "start": 724,
                    "end": 741,
                    "matchedPaperCorpusId": "52979524"
                },
                {
                    "start": 1047,
                    "end": 1063,
                    "matchedPaperCorpusId": "497108"
                },
                {
                    "start": 1623,
                    "end": 1641,
                    "matchedPaperCorpusId": "53017700"
                },
                {
                    "start": 1699,
                    "end": 1718,
                    "matchedPaperCorpusId": "211003735"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.701171875
        },
        {
            "corpus_id": "258546873",
            "title": "From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base",
            "text": "KoPL (Cao et al., 2022a) is a compositional and interpretable programming language for complex reasoning. The KoPL parser provides human understandable descriptions of logical forms. Based on the large-scale pretrained language model BART (Lewis et al., 2019), the BART-based KoPL has shown impressive results on KQA Pro dataset. \n\nAlthough KoPL can explicitly describe the reasoning processes, the logical forms generated from semantic parser are the intermediate results but not the final answer, which may result in sub-optimized solutions. For example, the slight generation deviation (Das et al., 2021) may cause the logical forms to run incorrectly. Thus a natural question arises: Can we use these intermediate results to further improve the complex reasoning ability? \n\nIn this paper, we further explore how to unlock KoPL and KBQA model for improving its capability of complex reasoning. We find that the complex reasoning ability can benefit from demonstrating the KoPL's intermediate results to the KBQA model. That is, with the intermediate step-by-step reasoning processes from KoPL, we can elicit the multi-step reasoning for complex KBQA and achieve better performances compared to KoPL. Observed from that, we propose a novel parse-execute-refine paradigm for complex KBQA. \n\nFigure 1 shows an overview of the proposed approach called PER-KBQA, which includes a parsing stage, an execution stage, and a refinement stage for complex KBQA. Given a question, the parser first utilizes a sequence-to-sequence model to generate the logical forms. Since KoPL can explicitly describe the reasoning process, we use KoPL as the logical forms in the parsing stage. Then, the execution stage aligns and executes the generated logical forms over the knowledge base. With that, we can obtain the intermediate step-bystep reasoning processes. Finally, in the refinement stage, the intermediate reasoning results are used as execution contexts. We combine the question and the execution contexts as inputs to the KBQA model. The main idea of this paradigm is that the answer may benefit from the intermediate step-bystep reasoning processes.",
            "score": 0.4510843456741583,
            "section_title": "Introduction",
            "char_start_offset": 1785,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 329
                },
                {
                    "start": 332,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1289
                },
                {
                    "start": 1292,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2142
                }
            ],
            "ref_mentions": [
                {
                    "start": 5,
                    "end": 24,
                    "matchedPaperCorpusId": "247362971"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.452392578125
        },
        {
            "corpus_id": "261065145",
            "title": "HopPG: Self-Iterative Program Generation for Multi-Hop Question Answering over Heterogeneous Knowledge",
            "text": "Question answering is a fundamental task and plays a crucial role in the field of natural language processing (Cui et al., 2020;Kwiatkowski et al., 2019;Liu et al., 2020;Choi et al., 2018;Fan et al., 2019). In recent years, question-answering tasks based on heterogeneous knowledge (HQA) have increasingly gained the attention of researchers (Chen et al., 2020b,a;Zhu et al., 2021;Talmor et al., 2021;Chen et al., 2021). These tasks require models to perform multi-hop reasoning on different structured knowledge, i.e., tables and texts. One category of the existing HQA method performs implicit answer reasoning, which takes the question and knowledge as input and performs reasoning in the semantic space, and then directly outputs the answer (Pan et al., 2020;Sun et al., 2021;Wang et al., 2022;Eisenschlos et al., 2021;Kumar et al., 2021). Although this approach has proven effective in HQA tasks, it lacks interpretability, scalability, and symbolic reasoning abilities. In contrast, the semantic parsing-based (SP-based) approach explicitly derives answers by generating and executable programs, remedying the above-mentioned deficiencies and enabling researchers to monitor and improve each step of the derivation process. \n\nSP-based methods have been widely used in question-answering tasks on homogeneous knowledge sources, such as tables, knowledge graphs, and texts (Yih et al., 2014;Bao et al., 2014Bao et al., , 2016;;Abdelaziz et al., 2021;Zhou et al., 2022a). Nevertheless, the SP-based question-answering methods over heterogeneous knowledge still require further exploration. Recently, Zhou et al. (2022b) introduced UniRPG, a SP-based model designed for HQA. They defined a set of general atomic and higher-order operations for discrete reasoning over heterogeneous knowledge resources. During the program generation process, UniRPG takes questions and supporting facts as input pairs of BART (Lewis et al., 2020) to generate a complete program in a single step.",
            "score": 0.4509839065348531,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1980
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 153,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 1377,
                    "end": 1395,
                    "matchedPaperCorpusId": "6343829"
                },
                {
                    "start": 1395,
                    "end": 1411,
                    "matchedPaperCorpusId": "1756650"
                },
                {
                    "start": 1411,
                    "end": 1431,
                    "matchedPaperCorpusId": "18549358"
                },
                {
                    "start": 1431,
                    "end": 1454,
                    "matchedPaperCorpusId": "235363625"
                },
                {
                    "start": 1603,
                    "end": 1622,
                    "matchedPaperCorpusId": "252918014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.433837890625
        },
        {
            "corpus_id": "53112478",
            "title": "Compositional Attention Networks for Interpretability in Natural Language Question Answering",
            "text": "By visualizing every reasoning step performed by our model, we are able to track the causal chain of reasoning starting with searching for initial piece of evidence, conditioned on the query, to arriving at the final answer. Thus, our model, in addition to answer the \"what\" question, also provides the logical steps taken in order to arrive at an answer, thus answering the \"how\" question. \n\n[6] has outlined different criteria to classify methods for machine learning interpretability. The model we propose is internpretable by design, hence it is intrinsically interpretable and it falls under the category of Model-specific interpretability. Our model provides explanations based on attention distributions extracted from model internals rather than studying relationship between the prediction and the features. \n\nA. Data bAbI [9] is a synthetic RC dataset, created by facebook researchers in 2015. The term synthetic data refers to data that is not extracted from a book or from the internet, but is generated by using a few rules that simulate natural language. This characteristic of bAbI places the weight of the task on the reasoning module rather than the understanding module.Question Answering data sets provide synthetic tasks for the goal of helping to develop learning algorithms for understanding and reasoning. AI's futuristic objective is that to create a dialogue platform automatically meanwhile, Question Answering data holds an important key in understanding dialogue. The way it works is the data's intent is to design different kinds of question sets which will further act as their skill set. This system typically replicates a software testing system. It has 20 different tasks of various reasoning patterns. Firstly it has a Single Supporting act, wherein the answer is usually a 'single word'. For example \"John went to the gym. Where is John?\". Then it moves on to harder tasks like to Two or Three Supporting Act. But the goal of such a data set is challenged while recognizing arXiv:1810.12698v1 [cs.LG] 30 Oct 2018 objects, negation and reasoning. For example those tasks with indefinite knowledge like \" Mary might like chocolate or vanilla\" such questions are answered with a \"maybe\" when asked about either. However the other sorts of simpler tasks the dataset does include is that of Yes/No Based or that of Counting of Lists/Sets.",
            "score": 0.45056569435017285,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2175,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2243
                },
                {
                    "start": 2244,
                    "end": 2368
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64892578125
        },
        {
            "corpus_id": "274464813",
            "title": "Perception Tokens Enhance Visual Reasoning in Multimodal Language Models",
            "text": "Reasoning with perception tokens. The second challenge is enabling the model to condition on tokens from V aux effectively when generating each subsequent reasoning step, thereby enhancing its reasoning capabilities. To achieve this, we introduce chain-of-thought reasoning progressively, beginning with simpler, single-step reasoning tasks and advancing to more complex, multi-step inference. The model begins by learning single-step reasoning, predicting an initial reasoning step s 1 based on the input x. It then progresses to multi-step reasoning, predicting sequences s 1 , \u2022 \u2022 \u2022 , s m and effectively utilizing auxiliary tokens to support extended chains of inference. We further reinforce this process with constrained decoding and an information bottleneck: in constrained decoding, we restrict sampling to auxiliary tokens, ensuring they serve as intermediate reasoning steps; in the information bottleneck approach, we truncate the reasoning chain before the auxiliary token when generating subsequent reasoning steps, forcing the model to rely solely on auxiliary tokens to reach the correct answer. Lastly, we provide a multi-task data synthesis approach to train the model using curriculum learning across various synthetic tasks. Further details are provided in Section 3.4.",
            "score": 0.450494094983768,
            "section_title": "Perception token prediction and reasoning",
            "char_start_offset": 11499,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1289
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56982421875
        },
        {
            "corpus_id": "278603068",
            "title": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging",
            "text": "Augmenting large language models (LLMs) with external knowledge retrieved via search tools is a common approach to address their inherent knowledge cutoff limitation [Lewis et al., 2020a, Zhao et al., 2024a]. Existing methods typically apply a static, pre-inference retrieval strategy by concatenating retrieved information into the input prompt, enabling the LLM to generate answers based on the provided external knowledge [Gao et al., 2024]. However, this approach often lacks adaptiveness, especially for complex tasks in which users' information needs are ambiguous, rationalebased, or not directly searchable, as these tasks require iterative reasoning to progressively uncover the necessary evidence for answer generation [Edge et al., 2024, Zhao et al., 2024b]. \n\nRecent advances in test-time scaling techniques have significantly strengthened LLMs' reasoning abilities, enabling complex behaviors such as long chain-of-thought reasoning and dynamic tool use [Snell et al., 2024, Muennighoff et al., 2025]. Reasoning-based models such as OpenAI's o1 and DeepSeek R1 have demonstrated significant gains on challenging tasks, particularly in areas like mathematical problem-solving and coding, where iterative self-reflection and refinement are essential for success [OpenAI, 2024, DeepSeek-AI, 2025]. Inspired by these developments, a natural extension for retrieval-augmented generation is to shift retrieval from a static, pre-inference step to a dynamic, inference-time process. This transition allows LLMs to iteratively retrieve and adaptively integrate external knowledge during reasoning, aligning more closely with the evolving and multi-layered information needs inherent to complex information seeking tasks [Li et al., 2025, Jin et al., 2025]. \n\nHowever, effectively supporting such dynamic search-enhanced reasoning for complex tasks presents two key challenges. First, due to the implicit and evolving nature of such tasks, it is difficult to retrieve all necessary evidence through a single retrieval action. Each retrieval typically uncovers only a local information patch-a partial subset of the expected knowledge space-requiring iterative accumulation and integration via reasoning to form a complete answer [Qian et al., 2024a, Edge et al., 2024].",
            "score": 0.4503104276500304,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1761
                },
                {
                    "start": 1764,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2273
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 186,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.232177734375
        },
        {
            "corpus_id": "265213146",
            "title": "Transformers in the Service of Description Logic-Based Contexts",
            "text": "Multiple surveys [27,11,28] in the literature describe the most recent research developments on the use of transformers for reasoning tasks. One of the first datasets generated for this purpose was from Clark et al. [3] with RuleTaker, demonstrating the potential of transformers to perform logical question answering under CWA by training LLMs on synthetic datasets. However, their approach was limited to short expressions of simple conjunctive rules. Tafjord et al. [22], generated the ProofWriter datasets (under CWA and OWA) and with a T5 [18]-based model fined-tuned on ProofWriter showed that LLMs can generate proofs with high accuracy (94.8% for depth 5). \n\nThe generation approach of the DELTAD was based on the approach for the generation of the datasets RuleTaker and ProofWriter, i.e., using PCFGs. However, DELTAD is different from these datasets as i) ALCQ is a much more expressive logic language hence we new PCFGs; ii) we have defined different PCFGs for each linguistic complexity level (which has not been done for any other dataset in the literature); iii) it is balanced regarding the aspects discussed in Section 3.5. \n\nIn more expressive contexts, Onta\u00f1\u00f3n et al. [16] showed that LLMs perform well (up to 90.5%) over contexts generated by propositional logic and a small subset of FOL. Han et al. [5], with the FOLIO dataset (1.4K), generated from FOL sentences -but without number restrictions-, tested the ability of various LLMs for the same reasoning task and concluded that RoBERTa [12] performed best among all tested models (including GPT-3 and Codex) but still, the performance was low. Tian et al. [24] introduced the much richer synthetic dataset LogicNLI (30K), under OWA for diagnosing LLMs' ability in FOL reasoning, showing that even their best-performing model does not learn to perform reasoning tasks and cannot generalize to different scenarios.",
            "score": 0.450086641153132,
            "section_title": "Related Work",
            "char_start_offset": 29750,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 1140
                },
                {
                    "start": 1143,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1887
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 24,
                    "matchedPaperCorpusId": "254877753"
                },
                {
                    "start": 216,
                    "end": 219,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 469,
                    "end": 473,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 544,
                    "end": 548,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7578125
        },
        {
            "corpus_id": "254018203",
            "title": "TSGP: Two-Stage Generative Prompting for Unsupervised Commonsense Question Answering",
            "text": "We first use knowledge generation prompts to let GPT generate knowledge (a series of coherent short sentences) related to the concepts or events in the question. Its essence is to fill knowledge gaps and clarify implicit intermediate reasoning steps to arrive at answers to reasoning questions. Instead of generating answers directly, we generate short sentences that imply the relationship between the concepts in the question and the answer, similar to a thought process or solution. \n\nFigure 2(a) shows the design of the knowledge generation prompt for the SocialIQA dataset. The knowledge generation prompt consists of instruction, some demos fixed for each task, and a new question placeholder. Demonstrations are handwritten, and each demonstration contains a taskstyle question and a knowledge statement that helps answer that question. When generating knowledge for a new question q, we insert the question into a placeholder and repeatedly sample the generated Figure 2: Overview of the proposed method Two-Stage Generative Prompting (TSGP), which contains three parts. 1) Knowledge Generation, using GPT to generate knowledge statements about concepts or events in questions based on the knowledge generation prompts. 2) Answer Generation, leveraging GPT again to generate pseudo-answers conditioned on questions and knowledge based on the answer generation prompts. 3) Answer Prediction, computing the semantic score between the generated pseudo-answer and each choice and voting for the final answer. continuation of that prompt to obtain a set of knowledge statements K q = {k 1 , k 2 , ..., k m }. \n\nSince we did not train the knowledge generator to generate coherent and factually correct knowledge, it can be assumed that some of the generated knowledge did not provide helpful information for the model. So we propose to use point-wise mutual information (PMI) to quantify the correlation between question q and each generated knowledge k. \n\nSince both q and k are sentences, we use a PrLM to estimate the probabilities. Conditional probabilities are calculated by treating conditional sentences as prefixes. It is important to note that while PMI can by definition be computed in two equivalent ways, language model estimates do not guarantee: \n\nTherefore, we use question q as the condition to calculate the mutual information between the knowledge and the question. \n\nIt measures the dependency between questions and knowledge. A positive score means that the question is positively related to this piece of knowledge.",
            "score": 0.4497340165674608,
            "section_title": "Knowledge Generation",
            "char_start_offset": 4998,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1611
                },
                {
                    "start": 1614,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1956
                },
                {
                    "start": 1959,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2125
                },
                {
                    "start": 2126,
                    "end": 2261
                },
                {
                    "start": 2264,
                    "end": 2385
                },
                {
                    "start": 2388,
                    "end": 2447
                },
                {
                    "start": 2448,
                    "end": 2538
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12213134765625
        },
        {
            "corpus_id": "277501797",
            "title": "TaMPERing with Large Language Models: A Field Guide for using Generative AI in Public Administration Research",
            "text": "Clearly defining the desired task output involves understanding the kinds of natural language processing tasks at which LLMs excel. LLMs have demonstrated proficiency across diverse NLP areas such as Natural Language Generation (NLG), Natural Language Understanding (NLU), Knowledge-Intensive Tasks, and Natural Language Inference (NLI) (Yang et al., 2024). NLG tasks focus on creating human-like text outputs, including summaries, translations, or content generation. In contrast, NLU tasks emphasize interpreting and comprehending input data, supporting text classification, sentiment analysis, or coding qualitative data into structured forms. Similarly, Knowledge-Intensive Tasks integrate factual and domain-specific information to generate detailed answers, summarize specialized documents, or explain concepts. Finally, NLI tasks involve evaluating logical relationships, which can directly inform tasks involving logical reasoning or hypothesis testing. \n\nDeciding how LLMs will be used-either to simulate human judgment or serve as analytical tools-depends largely on their ability to replicate human cognitive processes. LLMs possess human-like cognitive capacities, enabling researchers to model social interactions and derive insights into human behaviors and social dynamics (Ke et al., 2024;Niu et al., 2024). They can simulate judgments or emulate different human personas (Argyle et al., 2023;Dillion et al., 2023), thus supporting tasks like synthetic data generation or scenario exploration. However, researchers must recognize their limitations: LLMs effectively capture syntax, grammar, and semantic aspects of language (Chang & Bergen, 2024) but may struggle with tasks requiring complex reasoning, context-specific interpretations, or expertlevel domain knowledge (Amirizaniani et al., 2024;Niu et al., 2024;Szymanski et al., 2024). Understanding these limitations guides researchers in clearly defining the LLM's function within their research task.",
            "score": 0.44925501373335996,
            "section_title": "Tasks",
            "char_start_offset": 10977,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1972
                }
            ],
            "ref_mentions": [
                {
                    "start": 337,
                    "end": 356,
                    "matchedPaperCorpusId": "258331833"
                },
                {
                    "start": 1388,
                    "end": 1409,
                    "matchedPaperCorpusId": "252280474"
                },
                {
                    "start": 1409,
                    "end": 1430,
                    "matchedPaperCorpusId": "258569852"
                },
                {
                    "start": 1640,
                    "end": 1661,
                    "matchedPaperCorpusId": "257636789"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.197265625
        },
        {
            "corpus_id": "276774742",
            "title": "OmniSQL: Synthesizing High-quality Text-to-SQL Data at Scale",
            "text": "Chain-of-thought (CoT) reasoning has demonstrated remarkable success across various challenging tasks [34,71]. By decomposing complex problems into smaller, manageable steps, this approach enables LLMs to tackle intricate tasks more effectively, improving both accuracy and interpretability. Building on this, we augment the synthetic <database, question, SQL query> triplets by generating CoT solutions that explicitly outline the reasoning process behind constructing the SQL query from the question. The prompt for synthesizing CoT solutions consists of the following components: \n\n\u2022 Task Instruction: Directs the LLM to generate a step-by-step \n\nCoT solution using the provided information. \u2022 Database Schema: Includes the CREATE TABLE statements for all relational tables in the database. \u2022 NL Question and SQL Query Pair: The natural language question and its corresponding SQL query. \n\nA typical CoT solution begins by analyzing the question to identify the key information required. It then determines the relevant tables, columns, and filtering criteria needed to retrieve the desired data. Finally, it constructs the SQL query step by step, incorporating necessary joins, filters, aggregations, groupings, and other operators, culminating in the complete SQL query as the final answer. \n\nInterestingly, in our preliminary experiments, we observe that the SQL queries generated by the synthetic CoT sometimes differ  from the original ones. Upon closer examination, we find that CoTgenerated SQL queries often better align with the questions compared to the original SQL queries. This improvement arises because the original <database, question, SQL query> triplets occasionally contain minor issues, such as unnecessary column selections, and incorrect join paths. The CoT synthesis process allows the LLM to identify and correct these issues during step-by-step reasoning, resulting in more accurate and refined SQL queries. This observation also aligns with prior research showing that LLMs excel at detecting and resolving minor errors in predicted SQL queries [22,52,64]. Thus, incorporating CoT not only provides detailed solutions but also enhances the overall quality of the synthetic data. \n\nTo enhance the diversity and reliability of synthetic CoT solutions, we generate multiple candidate CoT solutions for each synthetic <database, question, SQL query> triplet. To select the most reliable CoT solution, we extract SQL queries from these candidates and perform a majority vote.",
            "score": 0.4491453662916123,
            "section_title": "Chain-of-Thought Solution Synthesis",
            "char_start_offset": 25307,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 647
                },
                {
                    "start": 650,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1295
                },
                {
                    "start": 1298,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2085
                },
                {
                    "start": 2086,
                    "end": 2207
                },
                {
                    "start": 2210,
                    "end": 2383
                },
                {
                    "start": 2384,
                    "end": 2499
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 106,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 106,
                    "end": 109,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76171875
        },
        {
            "corpus_id": "268297389",
            "title": "RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation",
            "text": "Retrieval-augmented Generation (RAG). Recently, RAG has gained popularity for boosting the performance of LLMs by guiding their generation process using the retrieved knowledge (Zhao et al., 2023). Without updating model parameters that may be expensive (Lewis et al., 2020a) or unstable (Ke et al., 2022a,b), RAG is a costeffective way for LLMs to interact with the external world (Gu et al., 2018;Lewis et al., 2020a). RAG is widely applied to downstream tasks, such as code generation (Lu et al., 2022;Nashid et al., 2023;Zhou et al., 2022b), question answering (Baek et al., 2023;Siriwardhana et al., 2023), and creative writing (Asai et al., 2023;Wen et al., 2023). \n\nReasoning-enhanced RAG. Some recent works also leverage reasoning to enhance the performance of RAG (Li et al., 2023b). For example, IRCoT (Trivedi et al., 2022b) exploits CoT to generate better queries for retrieval, IRGR (Ribeiro et al., 2022) performs iteratively retrieval to search for suitable premises for multi-hop QA, GEEK (Liu et al., 2023a) can choose to query external knowledge or perform a single logical reasoning step in long-horizon generation tasks, and ITRG (Feng et al., 2023a) performs retrieval based on the last-step generation. However, these previous RAG methods simply adopt a single query to retrieve the knowledge for question-answering tasks (Feng et al., 2023b;Gao et al., 2023), while our proposed RAT performs retrieval using reasoning and draft answers in an autoregressive way, which significantly improves the performance of RAG in various tasks as demonstrated in Figure 2.  2021) for enhancing multi-step computation. Tree-of-Thought (Yao et al., 2023) and Graph-of-Thought (Besta et al., 2023) also expand the reasoning paths into a complex structure instead of linear CoT.",
            "score": 0.4490083719111122,
            "section_title": "Related Works",
            "char_start_offset": 24871,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1784
                }
            ],
            "ref_mentions": [
                {
                    "start": 382,
                    "end": 399,
                    "matchedPaperCorpusId": "19206366"
                },
                {
                    "start": 1005,
                    "end": 1024,
                    "matchedPaperCorpusId": "256194112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2022705078125
        },
        {
            "corpus_id": "273901418",
            "title": "QuBE: Question-based Belief Enhancement for Agentic LLM Reasoning",
            "text": "LLMs guided by human-demonstrated rationales, such as Chain of Thoughts (Wei et al., 2022), Least-to-Most prompting (Zhou et al., 2023), have been proven effective in reasoning tasks. Nevertheless, such rationalizing strategies often fail to generalize to unseen, complex scenarios which diverge from the few-shot demonstrations (Zhou et al., 2023). \n\nTo improve the accuracy and explainability of LLM reasoning, Lyu et al. (2023) propose Faithful CoT, which uses a deterministic solver for solving intermediate steps in math word problem and question answering tasks. Creswell and Shanahan (2022) propose to finetune a language model to better ensure the logical validity of LLM reasoning, in logic and question answering tasks. Radhakrishnan et al. (2023) propose reasoning by decomposing questions into subquestions, for question answering tasks. \n\nWhile our work also pursues reliable and accurate LLM reasoning, our proposed solution of belief state construction and grounded rationale generation solves the unique problem of reasoning derailment in LLM agents.",
            "score": 0.44895896010642106,
            "section_title": "Reasoning in Large Language Models",
            "char_start_offset": 26731,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 349
                },
                {
                    "start": 352,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 1066
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 90,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 116,
                    "end": 135,
                    "matchedPaperCorpusId": "248986239"
                },
                {
                    "start": 329,
                    "end": 348,
                    "matchedPaperCorpusId": "248986239"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.254150390625
        },
        {
            "corpus_id": "267750908",
            "title": "Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs",
            "text": "We further analyze the effectiveness of our inference engine in generating logical rules or explanations to enhance downstream reasoning tasks. We evaluate on following commonsense reasoning datasets: StrategyQA (Geva et al., 2021), SOCIAL IQA (Sap et al., 2019b), LINK (Li et al., 2023), PIQA (Bisk et al., 2020) and CSQA2.0 (Talmor et al., 2022). We use a zero-shot CoT strategy to prompt two baseline models, Mistral-7B-Instruct-v0.1 and Llama-2-7b-chat (Touvron et al., 2023), to answer questions with following explanations. We then utilize our inference engine to generate logical rules or explanations relevant to answer questions, and supplement these generated rationals to baseline models as input to enhance their performance. We compare the prediction accuracy of our inference engine augmented models against baselines. \n\nThe comparative results are shown in Tabel 4. Our inference engine can generate logical rules or explanations that benefit multiple downstream commonsense reasoning tasks on top of different backbone models. For the lack of clear advantage on PIQA and performance decline on CSQA2.0, we speculate that PIQA may be contaminated during Mistral's training process, and CSQA2.0's focus is mainly on longtail commonsense knowledge rather than requiring logical rules inference, like \"Is cotton candy sometimes made out of cotton?\" Abstract and complex rules are essential in tackling diverse complex questions, paving the way for complex reasoning and decision-making. Although LLMs have opened new avenues for generating inferential rules (Zhu et al., 2023), they still struggle to automatically craft abstract and complex rules. \n\nIntegration of Logical Rules and LLMs Recently, the integration of inferential rules with neural networks, particularly LLMs, has gained significant attention. This approach combines the logical interpretability of symbolic reasoning and adaptive power of neural computing, improving LLMs' logical reasoning ability (Wang et al., 2022a;Mu et al., 2023). Wang et al. (2021) andOlausson et al. (2023) transform textual statements into logical expressions and conduct symbolic reasoning following logical rules.",
            "score": 0.44887181795238246,
            "section_title": "Downstream Reasoning Evaluation",
            "char_start_offset": 23169,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1660
                },
                {
                    "start": 1663,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2171
                }
            ],
            "ref_mentions": [
                {
                    "start": 212,
                    "end": 231,
                    "matchedPaperCorpusId": "230799347"
                },
                {
                    "start": 294,
                    "end": 313,
                    "matchedPaperCorpusId": "208290939"
                },
                {
                    "start": 1979,
                    "end": 1999,
                    "matchedPaperCorpusId": "236772032"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18212890625
        },
        {
            "corpus_id": "275921050",
            "title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models",
            "text": "Logical reasoning is a critical component of Large Language Models (LLMs), and substantial research efforts in recent years have aimed to enhance their deductive reasoning capabilities. However, existing deductive reasoning benchmarks, which are crucial for evaluating and advancing LLMs, are inadequate due to their lack of task complexity, presence of prior knowledge as a confounder, and superficial error analysis. To address these deficiencies, we introduce JustLogic, a synthetically generated deductive reasoning benchmark designed for rigorous evaluation of LLMs. JustLogic is (i) highly complex, capable of generating a diverse range of linguistic patterns, vocabulary, and argument structures; (ii) prior knowledge independent, eliminating the advantage of models possessing prior knowledge and ensuring that only deductive reasoning is used to answer questions; and (iii) capable of in-depth error analysis on the heterogeneous effects of reasoning depth and argument form on model accuracy. Our experimental results on JustLogic reveal that (i) state-of-the-art (SOTA) reasoning LLMs perform on par or better than the human average but significantly worse than the human ceiling, and (ii) SOTA non-reasoning models still underperform the human average. All code and data are available at https://github.com/michaelchen-lab/JustLogic",
            "score": 0.44876732152189597,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6640625
        },
        {
            "corpus_id": "273655169",
            "title": "Belief in the Machine: Investigating Epistemological Blind Spots of Language Models",
            "text": "Our work is closely related to the following bodies of work. \n\nLogical and commonsense reasoning. Endowing LMs with logical and commonsense reasoning abilities remains a central challenge in AI. The emergence of large, pre-trained models such as GPT-3 and GPT-4 [73,74] has intensified efforts to evaluate their capacity for human-like reasoning across domains such as arithmetic, logical deduction, probabilistic inference, and commonsense understanding. While LMs have demonstrated impressive performance on various generation tasks, questions persist about whether they truly reason or merely replicate patterns from training data [75,76]. Techniques like chain-of-thought (CoT) prompting [77], which encourage models to generate intermediate reasoning steps, have improved performance on complex tasks [78,79]. However, studies indicate that these models still struggle with multi-step logical deduction tasks and maintaining logical consistency [80,81]. Benchmarks like ReClor [82] and LogiQA [83] have been used to show limitations in LMs' logical reasoning compared to humans. \n\nCommonsense reasoning poses additional challenges for LMs [84]. For instance, benchmarks such as CommonsenseQA [85,86] and Winograd Schema [87] assess LMs' understanding of everyday scenarios. While progress has been made, LMs still find it difficult to perform well on tasks requiring subtle disambiguation or long, deep contextual comprehension [88,89]. Recent efforts to enhance reasoning in LMs include fine-tuning on data with high-quality natural-language explanations [90], integrating explicit reasoning prompting techniques or modules [91][92][93][94], and employing self-consistency techniques [95,96], among others. Despite these advancements, however, important challenges still persist, since these models can be sensitive to prompt phrasing, may produce logically plausible but incorrect reasoning, and tend to overfit to specific task formats [80,97,98,38,99].",
            "score": 0.44867321969973206,
            "section_title": "B Related Work: Commonsense Reasoning and Theory of Mind Evaluations",
            "char_start_offset": 52066,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 63,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 262,
                    "end": 266,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 634,
                    "end": 638,
                    "matchedPaperCorpusId": "261582594"
                },
                {
                    "start": 638,
                    "end": 641,
                    "matchedPaperCorpusId": "253522998"
                },
                {
                    "start": 692,
                    "end": 696,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 806,
                    "end": 810,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 810,
                    "end": 813,
                    "matchedPaperCorpusId": "252917648"
                },
                {
                    "start": 998,
                    "end": 1002,
                    "matchedPaperCorpusId": "220483148"
                },
                {
                    "start": 1144,
                    "end": 1148,
                    "matchedPaperCorpusId": "248377876"
                },
                {
                    "start": 1197,
                    "end": 1201,
                    "matchedPaperCorpusId": "53296520"
                },
                {
                    "start": 1225,
                    "end": 1229,
                    "matchedPaperCorpusId": "15710851"
                },
                {
                    "start": 1433,
                    "end": 1437,
                    "matchedPaperCorpusId": "246430982"
                },
                {
                    "start": 1437,
                    "end": 1440,
                    "matchedPaperCorpusId": "256697203"
                },
                {
                    "start": 1561,
                    "end": 1565,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 1630,
                    "end": 1634,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 1634,
                    "end": 1638,
                    "matchedPaperCorpusId": "248986239"
                },
                {
                    "start": 1638,
                    "end": 1642,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 1694,
                    "end": 1697,
                    "matchedPaperCorpusId": "253510790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.227294921875
        },
        {
            "corpus_id": "235898880",
            "title": "Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills",
            "text": "Large pre-trained language models (LMs) Brown et al., 2020) have become the backbone of natural language processing in recent years. However, recent work has shown that they struggle in performing symbolic reasoning operations, such as composition or conjunction of facts (Talmor et al., , 2020, numerical operations (Wallace et al., 2019;Hidey et al., 2020), and quantification (Warstadt et al., 2019), without substantial amounts of additional data.\n\nPast work on improving reasoning in pre-trained models has taken two flavors: (a) adding specialized components for specific skills, like numerical and temporal reasoning (Ran et al., 2019;Gupta et al., 2020a;Khot et al., 2021;Chen et al., 2020a), or (b) generating synthetic examples at scale, for example, by using grammars or templates (Rozen \u2020 Work done while working at the Allen Institute for Artificial Intelligence. Figure 1: An example table and question-contextanswer triplets generated from the table as synthetic data. Each color corresponds to a different reasoning skill and colored cells are necessary to answer the question. The contexts shown are partial, such that the actual context contains the necessary information to answer the question and additional distractors. Answers are not necessarily extractive (e.g., date difference). Zhao et al., 2019;Andreas, 2020;Asai and Hajishirzi, 2020;Campagna et al., 2020), and question generation models (Alberti et al., 2019;Puri et al., 2020;Bartolo et al., 2021).\n\nIn this work, we take the latter approach and argue that semi-structured tables are a valuable resource for automatic generation of training data that can endow LMs with reasoning skills. Tables can be crawled from the web at scale, and cover a wide range of domains and topics. Moreover, their structured nature makes them amenable to automatic processes of data generation. Specifically, given a table, we use templates to generate reading comprehension (RC) examples, that is, questioncontext-answer triplets, where answering the question requires diverse types of reasoning over facts mentioned in the context. Fig. 1 shows an example table, and three generated question-context-answer examples",
            "score": 0.4486043796076775,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 40,
                    "end": 59,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 317,
                    "end": 339,
                    "matchedPaperCorpusId": "202583694"
                },
                {
                    "start": 339,
                    "end": 358,
                    "matchedPaperCorpusId": "216553790"
                },
                {
                    "start": 379,
                    "end": 402,
                    "matchedPaperCorpusId": "202233787"
                },
                {
                    "start": 624,
                    "end": 642,
                    "matchedPaperCorpusId": "202788490"
                },
                {
                    "start": 662,
                    "end": 680,
                    "matchedPaperCorpusId": "221448158"
                },
                {
                    "start": 1305,
                    "end": 1323,
                    "matchedPaperCorpusId": "201660404"
                },
                {
                    "start": 1323,
                    "end": 1337,
                    "matchedPaperCorpusId": "128000127"
                },
                {
                    "start": 1337,
                    "end": 1363,
                    "matchedPaperCorpusId": "216035859"
                },
                {
                    "start": 1363,
                    "end": 1385,
                    "matchedPaperCorpusId": "214816511"
                },
                {
                    "start": 1418,
                    "end": 1440,
                    "matchedPaperCorpusId": "189762081"
                },
                {
                    "start": 1440,
                    "end": 1458,
                    "matchedPaperCorpusId": "211258652"
                },
                {
                    "start": 1458,
                    "end": 1479,
                    "matchedPaperCorpusId": "233296924"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85302734375
        },
        {
            "corpus_id": "276259248",
            "title": "Bag of Tricks for Inference-time Computation of LLM Reasoning",
            "text": "Large language models (LLMs) have demonstrated remarkable reasoning capabilities, enabling them to tackle increasingly sophisticated tasks in fields such as science, mathematics, and coding (Zhang et al., 2024a;Chen et al., 2021). While scaling model size and expanding high-quality training datasets have significantly driven these advancements, researchers are actively exploring complementary approaches to further enhance model performance. Inspired by human problem-solving behavior-where individuals often dedicate more time deliberating on complex problems to improve their decisions-there is growing interest (Snell et al., 2024) in leveraging inference-time computation (e.g., utilizing additional computation during testing to enhance the performance of reasoning tasks) to strengthen the reasoning abilities of LLMs. \n\nWhile inference-time computation holds significant potential for enhancing the reasoning performance of LLMs (Wang et al., 2022), existing studies reveal mixed results in inference-time computation (e.g., limited selfcorrection capabilities (Huang et al., 2023)). Its effectiveness on broader reasoning tasks (e.g., logical reasoning, code generation, question answering, and fact verification) remains limited, with most research narrowly focused on domains like math problems. Moreover, inference-time methods are sensitive to hyperparameters, such as temperature and top-p sampling, where small adjustments can lead to notable performance differences (e.g., a 5% improvement in solving math problems by tuning temperature). These challenges underscore the critical role of inference-time techniques (e.g., instructing prompt, sampling strategies, reward models), as shown in Table 1. Despite recent advancements, these gaps indicate that the field remains nascent, with many challenges yet to be addressed. \n\nIn this study, we investigate key tricks that influence the effectiveness of inference-time computation methods in LLM reasoning. Since most current methods rely on a proposer-verifier pipeline that first generates candidate solu-Table 1: Configuration of inference-time computation methods. Inference-time computation involves two main steps: generating candidate solutions (e.g., chain-of-thought reasoning) and selecting the optimal solution. These configurations, though significant, often receive less attention and lack standardization.",
            "score": 0.44843361776062307,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1839
                },
                {
                    "start": 1842,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2287
                },
                {
                    "start": 2288,
                    "end": 2384
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20166015625
        },
        {
            "corpus_id": "270710993",
            "title": "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models",
            "text": "Past attempts have been made to assess the logical reasoning ability of language models. For instance, LogiQA (Liu et al., 2021) and ReClor (Yu et al., 2020) evaluate diverse forms of logical reasoning by compiling multi-choice questions from standardized examinations, including multi-step reasoning. However, in contrast to our Multi-LogiEval, these datasets involve mixed forms of reasoning and do not focus on assessing logical reasoning independently. Past attempts have been made to create datasets focusing on logical reasoning (Luo et al., 2023). In terms of task formulation, our proposed dataset is similar to ProofWriter (Tafjord et al., 2021), RuleTaker (Clark et al., 2021), FOLIO (Han et al., 2022), ProntoQA (Saparov and He, 2023), and LogicBench (Parmar et al., 2024) which are QA datasets designed to evaluate logical reasoning ability independently. ProofWriter provides multihop proofs for each example, RuleTaker mainly covers the simple implication rules such as modus ponens, while FOLIO gives diverse and complex logical expressions and covers multi-step reasoning. However, it is only limited to FOL. ProntoQA (Saparov and He, 2023) provides a QA dataset with explanation and reasoning steps but is limited to single-step modus ponens in FOL. Although Log-icBench (Parmar et al., 2024) covers various inference rules and reasoning patterns comprehensively, it only contains single-step logical reasoning (see Table 1 for comparison). Additional datasets for evaluating multi-step logical reasoning also exist, such as SimpleLogic (Zhang et al., 2022), which only covers modus ponens inference rule, and Rule-Bert (Saeed et al., 2021) which covers only soft logical rules. In contrast, Multi-LogiEval evaluates logical reasoning independently beyond modus ponens. In addition, FLD (Formal Logic Deduction) (Morishita et al., 2023) has formal logic theorybased inference rules, and their combinations to create multi-step reasoning, but limited to PL and FOL.",
            "score": 0.44829654603340313,
            "section_title": "Related Work",
            "char_start_offset": 5099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1981
                }
            ],
            "ref_mentions": [
                {
                    "start": 110,
                    "end": 128,
                    "matchedPaperCorpusId": "220483148"
                },
                {
                    "start": 140,
                    "end": 157,
                    "matchedPaperCorpusId": "209485573"
                },
                {
                    "start": 632,
                    "end": 653,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 723,
                    "end": 745,
                    "matchedPaperCorpusId": "252693237"
                },
                {
                    "start": 762,
                    "end": 783,
                    "matchedPaperCorpusId": "269330143"
                },
                {
                    "start": 1134,
                    "end": 1156,
                    "matchedPaperCorpusId": "252693237"
                },
                {
                    "start": 1288,
                    "end": 1309,
                    "matchedPaperCorpusId": "269330143"
                },
                {
                    "start": 1637,
                    "end": 1657,
                    "matchedPaperCorpusId": "237940562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.603515625
        },
        {
            "corpus_id": "276107648",
            "title": "Position: Empowering Time Series Reasoning with Multimodal LLMs",
            "text": "In the field of Natural Language Processing (NLP), reasoning refers to the process of deriving conclusions from textual evidence and logical principles (Besta et al., 2025). It involves tasks such as understanding implicit information, performing logical inferences, and applying commonsense knowledge. Reasoning capabilities are crucial for addressing complex language tasks like natural language inference, multi-hop question answering, and commonsense reasoning (Yu et al., 2024). Types of reasoning include Chain-of-Thought (CoT), which breaks problems into intermediate steps for clarity, deductive reasoning which applies general rules to specific cases, and inductive reasoning which generalizes from observations (Xia et al., 2024). Abductive reasoning identifies the most plausible explanations, while analogical reasoning transfers knowledge based on similarities (Shi et al., 2024;Lewis & Mitchell, 2024). Others include commonsense reasoning, probabilistic reasoning, and causal reasoning (Yu et al., 2024). These approaches enhance the interpretability and performance of NLP systems.",
            "score": 0.44819804514386424,
            "section_title": "A.2.1. REASONING IN NLP",
            "char_start_offset": 35948,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1097
                }
            ],
            "ref_mentions": [
                {
                    "start": 465,
                    "end": 482,
                    "matchedPaperCorpusId": "257766470"
                },
                {
                    "start": 874,
                    "end": 892,
                    "matchedPaperCorpusId": "258947766"
                },
                {
                    "start": 1001,
                    "end": 1018,
                    "matchedPaperCorpusId": "257766470"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.343994140625
        },
        {
            "corpus_id": "253107930",
            "title": "Leveraging QA Datasets to Improve Generative Data Augmentation",
            "text": "The ability of generative language models (GLMs) to generate text has improved considerably in the last few years, enabling their use for generative data augmentation. In this work, we propose CONDA, an approach to further improve GLM\u2019s ability to generate synthetic data by reformulating data generation as context generation for a given question-answer (QA) pair and leveraging QA datasets for training context generators. Then, we cast downstream tasks into the same question answering format and adapt the fine-tuned context generators to the target task domain. Finally, we use the fine-tuned GLM to generate relevant contexts, which are in turn used as synthetic training data for their corresponding tasks. We perform extensive experiments on multiple classification datasets and demonstrate substantial improvements in performance for both few- and zero-shot settings. Our analysis reveals that QA datasets that require high-level reasoning abilities (e.g., abstractive and common-sense QA datasets) tend to give the best boost in performance in both few-shot and zero-shot settings.",
            "score": 0.44781811627713985,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95458984375
        },
        {
            "corpus_id": "265150405",
            "title": "Simulating Public Administration Crisis: A Novel Generative Agent-Based Simulation System to Lower Technology Barriers in Social Science Research",
            "text": "The development of large language models (LLMs) such as Transformer [38], XLNet [39], and GPT-3 [9] has enabled powerful natural language processing capabilities like text generation, translation, and question answering. This has sparked interest in applying LLMs to computational social science (CSS) tasks [12]. Recent research has evaluated LLMs on diverse CSS tasks and found they can expand CSS capabilities despite lacking professional vocabulary knowledge [40]. LLMs reduce the cost of analyzing raw data, and their comprehensive databases and logical capabilities can propose new models and hypotheses [10]. \n\nA key innovative direction is LLMs' simulation capabilities [41]. LLMs can generate synthetic interview responses that position real interviews in advance [42], though cannot replace real survey data. LLMs also generate synthetic social media content that is difficult for humans to distinguish from real content [43]. Overall, LLMs have many promising applications in CSS, though fine-tuning may be needed for optimal performance on specialized tasks. More research is needed to responsibly leverage LLMs' strengths while mitigating their limitations.",
            "score": 0.44776473099106295,
            "section_title": "B. LLM in Computational Social Science",
            "char_start_offset": 5832,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1170
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 72,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 773,
                    "end": 777,
                    "matchedPaperCorpusId": "252280474"
                },
                {
                    "start": 931,
                    "end": 935,
                    "matchedPaperCorpusId": "257633591"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73681640625
        },
        {
            "corpus_id": "271891972",
            "title": "Towards Realistic Synthetic User-Generated Content: A Scaffolding Approach to Generating Online Discussions",
            "text": "The emergence of synthetic data represents a pivotal shift in modern machine learning, offering a source of large volumes of data in diverse domains where real data is scarce, highly private, or difficult to obtain. Examples include generating images for various computer vision problems [29], question answering corpora for pre-training neural models [3], query logs for evaluating query auto-completion systems [17], and dialogue datasets for improving conversational systems [16,19,20]. Synthetic data is particularly useful in the realm of user-generated content (UGC), where research and development critically depends on the availability of large-scale datasets that allow for the modeling and analysis of the dynamics of users and content. There, collecting real data is subject to complex privacy legislation, often expensive to collect, and problematic to distribute or share with other researchers, thereby hindering progress. In this paper, we investigate the feasibility of the creation of realistic synthetic corpora involving a wide variety of discussions among different users as may be observed across a broad range of usergenerated content platforms. \n\nExisting work has explored the generation of both graph and text data for social media data [26,31,33,35,42]. However, realistic natural language back-and-forth in conversations common in UGC has only attracted limited attention. In particular, UGC often consists of a sequence of utterances, which progress the topic of a conversation to a greater or lesser degree depending on the topic and participants that the platform attracts. We argue that modeling this structure is key to synthetic collections being usefully representative of actual interactions. We thus focus on generating synthetic collections that accurately represent user dynamics. \n\nLarge language models (LLMs) offer a natural starting point for generating synthetic social media discussion threads, given their capabilities to produce diverse responses that typify online interactions. However, as we demonstrate in this paper, LLMs out of the box have limited success in realistically capturing the complex structure of online discussion threads, and the standard prompting mechanism does not offer enough control over the generation process. Our solution is a multi-step data generation pipeline, predicated on the idea of creating compact representations of discussion threads, referred to as scaffolds. Our framework is designed to be generic in nature, yet provide the flexibility for individual components to be customized according to the unique characteristics of specific platforms.",
            "score": 0.4475229114751848,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1167
                },
                {
                    "start": 1170,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1818
                },
                {
                    "start": 1821,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2283
                },
                {
                    "start": 2284,
                    "end": 2446
                },
                {
                    "start": 2447,
                    "end": 2631
                }
            ],
            "ref_mentions": [
                {
                    "start": 352,
                    "end": 355,
                    "matchedPaperCorpusId": "189762081"
                },
                {
                    "start": 413,
                    "end": 417,
                    "matchedPaperCorpusId": "215746329"
                },
                {
                    "start": 478,
                    "end": 482,
                    "matchedPaperCorpusId": "253098861"
                },
                {
                    "start": 485,
                    "end": 488,
                    "matchedPaperCorpusId": "266650704"
                },
                {
                    "start": 1262,
                    "end": 1266,
                    "matchedPaperCorpusId": "249855800"
                },
                {
                    "start": 1266,
                    "end": 1269,
                    "matchedPaperCorpusId": "251403008"
                },
                {
                    "start": 1269,
                    "end": 1272,
                    "matchedPaperCorpusId": "10222268"
                },
                {
                    "start": 1272,
                    "end": 1275,
                    "matchedPaperCorpusId": "52281452"
                },
                {
                    "start": 1275,
                    "end": 1278,
                    "matchedPaperCorpusId": "257038527"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65625
        },
        {
            "corpus_id": "270738112",
            "title": "Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback",
            "text": "Large language models, such as GPT series (Brown et al., 2020;Achiam et al., 2023), PaLM (Anil et al., 2023), Mistral (Jiang et al., 2023), andLLaMA (Touvron et al., 2023a,b;AI@Meta, 2024), have shown great success in numerous tasks that require reasoning.Besides the approach to scaling up the size of large language models and training data to enhance their reasoning ability, many prompting methods have been proposed to improve their reasoning performance.Previous works (Wei et al., 2022;Kojima et al., 2022;Zelikman et al., 2022;Gao et al., 2023), which aim to enhance the reasoning ability of large language models, can be categorized into two main types: natural language-based Figure 1: Schematic overview of our framework, ART.\"Q\" denotes a question.\"NL\" means \"Natural Language\".\"RT\" means \"Relation Tuple\".The left subfigure shows our proposed framework ART without Self-Consistency (Wang et al., 2023).The right subfigure shows that our framework can be integrated with Self-Consistency seamlessly.\n\napproaches and non-natural language-based approaches.The natural language-based approaches include Chain-of-Thought (CoT) (Wei et al., 2022) and Zero-shot CoT (Kojima et al., 2022), which utilize intermediate reasoning steps in natural language to elicit the reasoning ability of large language models.The non-natural language-based approaches include PAL (Gao et al., 2023), which proposes to use Python code to solve math word problems.\n\nHowever, the reasoning steps represented in nat-ural language are usually long, which can significantly increase inference cost and may contain computational errors and unjustified logical leaps (Zhou et al., 2024b).Besides, unlike graphs or formal languages, they are difficult to verify because of the nature of natural language (Zhou et al., 2024b).",
            "score": 0.44749074186113386,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 256,
                    "end": 460
                },
                {
                    "start": 460,
                    "end": 737
                },
                {
                    "start": 737,
                    "end": 760
                },
                {
                    "start": 760,
                    "end": 790
                },
                {
                    "start": 790,
                    "end": 818
                },
                {
                    "start": 818,
                    "end": 915
                },
                {
                    "start": 915,
                    "end": 1011
                },
                {
                    "start": 1013,
                    "end": 1066
                },
                {
                    "start": 1066,
                    "end": 1315
                },
                {
                    "start": 1315,
                    "end": 1451
                },
                {
                    "start": 1453,
                    "end": 1669
                },
                {
                    "start": 1669,
                    "end": 1805
                }
            ],
            "ref_mentions": [
                {
                    "start": 475,
                    "end": 493,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 513,
                    "end": 535,
                    "matchedPaperCorpusId": "247762790"
                },
                {
                    "start": 895,
                    "end": 914,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1135,
                    "end": 1153,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1648,
                    "end": 1668,
                    "matchedPaperCorpusId": "268724184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.378662109375
        },
        {
            "corpus_id": "268247692",
            "title": "CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments",
            "text": "In this paper, our emphasis lies on the process of reasoning which depends on a formal system grounded in logical rules and principles. Such a system ensures that all transformations or manipulations of symbols within it, leading to new inferences, adhere to the logical rules and principles governing the system (MacColl, 1897). While LLMs can also be seen as performing symbolic manipulations, these manipulations unlike traditional symbolic reasoning are based on statistical associations or patterns learned from data (Huang and Chang, 2023), because of which it may or may not be logically sound. Despite the progress in the development of large language models (LLMs), many still struggle with a deep understanding of symbols like humans do (Abraham and Alirezaie, 2022;Yao et al., 2022). To address this gap, there are ongoing efforts to create benchmarks (Huang and Chang, 2023), like the proposed CLEVR-POC, to evaluate the reasoning capabilities of LLMs. In CLEVR-POC, we introduce a VQA task that involves constraint-based reasoning, a form of logical reasoning, where the generated response must sat-isfy a set of constraints given. These benchmarks are used to assess the capacity of language models in handling symbolic reasoning, contributing to the advancements in the development of more logically sound systems.",
            "score": 0.44748813867315756,
            "section_title": "LLMs and Reasoning",
            "char_start_offset": 6807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1329
                }
            ],
            "ref_mentions": [
                {
                    "start": 522,
                    "end": 545,
                    "matchedPaperCorpusId": "254877753"
                },
                {
                    "start": 747,
                    "end": 776,
                    "matchedPaperCorpusId": "247693677"
                },
                {
                    "start": 863,
                    "end": 886,
                    "matchedPaperCorpusId": "254877753"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28564453125
        },
        {
            "corpus_id": "271962004",
            "title": "Large Language Model with Curriculum Reasoning for Visual Concept Recognition",
            "text": "LLM for Reasoning Tasks. Large language models (LLM) [12,16,19,48,52,70] exhibit a high capacity to understand, generate, and manipulate textual information, making them valuable tools for various natural language processing tasks such as machine translation, text generation, sentiment analysis, and questionanswering [27,32,34,42]. \n\nAs the extensive knowledge repository possessed by LLMs across various domains, some researchers have tried to enable LLMs to engage in reasoning tasks through several novel techniques such as prompt engineering. The Chain-of-thought [58] (CoT) method serves as a representative method in which large language models are guided through prompts to provide rationales or justifications before generating answers. Tree-of-thoughts [64] expands upon the CoT approach by transforming the reasoning process from a linear chain to a tree structure, which enables large language models to better handle the reasoning tasks. And many previous works [10,20,45,50,60,65] have extended large language models reasoning to more complex tasks, such as Symbol-LLM [60], which applies LLM to reason human action recognition tasks and derive human-interpretable reasoning rules for this task. \n\nHowever, these methods rely on self-evaluation by the LLMs, which is not entirely reliable in sensitive domains like risk assessment due to the existence of knowledge hallucination. Additionally, the high computational cost associated with training multimodal LLMs poses challenges in accurately evaluating and processing multimodal data in dynamic and open environments. \n\nNeural Symbolic Reasoning. Deep neural networks have achieved tremendous success in many tasks, but when it comes to tasks requiring reasoning ability and explainability, there still remain a lot of problems for deep neural networks. In contrast, symbolic reasoning which emulates human cognitive reasoning processes using symbolic operators, has shown promising results in various numerical reasoning tasks [13,14,22,53]. However, tasks involving semantic understanding still pose significant challenges [5,43,44]. \n\nTo combine the reasoning capabilities of traditional symbolic methods with the learning abilities of deep neural networks, some approaches introduce novel structures that enable neural networks to possess reasoning capabilities.",
            "score": 0.44719118890118836,
            "section_title": "Related Works",
            "char_start_offset": 6793,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 333
                },
                {
                    "start": 336,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1584
                },
                {
                    "start": 1587,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2102
                },
                {
                    "start": 2105,
                    "end": 2333
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 60,
                    "matchedPaperCorpusId": "247519241"
                },
                {
                    "start": 323,
                    "end": 326,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 326,
                    "end": 329,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 329,
                    "end": 332,
                    "matchedPaperCorpusId": "240420063"
                },
                {
                    "start": 570,
                    "end": 574,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 976,
                    "end": 980,
                    "matchedPaperCorpusId": "253801709"
                },
                {
                    "start": 989,
                    "end": 992,
                    "matchedPaperCorpusId": "265498480"
                },
                {
                    "start": 1084,
                    "end": 1088,
                    "matchedPaperCorpusId": "265498480"
                },
                {
                    "start": 1995,
                    "end": 1999,
                    "matchedPaperCorpusId": "202781424"
                },
                {
                    "start": 1999,
                    "end": 2002,
                    "matchedPaperCorpusId": "108306725"
                },
                {
                    "start": 2005,
                    "end": 2008,
                    "matchedPaperCorpusId": "51907763"
                },
                {
                    "start": 2092,
                    "end": 2095,
                    "matchedPaperCorpusId": "8662046"
                },
                {
                    "start": 2095,
                    "end": 2098,
                    "matchedPaperCorpusId": "4642944"
                },
                {
                    "start": 2098,
                    "end": 2101,
                    "matchedPaperCorpusId": "19454866"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.297119140625
        },
        {
            "corpus_id": "261822699",
            "title": "Dynamic MOdularized Reasoning for Compositional Structured Explanation Generation",
            "text": "Compositional Generalization in Reasoning Compositional generalization has been researched for decades (Fodor and Pylyshyn, 1988;Marcus, 2003;Lake and Baroni, 2018) and formalized in Hupkes et al. (2020). Recently, there has been a renewed interest in exploring the compositional generalization of reasoning tasks. Johnson et al. (2017); Hudson and Manning (2019); Bogin et al. (2021); Gao et al. (2022) proposed challenging compositional tasks in visual question answering. Liu et al. (2022) designed compositional questions for question answering and found even the strongest model struggled with these challenging questions. Other works probed the compositional ability of models in natural language inference (NLI) (Geiger et al., 2020;Goodwin et al., 2020;Yanaka et al., 2020Yanaka et al., , 2021)). They focus on specific linguistic phenomena, such as quantifiers, negation, predicate replacements, etc. I.e., they only investigate compositionality in phenomena manifesting at the word level, lacking understanding on the sentence level. To fill this gap, our work examines compositional generalization in the multi-step entailment tree generation task, which aims to compose different inference rules. \n\nNeural-Symbolic and Neural Methods Prior works show that symbolic approaches (Angeli and Manning, 2014;Mineshima et al., 2015;Mart\u00ednez-G\u00f3mez et al., 2017) that adopt pre-defined inference rules to establish the derivation through iterative reasoning, endow models with strong compositionality. However, they rely on prerequisite rules, hence limited to well-defined tasks and symbolic systems. Recently, several approaches (Yi et al., 2018;Yin et al., 2018;Li et al., 2020;Jiang et al., 2021) used neural networks to map raw signals to symbolic representations and subsequently performed symbolic reasoning to make predictions. Since symbolic reasoning is brittle, novel works based on Neural Modular Networks (NMN) (Andreas et al., 2016;Hu et al., 2017) have been introduced. These methods compose individual neural modules endowed with specialized reasoning capabilities.",
            "score": 0.4471295778334573,
            "section_title": "Related Work",
            "char_start_offset": 5323,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1208
                },
                {
                    "start": 1211,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2084
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 129,
                    "matchedPaperCorpusId": "29043627"
                },
                {
                    "start": 142,
                    "end": 164,
                    "matchedPaperCorpusId": "46761158"
                },
                {
                    "start": 183,
                    "end": 203,
                    "matchedPaperCorpusId": "211259383"
                },
                {
                    "start": 315,
                    "end": 336,
                    "matchedPaperCorpusId": "15458100"
                },
                {
                    "start": 365,
                    "end": 384,
                    "matchedPaperCorpusId": "237592834"
                },
                {
                    "start": 386,
                    "end": 403,
                    "matchedPaperCorpusId": "239998781"
                },
                {
                    "start": 475,
                    "end": 492,
                    "matchedPaperCorpusId": "237417170"
                },
                {
                    "start": 719,
                    "end": 740,
                    "matchedPaperCorpusId": "222396695"
                },
                {
                    "start": 740,
                    "end": 761,
                    "matchedPaperCorpusId": "218581294"
                },
                {
                    "start": 761,
                    "end": 780,
                    "matchedPaperCorpusId": "216868459"
                },
                {
                    "start": 780,
                    "end": 803,
                    "matchedPaperCorpusId": "231709707"
                },
                {
                    "start": 1288,
                    "end": 1314,
                    "matchedPaperCorpusId": "2854390"
                },
                {
                    "start": 1314,
                    "end": 1337,
                    "matchedPaperCorpusId": "17291409"
                },
                {
                    "start": 1337,
                    "end": 1365,
                    "matchedPaperCorpusId": "17619888"
                },
                {
                    "start": 1634,
                    "end": 1651,
                    "matchedPaperCorpusId": "52919654"
                },
                {
                    "start": 1651,
                    "end": 1668,
                    "matchedPaperCorpusId": "49325612"
                },
                {
                    "start": 1668,
                    "end": 1684,
                    "matchedPaperCorpusId": "219573265"
                },
                {
                    "start": 1684,
                    "end": 1703,
                    "matchedPaperCorpusId": "235485085"
                },
                {
                    "start": 1927,
                    "end": 1949,
                    "matchedPaperCorpusId": "5276660"
                },
                {
                    "start": 1949,
                    "end": 1965,
                    "matchedPaperCorpusId": "18682"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.341796875
        },
        {
            "corpus_id": "258588454",
            "title": "Multi-hop Commonsense Knowledge Injection Framework for Zero-Shot Commonsense Question Answering",
            "text": "We follow the definition in the previous frameworks and propose two methods for synthetic multihop QA generation on this basis, as shown in Compositive Commonsense QA Generation. Compositive commonsense QA is generated by triples with the logical relationship in the knowledge graph. This is the most common form of multi-hop commonsense knowledge, which shows the continuous reasoning ability of humans. For example, when you see the word \"university\", you will think of \"library\", and then think of \"books\". Our proposed generation method is as follows, divided into two steps. (1) Generating question and answer. Formally, given a triple (e head 1 , r 1 , e tail 1 ), we find another triple (e head 2 , r 2 , e tail 2 ), while e tail 1 = e head 2 = e key . Each triple will be converted into a sentence through the natural language template. Then, we integrate these two sentences and mask the connected entities e key in them as the question Q. Meanwhile, entity e key is also the correct answer A c . (2) Generating hard negative samples (distractors). We generate distractors following the above criteria as follows. Formally, for each distractor entity e 3 , in order to ensure it is relevant to the question, it needs to satisfy the condition: (e head 1 , r 1 , e 3 ). Meanwhile, to ensure that it is an error option, it needs to satisfy the condition: a): e 3 = e key , b): for any (e 3 , r 2 , e tail 3 ), e tail 3 = e tail 2 . We take e 3 as a distractor A i and select the distractor set {A 1 , \u2022 \u2022 \u2022 , A n } in this way. Finally, we can get a compositive commonsense QA pairs (Q, A) sample. Figure 2 (a) shows an example of this process. \n\nConjunctive Commonsense QA Generation. Conjunctive commonsense QA is generated by entities with multiple attributes in the knowledge graph, which shows that human thinking ability is multi-faceted. For example, when you see a \"gym\", you think of \"basketball\", \"football\", and other sports simultaneously.",
            "score": 0.4471003703735583,
            "section_title": "Synthetic Multi-hop QA Generation",
            "char_start_offset": 8567,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1650
                },
                {
                    "start": 1653,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1957
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39794921875
        },
        {
            "corpus_id": "258437058",
            "title": "Beyond Classification: Financial Reasoning in State-of-the-Art Language Models",
            "text": "Large Language Models (LLMs), consisting of 100 billion or more parameters, have demonstrated remarkable ability in complex multi-step reasoning tasks. However, the application of such generic advancements has been limited to a few fields, such as clinical or legal, with the field of financial reasoning remaining largely unexplored. To the best of our knowledge, the ability of LLMs to solve financial reasoning problems has never been dealt with, and whether it can be performed at any scale remains unknown. To address this knowledge gap, this research presents a comprehensive investigation into the potential application of LLMs in the financial domain. The investigation includes a detailed exploration of a range of subjects, including task formulation, synthetic data generation, prompting methods, and evaluation capability. Furthermore, the study benchmarks various GPT variants with parameter scales ranging from 2.8B to 13B, with and without instruction tuning, on diverse dataset sizes. By analyzing the results, we reveal that the ability to generate coherent financial reasoning first emerges at 6B parameters, and continues to improve with better instruction-tuning or larger datasets. Additionally, the study provides a publicly accessible dataset named sFIOG (Synthetic-Financial Investment Opinion Generation), consisting of 11,802 synthetic investment thesis samples, to support further research in the field of financial reasoning. Overall, this research seeks to contribute to the understanding of the efficacy of language models in the field of finance, with a particular emphasis on their ability to engage in sophisticated reasoning and analysis within the context of investment decision-making.",
            "score": 0.4470942948783372,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.452392578125
        },
        {
            "corpus_id": "239016123",
            "title": "Generated Knowledge Prompting for Commonsense Reasoning",
            "text": "A multiple-choice commonsense reasoning task involves predicting an answer a \u2208 A q given a ques-tion q \u2208 Q, where the set of choices A q is finite and can vary by question, and both questions and answers are variable-length text sequences. Our method answers commonsense questions in two steps.\n\nThe first step is knowledge generation, where we use a language model p G (k|q) to generate knowledge statements conditioned on the question:\n\nwhere each knowledge statement k m is a variablelength text sequence. Intuitively, each statement contains information that is helpful for answering the question (e.g. Table 1).\n\nThe second step is knowledge integration, where generated knowledge is integrated into the decision process of a language model used for inference, a = arg max a\u2208Aq p I (a|q, K q )\n\nIn contrast, the vanilla setting of using the inference model without knowledge is represented b\u0177 a = arg max a\u2208Aq p I (a|q).\n\nNext, we describe the knowledge generation and integration steps in detail.",
            "score": 0.44676794302605993,
            "section_title": "Generated Knowledge Prompting",
            "char_start_offset": 4383,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.205078125
        },
        {
            "corpus_id": "263909600",
            "title": "Does Synthetic Data Make Large Language Models More Efficient?",
            "text": "In the burgeoning field of Natural Language Processing (NLP), acquiring substantial data for training and fine-tuning models is a continual challenge [Vaswani et al., 2017]. While real-world annotated datasets are invaluable, their availability is often constrained, making them expensive to produce, and they can sometimes carry inherent biases from their collection methods [Bowman et al., 2015]. This context underscores the potential of synthetic data generation techniques, with synthetic questionanswer pairs emerging as a notable subset [Rajpurkar et al., 2016]. Among the diverse strategies available, template-based question generation, recognized for its rule-driven approach, provides a systematic avenue for data generation [Chen et al., 2016]. \n\nHowever, as with many techniques in the realm of computational linguistics, the adoption of templatebased generation in transformer models within NLP presents a complex landscape to navigate [Devlin et al., 2018]. This paper seeks to illuminate the intricacies of this approach, offering insights into its methodologies, advantages, and inherent limitations. Through our examination, our aim is to equip readers with a nuanced understanding of the technique, its impact on transformer architectures, and the potential avenues for its evolution in NLP research. \n\nThe implementation of template-based question generation for creating synthetic question-answer pairs can significantly impact the performance of a LLM in several ways: \n\n1. Data Augmentation: The most direct impact is the increase in training data. When you create synthetic question-answer pairs from existing text, you're effectively augmenting your dataset, which can be particularly useful when dealing with tasks where the amount of available labeled data is limited. This increased data volume helps the model better understand language patterns and variations, which can enhance the model's ability to generalize, ultimately improving performance. \n\n2. Exposure to Diverse Structures: Template-based question generation exposes the transformer model to a wider variety of question structures and types. This increased exposure helps the model develop a more comprehensive understanding of language and better performance on a broader range of questions. \n\n3. Model Robustness: By creating synthetic data that includes a variety of linguistic features and structures, the model becomes more robust.",
            "score": 0.44639955441161105,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 756
                },
                {
                    "start": 759,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1319
                },
                {
                    "start": 1322,
                    "end": 1490
                },
                {
                    "start": 1493,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1977
                },
                {
                    "start": 1980,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2283
                },
                {
                    "start": 2286,
                    "end": 2427
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 172,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8623046875
        },
        {
            "corpus_id": "258833020",
            "title": "Teaching Probabilistic Logical Reasoning to Transformers",
            "text": "Language models have demonstrated high performance across a wide range of Natural Language Processing (NLP) tasks (Liu et al., 2019) which in the case of Large Language Models holds even in zero-shot setting (Chen, 2023). However, they struggle to reason over uncertain text involving logical probabilistic rules (Saeed et al., 2021;Jin et al., 2023). This is confirmed by the reported poor results in arithmetic reasoning when using transformers (Mishra et al., 2022) which is required for probabilistic logical reasoning. Additionally, logical probabilistic inference requires coherent step-by-step reasoning. However, PLMs' evaluation of various question-answering (QA) benchmarks shows they produce contradictory results that violate the expected steps of reasoning, such as following transitivity or symmetry rules (Asai and Hajishirzi, 2020). This has led to the development of hybrid approaches, where reasoning tasks are outsourced to Neuro-Symbolic engines, bypassing the need for reasoning by transformers (Zhang et al., 2023). To overcome these limitations, we embed probabilistic reasoning into transformers by imposing the rules of logical probabilistic reasoning as constraints during their training phase. \n\nThere are only a few research efforts dealing with uncertainty in text. Understanding logical and uncertain rules in natural language form has been investigated in recent research on question answering (Clark et al., 2020;Saeed et al., 2021), and there have been several attempts to teach transformers how to follow these rules (Asai and Hajishirzi, 2020;Faghihi et al., 2023). While incorporating hard logical rules is undoubtedly important and is still being investigated, in the real world, most of the external knowledge and rules involve uncertainty. For example, only a small fraction of the logical rules in DBpedia can be deemed certain (Saeed et al., 2021). Inference over text that includes uncertainty concerning facts, relations, and rules is required in many natural language comprehension tasks. For example, scientific content often utilizes hedges to express the measure of certainty in factual statements (Pei and Jurgens, 2021;National Academies of Sciences et al., 2017).",
            "score": 0.44631271206679357,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1220
                },
                {
                    "start": 1223,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 208,
                    "end": 220,
                    "matchedPaperCorpusId": "252872943"
                },
                {
                    "start": 313,
                    "end": 333,
                    "matchedPaperCorpusId": "237940562"
                },
                {
                    "start": 447,
                    "end": 468,
                    "matchedPaperCorpusId": "248118588"
                },
                {
                    "start": 820,
                    "end": 847,
                    "matchedPaperCorpusId": "216035859"
                },
                {
                    "start": 1016,
                    "end": 1036,
                    "matchedPaperCorpusId": "252760744"
                },
                {
                    "start": 1445,
                    "end": 1464,
                    "matchedPaperCorpusId": "237940562"
                },
                {
                    "start": 1551,
                    "end": 1578,
                    "matchedPaperCorpusId": "216035859"
                },
                {
                    "start": 1868,
                    "end": 1888,
                    "matchedPaperCorpusId": "237940562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.166015625
        },
        {
            "corpus_id": "276249784",
            "title": "Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation",
            "text": "Logical soundness is a fundamental requirement in constructing a robust FOL reasoning dataset. While LLMs are highly proficient at generating natural language, they often fall short in maintaining logical coherence, particularly when dealing with intricate, multi-step reasoning processes. To address this limitation, we introduce a novel approach that integrates symbolic prover into the data generation pipeline, allowing for the construction of logically rigorous FOL reasoning problems. \n\nIn our data generation framework, we start with constructing a logic skeleton for each problem, represented as a reasoning tree with the ultimate goal at its root (as shown in Figure 1(b)). Previous works typically generate reasoning problems in a bottom-up approach by starting with premises and building toward conclusions. We argue that this approach could be flawed as it can often result in disconnected or contradictory premises, making it difficult to ensure that all facts are logically coherent with each other. For instance, if the problem includes facts such as f 1 : Sawyer is a poet, f 2 : Sawyer comes from Mars, and f 3 : Sawyer is a large, fluffy Maine Coon cat, then integrating these elements into a consistent reasoning chain is challenging. Therefore, our framework adopts a novel top-down approach, which begins with the goal and works backward to derive the relevant premises and rules needed to logically support that goal. This method ensures that each fact and rule contributes directly to the reasoning process, improving both the coherence and logical soundness of the generated problems. Moreover, this top-down structure simplifies the problem of logical consistency by allowing LLMs to focus on facts that are directly relevant to the conclusion, without needing to reconcile them with unrelated information. \n\nMore specifically, as shown in Figure 1(b), we begin by setting the truth value of the goal, denoted as f 3 = True, and treat this as the root node of the reasoning tree. Next, we sample a logical expression for this node, (f 1 \u2227 f 2 ) \u2192 f 3 , which establishes a relationship between the goal f 3 and the premises f 1 and f 2 . At this stage, we use the symbolic prover Prover9 to calculate the truth values of f 1 and f 2 , ensuring that the premises logically support the goal.",
            "score": 0.44608353231610587,
            "section_title": "LOGIC SKELETON GENERATION",
            "char_start_offset": 13218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1831
                },
                {
                    "start": 1834,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2314
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.484130859375
        },
        {
            "corpus_id": "233405184",
            "title": "Explainable Multi-hop Verbal Reasoning Through Internal Monologue",
            "text": "Many state-of-the-art (SOTA) language models have achieved high accuracy on several multi-hop reasoning problems. However, these approaches tend to not be interpretable because they do not make the intermediate reasoning steps explicit. Moreover, models trained on simpler tasks tend to fail when directly tested on more complex problems. We propose the Explainable multi-hop Verbal Reasoner (EVR) to solve these limitations by (a) decomposing multi-hop reasoning problems into several simple ones, and (b) using natural language to guide the intermediate reasoning hops. We implement EVR by extending the classic reasoning paradigm General Problem Solver (GPS) with a SOTA generative language model to generate subgoals and perform inference in natural language at each reasoning step. Evaluation of EVR on the RuleTaker synthetic question answering (QA) dataset shows that EVR achieves SOTA performance while being able to generate all reasoning steps in natural language. Furthermore, EVR generalizes better than other strong methods when trained on simpler tasks or less training data (up to 35.7% and 7.7% absolute improvement respectively).",
            "score": 0.4460584822973651,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66748046875
        },
        {
            "corpus_id": "269005156",
            "title": "Reason from Fallacy: Enhancing Large Language Models' Logical Reasoning through Logical Fallacy Understanding",
            "text": "Since then, logical fallacies have gradually gained attention in human society.In recent years, the studies related to logical fallacies mainly focused on dataset construction (Habernal et al., 2018;Martino et al., 2020;Jin et al., 2022) and fallacy classification (Stab and Gurevych, 2017;Goffredo et al., 2022;Jin et al., 2022;Payandeh et al., 2023).For instance, Jin et al. first proposed the task of Logical Fallacy Detection, presenting a framework of 13 logical fallacy types, and evaluated all sentence samples on a classification task.Sourati et al. proposed a Case-Based Reasoning method that classifies new cases of logical fallacy by language-modeling-driven retrieval and the adaptation of historical cases.However, there is no work to systematically evaluate LLMs' capability of logical fallacy understanding (LFU).For the first time, our work in this paper proposes a new dataset specific to LFU represented by five concrete tasks corresponding to three cognitive di-mensions of WHAT, WHY, and HOW.\n\nLearning from Synthetic Data Synthesizing data for model training has gradually gained popularity along with the advancements of language models.This approach is particularly beneficial for tasks that are difficult to be constructed or those with scarce data resources (M\u00f8ller et al., 2023).Currently, synthetic data has been applied in various tasks such as relation extraction (Papanikolaou and Pierleoni, 2020), text classification (Chung et al., 2023), irony detection (Abaskohi et al., 2022), translation (Sennrich et al., 2015), and sentiment analysis (Maqsud, 2015).For example, Josifoski et al. proposed a strategy to design an effective synthetic data generation pipeline and applied it to closed information extraction.In addition, Li et al. conducted a series of experiments to evaluate the effectiveness of LLMs in generating synthetic data to support model training for different text classification tasks.Beyond these fundamental tasks, Eldan and Li proposed to use LLMs with synthetic data to generate short stories typically for 3 to 4year-old only containing words.But they did not focus on logical fallacy.",
            "score": 0.445591913760288,
            "section_title": "Related Work",
            "char_start_offset": 8057,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 79,
                    "end": 352
                },
                {
                    "start": 352,
                    "end": 543
                },
                {
                    "start": 543,
                    "end": 719
                },
                {
                    "start": 719,
                    "end": 828
                },
                {
                    "start": 828,
                    "end": 1012
                },
                {
                    "start": 1014,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1305
                },
                {
                    "start": 1305,
                    "end": 1587
                },
                {
                    "start": 1587,
                    "end": 1743
                },
                {
                    "start": 1743,
                    "end": 1933
                },
                {
                    "start": 1933,
                    "end": 2096
                },
                {
                    "start": 2096,
                    "end": 2138
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 199,
                    "matchedPaperCorpusId": "3504755"
                },
                {
                    "start": 220,
                    "end": 237,
                    "matchedPaperCorpusId": "247158013"
                },
                {
                    "start": 265,
                    "end": 290,
                    "matchedPaperCorpusId": "6801402"
                },
                {
                    "start": 290,
                    "end": 312,
                    "matchedPaperCorpusId": "250631593"
                },
                {
                    "start": 312,
                    "end": 329,
                    "matchedPaperCorpusId": "247158013"
                },
                {
                    "start": 1487,
                    "end": 1510,
                    "matchedPaperCorpusId": "248227550"
                },
                {
                    "start": 1572,
                    "end": 1586,
                    "matchedPaperCorpusId": "11815268"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6005859375
        },
        {
            "corpus_id": "274597917",
            "title": "Knowledge Graphs are all you need: Leveraging KGs in Physics Question Answering",
            "text": "As LLMs become more prevalent, ensuring their faithfulness (adherence to factual and logical constraints) has emerged as a critical concern (Lin and Chen, 2021;Zhang et al., 2022a). In the context of question decomposition for large language models (LLMs), ensuring that generated sub-questions remain faithful to the original intent and logic is essential. Several recent studies have explored various techniques to improve this aspect of LLM functionality: \n\n1. Prompting Techniques: An in-depth analysis of different prompting methods for LLMs suggests that well-structured prompts can significantly enhance the performance and applicability of these models in complex reasoning tasks. Techniques such as \"Chain-of-Thought\" prompting enable LLMs to process information sequentially, thereby maintaining logical consistency and alignment with the initial query's intent (Wei et al., 2022). \n\n2. Multi-task Learning: Studies like (Gao et al., 2021) have shown that multi-task learning can be effectively leveraged to improve the few-shot learning capabilities of LLMs. By fine-tuning models on a small set of examples across various tasks, these models can generalize better and respond more accurately to diverse inputs. \n\n3. Constrained Decoding: Efficient prompting methods not only guide the generation process but also constrain it to generate outputs that adhere more closely to desired outcomes. This is particularly useful in tasks where maintaining the integrity of the generated content is crucial. Techniques for efficient prompting include optimizing the design of prompts and compressing them to reduce the computational burden (Chang et al., 2022). \n\nPrevious work on retrieval-augmented generation, such as SciPhyRAG (Anand et al., 2023b), highlights the benefits of incorporating relevant documents to enhance LLM performance in physics question answering. Our approach extends this concept by using knowledge graphs to maintain logical consistency in sub-question generation.",
            "score": 0.4454887138408642,
            "section_title": "Faithfulness in LLMs",
            "char_start_offset": 7478,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1222
                },
                {
                    "start": 1225,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 931,
                    "end": 949,
                    "matchedPaperCorpusId": "229923710"
                },
                {
                    "start": 1733,
                    "end": 1754,
                    "matchedPaperCorpusId": "266126733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.237548828125
        },
        {
            "corpus_id": "257771900",
            "title": "Explicit Planning Helps Language Models in Logical Reasoning",
            "text": "Our system is a kind of general model programs (Dohan et al., 2022)especially those with verification models (Cobbe et al., 2021)-which use language models inside as probabilistic programs and apply disparate inference algorithms to the models. Other kinds of approaches to use LMs for reasoning include training discriminative models (Clark et al., 2020;Picco et al., 2021;Ghosal et al., 2022;Zhang et al., 2023), prompting GPT-3 with spelled-out reasoning procedure (Wei et al., 2022;Talmor et al., 2020), and distilling GPT-3.5 (Fu et al., 2023). \n\nAnother straightforward approach for text-based logical reasoning is to first translate natural language statements into formal logic expressions and then use a formal logic inference engine (Weber et al., 2019;Levkovskyi and Li, 2021;Nye et al., 2021;Lu et al., 2022b;Betz and Richardson, 2022). We tried this approach in our experiments; please see Appendix C.3 for details. \n\nAnother research area related to multi-step logical reasoning is to reason over graph-structured data. A popular kind of graph is knowledge graphs, i.e., relational graphs over symbolic tuples (Lao and Cohen, 2010;Wang et al., 2013;Neelakantan et al., 2015;Cohen et al., 2017;Xiong et al., 2017;Chen et al., 2018;Das et al., 2018). Another kind of graph is built by linking texts via lexical overlap or hyperlink connections (Welbl et al., 2018;Yang et al., 2018;Khot et al., 2020Khot et al., , 2021)). Methods in this area involve multi-step navigation through graphs. But they rely on pre-defined symbolic and relational structures, thus not directly applicable to our setting. Additionally, recent research (Chen and Durrett, 2019; Min et al., 2019) shows that optimizing the performance on these datasets is not well aligned to improving the models' fundamental reasoning abilities.",
            "score": 0.4453959027655417,
            "section_title": "Related Work",
            "char_start_offset": 24031,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1817
                }
            ],
            "ref_mentions": [
                {
                    "start": 335,
                    "end": 355,
                    "matchedPaperCorpusId": "263869790"
                },
                {
                    "start": 355,
                    "end": 374,
                    "matchedPaperCorpusId": "237562817"
                },
                {
                    "start": 394,
                    "end": 413,
                    "matchedPaperCorpusId": "248986434"
                },
                {
                    "start": 468,
                    "end": 486,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 486,
                    "end": 506,
                    "matchedPaperCorpusId": "263869790"
                },
                {
                    "start": 743,
                    "end": 763,
                    "matchedPaperCorpusId": "189898046"
                },
                {
                    "start": 787,
                    "end": 804,
                    "matchedPaperCorpusId": "235743241"
                },
                {
                    "start": 804,
                    "end": 821,
                    "matchedPaperCorpusId": "252571115"
                },
                {
                    "start": 1145,
                    "end": 1163,
                    "matchedPaperCorpusId": "1981826"
                },
                {
                    "start": 1163,
                    "end": 1188,
                    "matchedPaperCorpusId": "16590714"
                },
                {
                    "start": 1207,
                    "end": 1226,
                    "matchedPaperCorpusId": "20667722"
                },
                {
                    "start": 1376,
                    "end": 1394,
                    "matchedPaperCorpusId": "52822214"
                },
                {
                    "start": 1666,
                    "end": 1683,
                    "matchedPaperCorpusId": "174801764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1939697265625
        },
        {
            "corpus_id": "277501721",
            "title": "Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models",
            "text": "The emergence of large language models (LLMs) has revolutionized natural language processing (NLP) capabilities across diverse applications (Minaee et al., 2024;Naveed et al., 2024). A critical component maximizing LLM efficacy lies in prompt engineering (Brown et al., 2020;Radford et al., 2019), which involves designing and refining the input prompts to elicit more accurate, relevant, and focused outputs. Among recent breakthroughs, chain-of-thought (CoT) prompting (Wei et al., 2023) has emerged as a transformative strategy. \n\nCoT prompting has proven particularly valuable in reasoning tasks that inherently demand multi-step logical inference to derive accurate conclusions. Traditional large language models (LLMs) demonstrate the capability to generate immediate responses, yet fundamentally lack the capacity to externalize their internal reasoning pathways or justify conclusions through explicit logical progression. The CoT methodology bridges this capability gap by structurally guiding models to articulate intermediate reasoning steps, thereby significantly enhancing performance on complex multi-step tasks requiring systematic analysis. Furthermore, the self-consistency technique (Wang X. et al., 2023) employs sampling strategies such as beam search and nucleus sampling (Holtzman et al., 2020) instead of greedy decoding, which has been empirically shown to amplify CoT's effectiveness in domains requiring rigorous deductive processesincluding mathematical reasoning, symbolic logic operations, and causal inference (Chen et al., 2023;P. Wang et al., 2023;Shao et al., 2024). Despite these advancements, several challenges remain in the reasoning domain. \n\nFirstly, although CoT prompting has demonstrated substantial improvements in multistep reasoning tasks, models exhibit constrained generalization capacity when handling novel complexity scenarios. LLMs still face difficulties when tasked with reasoning about unfamiliar domains or situations not well-represented in the training data (Stechly et al., 2025). Secondly, while the self-consistency methodology enhances models' performance by sampling the reasoning paths over breadth, issues of reasoning incorrectness persist (Chen et al., 2023;P. Wang et al., 2023), particularly when most of the sampled reasoning paths trapped in local optima.",
            "score": 0.4452029854356551,
            "section_title": "Introduction",
            "char_start_offset": 109,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 531
                },
                {
                    "start": 534,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1678
                },
                {
                    "start": 1681,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2325
                }
            ],
            "ref_mentions": [
                {
                    "start": 275,
                    "end": 296,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64208984375
        },
        {
            "corpus_id": "263608847",
            "title": "Large Language Models as Analogical Reasoners",
            "text": "Reasoning involves the application of knowledge to derive solutions for new problems, often through a series of steps. Teaching language models to reason has been a long-standing area of research (Bottou, 2014;Zhao et al., 2023;Wei et al., 2022b). \n\nTo assess the reasoning capabilities of language models, researchers have created datasets for various tasks that demand reasoning skills. These tasks include multi-step question answering (Yang et al., 2018;Dua et al., 2019;Talmor et al., 2018), mathematical problem-solving (Cobbe et al., 2021;Hendrycks et al., 2021b), and code generation (Yu et al., 2018;Chen et al., 2021;Hendrycks et al., 2021a;Austin et al., 2021). In this study, we evaluate our methods using these diverse datasets. \n\nTo teach language models to reason effectively, one line of approaches involve training or fine-tuning them. This can include using reasoning-intensive data during training (Wu et al., 2021;Yasunaga et al., 2022b;Lightman et al., 2023;Moor et al., 2023), retrieving structured knowledge (Lin et al., 2019;Feng et al., 2020;Zhang et al., 2022a;Yasunaga et al., 2021;2022a;Xie et al., 2022), and incorporating external modules for reasoning such as logic and program execution (Chen et al., 2018;2019;Yasunaga & Liang, 2020;Gupta et al., 2020;Ren et al., 2021;Zhang et al., 2023). \n\nRecently, with the rise of large language models (LLMs), prompting them to engage in reasoning has proven effective and gained attention. A common approach is prompting LLMs to generate intermediate reasoning steps, as demonstrated by the chain-of-thought method (Wei et al., 2022b;Kojima et al., 2022;Zhou et al., 2022;Wang et al., 2022), which assists LLMs in tackling complex reasoning tasks.",
            "score": 0.4451995834009238,
            "section_title": "A.1 LANGUAGE MODELS AND REASONING",
            "char_start_offset": 35845,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 247
                },
                {
                    "start": 250,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1322
                },
                {
                    "start": 1325,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1720
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 210,
                    "matchedPaperCorpusId": "1067591"
                },
                {
                    "start": 228,
                    "end": 246,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 917,
                    "end": 934,
                    "matchedPaperCorpusId": "231627491"
                },
                {
                    "start": 934,
                    "end": 957,
                    "matchedPaperCorpusId": "247793456"
                },
                {
                    "start": 979,
                    "end": 997,
                    "matchedPaperCorpusId": "260316059"
                },
                {
                    "start": 1031,
                    "end": 1049,
                    "matchedPaperCorpusId": "202540096"
                },
                {
                    "start": 1049,
                    "end": 1067,
                    "matchedPaperCorpusId": "218486837"
                },
                {
                    "start": 1067,
                    "end": 1087,
                    "matchedPaperCorpusId": "246240437"
                },
                {
                    "start": 1087,
                    "end": 1109,
                    "matchedPaperCorpusId": "235421942"
                },
                {
                    "start": 1219,
                    "end": 1238,
                    "matchedPaperCorpusId": "53317540"
                },
                {
                    "start": 1243,
                    "end": 1266,
                    "matchedPaperCorpusId": "218763496"
                },
                {
                    "start": 1266,
                    "end": 1285,
                    "matchedPaperCorpusId": "220546239"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5234375
        },
        {
            "corpus_id": "273375535",
            "title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
            "text": "Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce $\\textbf{F}$aithful $\\textbf{L}$ogic-$\\textbf{A}$ided $\\textbf{R}$easoning and $\\textbf{E}$xploration ($\\textbf{FLARE}$), a novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on $\\mathbf{7}$ out of $\\mathbf{9}$ diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that $\\textbf{FLARE}$ allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search.",
            "score": 0.44494719835932967,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1536865234375
        },
        {
            "corpus_id": "269330143",
            "title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
            "text": "Large language models such as GPT-4, ChatGPT, Google Gemini, Llama-2 (Touvron et al., 2023), and Mistral (Jiang et al., 2023) have made remarkable progress in NLP research enabling machines to perform a variety of language tasks that were previously thought to be exclusive to humans (OpenAI, 2023;Brown et al., 2020;Zhao et al., 2023).However, the ability of these LLMs to reason \"logically\" over natural language text remains under-explored, even though logical reasoning is a fundamental aspect of intelligence and a crucial requirement for many practical applications, such as questionanswering systems (Khashabi, 2019) and conversational agents (Beygi et al., 2022).Although several datasets have been proposed (Clark et al., 2021;Tian et al., 2021;Joshi et al., 2020;Saeed et al., 2021) to evaluate the logical reasoning capabilities of LLMs, these datasets are limited in their scope by (1) not evaluating logical reasoning independently of other forms of reasoning such as LogiQA (Liu et al., 2021a) and ReClor (Yu et al., 2020); and (2) evaluating only a single type of logic and covering only few logical inference rules as done in FOLIO (Han et al., 2022) and ProntoQA (Saparov and He, 2023).Thus, our aim in this work is to address the lacuna of having a more comprehensive set of inference rules for evaluating the logical reasoning ability of LLMs.\n\nTo this end, we introduce a systematically created question-answering dataset for the evaluation of logical reasoning ability using a single inference rule, called LogicBench.Besides evaluating the logical reasoning ability of LLMs, by evaluating models on single inference rules, we can also gain insights into the frequency of text sequences corresponding to these rules in the pre-training data and their impact on model performance.As illustrated in Figure 1, LogicBench includes a total of 25 reasoning patterns across 'propositional, first-order, and non-monotonic' logics.",
            "score": 0.44411339365150554,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 336
                },
                {
                    "start": 336,
                    "end": 671
                },
                {
                    "start": 671,
                    "end": 1203
                },
                {
                    "start": 1203,
                    "end": 1362
                },
                {
                    "start": 1364,
                    "end": 1539
                },
                {
                    "start": 1539,
                    "end": 1800
                },
                {
                    "start": 1800,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 650,
                    "end": 670,
                    "matchedPaperCorpusId": "246680186"
                },
                {
                    "start": 716,
                    "end": 736,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 754,
                    "end": 773,
                    "matchedPaperCorpusId": "222066957"
                },
                {
                    "start": 1019,
                    "end": 1036,
                    "matchedPaperCorpusId": "209485573"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.375244140625
        },
        {
            "corpus_id": "277452727",
            "title": "WHERE and WHICH: Iterative Debate for Biomedical Synthetic Data Augmentation",
            "text": "The synthetic data augmentation aims to generate pseudo instances corresponding to the given instances in semantics but with diverse syntax. The existing methods can be divided into two categories: (a) Amendment-based methods. This kind of method tries to amend the exact token or order of input sentences according to pre-defined rules to augment the data. [8] adopts synonym replacement, random insertion, random swap, and random deletion to augment the original sentences. \n\nBeyond the token-level, [16] interpolates the embeddings and labels of two or more sentences from representation-level. and (b) Generation-based methods. These years, leveraging generative large language models to generate kinds of data has become popular. [17] and [18] fine-tune multiple generative models for each relation type to generate augmentations. [10] proposes a sophisticated generation-based method that generates augmented data by incorporating new linguistic patterns. Beyond that, [19] introduced ChatGPT to make great progress in nearly all kinds of NLP tasks. It has proved to be effective utilizing ChatGPT as a data augmentation technique to enrich the data instances [20]. \n\nHowever, the existing data augmentation methods generally lean to generate instances leaving faithfulness and factuality alone which poison the model in understanding the interaction among biomedical entities and relations, and further, misleading models cannot be adapted to the real scenarios.",
            "score": 0.4439982908604859,
            "section_title": "B. Synthetic Data Augmentation in BioNLP",
            "char_start_offset": 8102,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1171
                },
                {
                    "start": 1174,
                    "end": 1469
                }
            ],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 361,
                    "matchedPaperCorpusId": "59523656"
                },
                {
                    "start": 735,
                    "end": 739,
                    "matchedPaperCorpusId": "212821571"
                },
                {
                    "start": 836,
                    "end": 840,
                    "matchedPaperCorpusId": "232379879"
                },
                {
                    "start": 1166,
                    "end": 1170,
                    "matchedPaperCorpusId": "260063065"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91650390625
        },
        {
            "corpus_id": "270562345",
            "title": "Can Large Language Models Code Like a Linguist?: A Case Study in Low Resource Sound Law Induction",
            "text": "We propose two synthetic data generation algorithms to generate language-agnostic data of different complexities to provide additional taskspecific supervision to the LLMs for the task.We outline these here.",
            "score": 0.4439982908604859,
            "section_title": "Synthetic Data Generation",
            "char_start_offset": 8664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 185,
                    "end": 207
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.603515625
        },
        {
            "corpus_id": "12307380",
            "title": "Natural Language Generation with Tree Conditional Random Fields",
            "text": "One of the ultimate goals in the field of natural language processing (NLP) is to enable computers to converse with humans through human languages. To achieve this goal, two important issues need to be studied. First, it is important for computers to capture the meaning of a natural language sentence in a meaning representation. Second, computers should be able to produce a humanunderstandable natural language sentence from its meaning representation. These two tasks are referred to as semantic parsing and natural language generation (NLG), respectively.\n\nIn this paper, we use corpus-based statistical methods for constructing a natural language generation system. Given a set of pairs, where each pair consists of a natural language (NL) sentence and its formal meaning representation (MR), a learning method induces an algorithm that can be used for performing language generation from other previously unseen meaning representations. A crucial question in any natural language processing system is the representation used. Meaning representations can be in the form of a tree structure. In Lu et al. (2008), we introduced a hybrid tree framework together with a probabilistic generative model to tackle semantic parsing, where tree structured meaning representations are used. The hybrid tree gives a natural joint tree representation of a natural language sentence and its meaning representation. A joint generative model for natural language and its meaning representation, such as that used in Lu et al. (2008) has several advantages over various previous approaches designed for semantic parsing. First, unlike most previous approaches, the generative approach models a simultaneous generation process for both NL and MR. One elegant property of such a joint generative model is that it allows the modeling of both semantic parsing and natural language generation within the same process. Second, the generative process proceeds as a recursive top-down Markov process in a way that takes advantage of the tree structure of the MR. The hybrid tree generative model proposed in Lu et al. (2008) was shown to give stateof-the-art accuracy in semantic parsing on benchmark corpora.\n\nWhile semantic parsing with hybrid trees has been studied in Lu et al. (2008), its inverse task -NLG with hybrid trees -has not yet been explored. We believe that the properties that make the hybrid trees effective for semantic parsing also make them effective for NLG. In this paper, we develop",
            "score": 0.4438970429730789,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1100,
                    "end": 1116,
                    "matchedPaperCorpusId": "8045822"
                },
                {
                    "start": 1507,
                    "end": 1523,
                    "matchedPaperCorpusId": "8045822"
                },
                {
                    "start": 2090,
                    "end": 2106,
                    "matchedPaperCorpusId": "8045822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5869140625
        },
        {
            "corpus_id": "275515421",
            "title": "Reasoning with Graphs: Structuring Implicit Knowledge to Enhance LLMs Reasoning",
            "text": "Large language models (LLMs) have demonstrated remarkable success across a wide range of tasks; however, they still encounter challenges in reasoning tasks that require understanding and inferring relationships between distinct pieces of information within text sequences. This challenge is particularly pronounced in tasks involving multi-step processes, such as logical reasoning and multi-hop question answering, where understanding implicit relationships between entities and leveraging multi-hop connections in the given context are crucial. Graphs, as fundamental data structures, explicitly represent pairwise relationships between entities, thereby offering the potential to enhance LLMs' reasoning capabilities. External graphs have proven effective in supporting LLMs across multiple tasks. However, in many reasoning tasks, no pre-existing graph structure is provided. Can we structure implicit knowledge derived from context into graphs to assist LLMs in reasoning? In this paper, we propose Reasoning with Graphs (RwG) by first constructing explicit graphs from the context and then leveraging these graphs to enhance LLM reasoning performance on reasoning tasks. Extensive experiments demonstrate the effectiveness of the proposed method in improving both logical reasoning and multi-hop question answering tasks.",
            "score": 0.4431086224740424,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.662109375
        },
        {
            "corpus_id": "274280623",
            "title": "CoA: Chain-of-Action for Generative Semantic Labels",
            "text": "Large language models (LLMs) [6,11,33,35,38,39,49,51] have garnered significant attention in recent years due to their ability to understand and generate natural language text. Models like GPT-3 [6] and LLaMA [39] have demonstrated impressive performance across various natural language processing tasks. These models are typically pre-trained on massive text data and fine-tuned on specific downstream tasks [11,39,49]. They have been employed in applications such as text generation, translation, summarization, and question answering. Research efforts have focused on enhancing the capabilities, efficiency, and interpretability of these large language models. The chain-of-thought (CoT) [5,22,40,41,45,47,48,[53][54][55][56] technique has emerged as a valuable approach to guide and structure the generation of content by LLMs. By utilizing a progressive hierarchical chain structure, researchers aim to enhance the coherence and contextuality of model-generated text [45,53,56]. This technique serves as a mechanism to direct the flow of information and ensure the generation of relevant and meaningful content [22,47]. CoT play a crucial role in constraining the output of LLMs, enabling them to produce text that aligns closely with desired contexts and themes. Integrating the CoT technique with LLMs has the potential to improve the quality and relevance of generated text across a wide range of applications and use cases. \n\nA key distinction of our approach from these methods is the use of generated context instead of pre-collected context during the inference step in CoA design. This allows us to fully leverage the inherent reasoning capabilities of LLMs. Additionally, our method employs a zero-shot approach during inference, making it adaptable across a range of LLM-based architectures.",
            "score": 0.4430351873392439,
            "section_title": "Large language models / Chain-of-Thought",
            "char_start_offset": 6890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1432
                },
                {
                    "start": 1435,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1806
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 32,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 32,
                    "end": 35,
                    "matchedPaperCorpusId": "247519241"
                },
                {
                    "start": 35,
                    "end": 38,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 38,
                    "end": 41,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 41,
                    "end": 44,
                    "matchedPaperCorpusId": "252780443"
                },
                {
                    "start": 47,
                    "end": 50,
                    "matchedPaperCorpusId": "252715691"
                },
                {
                    "start": 195,
                    "end": 198,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 409,
                    "end": 413,
                    "matchedPaperCorpusId": "247519241"
                },
                {
                    "start": 416,
                    "end": 419,
                    "matchedPaperCorpusId": "252715691"
                },
                {
                    "start": 691,
                    "end": 694,
                    "matchedPaperCorpusId": "261030303"
                },
                {
                    "start": 697,
                    "end": 700,
                    "matchedPaperCorpusId": "258546810"
                },
                {
                    "start": 700,
                    "end": 703,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 703,
                    "end": 706,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 706,
                    "end": 709,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 712,
                    "end": 716,
                    "matchedPaperCorpusId": "252762275"
                },
                {
                    "start": 716,
                    "end": 720,
                    "matchedPaperCorpusId": "256504063"
                },
                {
                    "start": 720,
                    "end": 724,
                    "matchedPaperCorpusId": "264451538"
                },
                {
                    "start": 724,
                    "end": 728,
                    "matchedPaperCorpusId": "248986239"
                },
                {
                    "start": 972,
                    "end": 976,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 976,
                    "end": 979,
                    "matchedPaperCorpusId": "252762275"
                },
                {
                    "start": 979,
                    "end": 982,
                    "matchedPaperCorpusId": "248986239"
                },
                {
                    "start": 1120,
                    "end": 1123,
                    "matchedPaperCorpusId": "258762525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.493896484375
        },
        {
            "corpus_id": "269149041",
            "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
            "text": "CoT [131] is an advanced reasoning method that solves complex problems by breaking them down into a series of logical steps.The key to this method is that it allows models to process multi-step reasoning problems through a series of intermediate steps,allocating more computational resources for problems that require deep reasoning.This process can be represented by the following illustrative mathematical formula:\n\nIn Equation ( 6):\n\n-Q is the problem we are trying to solve.\n\n-Init represents the initial state or known information of the problem-solving process.\n\n-Step i denotes the reasoning process at the ith step,which is based on the result of the previous step Step i\u22121 .\n\n-Answer is the final answer,derived from the result of the last step of reasoning Step n .\n\nKey features of CoT [131] include:\n\n\u2022 Step-by-step reasoning: The CoT [131] method generates a reasoning chain through step-by-step reasoning in natural language before presenting the final answer.\u2022 Explainability: CoT [131] provides a window that allows us to see how the model solves problems step by step,increasing the transparency and explainability of the reasoning process.\u2022 Broad applicability: CoT [131] can be applied to mathematical problems,common sense reasoning,symbolic operations,and more,even suitable for any problem that needs to be solved through language.Implementation of CoT: The implementation of CoT [131] relies on the capabilities of large language models,which can guide reasoning abilities even without directly provided samples.The inception of CoT [131] is attributed to two important papers,with \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\" [131](Fig.28) being the seminal work authored by Jason Wei and others.The second paper,\"Large Language Models are Zero-Shot Reasoners,\" [52] further simplifies the CoT process by adding \"Let's think step by step\" to the prompt,guiding the model to generate reasoning chains automatically.Fig. 28: Chain-of-thought prompting enables large language models to tackle complex arithmetic, commonsense, and symbolic reasoning tasks.Chain-of-thought reasoning processes are highlighted.",
            "score": 0.4429461504912872,
            "section_title": "C. Chain-of-Thought Reasoning (CoT)",
            "char_start_offset": 80505,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 124,
                    "end": 416
                },
                {
                    "start": 418,
                    "end": 435
                },
                {
                    "start": 437,
                    "end": 478
                },
                {
                    "start": 480,
                    "end": 567
                },
                {
                    "start": 569,
                    "end": 683
                },
                {
                    "start": 685,
                    "end": 775
                },
                {
                    "start": 777,
                    "end": 811
                },
                {
                    "start": 813,
                    "end": 974
                },
                {
                    "start": 974,
                    "end": 1157
                },
                {
                    "start": 1157,
                    "end": 1353
                },
                {
                    "start": 1353,
                    "end": 1535
                },
                {
                    "start": 1535,
                    "end": 1687
                },
                {
                    "start": 1687,
                    "end": 1747
                },
                {
                    "start": 1747,
                    "end": 1965
                },
                {
                    "start": 1965,
                    "end": 2103
                },
                {
                    "start": 2103,
                    "end": 2156
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50390625
        },
        {
            "corpus_id": "237450610",
            "title": "Flexible Generation of Natural Language Deductions",
            "text": "in terms of available operations, among other desirable properties (Rudin, 2019).\n\nIn this work, we describe a generative model for single-step deductive reasoning, building towards models capable of generating the range of logical transformations needed for the full reasoning process. We use a BART-based sequence-tosequence model (Lewis et al., 2020) to represent the distribution of valid conclusion statements conditioned on one or more premise statements. To make sound inferences, the model must be fine-tuned on well-formed training data. We describe a pipeline for crafting this data based on syntactic retrieval from Wikipedia, rule-based example construction, and automatic paraphrasing to increase diversity. Our hypothesis is that the logical regularities in the constructed examples will teach models to generate correct deductions, while paraphrasing coupled with the inductive bias from pretraining will regularize models, allowing them to robustly tolerate natural lexical and syntactic variation in their inputs.\n\nWe demonstrate our method's effectiveness by using it to create models for two distinct logical operations, substitution and contraposition, examples of which are shown in Figure 1. Through experiments on manually-constructed English perturbation sets, as well as on the English Question Answering via Sentence Composition (QASC) and EntailmentBank datasets (Khot et al., 2020;, we show that our proposed data generation method leads to accurate and robust operation models. While baseline methods tend to default to trivial input copying and fail to assign significant likelihood to valid novel conclusions, we show that our operation models reliably generate consistent inferences. Evaluating our substitution model on fact compositions from the QASC and EntailmentBank datasets, we find that our method produces valid conclusions at rates equivalent to models trained on in-domain, humanannotated data, indicating that our method is a viable substitute for expensive direct supervision.",
            "score": 0.4428274102001861,
            "section_title": "Contraposition model",
            "char_start_offset": 2486,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 67,
                    "end": 80,
                    "matchedPaperCorpusId": "182656421"
                },
                {
                    "start": 333,
                    "end": 353,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1390,
                    "end": 1409,
                    "matchedPaperCorpusId": "204915921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75048828125
        },
        {
            "corpus_id": "265301936",
            "title": "Certified Deductive Reasoning with Language Models",
            "text": "We use this framework to allow language models to locally constrain generation to a set of valid statements determined by an external tool. We leverage the Peano theorem-proving environment (Poesia & Goodman, 2022) to construct LOGICGUIDE, which an LM can use to formalize its assumptions, set proof goals and make sound inferences step-by-step. The model can intersperse formal reasoning and natural language during generation. Language conditioned on previous formal steps is highly reliable, since the generations allowed by LOGICGUIDE are formally certified. \n\nWe first validate our method on three logical reasoning datasets, PrOntoQA (Saparov & He, 2022), ProofWriter (Tafjord et al., 2021), and Syllogistic Validity (Dasgupta et al., 2022). We also follow the format and methodology of PrOntoQA to introduce a new dataset, DeontiQA, where problems require reasoning using deontic logic principles to determine whether actions are permissible, obligatory or forbidden. When used with few-shot prompting, we find that LOGICGUIDE significantly improves the accuracy of OpenAI GPT-3 and GPT-3.5 Turbo, and LLaMA 13B. Moreover, models using LOGICGUIDE have drastically lower content effects: we show this both with PrOntoQA and in the Syllogism Validity dataset, used previously to measure content effects in LLMs. \n\nWhile for these problems we could leverage an external solver to obtain a final answer after the model produces a sufficient formalization, a key advantage of LOGICGUIDE is that the LLM still generates a step-by-step rationale. This allows us to then apply self-improvement methods, such as the Self-Taught Reasoner (STaR; Zelikman et al. (2022)), which improves the model's own reasoning by fine-tuning on rationales that led to correct answers. In the tasks we analyze here, there's a high probability of guessing the answer (e.g. true or false, so at least 50%). Hence, STaR alone fails to yield meaningful improvements, as it fine tunes on incorrect reasoning that does not generalize.",
            "score": 0.44251989909438016,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3958,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1316
                },
                {
                    "start": 1319,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 674,
                    "end": 696,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 1642,
                    "end": 1664,
                    "matchedPaperCorpusId": "247762790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.346435546875
        },
        {
            "corpus_id": "272368345",
            "title": "Path-Consistency: Prefix Enhancement for Efficient Inference in LLM",
            "text": "The range of tasks that large language models (LLMs) can accomplish is continuously expanding, as the scale and complexity of models continue to grow recently. However, this advancement has not yet endowed LLMs with sufficiently robust reasoning capabilities (Rae et al., 2021;Wu et al., 2016;Guo et al., 2018;Chen et al., 2022). To address this shortcoming and further extend the application scope of LLMs, chain-of-thought (CoT) (Wei et al., 2022) prompting has emerged in response. CoT prompting uses reasoning problems and processes as input prompts to guide language models in generating reasoning paths and final answers, aim-1 Shanghai Jiao Tong University Joint Institute, Shanghai, China 2 Microsoft Corporation. Correspondence to: An Zou <an.zou@sjtu.edu.cn>. ing to mimic the thought processes humans might use when solving mathematical or logical problems. This enables LLMs to be applied in an increasing number of specific scenarios, such as mathematical reasoning (Cobbe et al., 2021;Miao et al., 2020), logical reasoning (Geva et al., 2021), and code generation (Chen et al., 2021). To better accomplish these complex tasks, there are several CoT-based optimization methods (Li et al., 2022;Chen et al., 2022). One of the common and effective methods is self-consistency (Wang et al., 2023), a technique involving multiple sampling and majority voting. In this approach, the model generates multiple reasoning paths for a given input, with the final decision based on the most frequently occurring output among the samples. \n\nSelf-consistency significantly enhances the reasoning capabilities of LLMs by sampling a large number of examples during inference. However, it has limitations in practical applications. The basic self-consistency technique frequently invokes the model to generate numerous reasoning paths when solving specific problems (Wang et al., 2023). As model size and task complexity increase, the time and computation cost of self-consistency increase sharply, making it a critical issue in practical applications. Motivated by this challenge, this paper proposes path-consistency, which is designed to reduce the computation and time cost by leveraging intermediate information from earlier generation results to assist in subsequent generations.",
            "score": 0.4423121544208161,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1539
                },
                {
                    "start": 1542,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2282
                }
            ],
            "ref_mentions": [
                {
                    "start": 293,
                    "end": 310,
                    "matchedPaperCorpusId": "3389583"
                },
                {
                    "start": 431,
                    "end": 449,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 999,
                    "end": 1017,
                    "matchedPaperCorpusId": "220047831"
                },
                {
                    "start": 1287,
                    "end": 1306,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1863,
                    "end": 1882,
                    "matchedPaperCorpusId": "247595263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31494140625
        },
        {
            "corpus_id": "235743241",
            "title": "Improving Coherence and Consistency in Neural Sequence Models with Dual-System, Neuro-Symbolic Reasoning",
            "text": "For example, when System 2 is engaged by asking people to take more time to respond, people's accuracy improves on the CRT task above (Kahneman, 2013). It has been argued that integrating System 2 processing could similarly improve AI systems (Goyal & Bengio, 2020;Garcez & Lamb, 2020), and here we explore this idea as applied to neural sequence models. \n\nIn this work, we take inspiration from dual process theories to explore a neuro-symbolic generation system, wherein predictions from a neural model are treated as System 1 proposals, and a logical, deliberative System 2 filters these proposals for consistency and soundness (see Figure 1). We further take inspiration from the fact that humans often do not need explicit supervision to reason about new problems or domains (e.g., see human evaluation task in Section 4.2) and require that the System 2 module not need additional problem-specific training, especially on example contradictions or commonsense violations. People can handle novelty by reconfiguring, rather than retraining, their internal models (Lake et al., 2017), and we strive to build machine systems capable of the same. We show how a lightweight, easy-to-implement System 2 model can help improve coherence and consistency by adding a small amount of symbolic reasoning. \n\nWe tackle two kinds of domains: text generation and instruction following. In both cases, we construct generative models over sequences by using a neural generation model to propose candidate generations and a symbolic world model that can accept or reject the generations and resample proposals if necessary. We first illustrate the approach by generating short stories based on the bAbI dataset (Weston et al., 2015); this pedagogical, synthetic example illustrates how basic commonsense knowledge of objects, agents, and places can inform a text generation model. We then test our approach on rich, natural language vignettes based on CLUTRR (Sinha et al., 2019), focusing on ensuring consistency of family and interpersonal relationships. In both text generation domains, we interface between the explicit logical knowledge/reasoning of System 2 and generations of System 1 using a few-shot learning approach with state-of-the-art neural language models (GPT-3), which requires no additional training or fine-tuning.",
            "score": 0.44223907514144745,
            "section_title": "Introduction",
            "char_start_offset": 3827,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 354
                },
                {
                    "start": 357,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1298
                },
                {
                    "start": 1301,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2321
                }
            ],
            "ref_mentions": [
                {
                    "start": 1067,
                    "end": 1086,
                    "matchedPaperCorpusId": "260496023"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.265625
        },
        {
            "corpus_id": "277244806",
            "title": "Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural Language Self-Critique",
            "text": "Large language models (LLMs) have significantly transformed natural language processing by enabling sophisticated reasoning and problem-solving abilities. However, enhancing the reasoning capabilities of LLMs, especially in complex tasks that require multi-step logical deductions, remains a significant challenge. One critical technique for addressing this challenge is inference time scaling, which strategically allocates computational resources during inference to explore a broader space of potential reasoning paths beyond single, deterministic trajectories. Recent methods employing inference time scaling have demonstrated the effectiveness of this strategy in improving the robustness and accuracy of LLMs' reasoning processes (Yao et al., 2024;OpenAI, 2024;Snell et al., 2025;Guo et al., 2025;Team, 2024). \n\nA prominent framework for implementing inference time scaling is step-level tree search, which iteratively explores possible reasoning steps to construct a solution path (Villalobos & Atkinson, 2023;Luo et al., 2024;Wan et al., 2024). Central to this framework is the mechanism for evaluating and selecting the most promising reasoning paths. Traditional approaches assess the quality of each candidate step using step-level verifiers, which often utilize scalar reward signals derived from process reward models (PRMs) (Lightman et al., 2024a;Wang et al., 2024). These verifiers output numerical scores representing the correctness or desirability of steps, guiding the search algorithm towards paths with higher scores. \n\nHowever, relying on scalar rewards introduces significant limitations. First, reducing complex reasoning steps to single numerical values inevitably sacrifices nuanced qualitative information essential for understanding and justifying each step. Important insights, justifications, and potential errors may be overlooked, hindering the model's ability to perform complex reasoning. Second, effective verifiers are often task-specific and require substantial training on annotated datasets that may not be available for many advanced reasoning tasks, particularly in STEM domains. Finally, the development and integration of these verifiers impose considerable computational overhead and complexity (Guo et al., 2025). \n\nIn this paper, we present a novel inference-time scaling approach called stepwise natural language self-critique (PANEL).",
            "score": 0.4420250403581273,
            "section_title": "Introduction",
            "char_start_offset": 2002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1539
                },
                {
                    "start": 1542,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2121
                },
                {
                    "start": 2122,
                    "end": 2259
                },
                {
                    "start": 2262,
                    "end": 2383
                }
            ],
            "ref_mentions": [
                {
                    "start": 736,
                    "end": 754,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 1034,
                    "end": 1051,
                    "matchedPaperCorpusId": "263310590"
                },
                {
                    "start": 1338,
                    "end": 1362,
                    "matchedPaperCorpusId": "258987659"
                },
                {
                    "start": 1362,
                    "end": 1379,
                    "matchedPaperCorpusId": "266209760"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51708984375
        },
        {
            "corpus_id": "44096233",
            "title": "Think Visually: Question Answering through Virtual Imagery",
            "text": "Natural language datasets for QA: Several natural language QA datasets have been proposed to test AI systems on various reasoning abilities (Levesque et al., 2011;Richardson et al., 2013). Our work differs from them in two key aspects: first, we use synthetic data instead of natural data; and second, we specialize in geometrical reasoning instead of general language understanding. Using synthetic data helps us simplify language parsing and thereby focus on geometric reasoning. However, additional research is necessary to generalize our work to natural data. \n\nSynthetic datasets for QA: Recently, synthetic datasets for QA are also becoming crucial in AI. In particular, bAbI (Weston et al., 2015) has driven the development of several recent DNN-based QA systems (Kumar et al., 2016;Sukhbaatar et al., 2015;Henaff et al., 2016). bAbI consists of 20 tasks to evaluate different reasoning abilities. Two tasks, Positional Reasoning (PR) and Path Finding (PF), are related to geometric reasoning. However, each Positional Reasoning question contains only two sentences, and can be solved through simple logical deduction such as 'A is left of B implies B is right of A'. Similarly, Path Finding involves a search problem that requires simple spatial deductions such as 'A is east of B implies B is west of A'. In contrast, the questions in our datasets involve longer descriptions, more entities, and more relations; they are thus harder to answer with simple deductions. We also provide reference visual representation for each sample, which is not available in bAbI.",
            "score": 0.44171240979001164,
            "section_title": "Related Work",
            "char_start_offset": 4464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1572
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 163,
                    "matchedPaperCorpusId": "15710851"
                },
                {
                    "start": 163,
                    "end": 187,
                    "matchedPaperCorpusId": "2100831"
                },
                {
                    "start": 770,
                    "end": 790,
                    "matchedPaperCorpusId": "2319779"
                },
                {
                    "start": 790,
                    "end": 814,
                    "matchedPaperCorpusId": "1399322"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80419921875
        },
        {
            "corpus_id": "276960938",
            "title": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection Decoding",
            "text": "Large Language Models (LLMs) have revolutionized natural language processing (NLP), demonstrating remarkable performance across a wide range of tasks, including question-answering, code generation, and machine translation. Scaling up model size has led to improvements in fluency, generalization, and sample efficiency (Wei et al., 2022b). However, despite these advancements, LLMs continue to struggle with complex reasoning tasks such as arithmetic, commonsense reasoning, and multi-step logical inference. To address these challenges, researchers have primarily relied on prompt engineering techniques, such as fewshot Chain-of-Thought (CoT) prompting and zeroshot CoT (Brown et al., 2020;Besta et al., 2023; Figure 1: Overview of our proposed method, Adaptive Injection Decoding, with injecting a designated phrase \"Well\" whenever the model is likely to conclude its response immaturely with a <eos> token. Madaan et al., 2023;Wei et al., 2022a;Yao et al., 2023). These approaches guide LLMs by providing structured reasoning examples or explicit step-bystep instructions, significantly improving performance on various reasoning benchmarks. However, they exhibit key limitations in that few-shot prompting requires human-crafted exemplars, while zeroshot prompting is highly sensitive to prompt phrasing. More recently, some studies have explored CoT reasoning without prompts by modifying the decoding strategy (Wang and Zhou, 2023;Wang et al., 2023a). Additionally, a test-time scaling method has been introduced, which intervenes during inference to control test-time compute (Muennighoff et al., 2025). \n\nDespite various efforts to enhance LLM reasoning, existing approaches have not directly addressed the fundamental causes of their reasoning failures. Through our analysis, we categorize these failures into three primary types, highlighting the need for a more targeted intervention. First, models terminate generation without producing a response to given questions, returning only the <eos> token, a phenomenon we refer to as silence. Second, models often produce responses that lack actual logical reasoning, responding with uninformative outputs or irrelevant patterns, such as numeric sequences or question repetition, a failure type we call no reasoning.",
            "score": 0.4416825334923292,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1611
                },
                {
                    "start": 1614,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2273
                }
            ],
            "ref_mentions": [
                {
                    "start": 319,
                    "end": 338,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 672,
                    "end": 692,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 949,
                    "end": 966,
                    "matchedPaperCorpusId": "249674500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44775390625
        },
        {
            "corpus_id": "271050320",
            "title": "H-STAR: LLM-driven Hybrid SQL-Text Adaptive Reasoning on Tables",
            "text": "Tabular data are among the most widely used formats for storing structured information in realworld applications. Tabular reasoning presents an inherently challenging problem, requiring logical, mathematical, and textual reasoning over unstructured queries and structured tables (Ye et al., 2023). Thus, understanding and inferring tabular data has become a significant area of research in natural language processing. Tabular reasoning tasks (Figure 1), such as table-based question answering (Pasupat and Liang, 2015;Nan et al., 2022) and table-based fact verification (Chen et al., 2019), have been extensively explored in the past. For each task, question Q is paired with its answer A, which varies by task. Evidence shows the relevant columns and rows needed to answer the question. \n\nAdvancements in Large Language Models (LLMs) have led to better performance across various tasks through carefully crafted prompts. In the domain of table reasoning, symbolic approaches such as Program of Thought (Chen et al., 2023) and textual methods such as Chain of Thought (CoT) (Wei et al., 2022;Brown et al., 2020) have been explored. However, when applied individually (see Figure 2) these methods often struggle due to the complexities imposed by the intricate mix of numerical, temporal, and textual data, coupled with complex table structures (Shi et al., 2023;Liu et al., 2023;Sui et al., 2024). Textual reasoning excels in natural language understanding but often misinterprets table structures and struggles with quantitative reasoning. In contrast, SQL-based approaches are strong in quantitative problem solving, but perform poorly on noisy or unstructured inputs (Liu et al., 2023). As a result, recent techniques (Cheng Figure 2: An illustration highlighting the complexity of table reasoning and the need for an integrated approach: (a) \n\nOriginal table, (b) Symbolic reasoning misinterprets the question and returns a value instead of a yes/no response, and (c) Text-based approach fails to solve a math question correctly, leading to an incorrect answer.",
            "score": 0.44167582069417943,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1846
                },
                {
                    "start": 1849,
                    "end": 2066
                }
            ],
            "ref_mentions": [
                {
                    "start": 279,
                    "end": 296,
                    "matchedPaperCorpusId": "256416408"
                },
                {
                    "start": 494,
                    "end": 519,
                    "matchedPaperCorpusId": "9027681"
                },
                {
                    "start": 519,
                    "end": 536,
                    "matchedPaperCorpusId": "232478685"
                },
                {
                    "start": 571,
                    "end": 590,
                    "matchedPaperCorpusId": "198917339"
                },
                {
                    "start": 1004,
                    "end": 1023,
                    "matchedPaperCorpusId": "253801709"
                },
                {
                    "start": 1075,
                    "end": 1093,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1093,
                    "end": 1112,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1345,
                    "end": 1363,
                    "matchedPaperCorpusId": "256459776"
                },
                {
                    "start": 1380,
                    "end": 1397,
                    "matchedPaperCorpusId": "258833533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37548828125
        },
        {
            "corpus_id": "277451521",
            "title": "Reasoning Beyond Limits: Advances and Open Problems for LLMs",
            "text": "Instead of decoding hidden states into textual tokens, Coconut directly feeds them back into the model as input, enabling a more flexible and efficient reasoning process. By utilizing two modes-Language Mode for standard text generation and Latent Mode for iterative hidden-state processing-Coconut facilitates a chain of continuous thoughts, allowing the model to explore multiple alternative reasoning paths simultaneously. This method, marked by special \u00a1bot\u00bf and \u00a1eot\u00bf tokens, enables breadth-first search (BFS)-like reasoning, which outperforms CoT in complex planning tasks such as ProntoQA and ProsQA, particularly those requiring substantial backtracking. Additionally, Coconut significantly reduces token generation during inference, making it more efficient while maintaining accuracy. Inspired by iterative CoT (iCoT), the model is trained using a multi-stage curriculum learning strategy that gradually replaces language-based reasoning with latent-space reasoning, ensuring a smooth transition. The findings demonstrate that leveraging latent representations enhances reasoning efficiency, making Coconut a promising advancement in LLM-based problem-solving and strategic planning. \n\n2) Chain-of-Retrieval Augmented Generation (CoRAG): Wang et al. [77] introduced Chain-of-Retrieval Augmented Generation (CoRAG), a novel approach designed to address a key limitation in traditional RAG systems-namely, the reliance on a single retrieval step that often falls short when handling complex, multi-hop queries. Conventional RAG methods retrieve supporting information once before the generation phase, which can lead to incomplete or suboptimal evidence for intricate queries. In contrast, CoRAG enables dynamic query reformulation by allowing the model to iteratively retrieve and integrate information, effectively building a chain of reasoning that better supports the final answer. To overcome the challenge of limited annotated data for intermediate steps, the authors employ rejection sampling to automatically generate plausible retrieval chains from existing RAG datasets that typically only include the final correct answer. During inference, they propose flexible decoding strategies that adjust the length and number of retrieval chains, providing a tradeoff between computational cost and accuracy. Experimental results demonstrate that CoRAG significantly outperforms strong baselines-improving exact match scores by over 10 points on multi-hop question answering tasks-and sets a new state-of-the-art on the KILT benchmark across various knowledge-intensive tasks.",
            "score": 0.44151720215903506,
            "section_title": "O. Chain-of-Thought (CoT)",
            "char_start_offset": 141921,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1194
                },
                {
                    "start": 1197,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2142
                },
                {
                    "start": 2143,
                    "end": 2319
                },
                {
                    "start": 2320,
                    "end": 2587
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5400390625
        },
        {
            "corpus_id": "260682724",
            "title": "Towards Controllable Natural Language Inference through Lexical Inference Types",
            "text": "Multi-step textual entailment Multi-step textual entailment refers to the generation of reasoning mechanisms that can interpret how an answer is inferred by given pieces of step-wise evidence (as shown in Figure 1). Clark et al. (2020) first framed the use of transformer-based approaches, such as RoBERTa (Liu et al., 2019), as a soft NLI reasoner. Based on their work, Tafjord et al. (2020) introduced ProofWriter, a model which is built on T5 for generating an answer to a question, taking into account facts and rules. Both works aim to control the inference process by natural language rules. E.g., given a fact: Erin is young and a rule: if someone is young then they are big, a conclusion can be inferred Erin is big. Recent work (Dalvi et al., 2021) focused on more natural and complex inference patterns where abstraction, specification, generalisation, and rule-based deduction are included in their corpus (the EntailmentBank dataset). Their model, EntailmentWriter (T5), can directly generate entailment trees to explain the inference process. However, the proposed model does not have specific mechanisms for step-wise symbolic control. In this work, we propose to control the inference process by guiding its generation through explanatory lexical inference types. \n\nLatent sentence embedding Latent sentence embeddings are widely deployed in AutoEncoder setup, such as Variational (Kingma and Welling, 2013), Denoising (Vincent et al., 2008), Adversarial (Makhzani et al., 2016). Due to their explainability and controllability, they have been applied in various tasks, including story generation (Fang et al., 2021a), dialogue generation (Zhao et al., 2017), text style transfer (John et al., 2019;Shen et al., 2020), text paraphrasing (Bao et al., 2019a), among others. However, those models are less explored in the NLI task. As the first step, this work introduces the T5 bottleneck and evaluates it on the NLI task.",
            "score": 0.4414952475014568,
            "section_title": "Related work",
            "char_start_offset": 4586,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1278
                },
                {
                    "start": 1281,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1935
                }
            ],
            "ref_mentions": [
                {
                    "start": 1434,
                    "end": 1456,
                    "matchedPaperCorpusId": "207168299"
                },
                {
                    "start": 1654,
                    "end": 1673,
                    "matchedPaperCorpusId": "14688760"
                },
                {
                    "start": 1695,
                    "end": 1714,
                    "matchedPaperCorpusId": "52190376"
                },
                {
                    "start": 1714,
                    "end": 1731,
                    "matchedPaperCorpusId": "211069137"
                },
                {
                    "start": 1752,
                    "end": 1771,
                    "matchedPaperCorpusId": "196176266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.359130859375
        },
        {
            "corpus_id": "263830352",
            "title": "Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models",
            "text": "Large Language Models for Reasoning. Large language models have demonstrated strong reasoning abilities through chain-of-thought (CoT) prompting, wherein LLM is prompted to generate its answer following a step-by-step explanation by using just a few examples as prompts. Recent works have shown that CoT prompting can improve performance on reasoning-heavy tasks such as multi-hop question answering, multi-step computation, and common sense reasoning (Nye et al., 2021;Zhou et al., 2022;Kojima et al., 2022). \n\nVerifying complex claims often requires multistep (multi-hop) reasoning (Mavi et al., 2022), which requires combining information from multiple pieces of evidence to predict the veracity of a claim. Multi-step reasoning can be categorized into forward-reasoning and backward-reasoning (Yu et al., 2023). Forward-reasoning (Creswell et al., 2022;Sanyal et al., 2022;Wei et al., 2022) employs a bottom-up approach that starts with existing knowledge and obtains new knowledge with inference until the goal is met. Backward-reasoning (Min et al., 2019;Press et al., 2022) on the other hand, is goal-driven, which starts from the goal and breaks it down into sub-goals until all of them are solved. Compared to forward reasoning, backward reasoning is more efficient, the divide-and-conquer search scheme can effectively reduce the problem search space. We propose FOLK, a FOL-guided backward reasoning method for claim verification. \n\nDespite the recent progress in using LLMs for reasoning tasks, their capability in verifying claims has not been extensively explored. (Yao et al., 2022) evaluate using LLMs to generate reasoning traces and task-specific actions on fact verification tasks. Their reasoning and action steps are more complex than simple CoT and rely on prompting much larger models (PaLM-540B). Additionally, they test their model's performance on the FEVER dataset (Thorne et al., 2018), which lacks manyhop relations and specialized domain claims. In contrast to their approach, our proposed method demonstrates effectiveness on significantly smaller LLMs without requiring any training, and we test our method on scientific claims.",
            "score": 0.4412627461814692,
            "section_title": "Introduction",
            "char_start_offset": 6059,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1441
                },
                {
                    "start": 1444,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2160
                }
            ],
            "ref_mentions": [
                {
                    "start": 857,
                    "end": 877,
                    "matchedPaperCorpusId": "247594506"
                },
                {
                    "start": 1043,
                    "end": 1061,
                    "matchedPaperCorpusId": "174801080"
                },
                {
                    "start": 1892,
                    "end": 1913,
                    "matchedPaperCorpusId": "257504992"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.332275390625
        },
        {
            "corpus_id": "273662478",
            "title": "Leveraging LLMs for Hypothetical Deduction in Logical Inference: A Neuro-Symbolic Approach",
            "text": "To address the challenge of huge search spaces of step-by-step reasoning, Kazemi et al. (2023) proposed a backward chaining algorithm that decomposes reasoning into sub-modules that are easier for LLMs to solve. Despite their success in enhancing the reasoning ability of LLMs in specific tasks, existing prompting-based algorithms confront with problems of expensive costs and unstable reasoning performance (Yao et al., 2024). \n\nSymbolic Methods for Logical Reasoning. Symbolic logical reasoning techniques utilize symbolic logical symbols and expressions for consistent and accurate reasoning, which overcomes the inconsistent reasoning and ordering sensitivity challenges of prompt-based reasoning algorithms (Chen et al., 2024;Bao et al., 2024a). One key idea is to enhance the reasoning ability of LLMs via invoking logical symbols and expressions (Wang et al., 2022;Wan et al., 2024). Along this line, Wang et al. (2022) proposed a symbol-enhanced text reasoning framework that extends natural language problems to logical symbols and expressions to enhance logical answer matching. Bao et al. (2024b) proposed a logic-driven data augmentation approach that transforms problem texts to structured semantic graphs to enhance language model-based reasoning frameworks. Another pivotal idea is to transform textualized problems into logical expressions via LLMs, then solve them with symbolic logic solvers (Olausson et al., 2023;Pan et al., 2023;Ye et al.). The choice of logic solvers, such as SAT solver (Ye et al.) or first-order logic solver (Pan et al., 2023), highly affects the accuracy and generalization ability of reasoning algorithms given datasets with different features. Despite their brilliant performance in consistent logical reasoning, symbolic methods usually confront with information loss in logical expression extraction, which can lead to inevitable reasoning ability dropping (Pan et al., 2023).",
            "score": 0.4408851004287485,
            "section_title": "RELATED WORK",
            "char_start_offset": 9908,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 428
                },
                {
                    "start": 431,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1924
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 94,
                    "matchedPaperCorpusId": "255186100"
                },
                {
                    "start": 409,
                    "end": 427,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 713,
                    "end": 732,
                    "matchedPaperCorpusId": "267657940"
                },
                {
                    "start": 854,
                    "end": 873,
                    "matchedPaperCorpusId": "234335834"
                },
                {
                    "start": 909,
                    "end": 927,
                    "matchedPaperCorpusId": "234335834"
                },
                {
                    "start": 1090,
                    "end": 1108,
                    "matchedPaperCorpusId": "258833579"
                },
                {
                    "start": 1411,
                    "end": 1434,
                    "matchedPaperCorpusId": "264436468"
                },
                {
                    "start": 1434,
                    "end": 1451,
                    "matchedPaperCorpusId": "258833332"
                },
                {
                    "start": 1551,
                    "end": 1569,
                    "matchedPaperCorpusId": "258833332"
                },
                {
                    "start": 1905,
                    "end": 1923,
                    "matchedPaperCorpusId": "258833332"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.311767578125
        },
        {
            "corpus_id": "258987341",
            "title": "Deliberate then Generate: Enhanced Prompting Framework for Text Generation",
            "text": "Given the English paragraph: [SRC] the already generated simplification is: [SYS] Please detect the error type firstly, and provide the refined simplification then. \n\nError type: incorrect simplification, the refined simplification is: [TGT] ... to various downstream tasks by prompting with natural language instructions, and some powerful capacities such as in-context learning [4] have also emerged. \n\nPrompting Methods. Prompting is a general method for humans to interact with LLMs, which is usually designed as an instruction for a task that guides LLMs toward intended outputs [37,36]. \n\nTo make the most of LLMs on downstream tasks, the prompts need to be carefully designed, either manually [13] or automatically [8,51]. Prompting also provides a way to interact with LLMs in natural language, such as letting them utilize external tools [38], resources [9] and models [47,40], or conducting Chain-of-Thought (CoT) reasoning in generation [45,16]. A concurrent work incorporates answers in previous rounds into prompts in an iterative process to improve the accuracy of LLMs on reasoning tasks [50]. Besides multi-step reasoning, basic prompts are still widely utilized in general text generation tasks such as machine translation and summarization, where previous advanced methods such as CoT have been shown ineffective [30]. In this paper, we propose Deliberate then Generate (DTG), a simple and general prompting method that consistently improves model performance across various text generation tasks, without task-specific designs.",
            "score": 0.4406585618678497,
            "section_title": "Simplification",
            "char_start_offset": 6119,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 167,
                    "end": 402
                },
                {
                    "start": 405,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 592
                },
                {
                    "start": 595,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1546
                }
            ],
            "ref_mentions": [
                {
                    "start": 380,
                    "end": 383,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1475830078125
        },
        {
            "corpus_id": "278000402",
            "title": "SplitReason: Learning To Offload Reasoning",
            "text": "Large language models (LLMs) are powerful general-purpose learners that excel at a wide range of tasks [1][2][3]. Recent advances in LLM post-training have shown that their performance on reasoningheavy tasks can be improved by inducing the ability to reason by generating explicit chain-of-thoughts (CoT) about a question before arriving at the final answer [4]. However, this shift towards more complex, multi-step reasoning during inference [5] significantly increases test-time compute cost. In practice, LLMs often have to generate thousands of tokens while referencing all previously produced tokens via a Key-Value Cache (KV-Cache) for every new token. This process is memory-bound and grows quadratically with respect to sequence length [6][7][8], making it very time-consuming as we scale up model sizes and rely on longer CoT to improve reasoning [9,10] In addition, further increasing compute at test time improves accuracy on reasoning tasks such as AIME24 and MATH500 [11]. This leads to an explosion in the thinking time needed: thousands of tokens are used for CoT reasoning before generating the final answer. \n\nWe hypothesize that reasoning segments are not uniformly difficult-certain parts of a problem can be generated with less effort using a small model, while others require more complex reasoning using larger models. Figure 2 illustrates an example of our approach: SplitReason. A small LLM (e.g., 1.5B parameters) begins generation by processing the question and starting to think about the problem, generating an initial CoT as it reasons through the steps. When the small model encounters a difficult part of the reasoning process, it independently emits a <bigmodel> token to request a reasoning segment from a much larger model (e.g., 32B parameters). In parallel, the large model batch-processes (prefill mode) the small model's output, enabling immediate continuation of generation whenever offloading is triggered.",
            "score": 0.4405963719514918,
            "section_title": "Introduction",
            "char_start_offset": 643,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1125
                },
                {
                    "start": 1128,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1947
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 362,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 745,
                    "end": 748,
                    "matchedPaperCorpusId": "253157377"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.383544921875
        },
        {
            "corpus_id": "277313791",
            "title": "A Survey of Large Language Model Agents for Question Answering",
            "text": "Answer generation synthesizes relevant information to produce a response to the given query. Several methods can enhance this process in LLM-based agents: a) Tool-augmented Generation: This approach allows LLMs to interact with external tools, such as calculators or code interpreters, to augment their reasoning capabilities. The Program-of-Thought (PoT) approach [61,62] focuses on using LLMs to generate executable Python code as part of the reasoning process. Instead of relying solely on the LLM to generate answers directly, PoT leverages the code interpreter to get the final answer. The LLM generates Python code as intermediate steps, which can be executed in a code interpreter, and the results are returned as the final answer. The advantage of this approach is that it allows LLMs to handle problems that require both complex reasoning and precise computation. \n\nHuggingGPT [63] and OpenAGI [64] extend this concept further by integrating domain-specific external models as tools for LLMs. These systems prompt the LLM to recognize tasks that require specialized knowledge beyond its internal capacity and then invoke external models to handle those tasks. For instance, in an image-related task, HuggingGPT can call upon a pre-trained image segmentation model to analyze an image, extract relevant features, or make predictions, which the LLM then incorporates into its reasoning process to produce a final answer. Similarly, OpenAGI acts as an orchestrator that connects LLMs to various task-specific models-whether for processing audio, images, or other modalities-allowing the LLM to integrate multimodal information into its responses. This approach enables the LLM to work alongside specialized AI models, thereby enhancing its ability to answer questions or solve problems that extend beyond the textual or conceptual knowledge encoded in its pre-trained weights. \n\nThe Binding approach [65,66] takes this integration of external tools one step further by using a hybrid system of natural language generation and targeted computational tools. In Binding, the LLM is first prompted to generate a preliminary solution to a problem in the form of a natural language skeleton. However, instead of producing a complete solution, the LLM intentionally leaves certain parts of the answer blank as placeholders for information that will be calculated or filled in by external tools.",
            "score": 0.4403629973762829,
            "section_title": "E. Answer Generation",
            "char_start_offset": 36033,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1882
                },
                {
                    "start": 1885,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2191
                },
                {
                    "start": 2192,
                    "end": 2393
                }
            ],
            "ref_mentions": [
                {
                    "start": 369,
                    "end": 372,
                    "matchedPaperCorpusId": "253708270"
                },
                {
                    "start": 886,
                    "end": 890,
                    "matchedPaperCorpusId": "257833781"
                },
                {
                    "start": 903,
                    "end": 907,
                    "matchedPaperCorpusId": "258049306"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28857421875
        },
        {
            "corpus_id": "274234049",
            "title": "AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous Knowledge Reasoning",
            "text": "Recent advancements in large language models (LLMs) have led to signicant improvements in various natural language processing tasks, but it is still challenging for LLMs to deal with tasks such as knowledge-intensive complex question answering due to LLMs' inecacy in reasoning planning and the hallucination problem. A typical solution is to employ retrieval-augmented generation (RAG) coupled with chain-of-thought (CoT) reasoning, which decomposes complex questions into chain-like sub-questions and applies iterative RAG at the sub-question level. However, prior works exhibit sub-optimal reasoning planning and have overlooked dynamic retrieval from multiple heterogeneous knowledge sources. In this paper, we propose AR, a novel heterogeneous knowledge reasoning framework that performs multi-source reasoning at the atomic level. Inspired by the inherent modular nature of knowledge, AR leverages LLMs to decompose complex questions into ne-grained atomic operators, eectively enhancing the reasoning process at both the planning and execution levels. We also introduce a novel dataset, BlendQA, tailored to assess complex heterogeneous knowledge reasoning. Experiments show that AR signicantly outperforms state-of-the-art baselines across both single-source and multi-source reasoning benchmarks. Knowledge-intensive complex question answering is a challenging task that requires the ability to reason over vast amounts of knowledge with various reasoning skills, such as multi-hop inference, comparison and calculation [3,9,25,36]. Although the recent advancements of Large Language Models (LLMs) [19] have enabled them to excel in various natural language processing tasks [1,41], it is arduous for LLMs to perform knowledge-intensive reasoning due to their inecacy in reasoning planning [11] and the hallucination problem [1,13], namely the phenomenon that LLMs condently make up factually incorrect answers. In order to address the above issues, a typical solution is to leverage retrieval augmentation with chain-of-thought [29] reasoning techniques. Specically, most recent works [4,21,26,31] propose to perform question decomposition on complex questions into simple questions, then retrieve knowledge facts for sub-question answering, thus alleviating the hallucination.",
            "score": 0.4403093625977305,
            "section_title": "Abstract",
            "char_start_offset": 47881,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2287
                }
            ],
            "ref_mentions": [
                {
                    "start": 1529,
                    "end": 1532,
                    "matchedPaperCorpusId": "247362971"
                },
                {
                    "start": 1532,
                    "end": 1534,
                    "matchedPaperCorpusId": "13919896"
                },
                {
                    "start": 1534,
                    "end": 1537,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 1607,
                    "end": 1611,
                    "matchedPaperCorpusId": "270491239"
                },
                {
                    "start": 2038,
                    "end": 2042,
                    "matchedPaperCorpusId": "5856552"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.354736328125
        },
        {
            "corpus_id": "252693237",
            "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
            "text": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
            "score": 0.44014416799227535,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6201171875
        },
        {
            "corpus_id": "271544102",
            "title": "CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large Language Models over Factual Knowledge",
            "text": "Large language models (OpenAI, 2022;OpenAI et al., 2023) (LLMs) have shown impressive results in various natural language processing tasks by acquiring rich factual knowledge from diverse training corpora (Wei et al., 2022b;a;Ouyang et al., 2022). However, their ability to synthesize and utilize this knowledge for complex logical reasoning tasks involving operations like intersections, unions, and multi-hop reasoning remains largely unexplored (Bang et al., 2023;Huang et al., 2023). \n\nWhile existing evaluations of factual knowledge primarily assess the memorization of simple facts (Thorne et al., 2018;Chen et al., 2023b;Sun et al., 2023;Huang et al., 2024), such as \"What is the capital of France?\" or \"Which proteins are associated with lung cancer?\", there is a lack of evaluation regarding how well language models can combine and synthesize those simple facts through multi-step logical reasoning. For example, while an LLM may know that Paris is the capital of France and that France borders Belgium, can it flexibly combine that knowledge to answer \"What is the closest capital city to Paris besides the capital of France itself?\" Many real-world applications require this type of complex reasoning over multiple facts, such as: (1) In healthcare, identifying patients that satisfy multiple criteria from electronic medical records for clinical trial recruitment. ( 2) In open-domain question answering, answering complex queries that build upon multiple pieces of provided information. Understanding the strengths and current limitations of LLMs in combining and logically reasoning over their broad factual knowledge can guide research toward developing more capable general reasoning systems. \n\nTo address this gap, we construct a new benchmark that utilizes high-quality knowledge graphs to automatically generate a diverse set of complex questions involving various reasoning patterns over factual knowledge. These questions require multi-step logical operations like intersections, unions, negations, and multi-hop reasoning over the knowledge graph entities and relations (Ren et al., 2020;Arakelyan et al., 2021;Bai et al., 2023b).",
            "score": 0.4400843853647279,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1709
                },
                {
                    "start": 1712,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2153
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5654296875
        },
        {
            "corpus_id": "248986946",
            "title": "Interpretable Proof Generation via Iterative Backward Reasoning",
            "text": "Question answering and reasoning. Endowing machines to do reasoning over explicit knowledge is a primitive task (Newell and Simon, 1956). Early works tried to solve it by converting texts into logic forms (Newell and Simon, 1956;Musen and Lei, 1988). But such kinds of approaches can be affected by the error propagation caused by semantic parsing (Zettlemoyer and Collins, 2012;Berant et al., 2013;Berant and Liang, 2014).\n\nLately, question answering (QA) is employed as an important task for machine reasoning. Numerous datasets were proposed, including synthesized data (Weston et al., 2016), comprehension on natural texts (Rajpurkar et al., 2016;Joshi et al., 2017;Fisch et al., 2019) or more complex relationship reasoning . There are also multi-hop QA tasks like Hot-potQA (Yang et al., 2018) or QAngaroo (Welbl et al., 2018), and logical QA datasets such as Re-Clor (Yu et al., 2020) and LogiQA , in which textual rules need to be inferred implicitly from a long supporting context. Plenty of studies try to solve these problems via neural networks and achieve remarkable performance (Joshi et al., 2020;Yu et al., 2018;Shao et al., 2020). Nevertheless, nearly all of them only focus on the prediction of final answers and neglect the acquisition of interpretable proofs. Although some datasets provide proof paths for better interpretability, these paths are only short chains with very few entities and cannot teach models to generate complex proofs.\n\nProof generation. NLProlog (Weber et al., 2019) first employs logic programming to search for a proof and then predicts the answer in multi-hop QA. Recently, Clark et al. (2020) propose new rulebased QA datasets for this line of research that include more complex proof paths, and present Rule-Taker to answer questions. Saha et al. (2020) argue that producing answer proofs makes models more reliable and propose PROVER, a transformer-based model that enumerates all possible nodes and edges of a proof path and predicts whether each one exists at once based on their em",
            "score": 0.4400444429799685,
            "section_title": "Related Work",
            "char_start_offset": 5339,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 136,
                    "matchedPaperCorpusId": "19454866"
                },
                {
                    "start": 205,
                    "end": 229,
                    "matchedPaperCorpusId": "19454866"
                },
                {
                    "start": 229,
                    "end": 249,
                    "matchedPaperCorpusId": "61565144"
                },
                {
                    "start": 379,
                    "end": 399,
                    "matchedPaperCorpusId": "6401679"
                },
                {
                    "start": 399,
                    "end": 422,
                    "matchedPaperCorpusId": "1336493"
                },
                {
                    "start": 573,
                    "end": 594,
                    "matchedPaperCorpusId": "3178759"
                },
                {
                    "start": 627,
                    "end": 651,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 651,
                    "end": 670,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 780,
                    "end": 799,
                    "matchedPaperCorpusId": "52822214"
                },
                {
                    "start": 812,
                    "end": 832,
                    "matchedPaperCorpusId": "9192723"
                },
                {
                    "start": 874,
                    "end": 891,
                    "matchedPaperCorpusId": "209485573"
                },
                {
                    "start": 1092,
                    "end": 1112,
                    "matchedPaperCorpusId": "198229624"
                },
                {
                    "start": 1128,
                    "end": 1146,
                    "matchedPaperCorpusId": "226191634"
                },
                {
                    "start": 1489,
                    "end": 1509,
                    "matchedPaperCorpusId": "189898046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4345703125
        },
        {
            "corpus_id": "252070866",
            "title": "FOLIO: Natural Language Reasoning with First-Order Logic",
            "text": "Developing models that can reason in texts has been a core goal in NLP since the field's early days (Cooper et al., 1996). Since then, there has been massive progress in reasoning over text. Various benchmarks that focus on different aspects of reasoning over textual inputs are proposed, including natural language inference (NLI) (Bowman et al., 2015;Wang et al., 2019b), reasoning for commonsense knowledge (Talmor et al., 2019;He et al., 2021) and multi-hop reasoning (Yang et al., 2018;Chen et al., 2020). Among these reasoning abilities, logical reasoning has recently attracted an increasing amount of study. ReClor (Yu et al., 2020) and LogiQA (Liu et al., 2021) both collected multiplechoice questions from standardized graduate admission examinations, answering which requires various types of logical reasoning. However, these datasets cover mixed forms of reasoning and are not intended to test logical reasoning in isolation. \n\nMeanwhile, testing logical reasoning in isolation without involving other forms of reasoning has also attracted researchers in recent years. CLUTRR (Sinha et al., 2019) covers inductive reasoning, which is beyond the scope of first-order logic. Synthetic corpuses of deductive reasoning are proposed to evaluate the deductive reasoning ability of pretrained LMs (Clark et al., 2021;Saeed et al., 2021;Tian et al., 2021). However, these datasets do not contain highly natural sentences and often cover limited forms of logic while FOL is much more expressive. Kazemi et al. (2023) created a dataset for reasoning with contradictory information. Kawabata and Sugawara (2023) crowdsourced rationales for over 3000 examples based on ReClor (Yu et al., 2020). ProntoQA (Saparov and He, 2023) is comprised solely of logical reasoning queries that can be resolved through applying the Modus Ponens inference rule while FOLIO questions require applications of multiple types of inference rules. As shown in Table 1, FOLIO is the first large-scale first-order logic (FOL) reasoning dataset with formal logic annotations in FOL.",
            "score": 0.4398528727289035,
            "section_title": "Datasets for reasoning from text",
            "char_start_offset": 5001,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 121,
                    "matchedPaperCorpusId": "62490646"
                },
                {
                    "start": 332,
                    "end": 353,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 353,
                    "end": 372,
                    "matchedPaperCorpusId": "143424870"
                },
                {
                    "start": 410,
                    "end": 431,
                    "matchedPaperCorpusId": "53296520"
                },
                {
                    "start": 431,
                    "end": 447,
                    "matchedPaperCorpusId": "243865246"
                },
                {
                    "start": 472,
                    "end": 491,
                    "matchedPaperCorpusId": "52822214"
                },
                {
                    "start": 491,
                    "end": 509,
                    "matchedPaperCorpusId": "215785913"
                },
                {
                    "start": 623,
                    "end": 640,
                    "matchedPaperCorpusId": "209485573"
                },
                {
                    "start": 652,
                    "end": 670,
                    "matchedPaperCorpusId": "220483148"
                },
                {
                    "start": 1089,
                    "end": 1109,
                    "matchedPaperCorpusId": "198184456"
                },
                {
                    "start": 1303,
                    "end": 1323,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 1323,
                    "end": 1342,
                    "matchedPaperCorpusId": "237940562"
                },
                {
                    "start": 1342,
                    "end": 1360,
                    "matchedPaperCorpusId": "243865235"
                },
                {
                    "start": 1677,
                    "end": 1694,
                    "matchedPaperCorpusId": "209485573"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52685546875
        },
        {
            "corpus_id": "265456287",
            "title": "Sibyl: Empowering Empathetic Dialogue Generation in Large Language Models via Sensible and Visionary Commonsense Inference",
            "text": "Ghosal et al. (2022); Shen et al. (2022) train language models to produce context-aware commonsense knowledge through natural language generation (NLG) and multiple-choice question (MCQ) tasks, advancing the application of commonsense knowledge in dialogue for further research. Recent studies indicate that commonsense reasoning (Han et al., 2023;Li et al., 2023), derived through multi-step methodologies that function like chain-of-thought prompting, markedly outperforms the approach of prompting LLMs to concurrently deduce implicit information and generate dialogue responses (Wang et al., 2023;Santra et al., 2023). By appending commonsense knowledge to the dialogue context (Wang et al., 2023;Chae et al., 2023), these inferences serve as intermedi-ate reasoning to trigger LLM analysis and produce high-quality responses.",
            "score": 0.4397407476992007,
            "section_title": "Related Work",
            "char_start_offset": 5841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 830
                }
            ],
            "ref_mentions": [
                {
                    "start": 330,
                    "end": 348,
                    "matchedPaperCorpusId": "263829391"
                },
                {
                    "start": 348,
                    "end": 364,
                    "matchedPaperCorpusId": "259251773"
                },
                {
                    "start": 601,
                    "end": 621,
                    "matchedPaperCorpusId": "258865713"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259033203125
        },
        {
            "corpus_id": "256461282",
            "title": "Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning",
            "text": "Multi-hop Question Answering involves retrieving supporting facts from multiple documents along with the explicit reasoning path and reasoning out the answer (Yang et al., 2018). As pre-trained language models evolved, the performance on this task improved spectacularly (Kenton and Toutanova, 2019;Beltagy et al., 2020;Zaheer et al., 2020;Joshi et al., 2020;Zhu et al., 2021a;Li et al., 2022). Despite the success, the black-box nature of pure neural networks has raised concerns among researchers that the unexplainable reasoning process is unacceptable for building trustworthy and robust intelligent systems (Min et al., 2019;Ding et al., 2019; Figure 1: Overall architecture of the proposed method. (a) gives an instance of the Question Semantic Graph. As (c) shows, we propose two simple yet effective improvements for the explainable reasoning method illustrated in (b), including Global Differentiable Learning and Dynamic Adaptive Reasoner (DA-Reasoner). Perez et al., 2020;Wolfson et al., 2020;Tang et al., 2021). \n\nA feasible way to realize an explainable reasoning mechanism is by modeling the reasoning path explicitly. Some researchers have successfully explored the idea of breaking up a multi-hop question into sub-questions and solving them step by step according to the logical relationships to arrive at the final answer. Due to the complexity and expense of constructing question decomposition datasets, early work explored unsupervised (Perez et al., 2020) or weakly supervised (Min et al., 2019) question decomposition methods. However, the sub-questions lack reasoning over logical relationships, thus they are only valuable for retrieving supporting facts. As Figure 1 (a) shows, the Allen institute (Wolfson et al., 2020) proposed the first large-scale question decomposition dataset, where each instance contains a multi-hop question and a question semantic graph consisting of sub-questions annotated by human experts according to a reasoning path. Based on this, they further explored BreakRC, a neural-symbolic reasoning method, and achieved good interpretability. However, the undifferentiable symbolic operations make the neural network reasoner untrainable.",
            "score": 0.4394679491711242,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2189
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 177,
                    "matchedPaperCorpusId": "52822214"
                },
                {
                    "start": 320,
                    "end": 340,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 340,
                    "end": 359,
                    "matchedPaperCorpusId": "198229624"
                },
                {
                    "start": 359,
                    "end": 377,
                    "matchedPaperCorpusId": "237502990"
                },
                {
                    "start": 612,
                    "end": 630,
                    "matchedPaperCorpusId": "174801080"
                },
                {
                    "start": 630,
                    "end": 648,
                    "matchedPaperCorpusId": "153312687"
                },
                {
                    "start": 964,
                    "end": 983,
                    "matchedPaperCorpusId": "211258645"
                },
                {
                    "start": 983,
                    "end": 1004,
                    "matchedPaperCorpusId": "211003735"
                },
                {
                    "start": 1004,
                    "end": 1022,
                    "matchedPaperCorpusId": "211258744"
                },
                {
                    "start": 1457,
                    "end": 1477,
                    "matchedPaperCorpusId": "211258645"
                },
                {
                    "start": 1499,
                    "end": 1516,
                    "matchedPaperCorpusId": "174801080"
                },
                {
                    "start": 1724,
                    "end": 1746,
                    "matchedPaperCorpusId": "211003735"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29052734375
        },
        {
            "corpus_id": "273549253",
            "title": "A Systematic Survey on Instructional Text: From Representation Formats to Downstream NLP Tasks",
            "text": "Safa, et al. \n\nquestion answering [190,195,202], classification [131,218], detecting implicit information in instructions [4], learning task hierarchy [217], generation (e.g., next step, all steps) [98,122] and benchmarking procedural language understanding and planning abilities [172,173] without any additional annotation layers. Even though recipes and WikiHow are mostly exploited, we find that many resources such as troubleshooting websites from tech companies (e.g., Canon are overlooked in the literature with some exceptions [51]. The reasons might be i) their small size (which is smaller) ii) the lack of a consistent structure that makes them hard to parse. [162] and ABCD [25], employ crowdworkers to generate dialogue datasets grounded on instructional documents such as recipes and call center guidelines by assigning different roles to crowdworkers (e.g., call center employee, information giver on a certain recipe). It is also commonly used to annotate existing small corpora for a specific tasks. Such tasks are mostly related to extracting some form of information from instructional text, e.g., tools from fixing manuals [117], ingredients from recipes [211]. Due to the associated costs with crowdsourcing, this technique has been mostly used for generating scripts, grounded dialogues or to add small annotation layers to existing corpora. We find that, even though, for example, the corpus DeScript [180] and the alignments are publicly available, such resources are overlooked and not used for related downstream tasks e.g., event paraphrasing or alignment to the best of our knowledge, with some exceptions [179]. \n\n4.1.3 Synthetic data generation. Synthetic datasets and environments are also common for creating toy setups under simplifying assumptions. For example, Weston et al. [181] formulate 20 question answering tasks to evaluate different linguistic and reasoning abilities such as co-reference resolution, temporal and spatial reasoning. The stories are generated in a simulated world which is defined by manually written rules (e.g., one should find food if hungry) on a predefined set of entities with predefined attributes such as location and size.",
            "score": 0.43926473700844904,
            "section_title": "Web Corpora.",
            "char_start_offset": 13891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 15,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1640
                },
                {
                    "start": 1643,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2190
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 39,
                    "matchedPaperCorpusId": "233219638"
                },
                {
                    "start": 43,
                    "end": 47,
                    "matchedPaperCorpusId": "221738970"
                },
                {
                    "start": 64,
                    "end": 69,
                    "matchedPaperCorpusId": "13825603"
                },
                {
                    "start": 69,
                    "end": 73,
                    "matchedPaperCorpusId": "201632901"
                },
                {
                    "start": 122,
                    "end": 125,
                    "matchedPaperCorpusId": "218974015"
                },
                {
                    "start": 151,
                    "end": 156,
                    "matchedPaperCorpusId": "247446828"
                },
                {
                    "start": 198,
                    "end": 202,
                    "matchedPaperCorpusId": "236469319"
                },
                {
                    "start": 202,
                    "end": 206,
                    "matchedPaperCorpusId": "22457893"
                },
                {
                    "start": 281,
                    "end": 286,
                    "matchedPaperCorpusId": "268248850"
                },
                {
                    "start": 286,
                    "end": 290,
                    "matchedPaperCorpusId": "261706169"
                },
                {
                    "start": 686,
                    "end": 690,
                    "matchedPaperCorpusId": "233004708"
                },
                {
                    "start": 1143,
                    "end": 1148,
                    "matchedPaperCorpusId": "218973875"
                },
                {
                    "start": 1175,
                    "end": 1180,
                    "matchedPaperCorpusId": "252624094"
                },
                {
                    "start": 1424,
                    "end": 1429,
                    "matchedPaperCorpusId": "10551763"
                },
                {
                    "start": 1810,
                    "end": 1815,
                    "matchedPaperCorpusId": "3178759"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.292236328125
        },
        {
            "corpus_id": "277999668",
            "title": "GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning",
            "text": "Beyond high-quality instructions, the generation of structured, verifiable reasoning chains is essential for training large language models capable of logical inference and multi-step deduction. To curate such high-quality solutions, we adopt a automated pipeline that incorporates web-scale retrieval to ensure factual correctness and logical coherence. \n\nGiven an instruction i \u2208 I, we first retrieve supplementary context c \u2208 C from the web. This retrieved information often includes relevant definitions, background knowledge, or factual references that are not explicitly included in the instruction. The context c serves as external knowledge to support the reasoning process, especially in tasks requiring factual grounding or domain-specific expertise. \n\nSubsequently, we generate a reasoning chain r = s 1 , s 2 , ..., s n , where each step s i is a logically valid and interpretable inference that incrementally bridges the gap between the given question and the final answer a \u2208 A. These steps are structured to reflect a natural flow of thought, ensuring that the reasoning path remains traceable, coherent, and grounded in both the instruction and the retrieved context. \n\nTo ensure the quality of the generated reasoning chains, we apply a multi-stage filtering and validation process: \n\n\u2022 Consistency Check: We verify that the reasoning steps logically lead to the final answer and are internally consistent. \n\n\u2022 Redundancy Elimination: Duplicate or unnecessary steps are pruned to maintain conciseness without sacrificing interpretability. \n\n\u2022 Format Conformity: The reasoning chain must follow a step-by-step format to ensure compatibility with chain-of-thought (CoT) supervision. \n\nMoreover, to promote generalization and robustness, we also include examples with multiple valid reasoning chains for the same instruction. This encourages the model to develop a flexible reasoning strategy rather than memorizing fixed templates. \n\nBy focusing on both correctness and interpretability, our approach to reasoning chain generation enables GreenMind to demonstrate superior performance in structured reasoning tasks, setting a strong foundation for Vietnamese LLMs with transparent and explainable outputs 4 GreenMind-Medium-14B-R1 \n\nIn this section, we present the base architecture, provide statistics on the Vietnamese training data, and describe the optimization strategy we used to transform the pretrained model into a Vietnamesefocused reasoning model. \n\nBase Model. We utilize Qwen2.5-14B-Instruct",
            "score": 0.43919006504556085,
            "section_title": "Reasoning Chain Generation",
            "char_start_offset": 7731,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 354
                },
                {
                    "start": 357,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 1183
                },
                {
                    "start": 1186,
                    "end": 1299
                },
                {
                    "start": 1302,
                    "end": 1423
                },
                {
                    "start": 1426,
                    "end": 1555
                },
                {
                    "start": 1558,
                    "end": 1697
                },
                {
                    "start": 1700,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1946
                },
                {
                    "start": 1949,
                    "end": 2245
                },
                {
                    "start": 2248,
                    "end": 2473
                },
                {
                    "start": 2476,
                    "end": 2487
                },
                {
                    "start": 2488,
                    "end": 2519
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33544921875
        },
        {
            "corpus_id": "248377876",
            "title": "The Curious Case of Commonsense Intelligence",
            "text": "First, natural language, not just words or phrases but the full scope of opentext descriptions, is the best way to communicate the content of intuitive inferences between humans and machines. Inventing a new labeling scheme (or logic \n\n143 Yejin Choi formalisms) can only be error prone and incomplete, since there always will be a significant representation gap between the labeling scheme and natural language. Second, the total number of all possible textual descriptions of intuitive inferences is too large for us, and even for AI, to enumerate and examine one by one in real time. \n\nThese observations motivate the need for computational models that can handle on-the-fly generative reasoning through language. The key underlying challenge is scale. Naively increasing the set of labels for discriminative models will not scale effectively to handle the sheer scope of intuitive reasoning, which requires complex and potentially novel compositional reasoning over diverse concepts. This calls for new machine-learning models and algorithms that can learn to generate intuitive inferences on the fly, word by word, just like how humans communicate their thoughts. \n\nIn fact, such word-by-word generation is exactly how text generation from neural language models operates today. For example, OpenAI's GPT-3 (Generative Pre-trained Transformer 3)-a language model that uses deep learning to produce speech-like text-has generated remarkably coherent paragraphs by sampling just one word at a time, without explicitly enumerating all other alternative sentences. 7 Advances in neural language models provide strong technical foundations to build language-based on-the-fly generative reasoning systems. Promising recent research is based on such generative reasoning: abductive reasoning, counterfactual story revision, and commonsense reasoning. But before we get there, let us discuss the importance of defeasible reasoning and commonsense knowledge.",
            "score": 0.43901599495392973,
            "section_title": "W",
            "char_start_offset": 8996,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 233
                },
                {
                    "start": 236,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1168
                },
                {
                    "start": 1171,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1954
                }
            ],
            "ref_mentions": [
                {
                    "start": 1566,
                    "end": 1567,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.135986328125
        },
        {
            "corpus_id": "269757843",
            "title": "Hypothesis Testing Prompting Improves Deductive Reasoning in Large Language Models",
            "text": "The release of large language models (LLMs) has revolutionized the NLP landscape recently (Thoppilan et al., 2022;Kaplan et al., 2020;Chowdhery et al., 2022).Scaling up the size of language models and conducting diversified prompt methods become mainstream (Liu et al., 2023c;Wei et al., 2022a;Yang et al., 2023).Given In-context learning or Chain-of-Thought prompts have already achieved high performance on challenging tasks such as commonsense, arithmetic, and symbolic reasoning (Imani et al., 2023;Lee et al., 2021;Kojima et al., 2022).Logical reasoning is one of the most important and long-standing problems in NLP (Hirschberg and Manning, 2015;Russell and Norvig, 2010), and integrating this ability into natural language understanding systems has always been a goal pursued (Du et al., 2022).\n\nNevertheless, scaling has been demonstrated to offer limited advantages in resolving complex logical reasoning issues (Kazemi et al., 2022).For example, Saparov and He (2022) show that Chainof-Thought prompting struggles with proof planning for more complex logical reasoning problems.Additionally, the performance suffers greatly while handling recently released and out-of-distribution logical reasoning datasets (Liu et al., 2023a).Despite many works have explored variants of Chain-of-Thought prompts to facilitate LLMs inference (Zelikman et al., 2022;Zheng et al., 2023), we discover that the present logical reasoning task prompts place an excessive amount of emphasis on the reasoning process while ignoring the origin, purpose, and effectiveness of reasoning (Creswell et al., 2022;Xi et al., 2023).As examples shown in  Figure 1, the difficulty in judging logical problems arises not only from the process of reasoning but also from the choice of facts and rules to use as a starting point.Even if we were provided the thought process for some of the issues, it would not be very beneficial for others, based on how we previously created the prompts.\n\nIn this paper, we propose Hypothesis Testing Prompting, a new and more considerate prompt template design idea.",
            "score": 0.43890334778391615,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 313
                },
                {
                    "start": 313,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 801
                },
                {
                    "start": 803,
                    "end": 943
                },
                {
                    "start": 943,
                    "end": 1088
                },
                {
                    "start": 1088,
                    "end": 1238
                },
                {
                    "start": 1238,
                    "end": 1611
                },
                {
                    "start": 1611,
                    "end": 1803
                },
                {
                    "start": 1803,
                    "end": 1963
                },
                {
                    "start": 1965,
                    "end": 2076
                }
            ],
            "ref_mentions": [
                {
                    "start": 520,
                    "end": 540,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 783,
                    "end": 800,
                    "matchedPaperCorpusId": "250144604"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29541015625
        },
        {
            "corpus_id": "265158225",
            "title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey",
            "text": "These methods generate knowledge using a language model and then use probing or API calls for tasks. Liu et al. (Liu et al., 2021) used a second model to produce question-related knowledge statements for deductions. Binder (Cheng et al., 2022) uses Codex to parse context and generate task API calls. KB-Binder (Li et al., 2023) also employs Codex to create logical drafts for questions, integrating knowledge graphs for complete answers. Brate et al. (Brate et al., 2022) create cloze-style prompts for entities in knowledge graphs, enhancing them with auxiliary data via SPARQL queries, improving recall and accuracy. KnowPrompt (Chen et al., 2022) generates prompts from a pre-trained model and tunes them for relation extraction in cloze-style tasks. BeamQA (Atif et al., 2023) uses a language model to generate inference paths for knowledge graph embeddingbased search in link prediction. ALCUNA (Yin et al., 2023a) and PRCA (Yang et al., 2023) are other significant methods in controlled generation. \n\nGuardrails in generative AI set operational boundaries for models, ensuring safe and secure output generation. NeMo guardrails (Rebedea et al., 2023) by Nvidia guide conversational flows in enterprise applications to meet safety and secu-rity standards. Knowledge-controlled generation ensures alignment with facts and prevents misinformation. Knowledge graph ontologies can provide specific domain constraints, aiding LLMs in defining output generation boundaries.",
            "score": 0.43883318239514857,
            "section_title": "Knowledge-Controlled Generation",
            "char_start_offset": 12030,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1005
                },
                {
                    "start": 1008,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1473
                }
            ],
            "ref_mentions": [
                {
                    "start": 631,
                    "end": 650,
                    "matchedPaperCorpusId": "237295186"
                },
                {
                    "start": 762,
                    "end": 780,
                    "matchedPaperCorpusId": "259949829"
                },
                {
                    "start": 901,
                    "end": 920,
                    "matchedPaperCorpusId": "264591445"
                },
                {
                    "start": 930,
                    "end": 949,
                    "matchedPaperCorpusId": "264590451"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.537109375
        },
        {
            "corpus_id": "270067866",
            "title": "Arithmetic Reasoning with LLM: Prolog Generation & Permutation",
            "text": "Large language models (LLMs), with their scaling of model size and data size, have demonstrated impressive performance across various understanding and generation tasks (Brown et al., 2020;Chowdhery et al., 2022;Rae et al., 2021;Thoppilan et al., 2022;Touvron et al., 2023;Almazrouei et al., 2023;Jiang et al., 2023).Nevertheless, such LLMs fall short in addressing mathematical problems that involves arithmetic, commonsense, and symbolic reasoning -topics that may appear deceptively simple to humans (Rae et al., 2021).Existing works leveraged Chain-of-Thought (CoT) reasoning that asks language models to generate both the answer and the step-by-step reasoning chain, which helps break down a complex reasoning task into a sequential thought process (Wei et al., 2022b).Particularly, arithmetic reasoning with CoT is shown to be an emergent ability that language models acquired during the scaling process (Wei et al., 2022a).Yet, natural language reasoning is not native to mathematical operations and symbolic manipulations.A line of work has focused on augmenting language models with deterministic computation resources like a calculator (Schick et al., 2023) or program-based tools (Gao et al., 2023;Gou et al., 2023).However, all such methods require a sequential reasoning trajectory, where models need to translate the natural language questions into sequential mathematical or logical operations.Our research probes into the application of Prolog, a logic programming language, in solving the arithmetic reasoning task.Prolog solves arithmetic reasoning tasks by defining an unordered set of predicates and running queries over them.We further explain the unique properties of Prolog in Section 2. In Prolog code generation for arithmetic reasoning, LLMs extract facts and rules in mathematical ques- Our research has the following contributions: 1) We curate and open-source the GSM8K-Prolog dataset with a semi-automatic approach, which contains arithmetic reasoning problems and their corresponding Prolog code solutions.",
            "score": 0.43880132933498295,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 317
                },
                {
                    "start": 317,
                    "end": 522
                },
                {
                    "start": 522,
                    "end": 774
                },
                {
                    "start": 774,
                    "end": 930
                },
                {
                    "start": 930,
                    "end": 1030
                },
                {
                    "start": 1030,
                    "end": 1227
                },
                {
                    "start": 1227,
                    "end": 1409
                },
                {
                    "start": 1409,
                    "end": 1532
                },
                {
                    "start": 1532,
                    "end": 1646
                },
                {
                    "start": 1646,
                    "end": 2037
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 189,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 754,
                    "end": 773,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.142578125
        },
        {
            "corpus_id": "276617498",
            "title": "General Intelligence Requires Reward-based Pretraining",
            "text": "Enabling Efficient Exploration via Synthetic Tasks (Section 4.2): Building on the insight that data containing reasoning traces may better enable the model to learn the underlying reasoning process compared to passive data, the natural question is how to obtain such data. We propose that this data can be obtained via a more active process where an agent optimizes task rewards through a sequence of actions and generates reasoning traces in the process. However, as natural language comprises of \u223c40K tokens, doing RL from scratch is infeasible. To address the exploration challenge, we propose using synthetic tasks with a reduced token space while preserving key structural properties to learn a reasoning prior (e.g., commutativity, associativity, in-context learning, etc.). The token space and complexity can gradually be increased via a task curriculum that increases difficulty. Once the reasoning prior is learned, it can leveraged to bootstrap reasoning in natural language. Supervised pretraining requires the model to directly predict answers, limiting its ability to refine intermediate solutions. In contrast, RPT enables step-by-step reasoning from the outset, allowing the model to iteratively approximate complex functions through simpler transitions that are easier to learn and more robust to errors (Carreira et al., 2016). (Right) Illustration of our proposed decoupled memory-reasoning architecture. This design allows the reasoning module to operate on shorter context windows that reduce the chances of learning spurious correlations and, thereby, more transferrable reasoning. The use of a shorter context window also encourages the model to learn how to dynamically read and write to memory, which facilitates the use of reasoning model on new problems and knowledge domains. \n\n3. Architectural bias to decouple knowledge and reasoning (Section 4.3): Training models using long context windows provide more chances for the model to learn spurious correlations between past and future tokens when minimizing the next-token prediction loss. \n\nPrevious work highlighted issues such as lost-in-themiddle phenomenon (Liu et al., 2024), where models become overly sensitive to token positions within a long context rather than learning a robust, transferable reasoning process.",
            "score": 0.43865861982473375,
            "section_title": "2.",
            "char_start_offset": 5962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1802
                },
                {
                    "start": 1805,
                    "end": 2065
                },
                {
                    "start": 2068,
                    "end": 2298
                }
            ],
            "ref_mentions": [
                {
                    "start": 1320,
                    "end": 1343,
                    "matchedPaperCorpusId": "10111903"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6064453125
        },
        {
            "corpus_id": "265150088",
            "title": "Are LLMs Rigorous Logical Reasoner? Empowering Natural Language Proof Generation with Contrastive Stepwise Decoding",
            "text": "Logical reasoning underpins the comprehension of human cognition and intelligence in machines (Goel et al., 2017). Large Language Models (LLMs) like GPT (Brown et al., 2020;Ouyang et al., 2022) and PaLM (Chowdhery et al., 2022;Anil et al., 2023) have pioneered using natural language as a platform for logical reasoning, complementing the traditional use of formal languages (Kazemi et al., 2022;Creswell et al., 2022). The incorporation of natural language broadens the scope of logical reasoning by allowing flexible querying and tapping into the extensive implicit knowledge encapsulated within LLMs. \n\nIn examining the capacity of LLMs for logical reasoning, it is crucial to consider not only the accuracy of their answers but also the correctness of their explanations (Xu et al., 2023). Utilizing prompting methods such as in-context learning (Brown et al., 2020) and chain-of-thought (Wei et al., 2022), LLMs have shown promising results across various deductive reasoning tasks in question-answering formats (Weston et al., 2015;Tafjord et al., 2021;Saparov and He, 2022;Han et al., 2022). These approaches decompose the final task goal by guiding the LLMs through intermediate reasoning steps in a carefully constructed context. However, providing correct explanations, which covers completeness, redundancy, correctness (Xu et al., 2023), emerges as a more daunting challenge. This is particularly evident in tasks that involve generating reasoning chains from premises leading to a conclusion, known as proof generation (Clark et al., 2020;Dalvi et al., 2021). Unfortunately, LLMs often fall short in creating concise and exact proof trees, commonly producing superfluous or imprecise intermediate steps. \n\nIn an attempt to better understand LLMs in the scope of deductive reasoning, this paper conducts an analysis from both the prompting and finetuning perspectives.",
            "score": 0.43816292498620557,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 603
                },
                {
                    "start": 606,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1880
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 173,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 173,
                    "end": 193,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 850,
                    "end": 870,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1038,
                    "end": 1059,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 1059,
                    "end": 1080,
                    "matchedPaperCorpusId": "252693237"
                },
                {
                    "start": 1532,
                    "end": 1552,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 1552,
                    "end": 1571,
                    "matchedPaperCorpusId": "233297051"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25732421875
        },
        {
            "corpus_id": "276884968",
            "title": "An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for Robust Reasoning",
            "text": "Building automated reasoning systems is one of the cornerstone goals of research in artificial intelligence, with wide-reaching applications in a variety of domains. LLMs have demonstrated remarkable capabilities in various natural language processing tasks Min et al. (2021), yet exhibit critical limitations in a variety of reasoning-related tasks, particularly in multistep and spatially complex scenarios Yang et al. (2023); Li et al. (2024). Although these models process and generate natural language with unprecedented fluency, their reasoning capabilities are still limited by several key constraints. \n\nMost LLMs frequently struggle with verifiable, robust and interpretable reasoning. Experimental studies have shown significant variability in LLM responses to identical reasoning related questions which highlights inherent instability Mirzaee & Kordjamshidi (2023). For instance, sampling from an LLM's responses to reasoning-based queries can produce widely varying answers with significantly divergent reasoning traces. This lack of robustness undermines their reliability in critical decision-making contexts Ji et al. (2023). Moreover, tasks that require multiple sequential reasoning steps, such as complex spatial reasoning, often expose fundamental weaknesses in LLM's systematic reasoning capabilities Shi et al. (2022). \n\nRecent research has increasingly focused on neural-symbolic approaches that combine the strengths of LLMs in learning from data and rigorous reasoning capabilities of symbolic systems Mao et al. (2019); Yu et al. (2024). Specifically, Answer Set Programming (ASP), a type of declarative logic programming, has emerged as a promising candidate for enhancing LLMs' reasoning capabilities (Yang et al., 2023). Recent studies have explored integrating LLMs with ASP to transform natural language into logical representations, particularly for spatial reasoning tasks (Yang et al., 2023;Li et al., 2024). \n\nHowever, existing approaches face significant challenges in generating reliable intermediate scaffolds: the logical representations that bridge natural language inputs and systematic symbolic reasoning systems. The translation of natural language into formal logic programs introduces substantial uncertainty, as LLMs can produce inconsistent or semantically incorrect translations Li et al. (2024). This variability may become a significant weakness in this reasoning pipeline, potentially compromising the reliability of downstream reasoning tasks Mirzaee & Kordjamshidi (2023).",
            "score": 0.4379534852417032,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1340
                },
                {
                    "start": 1343,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1942
                },
                {
                    "start": 1945,
                    "end": 2155
                },
                {
                    "start": 2156,
                    "end": 2344
                },
                {
                    "start": 2345,
                    "end": 2525
                }
            ],
            "ref_mentions": [
                {
                    "start": 409,
                    "end": 427,
                    "matchedPaperCorpusId": "271571434"
                },
                {
                    "start": 1124,
                    "end": 1140,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 1546,
                    "end": 1562,
                    "matchedPaperCorpusId": "257766470"
                },
                {
                    "start": 1729,
                    "end": 1748,
                    "matchedPaperCorpusId": "271571434"
                },
                {
                    "start": 1906,
                    "end": 1925,
                    "matchedPaperCorpusId": "271571434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26123046875
        },
        {
            "corpus_id": "271470933",
            "title": "Development of a Geographical Question- Answering System in the Kazakh Language",
            "text": "For example, a factoid question could be: ''What is the capital of France?'' The expected answer would be a single piece of factual information, in this case, ''Paris.'' QAS with Simple Reasoning questions refers to Question-Answering Systems that are designed to handle questions that involve basic logic or reasoning to arrive at an answer [9]. While factoid questions typically have straightforward and direct answers, simple reasoning questions may require the system to make basic inferences or deductions based on the information available. For instance, a simple reasoning question could be: ''If it is raining outside, what should you carry with you?'' The answer involves a logical inference, where the system needs to understand that carrying an umbrella is advisable when it's raining. QAS with simple reasoning questions often involves incorporating basic logical rules or patterns into the system to allow it to analyze and process information in a more nuanced way. These systems may use techniques from natural language processing and machine learning to understand the context, identify relationships, and provide reasoned responses to questions that go beyond straightforward factual queries. \n\nQAS with Complex Reasoning questions refers to Question-Answering Systems that are designed to handle questions that involve more intricate logic, multi-step reasoning, and possibly deeper understanding of contextual information [10]. Unlike simple reasoning questions, which often require basic inferences, complex reasoning questions demand a more sophisticated level of cognitive processing. For example, a complex reasoning question could be: ''If a train leaves City A at a certain time and travels at a certain speed, while another train leaves City B at a different time and speed, when and where will the two trains meet?'' This question involves multiple steps of computation and reasoning, including calculating distances, speeds, and determining the point of intersection. To handle complex reasoning questions, QAS may utilize advanced techniques in artificial intelligence, machine learning, and natural language processing. These systems may need to understand and manipulate more abstract concepts, engage in logical deduction, and navigate through complex scenarios to provide accurate and meaningful answers. Such systems are valuable for applications that require a deeper level of comprehension and problem-solving ability, such as advanced decision support systems or expert systems.",
            "score": 0.4379041903697485,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2454,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2337
                },
                {
                    "start": 2338,
                    "end": 2515
                }
            ],
            "ref_mentions": [
                {
                    "start": 342,
                    "end": 345,
                    "matchedPaperCorpusId": "216076950"
                },
                {
                    "start": 1441,
                    "end": 1445,
                    "matchedPaperCorpusId": "249439927"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2091064453125
        },
        {
            "corpus_id": "276557738",
            "title": "Empowering LLMs with Logical Reasoning: A Comprehensive Survey",
            "text": "Large language models (LLMs) have demonstrated remarkable performance in a broad range of natural language tasks * Haoxuan Li is the corresponding author. \n\nincluding language generation, classification and translation. However, recent studies have found that there are still significant challenges to the logical reasoning abilities of LLMs. On one hand, learning syntax, semantics, and world knowledge through tasks such as next-word prediction or masked language modeling does not ensure the logical reasoning ability of LLMs [Luo et al., 2023]. On the other hand, the pre-training corpus of LLMs primarily consists of humanwritten texts, which lack high-quality logical reasoning samples such as logical deduction and proofs [Morishita et al., 2024]. These challenges significantly limit the applicability of LLMs due to the following two summarized aspects. \n\nLLMs often fail to generate the correct answer in logical question answering, which requires sophisticated deductive, inductive or abductive reasoning given a collection of premises and constraints. Specifically, these logical questions can be broadly divided into two categories: (1) Determine whether a related statement can be deduced from the given information, namely, output the truth value of the statement: true, false or unknown. For example, the premise and constraints could be Metals conduct electricity. Insulators do not conduct electricity. If something is made of iron, then it is metal. Nails are made of iron., followed by the logical question Is the following statement true, false, or unknown? Nails cannot conduct electricity. To answer this question correctly, LLMs need to conduct logical reasoning nails\u2192made of iron\u2192metal\u2192conduct electricity before conclude that the statement is actually false. (2) Find the correct option that can satisfy all the given premises and constraints from the multiple choices. Surprisingly, LLaMA-13B achieves 33.63% accuracy under 8-shot prompting on the logical questions dataset FOLIO, which is only slightly better than random guess from true, false and unknown with an accuracy of 33.33% [Han et al., 2024]. This significantly restricts the application of LLMs in complicated real-world situations, such as problem-solving and decision-making.",
            "score": 0.4375623562222908,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 157,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2268
                }
            ],
            "ref_mentions": [
                {
                    "start": 729,
                    "end": 753,
                    "matchedPaperCorpusId": "264436468"
                },
                {
                    "start": 2113,
                    "end": 2131,
                    "matchedPaperCorpusId": "258833579"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1083984375
        },
        {
            "corpus_id": "266844589",
            "title": "Computational Argumentation-based Chatbots: a Survey",
            "text": "Hallucination. Defined as 'the generated content that is nonsensical or unfaithful to the provided source content' (Ji et al., 2023) the phenomenon of hallucination in natural language generation can be divided into intrinsic and extrinsic. The former refers to generated output that contradicts the source upon which the model was trained. The second, instead, represents an output that cannot be verified. The employment of an argumentation reasoning engine can reduce the intrinsic hallucination kind by stipulating that only grounded arguments (hence, members of conflict-free sceptical extensions) will be output by the chatbot. On the other hand, extrinsic hallucinations can be probed by argumentative XAI methods, thus ensuring, in the worst-case scenario, the retrieval of additional information over the produced content. \n\nReasoning. Different scholars argue that, although LLMs provide a good representation of language generation, they lack reasoning skills and logical thinking (Mahowald et al., 2023;Bang et al., 2023;Frieder et al., 2023;Thorp, 2023). In an attempt to provide effective solutions, Chain and Tree of Thoughts (respectively, CoT and ToT) have been introduced to address such weaknesses. CoT consists of a prompting strategy that details a series of intermediate reasoning steps in order to achieve better performance in arithmetic, symbolic and commonsense inferences (Wei et al., 2022b). The limitations of this approach mostly concern the absence of a procedure to plan or analyse multiple reasoning paths before generating the output and this is exactly the enhancement yielded by ToT. Indeed, Tree of Thoughts frames each problem as a search over a tree, where each node is a partial solution (Yao et al., 2023). Against these two options, we argue that endowing generative-LLM-based chatbots with a reasoning engine driven by computational argumentation may provide a more intuitive and cheaper alternative (e.g. it does not require expensive resources to be implemented, unlike ToT). Argumentative reasoning is particularly suited for models that parse, work and generate natural language. Recall that AFs are graphs whose edges represent paths determining the status of each node.",
            "score": 0.43744063702849345,
            "section_title": "Benefits of Leveraging Computational Argumentation Approaches in",
            "char_start_offset": 61590,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2126
                },
                {
                    "start": 2127,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 132,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 1054,
                    "end": 1066,
                    "matchedPaperCorpusId": "256275435"
                },
                {
                    "start": 1399,
                    "end": 1418,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07989501953125
        },
        {
            "corpus_id": "227217290",
            "title": "Tansformer based Natural Language Generation for Question-Answering",
            "text": "This paper explores Natural Language Generation within the context of Question-Answering task. The several works addressing this task only focused on generating a short answer or a long text span that contains the answer, while reasoning over a Web page or processing structured data. Such answers\u2019 length are usually not appropriate as the answer tend to be perceived as too brief or too long to be read out loud by an intelligent assistant. In this work, we aim at generating a concise answer for a given question using an unsupervised approach that does not require annotated data. Tested over English and French datasets, the proposed approach shows very promising results.",
            "score": 0.43719950413932385,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.515625
        },
        {
            "corpus_id": "272753636",
            "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
            "text": "This requires the answerer to perform specific post-processing steps after all necessary facts have been gathered. For example, consider the question: \"What is five years after the founding of the largest country in North America in Roman numerals?\". This question requires the following sub-instructions: (1) Numerical reasoning: Add five years to the founding date, and (2) Post-processing: Convert the resulting year into Roman numerals. \n\nof \u223c32%, suggesting that the legitimate questions generated by LLMs were indeed challenging for state-of-the-art models. There are two key takeaways from our experimentation with synthetic data generation: \n\n(1) Synthetic test data requires heavy manual cleaning before usage, which suggests that we will need to rely on human annotations instead of LLMs to generate the final evaluation set; and \n\n(2) models performed significantly poorly on the correct test samples we tested on, suggesting that the instruction to create questions can be used to generate a challenging evaluation set. \n\nHuman annotation. Given these findings, we decided to use the core instruction for generating questions that combine information from multiple articles as a guide for human annotation, shown in Figure 1. This approach aimed to leverage the challenging nature of the synthetic questions while also mitigating the issues of hallucination present in LLM-generated content. Human annotators were tasked with creating questions that required information from multiple Wikipedia articles, following a similar structure to the synthetic prompts but with greater reliability and accuracy. The outcome of this human annotation resulted in 824 questions with their correct responses along with the list of Wikipedia articles needed to answer the questions. We also ask the human annotators to label each question based on five reasoning types, i.e, Numerical Reasoning, Tabular Reasoning, Multiple Constraints, Temporal Reasoning, and Post-Processing, described in more details in Table 2. Please note that a question can belong to multiple reasoning types. To ensure the highest quality annotations, we engaged a team of carefully vetted experts with extensive experience in question generation and complex reasoning tasks.",
            "score": 0.4366452991217622,
            "section_title": "Post-Processing",
            "char_start_offset": 9993,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 440
                },
                {
                    "start": 443,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 839
                },
                {
                    "start": 842,
                    "end": 1031
                },
                {
                    "start": 1034,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2248
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10992431640625
        },
        {
            "corpus_id": "268363344",
            "title": "Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs",
            "text": "Instead of relying on embeddings or limited query types for matching synthetic logical queries, we leverage the concept of logical queries to effectively acquire complex reasoning data from CSKGs with minimum human efforts.\n\nComplex Commonsense Reasoning Recent advances in commonsense reasoning have been driven by the construction of human-annotated (Speer et al., 2017;Sap et al., 2019a;Hwang et al., 2021; Jiang et al., 2021;Mostafazadeh et al., 2020;Krishna et al., 2017;Shen et al., 2024) and humanvalidated (West et al., 2022;Gao et al., 2023) Com-monSense Knowledge Graphs (CSKG).A common approach to create challenges for commonsense reasoning involves constructing tasks in the form of question-answering (Talmor et al., 2019;Sap et al., 2019b), knowledge base completion (Malaviya et al., 2020;Yang et al., 2023) and population (Fang et al., 2021b,a), grounding (Gao et al., 2022), and daily dialogue (Kim et al., 2023), based on CSKGs.However, most of those previous benchmarks are based on one-hop triples.\n\nIn contrast, real-world situations in narratives usually involve more complicated reasoning across multiple events, sentences, and paragraphs (Schank and Abelson, 1975).Previous works learn representations of narrative chains (Chambers and Jurafsky, 2008;Pichotta and Mooney, 2014) and draw inferences (Fang et al., 2022;Yuan et al., 2023).To address more complex paragraph-level or multievent reasoning, ParaCOMET (Gabriel et al., 2021) proposed to pre-train on distantly supervised onehop paragraph-level commonsense inferences, and COMET-M (Ravi et al., 2023) was fine-tuned on a crowdsourced corpus focusing on reasoning on multiple events.Instead of crowdsourcing or using language models to distill complex inferences, we provide narrative-level inference by verbalizing complex logical queries over CSKGs, to effectively acquire grounded inferences at scale.",
            "score": 0.43647581075889497,
            "section_title": "Background and Related Work",
            "char_start_offset": 6274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 225,
                    "end": 588
                },
                {
                    "start": 588,
                    "end": 947
                },
                {
                    "start": 947,
                    "end": 1019
                },
                {
                    "start": 1021,
                    "end": 1190
                },
                {
                    "start": 1190,
                    "end": 1361
                },
                {
                    "start": 1361,
                    "end": 1665
                },
                {
                    "start": 1665,
                    "end": 1886
                }
            ],
            "ref_mentions": [
                {
                    "start": 352,
                    "end": 372,
                    "matchedPaperCorpusId": "15206880"
                },
                {
                    "start": 372,
                    "end": 390,
                    "matchedPaperCorpusId": "53170360"
                },
                {
                    "start": 390,
                    "end": 402,
                    "matchedPaperCorpusId": "222310337"
                },
                {
                    "start": 410,
                    "end": 429,
                    "matchedPaperCorpusId": "233231245"
                },
                {
                    "start": 429,
                    "end": 455,
                    "matchedPaperCorpusId": "221739295"
                },
                {
                    "start": 455,
                    "end": 476,
                    "matchedPaperCorpusId": "4492210"
                },
                {
                    "start": 514,
                    "end": 533,
                    "matchedPaperCorpusId": "238857304"
                },
                {
                    "start": 533,
                    "end": 550,
                    "matchedPaperCorpusId": "258480238"
                },
                {
                    "start": 715,
                    "end": 736,
                    "matchedPaperCorpusId": "53296520"
                },
                {
                    "start": 736,
                    "end": 754,
                    "matchedPaperCorpusId": "128296356"
                },
                {
                    "start": 782,
                    "end": 805,
                    "matchedPaperCorpusId": "209439910"
                },
                {
                    "start": 805,
                    "end": 823,
                    "matchedPaperCorpusId": "256462868"
                },
                {
                    "start": 873,
                    "end": 891,
                    "matchedPaperCorpusId": "253098370"
                },
                {
                    "start": 912,
                    "end": 930,
                    "matchedPaperCorpusId": "254877312"
                },
                {
                    "start": 1163,
                    "end": 1189,
                    "matchedPaperCorpusId": "18113275"
                },
                {
                    "start": 1247,
                    "end": 1276,
                    "matchedPaperCorpusId": "529375"
                },
                {
                    "start": 1276,
                    "end": 1302,
                    "matchedPaperCorpusId": "2597568"
                },
                {
                    "start": 1323,
                    "end": 1342,
                    "matchedPaperCorpusId": "252907653"
                },
                {
                    "start": 1342,
                    "end": 1360,
                    "matchedPaperCorpusId": "258564677"
                },
                {
                    "start": 1436,
                    "end": 1458,
                    "matchedPaperCorpusId": "222134165"
                },
                {
                    "start": 1564,
                    "end": 1583,
                    "matchedPaperCorpusId": "258865448"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.439697265625
        },
        {
            "corpus_id": "266693275",
            "title": "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models",
            "text": "Large language models (LLMs), such as OpenAI's GPT series have significantly impacted natural language processing, excelling in a variety of tasks including text generation, machine translation, and code generation (Gao et al., 2022(Gao et al., , 2023a;;Jiao et al., 2023). \n\nReasoning, defined as the cognitive process of using logic to draw conclusions from given facts (Wei et al., 2022b,a), is crucial for complex interactions that go beyond text generation. Accurately assessing this ability in LLMs is essen-tial, yet challenging, as models may correctly perform tasks merely relying on shortcuts such as pattern recognition without truly engaging in logical reasoning (Huang and Chang, 2022;Huang et al., 2023;Liu et al., 2023a). Consider the following inference example: Either it is raining, or Tom will play football; if it rains, then the floor will be wet; the floor is dry; therefore, Tom will play football. We may encounter the following challenges: 1) It's unclear if a correct LLM response is due to reasoning or simple heuristics like word correlations (e.g., \"dry floor\" is more likely to correlate with \"playing football\"). 2) If an LLM fails, pinpointing the specific breakdown in reasoning is difficult (i.e., inferring not raining from the floor being dry or inferring playing football from not raining). 3) Current systems lack comprehensive test cases that encompass various formal reasoning types beyond implication, such as logical equivalence (e.g., A and B are true; therefore, B and A are true. 4) Evaluating an LLM's reasoning on such cases offers limited insight into enhancing its reasoning capabilities. \n\nTo better handle these challenges, a wellperforming testing framework should be able to define a set of skills that a) directly correspond to the reasoning process, b) cannot be further divided, c) cover all formal logical reasoning scenarios, and d) can identify LLMs' weaknesses and facilitate improving LLMs' performance.",
            "score": 0.4363601819864087,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1637
                },
                {
                    "start": 1640,
                    "end": 1964
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.199462890625
        },
        {
            "corpus_id": "222141025",
            "title": "PRover: Proof Generation for Interpretable Reasoning over Rules",
            "text": "Developing systems that can understand and reason over explicitly provided knowledge has been a fundamental goal of AI (Newell and Simon, 1956). Owing to the challenges posed in reasoning over formal representations (Musen and Van Der Lei, 1988), and backed by the recent successes of transformers (Vaswani et al., 2017) in NLP, Clark et al. (2020) propose a new version of the problem by replacing the formal representations of rule-bases with natural language (English). Specifically, their task requires predicting the truth value of a statement by reasoning over a set of facts and rules, all expressed in natural language. Figure 2 shows some examples of the task. Clark et al. (2020) propose RuleTakers, a fine-tuned RoBERTa model (Liu et al., 2019b) to show that transformers can act as \"soft theorem provers\" by predicting the final answer in such reasoning-based problems with high accuracy. \n\nWe argue that to use transformers for natural language reasoning reliably, they should be able to generate proofs that provide rationales for the predicted answer. Proof generation is vital for emulating formal reasoning but also for moving towards human-interpretable models that alleviate concerns about the black-box nature of deep architectures (Rudin, 2019). Towards this, we present PROVER, a transformer-based model that jointly answers questions over natural language rule-bases and generates corresponding proofs. Figure 1 illustrates our method as a closer linguistic analog of formal reasoning, as it generates proofs along with answers. However, unlike formal reasoners, PROVER can operate on natural language text that provides the underlying theory, rather than rely on formal logical representations. Such methods that combine interpretability and flexibility in reasoning can have wide applications across domains. \n\nPROVER's architecture consists of three modules that together generate answers along with proofs. In this work, proofs are represented as directed graphs consisting of the relevant rules and facts needed to prove or disprove the question statement. Section 3.1 contains details of this representation. A QA module predicts a binary answer for the question, a node module chooses which rules and facts are part of the proof, and an edge module predicts the presence and the direction of the edges between the chosen nodes. Model training minimizes a joint cross-entropy loss over the three modules.",
            "score": 0.43621520891594046,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1833
                },
                {
                    "start": 1836,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2137
                },
                {
                    "start": 2138,
                    "end": 2357
                },
                {
                    "start": 2358,
                    "end": 2433
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 143,
                    "matchedPaperCorpusId": "19454866"
                },
                {
                    "start": 298,
                    "end": 320,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 329,
                    "end": 348,
                    "matchedPaperCorpusId": "15820365"
                },
                {
                    "start": 670,
                    "end": 689,
                    "matchedPaperCorpusId": "15820365"
                },
                {
                    "start": 1252,
                    "end": 1265,
                    "matchedPaperCorpusId": "182656421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33544921875
        },
        {
            "corpus_id": "276776226",
            "title": "The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models",
            "text": "Large language models (LLMs) have achieved significant progress in natural language processing tasks (Longpre et al., 2023;Touvron et al., 2023). However, enabling LLMs to perform complex reasoning remains challenging. \n\nRecent work has demonstrated strong reasoning capabilities in LLMs (Yang et al., 2024;Dubey et al., 2024;Guo et al., 2025;OpenAI, 2023). One research direction varies the input query of LLMs to obtain consistent responses through prompting techniques, such as chain-of-thought (Wei et al., 2022) and tree-of-thought (Yao et al., 2023). Another line of research leverages the verification of output trajectories for LLMs, for instance, ensembling and reranking inference paths, verification (Cobbe et al., 2021;Uesato et al., 2022) and self-consistency (Wang et al., 2023b). Trainable approaches like ReST (Zelikman et al., 2022), Rejection Sampling Fine-Tuning (RFT) (Yuan et al., 2023), and Self-Taught Reasoner (STaR) (Zelikman et al., 2022) also exemplify this paradigm. \n\nHowever, a significant limitation of many existing approaches, particularly when considering complex tasks like mathematical reasoning, lies in their explicit reliance on sampling complete correct reasoning traces from LLMs. Although more recent studies such as s1 (Muennighoff et al., 2025) and LIMO (Ye et al., 2025) show that a small number of data can unlock strong generalization in LLMs for reasoning tasks, these approaches require external strong LLMs, which naturally are limited by external LLMs' ability and cannot broaden the knowledge boundary by itself. Consequently, a critical question arises: How can we design an efficient and scalable method to enhance LLM reasoning without external supervision? \n\nOur approach answers this question by leveraging the latent reasoning structures LLMs acquire during pre-training on large corpora (Brown et al., 2020), which can be aligned using a small subset of high-quality data (Muennighoff et al., 2025;Ye et al., 2025).",
            "score": 0.436152247877645,
            "section_title": "Related Work",
            "char_start_offset": 28226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 218
                },
                {
                    "start": 221,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 994
                },
                {
                    "start": 997,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1712
                },
                {
                    "start": 1715,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 498,
                    "end": 516,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 773,
                    "end": 793,
                    "matchedPaperCorpusId": "247595263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.454345703125
        },
        {
            "corpus_id": "270619500",
            "title": "Instruction Data Generation and Unsupervised Adaptation for Speech Language Models",
            "text": "Speech models are traditionally trained and fine-tuned using labeled data for single or multiple tasks.They achieve high performance in many targeted tasks but often exhibit limited generalization to out-of-domain tasks [1].On the other hand, Large Language Models (LLMs) have demonstrated impressive text understanding and a wide range of emergent capabilities through training on predominantly unsupervised and unlabeled text data [2].The success of LLMs has paved the way for the development of speech-language models capable of processing both text and speech inputs.These models are expected to leverage the text-understanding prowess of LLMs while comprehending speech inputs [3,4,5,6,7].One major challenge in training such multimodal models is the scarcity of sufficient labeled data, necessitating paired speech and text samples.To address this challenge, we propose that the synthetic generation of data by utilizing the text-understanding capabilities of LLMs or text-tospeech systems (TTS) can significantly contribute to expanding the dataset required for modeling both text and speech inputs.\n\nIn this paper, we present and evaluate three strategies for generating instruction data to train speech language models: Synthetic Speech Instruction Data with TTS Systems: Text-to-speech (TTS) systems are being used to generate the speech component of samples from a textual dataset.This approach is applied to a question-answering dataset, where the model must comprehend both textual and speech parts of the input, as well as their interrelation, to successfully answer the question.This method is versatile, applicable to any textual or instructional dataset for pre-training and instruction tuning.How-ever, it faces limitations in speech diversity and the availability of efficient TTS models for certain languages, challenging the extension of speech models to a wider linguistic range.\n\nText Generation from Labeled Speech Data: We leverage the text-understanding capabilities of LLMs to generate textual content for labeled speech data.In this approach, we use LLMs to generate questions and answers based on the context provided by the transcription of speech data.By treating the speech segment as the context, we create triplets of question/context/answer, where the context is real speech, and the remaining elements are in textual form.This method significantly expands data generation with real and diverse speech data.",
            "score": 0.43581294323714176,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 103,
                    "end": 224
                },
                {
                    "start": 224,
                    "end": 437
                },
                {
                    "start": 437,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 694
                },
                {
                    "start": 694,
                    "end": 838
                },
                {
                    "start": 838,
                    "end": 1106
                },
                {
                    "start": 1108,
                    "end": 1392
                },
                {
                    "start": 1392,
                    "end": 1594
                },
                {
                    "start": 1594,
                    "end": 1711
                },
                {
                    "start": 1711,
                    "end": 1901
                },
                {
                    "start": 1903,
                    "end": 2053
                },
                {
                    "start": 2053,
                    "end": 2183
                },
                {
                    "start": 2183,
                    "end": 2358
                },
                {
                    "start": 2358,
                    "end": 2442
                }
            ],
            "ref_mentions": [
                {
                    "start": 220,
                    "end": 223,
                    "matchedPaperCorpusId": "252923993"
                },
                {
                    "start": 433,
                    "end": 436,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 685,
                    "end": 687,
                    "matchedPaperCorpusId": "259501685"
                },
                {
                    "start": 689,
                    "end": 691,
                    "matchedPaperCorpusId": "264406171"
                },
                {
                    "start": 691,
                    "end": 693,
                    "matchedPaperCorpusId": "264146423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6806640625
        },
        {
            "corpus_id": "257766470",
            "title": "Natural Language Reasoning, A Survey",
            "text": "Potentials. Compared to reasoning with precise formal language, natural language provides a better humancomputer interaction interface. Besides, natural language opens a door to play with defeasible reasoning, where formal language fails. \n\nChallenges. Firstly, natural language suffers from ambiguity and variety, since there are polysemy, synonymy and diverse structures. Therefore, while triples and formal languages are precise, statements and propositions are many-to-many in natural language, which poses a challenge on natural language understanding. Secondly, supervised data of inference is difficult to obtain, which may prevent it from large-scale training. Moreover, the step of reasoning is diverse at the instance level, i.e. different questions may require different inference steps to answer, and it is important to generalize to the unseen steps. \n\nRequirements. Based on the definition (Definition 2.4), the key components in NLP to achieve reasoning are (1) (multiple) knowledge and (2) an algorithm capable of understanding and inference. Correspondingly, there are three stages: knowledge acquisition, knowledge understanding, and inference. Firstly, it requires collecting the relevant knowledge required for reasoning (knowledge acquisition). Then, the algorithm requires to capture propositions underlying the given knowledge (knowledge understanding). In addition to the general semantics, it should also capture the logical semantics such as negation, conjunction and disjunction. Subsequently, beginning from these propositions, the algorithm requires integrating some knowledge to infer a new conclusion with one or more steps to reach the final answer (inference). Though knowledge acquisition and understanding are also necessary for reasoning, the two topics are big enough to write another survey, thus we just focus on inference in this article.",
            "score": 0.43570017786838133,
            "section_title": "Potentials, Challenges, and Requirements of NLR",
            "char_start_offset": 18991,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 238
                },
                {
                    "start": 241,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1878
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1678466796875
        },
        {
            "corpus_id": "270063126",
            "title": "Learning to Reason via Program Generation, Emulation, and Search",
            "text": "Recently there have been rapid advances in training language models (LMs) to generate code rather than natural language (NL), following the intuition that code may be more effective than NL for certain tasks, such as those requiring complex calculations, iteration, or data structure manipulation (Chen et al., 2022;Gao et al., 2023). Although successful, these works have mostly studied tasks conducive to a programmatic paradigm, such as symbolic manipulation or algorithmic reasoning, i.e., tasks for which a clear compilable program can be devised. However, it is unclear how to apply this approach to \"softer\" reasoning tasks such as commonsense and social reasoning, where algorithmic solutions are less obvious (Zhang et al., 2023a). (Taori et al., 2023) instance via LLM prompting. We train the model to receive the instruction and input and generate the Python program and function call (as an intermediate), before outputting the final dictionary that contains the answer and any intermediate reasoning steps. \n\nOur goal is to expand an LM's program synthesis skills to such softer reasoning tasks. Our approach builds on the insight that, beyond generating code, LMs can also emulate the execution of code. This includes handling function calls that are defined only by name and documentation, even if they lack a full implementation. We refer to these types of programs-where only the function skeletons are provided without actual code implementations-as pseudo-programs. Such pseudo-programs can encompass both well-defined reasoning steps, such as mathematical or algorithmic operations, as well as function calls representing less precise reasoning, such as commonsense logic. This work investigates whether generating and pseudo-executing such programs can effectively address soft reasoning tasks in addition to traditional algorithmic problems. \n\nTo achieve this, we propose a novel approach: training models to follow NL instructions by generating a program and then emulating that program's code execution. Our paradigm, called COGEX, changes the inference process to (1) generate a Python function given an arbitrary instruction and optional input, (2) generate a call to that function, and (3) produce the result of simulating its execution.",
            "score": 0.43557057113999553,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1863
                },
                {
                    "start": 1866,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2264
                }
            ],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 316,
                    "matchedPaperCorpusId": "259075794"
                },
                {
                    "start": 316,
                    "end": 333,
                    "matchedPaperCorpusId": "253708270"
                },
                {
                    "start": 718,
                    "end": 739,
                    "matchedPaperCorpusId": "258332119"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47314453125
        },
        {
            "corpus_id": "276725054",
            "title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
            "text": "C ontemporary Large Language Models (LLMs) exhibit remarkable capabilities across a vast spectrum of tasks, encompassing not only text generation [1,2,3] and questionanswering [4,5,6,7], but also sophisticated multi-step reasoning [8,9,10,11]. They power applications in natural language understanding [12,13,14,15,16,17], content generation [18,19,20,21,22,23,24,25], automated reasoning [26,27,28,29], and multimodal interactions [30, 31? , 33]. By leveraging vast self-supervised training corpora, these models often approximate human-like cognition [34,35,36,37,38], demonstrating impressive adaptability in real-world settings. \n\nDespite these impressive achievements, LLMs remain prone to critical shortcomings. They can generate misleading or factually incorrect content (commonly referred to as \"hallucinations\") and may struggle to maintain logical consistency throughout extended discourse [41,42,43,44,45,46]. Moreover, the concept of reasoning in LLMs remains a topic of debate. While these models can produce responses that appear logically coherent, their reasoning is fundamentally distinct from human-like logical inference [47,34,48,49]. This distinction is crucial, as it helps explain why LLMs can  Fig. 1: A taxonomy of post-training approaches for LLMs (LLMs), categorized into Fine-tuning, Reinforcement Learning, and Test-time Scaling methods. We summarize the key techniques used in recent LLM models, such as GPT-4 [39], LLaMA 3.3 [13], and Deepseek R1 [40]. \n\nproduce compelling outputs while still stumbling on relatively simple logical tasks. Unlike symbolic reasoning that manipulates explicit rules and facts, LLMs operate in an implicit and probabilistic manner [50,42,51]. For the scope of this work, 'reasoning' in LLMs refers to their ability to generate logically coherent responses based on statistical patterns in data rather than explicit logical inference or symbolic manipulation.",
            "score": 0.4349107967684234,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1483
                },
                {
                    "start": 1486,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1920
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 149,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 149,
                    "end": 151,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 181,
                    "end": 183,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "268710464"
                },
                {
                    "start": 239,
                    "end": 242,
                    "matchedPaperCorpusId": "264172238"
                },
                {
                    "start": 358,
                    "end": 361,
                    "matchedPaperCorpusId": "266335508"
                },
                {
                    "start": 361,
                    "end": 364,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 364,
                    "end": 367,
                    "matchedPaperCorpusId": "253734939"
                },
                {
                    "start": 396,
                    "end": 399,
                    "matchedPaperCorpusId": "258426922"
                },
                {
                    "start": 553,
                    "end": 557,
                    "matchedPaperCorpusId": "259145108"
                },
                {
                    "start": 566,
                    "end": 569,
                    "matchedPaperCorpusId": "269751990"
                },
                {
                    "start": 900,
                    "end": 904,
                    "matchedPaperCorpusId": "270361439"
                },
                {
                    "start": 910,
                    "end": 913,
                    "matchedPaperCorpusId": "270615909"
                },
                {
                    "start": 913,
                    "end": 916,
                    "matchedPaperCorpusId": "268358769"
                },
                {
                    "start": 916,
                    "end": 919,
                    "matchedPaperCorpusId": "260887222"
                },
                {
                    "start": 1140,
                    "end": 1144,
                    "matchedPaperCorpusId": "258865812"
                },
                {
                    "start": 1144,
                    "end": 1147,
                    "matchedPaperCorpusId": "259145108"
                },
                {
                    "start": 1700,
                    "end": 1703,
                    "matchedPaperCorpusId": "246652372"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2001953125
        },
        {
            "corpus_id": "258714564",
            "title": "A Preliminary Analysis on the Code Generation Capabilities of GPT-3.5 and Bard AI Models for Java Functions",
            "text": "Artificial Intelligence (AI) has made significant strides in recent years, particularly in the area of natural language understanding and generation. With the widespread proliferation of large language models capable of performing intricate tasks, research efforts have been undertaken to benchmark and compare the performance of these models [1]. Our analysis focuses on two specific AI models, GPT-3.5 and Bard, investigating their capabilities to generate Java code based on function descriptions obtained from CodingBat.com. \n\nIn previous research, the mathematical reasoning capabilities of Generative Pre-trained Tranformers (GPTs) were assessed against state-of-the-art large language models (LLMs) [2]. The system was presented with a set of mathematics questions taken, and the output was rated manually to assess their performance. In [3] GPT-3.5, 4 and Bard's performances in answering medical questions from a neurosurgery question bank were compared, showing that GPT models systematically outperformed Bard. GPT-3.5 and 4 were also shown to be able to pass complex tests, such as the Bar Exam in U.S., with GPT-4 consistently surpassing its LLMs predecessors as well as previous recorded students' performance [4]. GPT-3 models were also tested for code vulnerability and bug detection, showing that their capabilities were limited [5]. Similarly, GPT-3.5 and GPT-4 abilities were tested against other LLMs on coding tasks [6], performing translation in different languages [8], and stock return forecasting [7].",
            "score": 0.43484353181889124,
            "section_title": "Introduction",
            "char_start_offset": 812,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1526
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.062103271484375
        },
        {
            "corpus_id": "254854559",
            "title": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning",
            "text": "Logical reasoning is an important ability of humans that helps us in making rational decisions based on known information. It is an important ability for text understanding across various downstream tasks, e.g., in open-domain question answering (Yang et al., 2018;Zhu et al., 2021), machine reading comprehension (MRC) (Baradaran et al., The blue words are some logical keywords that are essential in understanding the overall logical structure of the context. The blue and pink words are some of the words we mask during pretraining as they pertain more towards logical reasoning (as opposed to the non-highlighted words that mainly contain factual knowledge or grammar). 2022), etc. Recently, there has been an increasing focus on evaluating the logical reasoning abilities of language models by using MRC tasks that specifically require a significant amount of logical reasoning to obtain the correct answer Liu et al., 2021). In these datasets, the model needs to understand a given context, reason logically about a question to infer new conclusions, and then select the correct answer from a set of options. With the advent of large pre-trained language models (PLMs) in NLP (Devlin et al., 2019;Radford et al., 2019;Raffel et al., 2020), understanding and improving the logical reasoning abilities of these models has become even more important as these are increasingly being used across a wide variety of real-world tasks.\n\nThere have been some recent works on improving the logical reasoning abilities of PLMs (Wang et al., 2022;Ouyang et al., 2022;Jiao et al., 2022). These works typically generate a dataset containing symbolic structures such as logical graphs from text, logical contrast sets, etc., and then train the LM using custom loss objectives to learn logical reasoning abilities. While the performance improvements achieved by these methods are encouraging, the proposed solutions generally require complex data processing to generate the additional structural information (graphs, contrast data, etc.) required for logical reasoning. Further, the loss functions proposed in these works are very specifically designed in accordance with their respective data augmentation technique and widely differs from the typical masked language modeling loss used for LM pretraining (Devlin et al., 2019). These complex processing steps usually",
            "score": 0.43474661012777255,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 265,
                    "matchedPaperCorpusId": "52822214"
                },
                {
                    "start": 912,
                    "end": 929,
                    "matchedPaperCorpusId": "220483148"
                },
                {
                    "start": 1182,
                    "end": 1203,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1224,
                    "end": 1244,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1521,
                    "end": 1540,
                    "matchedPaperCorpusId": "234335834"
                },
                {
                    "start": 1560,
                    "end": 1578,
                    "matchedPaperCorpusId": "247187518"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61328125
        },
        {
            "corpus_id": "269604995",
            "title": "Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection",
            "text": "However, these approaches still face challenges in accurately capturing implicit information or transforming complex ambiguous sentences into logical form, mainly attributed to linguistic ambiguity. Aly et al. (2023) integrated LLMs with logical inference for fact verification, and while our method shares the fundamental idea of employing LLMs to construct proofs and analyze relationships between textual spans, our task adds a layer of contextual reasoning by requiring the incorporation of background knowledge and maintaining interdependency between proof steps, which is not present in approaches where each proof step is treated as an independent, isolated process. \n\nTheory solvers. Recent work by Hahn et al. (2022) demonstrated the potential of integrating symbolic solvers with large language models (LLMs), such as tool-augmented LLMs, to combine neural and symbolic reasoning. While such approaches are promising, they often struggle to translate natural language into symbolic representations and effectively capture background knowledge. Other recent approaches (Olausson et al., 2023;Pan et al., 2023) have used theory solvers to logically reason with natural language, which we build on with several key advancements. First, we introduce a framework that handles naturalistic, real-world data and tasks with ambiguous premises and conclusions. Then, we present a method to incorporate background knowledge into logical formulas. Finally, we show that our approach introduces interpretability by allowing human verification and modification throughout the intermediate reasoning steps.",
            "score": 0.4343191561949402,
            "section_title": "Related Work",
            "char_start_offset": 6561,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1602
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 216,
                    "matchedPaperCorpusId": "264426758"
                },
                {
                    "start": 707,
                    "end": 725,
                    "matchedPaperCorpusId": "251197051"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.212890625
        },
        {
            "corpus_id": "276249607",
            "title": "LawGPT: Knowledge-Guided Data Generation and Its Application to Legal LLM",
            "text": "Large language models (LLMs) (OpenAI, 2023b;Touvron et al., 2023) have achieved remarkable success in various natural language processing tasks, including natural language understanding (Dong et al., 2019), reasoning (Huang & Chang, 2023), and generation (Yu et al., 2022). Both proprietary and open-source LLMs exhibit strong generalization capabilities, enabling their application in diverse downstream scenarios, such as medicine (Thirunavukarasu et al., 2023), finance (Yang et al., 2023), education (Gan et al., 2023). Recent studies (Fei et al., 2023;Nguyen, 2023) have demonstrated the preliminary effectiveness of existing general LLMs in legal reasoning tasks, including legal documents retrieval (Chen et al., 2013), legal judgment prediction (Luo et al., 2017), and legal question answering (Zhong et al., 2020). \n\nDespite their preliminary success in legal reasoning applications, LLMs face significant practical limitations. Proprietary LLMs such as GPT-4 (OpenAI, 2023b) and GPT-3.5 Turbo (OpenAI, 2023a), as well as extremely large open-source models like DeepSeek V3 (DeepSeek-AI et al., 2024), require API access, introducing substantial data privacy risks and high inference costs. Opensource LLMs like Qwen (Yang et al., 2024) and ChatGLM (Du et al., 2022) show suboptimal per-Preprint formance due to training with insufficient legal data. These limitations create an opportunity to leverage proprietary LLMs for generating legal reasoning data to build open-source legal LLMs. \n\nPrevious studies have developed various data generation methods using proprietary LLMs for downstream reasoning tasks, which have been effective for mathematical reasoning (Luo et al., 2025). These methods assume that the LLMs used for generation have sufficient knowledge about the downstream tasks and can generate diverse data through appropriate prompts (Yu et al., 2024).",
            "score": 0.4340123687853982,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1876
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 205,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 255,
                    "end": 272,
                    "matchedPaperCorpusId": "222272210"
                },
                {
                    "start": 433,
                    "end": 463,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 504,
                    "end": 522,
                    "matchedPaperCorpusId": "265352038"
                },
                {
                    "start": 706,
                    "end": 725,
                    "matchedPaperCorpusId": "30879425"
                },
                {
                    "start": 753,
                    "end": 771,
                    "matchedPaperCorpusId": "7699280"
                },
                {
                    "start": 802,
                    "end": 822,
                    "matchedPaperCorpusId": "216552897"
                },
                {
                    "start": 1258,
                    "end": 1275,
                    "matchedPaperCorpusId": "247519241"
                },
                {
                    "start": 1672,
                    "end": 1690,
                    "matchedPaperCorpusId": "261030818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3251953125
        },
        {
            "paperId": "b01f9dd1e73f0a9221b1bf7e6eb4baf34c04e5a2",
            "corpusId": 270738122,
            "title": "Multi-step Inference over Unstructured Data",
            "venue": "",
            "year": 2024,
            "referenceCount": 23,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.17987, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308097249",
                    "name": "Aditya Kalyanpur"
                },
                {
                    "authorId": "2139514542",
                    "name": "K. Saravanakumar"
                },
                {
                    "authorId": "2308097509",
                    "name": "Victor Barres"
                },
                {
                    "authorId": "2308275474",
                    "name": "CJ McFate"
                },
                {
                    "authorId": "2308275315",
                    "name": "Lori Moon"
                },
                {
                    "authorId": "2308275650",
                    "name": "Nati Seifu"
                },
                {
                    "authorId": "2308275705",
                    "name": "Maksim Eremeev"
                },
                {
                    "authorId": "2308275929",
                    "name": "Jose Barrera"
                },
                {
                    "authorId": "2312902277",
                    "name": "Abraham Bautista-Castillo"
                },
                {
                    "authorId": "2308440649",
                    "name": "Eric Brown"
                },
                {
                    "authorId": "2263068247",
                    "name": "David Ferrucci"
                }
            ],
            "abstract": "The advent of Large Language Models (LLMs) and Generative AI has revolutionized natural language applications across various domains. However, high-stakes decision-making tasks in fields such as medical, legal and finance require a level of precision, comprehensiveness, and logical consistency that pure LLM or Retrieval-Augmented-Generation (RAG) approaches often fail to deliver. At Elemental Cognition (EC), we have developed a neuro-symbolic AI platform to tackle these problems. The platform integrates fine-tuned LLMs for knowledge extraction and alignment with a robust symbolic reasoning engine for logical inference, planning and interactive constraint solving. We describe Cora, a Collaborative Research Assistant built on this platform, that is designed to perform complex research and discovery tasks in high-stakes domains. This paper discusses the multi-step inference challenges inherent in such domains, critiques the limitations of existing LLM-based methods, and demonstrates how Cora's neuro-symbolic approach effectively addresses these issues. We provide an overview of the system architecture, key algorithms for knowledge extraction and formal reasoning, and present preliminary evaluation results that highlight Cora's superior performance compared to well-known LLM and RAG baselines.",
            "corpus_id": "270738122",
            "text": "The advent of Large Language Models (LLMs) and Generative AI has revolutionized natural language applications across various domains. However, high-stakes decision-making tasks in fields such as medical, legal and finance require a level of precision, comprehensiveness, and logical consistency that pure LLM or Retrieval-Augmented-Generation (RAG) approaches often fail to deliver. At Elemental Cognition (EC), we have developed a neuro-symbolic AI platform to tackle these problems. The platform integrates fine-tuned LLMs for knowledge extraction and alignment with a robust symbolic reasoning engine for logical inference, planning and interactive constraint solving. We describe Cora, a Collaborative Research Assistant built on this platform, that is designed to perform complex research and discovery tasks in high-stakes domains. This paper discusses the multi-step inference challenges inherent in such domains, critiques the limitations of existing LLM-based methods, and demonstrates how Cora's neuro-symbolic approach effectively addresses these issues. We provide an overview of the system architecture, key algorithms for knowledge extraction and formal reasoning, and present preliminary evaluation results that highlight Cora's superior performance compared to well-known LLM and RAG baselines.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.33251953125
        },
        {
            "paperId": "8bf01e11b54c1dcb6d411f88eb82cd3d862870ea",
            "corpusId": 274776138,
            "title": "AutoPrep: Natural Language Question-Aware Data Preparation with a Multi-Agent Framework",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 58,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.10422, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2271233300",
                    "name": "Meihao Fan"
                },
                {
                    "authorId": "2295796111",
                    "name": "Ju Fan"
                },
                {
                    "authorId": "2213989628",
                    "name": "Nan Tang"
                },
                {
                    "authorId": "2251218532",
                    "name": "Lei Cao"
                },
                {
                    "authorId": "2257315172",
                    "name": "Guoliang Li"
                },
                {
                    "authorId": "2200839859",
                    "name": "Xiaoyong Du"
                }
            ],
            "abstract": "Answering natural language (NL) questions about tables, known as Tabular Question Answering (TQA), is crucial because it allows users to quickly and efficiently extract meaningful insights from structured data, effectively bridging the gap between human language and machine-readable formats. Many of these tables are derived from web sources or real-world scenarios, which require meticulous data preparation (or data prep) to ensure accurate responses. However, preparing such tables for NL questions introduces new requirements that extend beyond traditional data preparation. This question-aware data preparation involves specific tasks such as column derivation and filtering tailored to particular questions, as well as question-aware value normalization or conversion, highlighting the need for a more nuanced approach in this context. Because each of the above tasks is unique, a single model (or agent) may not perform effectively across all scenarios. In this paper, we propose AutoPrep, a large language model (LLM)-based multi-agent framework that leverages the strengths of multiple agents, each specialized in a certain type of data prep, ensuring more accurate and contextually relevant responses. Given an NL question over a table, AutoPrep performs data prep through three key components. Planner: Determines a logical plan, outlining a sequence of high-level operations. Programmer: Translates this logical plan into a physical plan by generating the corresponding low-level code. Executor: Executes the generated code to process the table. To support this multi-agent framework, we design a novel Chain-of-Clauses reasoning mechanism for high-level operation suggestion, and a tool-augmented method for low-level code generation...",
            "corpus_id": "274776138",
            "text": "Answering natural language (NL) questions about tables, known as Tabular Question Answering (TQA), is crucial because it allows users to quickly and efficiently extract meaningful insights from structured data, effectively bridging the gap between human language and machine-readable formats. Many of these tables are derived from web sources or real-world scenarios, which require meticulous data preparation (or data prep) to ensure accurate responses. However, preparing such tables for NL questions introduces new requirements that extend beyond traditional data preparation. This question-aware data preparation involves specific tasks such as column derivation and filtering tailored to particular questions, as well as question-aware value normalization or conversion, highlighting the need for a more nuanced approach in this context. Because each of the above tasks is unique, a single model (or agent) may not perform effectively across all scenarios. In this paper, we propose AutoPrep, a large language model (LLM)-based multi-agent framework that leverages the strengths of multiple agents, each specialized in a certain type of data prep, ensuring more accurate and contextually relevant responses. Given an NL question over a table, AutoPrep performs data prep through three key components. Planner: Determines a logical plan, outlining a sequence of high-level operations. Programmer: Translates this logical plan into a physical plan by generating the corresponding low-level code. Executor: Executes the generated code to process the table. To support this multi-agent framework, we design a novel Chain-of-Clauses reasoning mechanism for high-level operation suggestion, and a tool-augmented method for low-level code generation...",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.294189453125
        },
        {
            "paperId": "d9293bafb3b7ff2d14f69f35b681d7ce8d192e46",
            "corpusId": 267897558,
            "title": "Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 55,
            "citationCount": 13,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.15131, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267039181",
                    "name": "Guanming Xiong"
                },
                {
                    "authorId": "2286303797",
                    "name": "Junwei Bao"
                },
                {
                    "authorId": "2267103606",
                    "name": "Wen Zhao"
                }
            ],
            "abstract": "This study explores the realm of knowledge base question answering (KBQA). KBQA is considered a challenging task, particularly in parsing intricate questions into executable logical forms. Traditional semantic parsing (SP)-based methods require extensive data annotations, which result in significant costs. Recently, the advent of few-shot in-context learning, powered by large language models (LLMs), has showcased promising capabilities. However, fully leveraging LLMs to parse questions into logical forms in low-resource scenarios poses a substantial challenge. To tackle these hurdles, we introduce Interactive-KBQA, a framework designed to generate logical forms through direct interaction with knowledge bases (KBs). Within this framework, we have developed three generic APIs for KB interaction. For each category of complex question, we devised exemplars to guide LLMs through the reasoning processes. Our method achieves competitive results on the WebQuestionsSP, ComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of examples (shots). Importantly, our approach supports manual intervention, allowing for the iterative refinement of LLM outputs. By annotating a dataset with step-wise reasoning processes, we showcase our model's adaptability and highlight its potential for contributing significant enhancements to the field.",
            "corpus_id": "267897558",
            "text": "This study explores the realm of knowledge base question answering (KBQA). KBQA is considered a challenging task, particularly in parsing intricate questions into executable logical forms. Traditional semantic parsing (SP)-based methods require extensive data annotations, which result in significant costs. Recently, the advent of few-shot in-context learning, powered by large language models (LLMs), has showcased promising capabilities. However, fully leveraging LLMs to parse questions into logical forms in low-resource scenarios poses a substantial challenge. To tackle these hurdles, we introduce Interactive-KBQA, a framework designed to generate logical forms through direct interaction with knowledge bases (KBs). Within this framework, we have developed three generic APIs for KB interaction. For each category of complex question, we devised exemplars to guide LLMs through the reasoning processes. Our method achieves competitive results on the WebQuestionsSP, ComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of examples (shots). Importantly, our approach supports manual intervention, allowing for the iterative refinement of LLM outputs. By annotating a dataset with step-wise reasoning processes, we showcase our model's adaptability and highlight its potential for contributing significant enhancements to the field.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.52734375
        },
        {
            "paperId": "e5bf9d2ff9294a2d5ebf25e2350f048bacbe6297",
            "corpusId": 271517980,
            "title": "Prompting Few-shot Multi-hop Question Generation via Comprehending Type-aware Semantics",
            "venue": "NAACL-HLT",
            "year": 2024,
            "referenceCount": 42,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2024.findings-naacl.236?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2024.findings-naacl.236, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313600081",
                    "name": "Zefeng Lin"
                },
                {
                    "authorId": "2221304979",
                    "name": "Weidong Chen"
                },
                {
                    "authorId": "2266777604",
                    "name": "Yan Song"
                },
                {
                    "authorId": "2266360133",
                    "name": "Yongdong Zhang"
                }
            ],
            "abstract": "Given several documents, multi-hop question generation (MQG) is a task aims to generate complicated questions that require reasoning over multiple pieces of these documents to find the answer. To perform this task, existing studies focus on designing advanced architectures to locate essential keywords or sentences in multiple documents and then generate questions accordingly, where they normally do not note that question types could provide crucial hints for extracting key information from the documents for MQG. In general, supervised approaches are used that rely on large annotated data, which is not available in many low-resource scenarios and thus makes MQG hard in these domains. Consider the recent success of large language models (LLMs) on natural language processing tasks using limited labeled data under few-shot settings, in this paper, we propose an approach named type-aware semantics extraction-based chain-of-thought method (TASE-CoT) for few-shot MQG. Specifically, our approach firstly extracts question types and essential semantic phrases from the given documents and the answer. Then, we design a three-step CoT template to leverage the extracted question type and semantic phrases to predict multi-hop questions. Extensive experiments and the results demonstrate the effectiveness of our approach and the proposed modules. 1",
            "corpus_id": "271517980",
            "text": "Given several documents, multi-hop question generation (MQG) is a task aims to generate complicated questions that require reasoning over multiple pieces of these documents to find the answer. To perform this task, existing studies focus on designing advanced architectures to locate essential keywords or sentences in multiple documents and then generate questions accordingly, where they normally do not note that question types could provide crucial hints for extracting key information from the documents for MQG. In general, supervised approaches are used that rely on large annotated data, which is not available in many low-resource scenarios and thus makes MQG hard in these domains. Consider the recent success of large language models (LLMs) on natural language processing tasks using limited labeled data under few-shot settings, in this paper, we propose an approach named type-aware semantics extraction-based chain-of-thought method (TASE-CoT) for few-shot MQG. Specifically, our approach firstly extracts question types and essential semantic phrases from the given documents and the answer. Then, we design a three-step CoT template to leverage the extracted question type and semantic phrases to predict multi-hop questions. Extensive experiments and the results demonstrate the effectiveness of our approach and the proposed modules. 1",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.470703125
        },
        {
            "paperId": "965933f221e1414e27d1177c1a145233d4f33e2b",
            "corpusId": 273507450,
            "title": "SG-FSM: A Self-Guiding Zero-Shot Prompting Paradigm for Multi-Hop Question Answering Based on Finite State Machine",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 28,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.17021, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2196915027",
                    "name": "Xiaochen Wang"
                },
                {
                    "authorId": "2266806217",
                    "name": "Junqing He"
                },
                {
                    "authorId": "2327179428",
                    "name": "Liang Chen"
                },
                {
                    "authorId": "2301453951",
                    "name": "Reza Haf"
                },
                {
                    "authorId": "2257389788",
                    "name": "Zhe Yang"
                },
                {
                    "authorId": "2309653937",
                    "name": "Yiru Wang"
                },
                {
                    "authorId": "2287260750",
                    "name": "Xiangdi Meng"
                },
                {
                    "authorId": "2184245974",
                    "name": "Kunhao Pan"
                },
                {
                    "authorId": "2287829088",
                    "name": "Zhifang Sui"
                }
            ],
            "abstract": "Large Language Models with chain-of-thought prompting, such as OpenAI-o1, have shown impressive capabilities in natural language inference tasks. However, Multi-hop Question Answering (MHQA) remains challenging for many existing models due to issues like hallucination, error propagation, and limited context length. To address these challenges and enhance LLMs' performance on MHQA, we propose the Self-Guiding prompting Finite State Machine (SG-FSM), designed to strengthen multi-hop reasoning abilities. Unlike traditional chain-of-thought methods, SG-FSM tackles MHQA by iteratively breaking down complex questions into sub-questions, correcting itself to improve accuracy. It processes one sub-question at a time, dynamically deciding the next step based on the current context and results, functioning much like an automaton. Experiments across various benchmarks demonstrate the effectiveness of our approach, outperforming strong baselines on challenging datasets such as Musique. SG-FSM reduces hallucination, enabling recovery of the correct final answer despite intermediate errors. It also improves adherence to specified output formats, simplifying evaluation significantly.",
            "corpus_id": "273507450",
            "text": "Large Language Models with chain-of-thought prompting, such as OpenAI-o1, have shown impressive capabilities in natural language inference tasks. However, Multi-hop Question Answering (MHQA) remains challenging for many existing models due to issues like hallucination, error propagation, and limited context length. To address these challenges and enhance LLMs' performance on MHQA, we propose the Self-Guiding prompting Finite State Machine (SG-FSM), designed to strengthen multi-hop reasoning abilities. Unlike traditional chain-of-thought methods, SG-FSM tackles MHQA by iteratively breaking down complex questions into sub-questions, correcting itself to improve accuracy. It processes one sub-question at a time, dynamically deciding the next step based on the current context and results, functioning much like an automaton. Experiments across various benchmarks demonstrate the effectiveness of our approach, outperforming strong baselines on challenging datasets such as Musique. SG-FSM reduces hallucination, enabling recovery of the correct final answer despite intermediate errors. It also improves adherence to specified output formats, simplifying evaluation significantly.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.58740234375
        },
        {
            "paperId": "2f3ce76e7d92c19a8c59ba6dbdc3db7cbae6cb56",
            "corpusId": 271923537,
            "title": "SymKGQA: Few-Shot Knowledge Graph Question Answering via Symbolic Program Generation and Execution",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 39,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2024.acl-long.545?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2024.acl-long.545, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316635729",
                    "name": "Prerna Agarwal"
                },
                {
                    "authorId": "2316676036",
                    "name": "Nishant Kumar"
                },
                {
                    "authorId": "1751538",
                    "name": "Srikanta J. Bedathur"
                }
            ],
            "abstract": "Semantic Parsing of natural language questions into their executable logical form (LF) has shown state-of-the-art (SOTA) performance for Knowledge Graph Question Answering (KGQA). However, these methods are not applicable for real-world applications, due to lack of KG-specific training data. Recent advances in the capabilities of Large Language Models (LLMs) has led towards generating low-level LFs such as SPARQL and S-Expression in a few-shot setting. Unfortunately, these methods: (1) are limited to the knowledge of underlying LLM about the LF, (2) performs inferior for the harder complex benchmarks such as KQA Pro, (3) suffers while grounding the generated LF to a specific Knowledge Graph. Recently, a new LF called KoPL (Cao et al., 2022a) has been introduced that explicitly models complex reasoning process step-by-step in a symbolic manner and has shown SOTA on KQA Pro in fully-supervised setting. Inspired by this, we propose SymKGQA 1 framework that generates step-by-step Sym bolic LF i.e., KoPL in a few-shot in-context learning setting using LLM. Our framework is not dependent on pre-trained knowledge of LLM about KoPL. We further build a Retrieval-Augmented Generation based Qu estion-A ware C ontextual K oPL (QUACK) resolver to ground the generated LF. Our experiments with different LLMs and few-shot settings demonstrate that SymKGQA outperforms all other few-shot and even many of the fully-supervised KGQA approaches.",
            "corpus_id": "271923537",
            "text": "Semantic Parsing of natural language questions into their executable logical form (LF) has shown state-of-the-art (SOTA) performance for Knowledge Graph Question Answering (KGQA). However, these methods are not applicable for real-world applications, due to lack of KG-specific training data. Recent advances in the capabilities of Large Language Models (LLMs) has led towards generating low-level LFs such as SPARQL and S-Expression in a few-shot setting. Unfortunately, these methods: (1) are limited to the knowledge of underlying LLM about the LF, (2) performs inferior for the harder complex benchmarks such as KQA Pro, (3) suffers while grounding the generated LF to a specific Knowledge Graph. Recently, a new LF called KoPL (Cao et al., 2022a) has been introduced that explicitly models complex reasoning process step-by-step in a symbolic manner and has shown SOTA on KQA Pro in fully-supervised setting. Inspired by this, we propose SymKGQA 1 framework that generates step-by-step Sym bolic LF i.e., KoPL in a few-shot in-context learning setting using LLM. Our framework is not dependent on pre-trained knowledge of LLM about KoPL. We further build a Retrieval-Augmented Generation based Qu estion-A ware C ontextual K oPL (QUACK) resolver to ground the generated LF. Our experiments with different LLMs and few-shot settings demonstrate that SymKGQA outperforms all other few-shot and even many of the fully-supervised KGQA approaches.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.406005859375
        },
        {
            "paperId": "ad6a83dba641ae571254dda676b9f3ad5324c677",
            "corpusId": 276961886,
            "title": "DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model for Driving Scenario Understanding",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 32,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10621, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2140344825",
                    "name": "Ayesha Ishaq"
                },
                {
                    "authorId": "49034704",
                    "name": "Jean Lahoud"
                },
                {
                    "authorId": "2332100847",
                    "name": "Ketan More"
                },
                {
                    "authorId": "1396268305",
                    "name": "Omkar Thawakar"
                },
                {
                    "authorId": "2339668248",
                    "name": "Ritesh Thawkar"
                },
                {
                    "authorId": "2332100854",
                    "name": "Dinura Dissanayake"
                },
                {
                    "authorId": "2322506250",
                    "name": "Noor Ahsan"
                },
                {
                    "authorId": "2305625382",
                    "name": "Yuhao Li"
                },
                {
                    "authorId": "2242949919",
                    "name": "F. Khan"
                },
                {
                    "authorId": "2951229",
                    "name": "Hisham Cholakkal"
                },
                {
                    "authorId": "2311508734",
                    "name": "Ivan Laptev"
                },
                {
                    "authorId": "3288214",
                    "name": "R. Anwer"
                },
                {
                    "authorId": "2347699468",
                    "name": "Salman Khan"
                }
            ],
            "abstract": "While large multimodal models (LMMs) have demonstrated strong performance across various Visual Question Answering (VQA) tasks, certain challenges require complex multi-step reasoning to reach accurate answers. One particularly challenging task is autonomous driving, which demands thorough cognitive processing before decisions can be made. In this domain, a sequential and interpretive understanding of visual cues is essential for effective perception, prediction, and planning. Nevertheless, common VQA benchmarks often focus on the accuracy of the final answer while overlooking the reasoning process that enables the generation of accurate responses. Moreover, existing methods lack a comprehensive framework for evaluating step-by-step reasoning in realistic driving scenarios. To address this gap, we propose DriveLMM-o1, a new dataset and benchmark specifically designed to advance step-wise visual reasoning for autonomous driving. Our benchmark features over 18k VQA examples in the training set and more than 4k in the test set, covering diverse questions on perception, prediction, and planning, each enriched with step-by-step reasoning to ensure logical inference in autonomous driving scenarios. We further introduce a large multimodal model that is fine-tuned on our reasoning dataset, demonstrating robust performance in complex driving scenarios. In addition, we benchmark various open-source and closed-source methods on our proposed dataset, systematically comparing their reasoning capabilities for autonomous driving tasks. Our model achieves a +7.49% gain in final answer accuracy, along with a 3.62% improvement in reasoning score over the previous best open-source model. Our framework, dataset, and model are available at https://github.com/ayesha-ishaq/DriveLMM-o1.",
            "corpus_id": "276961886",
            "text": "While large multimodal models (LMMs) have demonstrated strong performance across various Visual Question Answering (VQA) tasks, certain challenges require complex multi-step reasoning to reach accurate answers. One particularly challenging task is autonomous driving, which demands thorough cognitive processing before decisions can be made. In this domain, a sequential and interpretive understanding of visual cues is essential for effective perception, prediction, and planning. Nevertheless, common VQA benchmarks often focus on the accuracy of the final answer while overlooking the reasoning process that enables the generation of accurate responses. Moreover, existing methods lack a comprehensive framework for evaluating step-by-step reasoning in realistic driving scenarios. To address this gap, we propose DriveLMM-o1, a new dataset and benchmark specifically designed to advance step-wise visual reasoning for autonomous driving. Our benchmark features over 18k VQA examples in the training set and more than 4k in the test set, covering diverse questions on perception, prediction, and planning, each enriched with step-by-step reasoning to ensure logical inference in autonomous driving scenarios. We further introduce a large multimodal model that is fine-tuned on our reasoning dataset, demonstrating robust performance in complex driving scenarios. In addition, we benchmark various open-source and closed-source methods on our proposed dataset, systematically comparing their reasoning capabilities for autonomous driving tasks. Our model achieves a +7.49% gain in final answer accuracy, along with a 3.62% improvement in reasoning score over the previous best open-source model. Our framework, dataset, and model are available at https://github.com/ayesha-ishaq/DriveLMM-o1.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.10614013671875
        },
        {
            "paperId": "95cd194aa578e791eed6a74a1563666d15f9c104",
            "corpusId": 277113175,
            "title": "Elevating Visual Question Answering through Implicitly Learned Reasoning Pathways in LVLMs",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 21,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.14674, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2311182584",
                    "name": "Liu Jing"
                },
                {
                    "authorId": "2311404877",
                    "name": "Amirul Rahman"
                }
            ],
            "abstract": "Large Vision-Language Models (LVLMs) have shown remarkable progress in various multimodal tasks, yet they often struggle with complex visual reasoning that requires multi-step inference. To address this limitation, we propose MF-SQ-LLaVA, a novel approach that enhances LVLMs by enabling implicit self-questioning through end-to-end training. Our method involves augmenting visual question answering datasets with reasoning chains consisting of sub-question and answer pairs, and training the LVLM with a multi-task loss that encourages the generation and answering of these intermediate steps, as well as the prediction of the final answer. We conduct extensive experiments on the ScienceQA and VQAv2 datasets, demonstrating that MF-SQ-LLaVA significantly outperforms existing state-of-the-art models, including the base LLaVA and the original SQ-LLaVA. Ablation studies further validate the contribution of each component of our approach, and human evaluation confirms the improved accuracy and coherence of the reasoning process enabled by our method.",
            "corpus_id": "277113175",
            "text": "Large Vision-Language Models (LVLMs) have shown remarkable progress in various multimodal tasks, yet they often struggle with complex visual reasoning that requires multi-step inference. To address this limitation, we propose MF-SQ-LLaVA, a novel approach that enhances LVLMs by enabling implicit self-questioning through end-to-end training. Our method involves augmenting visual question answering datasets with reasoning chains consisting of sub-question and answer pairs, and training the LVLM with a multi-task loss that encourages the generation and answering of these intermediate steps, as well as the prediction of the final answer. We conduct extensive experiments on the ScienceQA and VQAv2 datasets, demonstrating that MF-SQ-LLaVA significantly outperforms existing state-of-the-art models, including the base LLaVA and the original SQ-LLaVA. Ablation studies further validate the contribution of each component of our approach, and human evaluation confirms the improved accuracy and coherence of the reasoning process enabled by our method.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.33447265625
        },
        {
            "paperId": "22230df98376f0b1229e23af2adc6b81575f79af",
            "corpusId": 275533852,
            "title": "React Agent-Based Question Answering Method for Biology in National College Entrance Examination",
            "venue": "International Symposium on Artificial Intelligence in Medical Sciences",
            "year": 2024,
            "referenceCount": 11,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3706890.3707043?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3706890.3707043, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2340138619",
                    "name": "Yulin Chen"
                },
                {
                    "authorId": "145026687",
                    "name": "Jing Luo"
                },
                {
                    "authorId": "2298273637",
                    "name": "Xinhui Tu"
                }
            ],
            "abstract": "Recent advancements in large language models have led to significant improvements in various natural language processing tasks, including automated question answering. However, these models still struggle with providing accurate responses to complex biology questions, such as those found in the National College Entrance Examination. To address this issue, researchers have explored retrieval-augmented generation techniques, which incorporate external knowledge sources to enhance answer accuracy. While these methods have shown promise in improving efficiency, they often lack the ability to perform in-depth analysis of complex questions. In response to these limitations, the paper proposes a novel ReAct agent-based question answering method. This method combines biology textbook document libraries with Google search engine capabilities, leveraging the strengths of large language models. By integrating these components, this method can engage in autonomous decision-making and multi-step reasoning, allowing for a more thorough analysis of complex biology questions. Experimental results demonstrate the effectiveness of this new approach. When compared to responses generated directly by ChatGPT and GPT-4o models, the method showed significant improvements in accuracy for biology questions from the National College Entrance Examination. Specifically, the new method achieved accuracy increases of 11.1% and 4.64% over ChatGPT and GPT-4o, respectively.",
            "corpus_id": "275533852",
            "text": "Recent advancements in large language models have led to significant improvements in various natural language processing tasks, including automated question answering. However, these models still struggle with providing accurate responses to complex biology questions, such as those found in the National College Entrance Examination. To address this issue, researchers have explored retrieval-augmented generation techniques, which incorporate external knowledge sources to enhance answer accuracy. While these methods have shown promise in improving efficiency, they often lack the ability to perform in-depth analysis of complex questions. In response to these limitations, the paper proposes a novel ReAct agent-based question answering method. This method combines biology textbook document libraries with Google search engine capabilities, leveraging the strengths of large language models. By integrating these components, this method can engage in autonomous decision-making and multi-step reasoning, allowing for a more thorough analysis of complex biology questions. Experimental results demonstrate the effectiveness of this new approach. When compared to responses generated directly by ChatGPT and GPT-4o models, the method showed significant improvements in accuracy for biology questions from the National College Entrance Examination. Specifically, the new method achieved accuracy increases of 11.1% and 4.64% over ChatGPT and GPT-4o, respectively.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.537109375
        },
        {
            "paperId": "0759076827798ca3e4a4b3c0002bc5687576a9a9",
            "corpusId": 268732704,
            "title": "MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering",
            "venue": "International Conference on Computing and Artificial Intelligence",
            "year": 2024,
            "referenceCount": 28,
            "citationCount": 4,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2403.19116",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.19116, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2293720207",
                    "name": "Che Guan"
                },
                {
                    "authorId": "2293768415",
                    "name": "Mengyu Huang"
                },
                {
                    "authorId": "2294328049",
                    "name": "Peng Zhang"
                }
            ],
            "abstract": "In today\u2019s fast-paced industry, professionals face the challenge of summarizing a large number of documents and extracting vital information from them on a daily basis. These metrics are frequently hidden away in tables and/or their nested hyperlinks. To address this challenge, the approach of Table Question Answering (QA) has been developed to extract the relevant information. However, traditional Table QA training tasks that provide a table and an answer(s) from a gold cell coordinate(s) for a question may not always ensure extracting the accurate answer(s). Recent advancements in Large Language Models (LLMs) have opened up new possibilities for extracting information from tabular data using prompts. In this paper, we introduce the Multi-hop Few-shot Open Rich Table QA (MFORT-QA) approach, which consists of two major steps. The first step involves Few-Shot Learning (FSL), where relevant tables and associated contexts of hyperlinks are retrieved based on a given question. The retrieved content is then used to construct few-shot prompts as inputs to an LLM, such as ChatGPT. To tackle the challenge of answering complex questions, the second step leverages Chain-of-thought (CoT) prompting to decompose the complex question into a sequential chain of questions and reasoning thoughts in a multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this process by retrieving relevant tables and contexts of hyperlinks that are relevant to the resulting reasoning thoughts and questions. These additional contexts are then used to supplement the prompt used in the first step, resulting in more accurate answers from an LLM. Empirical results from OTT-QA demonstrate that our abstractive QA approach significantly improves the accuracy of extractive Table QA methods.",
            "corpus_id": "268732704",
            "text": "In today\u2019s fast-paced industry, professionals face the challenge of summarizing a large number of documents and extracting vital information from them on a daily basis. These metrics are frequently hidden away in tables and/or their nested hyperlinks. To address this challenge, the approach of Table Question Answering (QA) has been developed to extract the relevant information. However, traditional Table QA training tasks that provide a table and an answer(s) from a gold cell coordinate(s) for a question may not always ensure extracting the accurate answer(s). Recent advancements in Large Language Models (LLMs) have opened up new possibilities for extracting information from tabular data using prompts. In this paper, we introduce the Multi-hop Few-shot Open Rich Table QA (MFORT-QA) approach, which consists of two major steps. The first step involves Few-Shot Learning (FSL), where relevant tables and associated contexts of hyperlinks are retrieved based on a given question. The retrieved content is then used to construct few-shot prompts as inputs to an LLM, such as ChatGPT. To tackle the challenge of answering complex questions, the second step leverages Chain-of-thought (CoT) prompting to decompose the complex question into a sequential chain of questions and reasoning thoughts in a multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this process by retrieving relevant tables and contexts of hyperlinks that are relevant to the resulting reasoning thoughts and questions. These additional contexts are then used to supplement the prompt used in the first step, resulting in more accurate answers from an LLM. Empirical results from OTT-QA demonstrate that our abstractive QA approach significantly improves the accuracy of extractive Table QA methods.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2763671875
        },
        {
            "paperId": "e9bab7f2340aa3b8d0c6fc4ea34c2cd147894822",
            "corpusId": 260776656,
            "title": "Multi-view consistency for multi-hop knowledge base question answering",
            "venue": "4th International Conference on Information Science, Electrical and Automation Engineering",
            "year": 2023,
            "referenceCount": 29,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/12.2689748?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/12.2689748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47118929",
                    "name": "Xin Wang"
                }
            ],
            "abstract": "The task of Knowledge Base Question Answering (KBQA) is to answer a question in natural language over a Knowledge Base. And multi-hop KBQA aims to reason over multiple hops of facts in KB to answer a complex question. Step-wised reasoning has been an important schema to solve multi-hop KBQA. But previous approaches suffer from lacking reasoning paths, causing models may answer in an incorrect way. To address the issue, we present a novel approach to enhance the KBQA model by leveraging consistency between different views of the data, with few intermediate-relation-labeled data. Previous retrieval-based methods proceeded by utilizing the data view of (question, intermediate entities, answer entities). In our method, we introduce the data view of (question, intermediate relations) and enhance the KBQA model through the consistency of different data views. Concretely, we first implement a question-to-intermediate relations(Q2R) model to obtain intermediate relations\u2019 distributions. By utilizing a pretrained text generation model, it performs well using a small part of relation-labeled data. Then we devise a map function to map distributions of intermediate entities to distributions of intermediate. Finally, a constraint that metrics the consistency between the intermediate path distributions obtained from the Q2R model and the original KBQA model is constructed to enhance the KBQA model. Experiments over three datasets of multi-hop KBQA are conducted, and the results demonstrate the effectiveness of our method.",
            "corpus_id": "260776656",
            "text": "The task of Knowledge Base Question Answering (KBQA) is to answer a question in natural language over a Knowledge Base. And multi-hop KBQA aims to reason over multiple hops of facts in KB to answer a complex question. Step-wised reasoning has been an important schema to solve multi-hop KBQA. But previous approaches suffer from lacking reasoning paths, causing models may answer in an incorrect way. To address the issue, we present a novel approach to enhance the KBQA model by leveraging consistency between different views of the data, with few intermediate-relation-labeled data. Previous retrieval-based methods proceeded by utilizing the data view of (question, intermediate entities, answer entities). In our method, we introduce the data view of (question, intermediate relations) and enhance the KBQA model through the consistency of different data views. Concretely, we first implement a question-to-intermediate relations(Q2R) model to obtain intermediate relations\u2019 distributions. By utilizing a pretrained text generation model, it performs well using a small part of relation-labeled data. Then we devise a map function to map distributions of intermediate entities to distributions of intermediate. Finally, a constraint that metrics the consistency between the intermediate path distributions obtained from the Q2R model and the original KBQA model is constructed to enhance the KBQA model. Experiments over three datasets of multi-hop KBQA are conducted, and the results demonstrate the effectiveness of our method.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.371337890625
        },
        {
            "paperId": "90099482402f12126ef2970337849a2190401fc3",
            "corpusId": 277820355,
            "title": "Debate on Graph: A Flexible and Reliable Reasoning Framework for Large Language Models",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1609/aaai.v39i23.34658",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v39i23.34658?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v39i23.34658, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319184164",
                    "name": "Jie Ma"
                },
                {
                    "authorId": "2319941137",
                    "name": "Zhitao Gao"
                },
                {
                    "authorId": "51255954",
                    "name": "Qianyi Chai"
                },
                {
                    "authorId": "2297333542",
                    "name": "Wangchun Sun"
                },
                {
                    "authorId": "2269757371",
                    "name": "Pinghui Wang"
                },
                {
                    "authorId": "2272282276",
                    "name": "Hongbin Pei"
                },
                {
                    "authorId": "2269513697",
                    "name": "Jing Tao"
                },
                {
                    "authorId": "2320172297",
                    "name": "Lingyun Song"
                },
                {
                    "authorId": "2265651722",
                    "name": "Jun Liu"
                },
                {
                    "authorId": "2319826987",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "2321151995",
                    "name": "Li-zhen Cui"
                }
            ],
            "abstract": "Large Language Models (LLMs) may suffer from hallucinations in real-world applications due to the lack of relevant knowledge. In contrast, knowledge graphs encompass extensive, multi-relational structures that store a vast array of symbolic facts. Consequently, integrating LLMs with knowledge graphs has been extensively explored, with Knowledge Graph Question Answering (KGQA) serving as a critical touchstone for the integration. This task requires LLMs to answer natural language questions by retrieving relevant triples from knowledge graphs. However, existing methods face two significant challenges: *excessively long reasoning paths distracting from the answer generation*, and *false-positive relations hindering the path refinement*. In this paper, we propose an iterative interactive KGQA framework that leverages the interactive learning capabilities of LLMs to perform reasoning and Debating over Graphs (DoG). Specifically, DoG employs a subgraph-focusing mechanism, allowing LLMs to perform answer trying after each reasoning step, thereby mitigating the impact of lengthy reasoning paths. On the other hand, DoG utilizes a multi-role debate team to gradually simplify complex questions, reducing the influence of false-positive relations. This debate mechanism ensures the reliability of the reasoning process. Experimental results on five public datasets demonstrate the effectiveness and superiority of our architecture. Notably, DoG outperforms the state-of-the-art method ToG by 23.7% and 9.1% in accuracy on WebQuestions and GrailQA, respectively. Furthermore, the integration experiments with various LLMs on the mentioned datasets highlight the flexibility of DoG.",
            "corpus_id": "277820355",
            "text": "Large Language Models (LLMs) may suffer from hallucinations in real-world applications due to the lack of relevant knowledge. In contrast, knowledge graphs encompass extensive, multi-relational structures that store a vast array of symbolic facts. Consequently, integrating LLMs with knowledge graphs has been extensively explored, with Knowledge Graph Question Answering (KGQA) serving as a critical touchstone for the integration. This task requires LLMs to answer natural language questions by retrieving relevant triples from knowledge graphs. However, existing methods face two significant challenges: *excessively long reasoning paths distracting from the answer generation*, and *false-positive relations hindering the path refinement*. In this paper, we propose an iterative interactive KGQA framework that leverages the interactive learning capabilities of LLMs to perform reasoning and Debating over Graphs (DoG). Specifically, DoG employs a subgraph-focusing mechanism, allowing LLMs to perform answer trying after each reasoning step, thereby mitigating the impact of lengthy reasoning paths. On the other hand, DoG utilizes a multi-role debate team to gradually simplify complex questions, reducing the influence of false-positive relations. This debate mechanism ensures the reliability of the reasoning process. Experimental results on five public datasets demonstrate the effectiveness and superiority of our architecture. Notably, DoG outperforms the state-of-the-art method ToG by 23.7% and 9.1% in accuracy on WebQuestions and GrailQA, respectively. Furthermore, the integration experiments with various LLMs on the mentioned datasets highlight the flexibility of DoG.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.302734375
        },
        {
            "paperId": "c27937c28a1b08411acdf1cb0f8bc91cc358a056",
            "corpusId": 272423947,
            "title": "Debate on Graph: a Flexible and Reliable Reasoning Framework for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 5,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.03155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319184164",
                    "name": "Jie Ma"
                },
                {
                    "authorId": "2319941137",
                    "name": "Zhitao Gao"
                },
                {
                    "authorId": "51255954",
                    "name": "Qianyi Chai"
                },
                {
                    "authorId": "2297333542",
                    "name": "Wangchun Sun"
                },
                {
                    "authorId": "2269757371",
                    "name": "Pinghui Wang"
                },
                {
                    "authorId": "2272282276",
                    "name": "Hongbin Pei"
                },
                {
                    "authorId": "2269513697",
                    "name": "Jing Tao"
                },
                {
                    "authorId": "2320172297",
                    "name": "Lingyun Song"
                },
                {
                    "authorId": "2265651722",
                    "name": "Jun Liu"
                },
                {
                    "authorId": "2319826987",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "2321151995",
                    "name": "Li-zhen Cui"
                }
            ],
            "abstract": "Large Language Models (LLMs) may suffer from hallucinations in real-world applications due to the lack of relevant knowledge. In contrast, knowledge graphs encompass extensive, multi-relational structures that store a vast array of symbolic facts. Consequently, integrating LLMs with knowledge graphs has been extensively explored, with Knowledge Graph Question Answering (KGQA) serving as a critical touchstone for the integration. This task requires LLMs to answer natural language questions by retrieving relevant triples from knowledge graphs. However, existing methods face two significant challenges: \\textit{excessively long reasoning paths distracting from the answer generation}, and \\textit{false-positive relations hindering the path refinement}. In this paper, we propose an iterative interactive KGQA framework that leverages the interactive learning capabilities of LLMs to perform reasoning and Debating over Graphs (DoG). Specifically, DoG employs a subgraph-focusing mechanism, allowing LLMs to perform answer trying after each reasoning step, thereby mitigating the impact of lengthy reasoning paths. On the other hand, DoG utilizes a multi-role debate team to gradually simplify complex questions, reducing the influence of false-positive relations. This debate mechanism ensures the reliability of the reasoning process. Experimental results on five public datasets demonstrate the effectiveness and superiority of our architecture. Notably, DoG outperforms the state-of-the-art method ToG by 23.7\\% and 9.1\\% in accuracy on WebQuestions and GrailQA, respectively. Furthermore, the integration experiments with various LLMs on the mentioned datasets highlight the flexibility of DoG. Code is available at \\url{https://github.com/reml-group/DoG}.",
            "corpus_id": "272423947",
            "text": "Large Language Models (LLMs) may suffer from hallucinations in real-world applications due to the lack of relevant knowledge. In contrast, knowledge graphs encompass extensive, multi-relational structures that store a vast array of symbolic facts. Consequently, integrating LLMs with knowledge graphs has been extensively explored, with Knowledge Graph Question Answering (KGQA) serving as a critical touchstone for the integration. This task requires LLMs to answer natural language questions by retrieving relevant triples from knowledge graphs. However, existing methods face two significant challenges: \\textit{excessively long reasoning paths distracting from the answer generation}, and \\textit{false-positive relations hindering the path refinement}. In this paper, we propose an iterative interactive KGQA framework that leverages the interactive learning capabilities of LLMs to perform reasoning and Debating over Graphs (DoG). Specifically, DoG employs a subgraph-focusing mechanism, allowing LLMs to perform answer trying after each reasoning step, thereby mitigating the impact of lengthy reasoning paths. On the other hand, DoG utilizes a multi-role debate team to gradually simplify complex questions, reducing the influence of false-positive relations. This debate mechanism ensures the reliability of the reasoning process. Experimental results on five public datasets demonstrate the effectiveness and superiority of our architecture. Notably, DoG outperforms the state-of-the-art method ToG by 23.7\\% and 9.1\\% in accuracy on WebQuestions and GrailQA, respectively. Furthermore, the integration experiments with various LLMs on the mentioned datasets highlight the flexibility of DoG. Code is available at \\url{https://github.com/reml-group/DoG}.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.26318359375
        },
        {
            "paperId": "1e426bd98107219d8e5107bfcbe385886d486d34",
            "corpusId": 277103848,
            "title": "DAgent: A Relational Database-Driven Data Analysis Report Generation Agent",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 44,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.13269, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292411692",
                    "name": "Wenyi Xu"
                },
                {
                    "authorId": "2293238106",
                    "name": "Yuren Mao"
                },
                {
                    "authorId": "2350860185",
                    "name": "Xiaolu Zhang"
                },
                {
                    "authorId": "2280285505",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "2223421628",
                    "name": "Xuemei Dong"
                },
                {
                    "authorId": "2350868043",
                    "name": "Mengfei Zhang"
                },
                {
                    "authorId": "2292513358",
                    "name": "Yunjun Gao"
                }
            ],
            "abstract": "Relational database-driven data analysis (RDB-DA) report generation, which aims to generate data analysis reports after querying relational databases, has been widely applied in fields such as finance and healthcare. Typically, these tasks are manually completed by data scientists, making the process very labor-intensive and showing a clear need for automation. Although existing methods (e.g., Table QA or Text-to-SQL) have been proposed to reduce human dependency, they cannot handle complex analytical tasks that require multi-step reasoning, cross-table associations, and synthesizing insights into reports. Moreover, there is no dataset available for developing automatic RDB-DA report generation. To fill this gap, this paper proposes an LLM agent system for RDB-DA report generation tasks, dubbed DAgent; moreover, we construct a benchmark for automatic data analysis report generation, which includes a new dataset DA-Dataset and evaluation metrics. DAgent integrates planning, tools, and memory modules to decompose natural language questions into logically independent sub-queries, accurately retrieve key information from relational databases, and generate analytical reports that meet the requirements of completeness, correctness, and conciseness through multi-step reasoning and effective data integration. Experimental analysis on the DA-Dataset demonstrates that DAgent's superiority in retrieval performance and analysis report generation quality, showcasing its strong potential for tackling complex database analysis report generation tasks.",
            "corpus_id": "277103848",
            "text": "Relational database-driven data analysis (RDB-DA) report generation, which aims to generate data analysis reports after querying relational databases, has been widely applied in fields such as finance and healthcare. Typically, these tasks are manually completed by data scientists, making the process very labor-intensive and showing a clear need for automation. Although existing methods (e.g., Table QA or Text-to-SQL) have been proposed to reduce human dependency, they cannot handle complex analytical tasks that require multi-step reasoning, cross-table associations, and synthesizing insights into reports. Moreover, there is no dataset available for developing automatic RDB-DA report generation. To fill this gap, this paper proposes an LLM agent system for RDB-DA report generation tasks, dubbed DAgent; moreover, we construct a benchmark for automatic data analysis report generation, which includes a new dataset DA-Dataset and evaluation metrics. DAgent integrates planning, tools, and memory modules to decompose natural language questions into logically independent sub-queries, accurately retrieve key information from relational databases, and generate analytical reports that meet the requirements of completeness, correctness, and conciseness through multi-step reasoning and effective data integration. Experimental analysis on the DA-Dataset demonstrates that DAgent's superiority in retrieval performance and analysis report generation quality, showcasing its strong potential for tackling complex database analysis report generation tasks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.451904296875
        },
        {
            "paperId": "c369e52b6fb990918929b198583aed2a29483638",
            "corpusId": 279305725,
            "title": "PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering",
            "venue": "",
            "year": 2025,
            "referenceCount": 41,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.09414, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2366433044",
                    "name": "Xiujun Zhou"
                },
                {
                    "authorId": "2197970055",
                    "name": "Pingjian Zhang"
                },
                {
                    "authorId": "2366668933",
                    "name": "Deyou Tang"
                }
            ],
            "abstract": "Knowledge Graph Question Answering (KGQA) is a crucial task in natural language processing that requires reasoning over knowledge graphs (KGs) to answer natural language questions. Recent methods utilizing large language models (LLMs) have shown remarkable semantic parsing capabilities but are limited by the scarcity of diverse annotated data and multi-hop reasoning samples. Traditional data augmentation approaches are focus mainly on single-hop questions and prone to semantic distortion, while LLM-based methods primarily address semantic distortion but usually neglect multi-hop reasoning, thus limiting data diversity. The scarcity of multi-hop samples further weakens models' generalization. To address these issues, we propose PGDA-KGQA, a prompt-guided generative framework with multiple data augmentation strategies for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by crafting meticulously engineered prompts that integrate the provided textual content, it leverages LLMs to generate large-scale (question, logical form) pairs for model training. Specifically, PGDA-KGQA enriches its training set by: (1) generating single-hop pseudo questions to improve the alignment of question semantics with KG relations; (2) applying semantic-preserving question rewriting to improve robustness against linguistic variations; (3) employing answer-guided reverse path exploration to create realistic multi-hop questions. By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA utilizes the augmented data to enhance the accuracy of logical form generation and thus improve answer retrieval performance. Experiments demonstrate that outperforms state-of-the-art methods on standard KGQA datasets, achieving improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by 1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.",
            "corpus_id": "279305725",
            "text": "Knowledge Graph Question Answering (KGQA) is a crucial task in natural language processing that requires reasoning over knowledge graphs (KGs) to answer natural language questions. Recent methods utilizing large language models (LLMs) have shown remarkable semantic parsing capabilities but are limited by the scarcity of diverse annotated data and multi-hop reasoning samples. Traditional data augmentation approaches are focus mainly on single-hop questions and prone to semantic distortion, while LLM-based methods primarily address semantic distortion but usually neglect multi-hop reasoning, thus limiting data diversity. The scarcity of multi-hop samples further weakens models' generalization. To address these issues, we propose PGDA-KGQA, a prompt-guided generative framework with multiple data augmentation strategies for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by crafting meticulously engineered prompts that integrate the provided textual content, it leverages LLMs to generate large-scale (question, logical form) pairs for model training. Specifically, PGDA-KGQA enriches its training set by: (1) generating single-hop pseudo questions to improve the alignment of question semantics with KG relations; (2) applying semantic-preserving question rewriting to improve robustness against linguistic variations; (3) employing answer-guided reverse path exploration to create realistic multi-hop questions. By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA utilizes the augmented data to enhance the accuracy of logical form generation and thus improve answer retrieval performance. Experiments demonstrate that outperforms state-of-the-art methods on standard KGQA datasets, achieving improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by 1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.89501953125
        },
        {
            "paperId": "a23bef1d8ba784d9ed222bab8cb725409d04b589",
            "corpusId": 275820289,
            "title": "ReasVQA: Advancing VideoQA with Imperfect Reasoning Process",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2025,
            "referenceCount": 55,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.13536, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2203813088",
                    "name": "Jianxin Liang"
                },
                {
                    "authorId": "2332816459",
                    "name": "Xiaojun Meng"
                },
                {
                    "authorId": "2302812779",
                    "name": "Huishuai Zhang"
                },
                {
                    "authorId": "2162636146",
                    "name": "Yueqian Wang"
                },
                {
                    "authorId": "2332882148",
                    "name": "Jiansheng Wei"
                },
                {
                    "authorId": "2258097332",
                    "name": "Dongyan Zhao"
                }
            ],
            "abstract": "Video Question Answering (VideoQA) is a challenging task that requires understanding complex visual and temporal relationships within videos to answer questions accurately. In this work, we introduce \\textbf{ReasVQA} (Reasoning-enhanced Video Question Answering), a novel approach that leverages reasoning processes generated by Multimodal Large Language Models (MLLMs) to improve the performance of VideoQA models. Our approach consists of three phases: reasoning generation, reasoning refinement, and learning from reasoning. First, we generate detailed reasoning processes using additional MLLMs, and second refine them via a filtering step to ensure data quality. Finally, we use the reasoning data, which might be in an imperfect form, to guide the VideoQA model via multi-task learning, on how to interpret and answer questions based on a given video. We evaluate ReasVQA on three popular benchmarks, and our results establish new state-of-the-art performance with significant improvements of +2.9 on NExT-QA, +7.3 on STAR, and +5.9 on IntentQA. Our findings demonstrate the supervising benefits of integrating reasoning processes into VideoQA. Further studies validate each component of our method, also with different backbones and MLLMs, and again highlight the advantages of this simple but effective method. We offer a new perspective on enhancing VideoQA performance by utilizing advanced reasoning techniques, setting a new benchmark in this research field.",
            "corpus_id": "275820289",
            "text": "Video Question Answering (VideoQA) is a challenging task that requires understanding complex visual and temporal relationships within videos to answer questions accurately. In this work, we introduce \\textbf{ReasVQA} (Reasoning-enhanced Video Question Answering), a novel approach that leverages reasoning processes generated by Multimodal Large Language Models (MLLMs) to improve the performance of VideoQA models. Our approach consists of three phases: reasoning generation, reasoning refinement, and learning from reasoning. First, we generate detailed reasoning processes using additional MLLMs, and second refine them via a filtering step to ensure data quality. Finally, we use the reasoning data, which might be in an imperfect form, to guide the VideoQA model via multi-task learning, on how to interpret and answer questions based on a given video. We evaluate ReasVQA on three popular benchmarks, and our results establish new state-of-the-art performance with significant improvements of +2.9 on NExT-QA, +7.3 on STAR, and +5.9 on IntentQA. Our findings demonstrate the supervising benefits of integrating reasoning processes into VideoQA. Further studies validate each component of our method, also with different backbones and MLLMs, and again highlight the advantages of this simple but effective method. We offer a new perspective on enhancing VideoQA performance by utilizing advanced reasoning techniques, setting a new benchmark in this research field.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2158203125
        }
    ],
    "quotes": {
        "cost": 0.23986500000000002,
        "quotes": [
            {
                "idx": 0,
                "key": "[10817864 | Wang et al. | 2016 | Citations: 58]",
                "snippets": "Synthetic datasets have also arisen recently for semantic tasks in natural language processing. bAbI is a dataset of facts, questions, and answers, generated by random simulation, for training machines to do simple logic (Weston et al., 2015)(Hermann et al., 2015) generate reading comprehension questions and their answers, based on a large set of newssummarization pairs, for training machine readers. (Serban et al., 2016) used RNNs to generate 30 million factoid questions about Freebase, with answers, for training question-answering systems. (Wang et al., 2015) obtain data to train semantic parsers in a new domain by first generating synthetic (utterance, logical form) pairs and then asking human annotators to paraphrase the synthetic utterances into more natural human language.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[12241221 | Serban et al. | 2016 | Citations: 288]": "Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale question-answer corpora available. In this paper we present the 30M Factoid Question-Answer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the question-generation model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear comparable in quality to real human-generated questions.",
                    "[14472576 | Wang et al. | 2015 | Citations: 364]": "How do we build a semantic parser in a new domain starting with zero training examples? We introduce a new methodology for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canonical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours.",
                    "[3178759 | Weston et al. | 2015 | Citations: 1182]": "One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.",
                    "[6203757 | Hermann et al. | 2015 | Citations: 3553]": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 825,
                        "end": 1613,
                        "sentence_offsets": [
                            {
                                "start": 825,
                                "end": 920
                            },
                            {
                                "start": 921,
                                "end": 1229
                            },
                            {
                                "start": 1230,
                                "end": 1372
                            },
                            {
                                "start": 1373,
                                "end": 1613
                            }
                        ],
                        "ref_mentions": [
                            "3178759",
                            "6203757",
                            "12241221",
                            "14472576"
                        ],
                        "quote": "Synthetic datasets have also arisen recently for semantic tasks in natural language processing. bAbI is a dataset of facts, questions, and answers, generated by random simulation, for training machines to do simple logic (Weston et al., 2015)(Hermann et al., 2015) generate reading comprehension questions and their answers, based on a large set of newssummarization pairs, for training machine readers. (Serban et al., 2016) used RNNs to generate 30 million factoid questions about Freebase, with answers, for training question-answering systems. (Wang et al., 2015) obtain data to train semantic parsers in a new domain by first generating synthetic (utterance, logical form) pairs and then asking human annotators to paraphrase the synthetic utterances into more natural human language."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[222090330 | Eisenschlos et al. | 2020 | Citations: 121]",
                "snippets": "Synthetic data generation Synthetic data has been used to improve learning in NLP tasks (Alberti et al., 2019)Lewis et al., 2019;(Wu et al., 2016)Leonandya et al., 2019). In semantic parsing for example (Wang et al., 2015)Iyer et al., 2017;(Weir et al., 2020), templates are used to bootstrap models that map text to logical forms or SQL. Salvatore et al. (2019) use synthetic data generated from logical forms to evaluate the performance of textual entailment models (e.g., BERT). (Geiger et al., 2019) use synthetic data to create fair evaluation sets for natural language inference. (Geva et al., 2020) show the importance of injecting numerical reasoning via generated data into the model to solve reading comprehension tasks. They propose different templates for generating synthetic numerical examples.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[14472576 | Wang et al. | 2015 | Citations: 364]": "How do we build a semantic parser in a new domain starting with zero training examples? We introduce a new methodology for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canonical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours.",
                    "[189762081 | Alberti et al. | 2019 | Citations: 249]": "We introduce a novel method of generating synthetic question answering corpora by combining models of question generation and answer extraction, and by filtering the results to ensure roundtrip consistency. By pretraining on the resulting corpora we obtain significant improvements on SQuAD2 and NQ, establishing a new state-of-the-art on the latter. Our synthetic data generation models, for both question generation and answer extraction, can be fully reproduced by finetuning a publicly available BERT model on the extractive subsets of SQuAD2 and NQ. We also describe a more powerful variant that does full sequence-to-sequence pretraining for question generation, obtaining exact match and F1 at less than 0.1% and 0.4% from human performance on SQuAD2.",
                    "[202763794 | Geiger et al. | 2019 | Citations: 48]": "Deep learning models for semantics are generally evaluated using naturalistic corpora. Adversarial testing methods, in which models are evaluated on new examples with known semantic properties, have begun to reveal that good performance at these naturalistic tasks can hide serious shortcomings. However, we should insist that these evaluations be fair \u2013 that the models are given data sufficient to support the requisite kinds of generalization. In this paper, we define and motivate a formal notion of fairness in this sense. We then apply these ideas to natural language inference by constructing very challenging but provably fair artificial datasets and showing that standard neural models fail to generalize in the required ways; only task-specific models that jointly compose the premise and hypothesis are able to achieve high performance, and even these models do not solve the task perfectly.",
                    "[215548225 | Geva et al. | 2020 | Citations: 226]": "Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 \u2013> 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.",
                    "[218981535 | Weir et al. | 2020 | Citations: 41]": "Natural language is a promising alternative interface to DBMSs because it enables non-technical users to formulate complex questions in a more concise manner than SQL. Recently, deep learning has gained traction for translating natural language to SQL, since similar ideas have been successful in the related domain of machine translation. However, the core problem with existing deep learning approaches is that they require an enormous amount of training data in order to provide accurate translations. This training data is extremely expensive to curate, since it generally requires humans to manually annotate natural language examples with the corresponding SQL queries (or vice versa). Based on these observations, we propose DBPal, a new approach that augments existing deep learning techniques in order to improve the performance of models for natural language to SQL translation. More specifically, we present a novel training pipeline that automatically generates synthetic training data in order to (1) improve overall translation accuracy, (2) increase robustness to linguistic variation, and (3) specialize the model for the target database. As we show, our DBPal training pipeline is able to improve both the accuracy and linguistic robustness of state-of-the-art natural language to SQL translation models.",
                    "[3099009 | Wu et al. | 2016 | Citations: 24]": "To alleviate the shortage of labeled data, we propose to use bilingually-constrained synthetic implicit data for implicit discourse relation recognition. These data are extracted from a bilingual sentence-aligned corpus according to the implicit/explicit mismatch be-tween different languages. Incorporating these data via a multi-task neural network model achieves signi\ufb01cant improvements over baselines, on both the English PDTB and Chinese CDTB data sets."
                },
                "metadata": [
                    {
                        "section_title": "Counterfactual data generation",
                        "pdf_hash": "",
                        "start": 771,
                        "end": 1575,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "189762081",
                            "3099009",
                            "14472576",
                            "218981535",
                            "202763794",
                            "215548225"
                        ],
                        "quote": "Synthetic data generation Synthetic data has been used to improve learning in NLP tasks (Alberti et al., 2019)Lewis et al., 2019;(Wu et al., 2016)Leonandya et al., 2019). In semantic parsing for example (Wang et al., 2015)Iyer et al., 2017;(Weir et al., 2020), templates are used to bootstrap models that map text to logical forms or SQL. Salvatore et al. (2019) use synthetic data generated from logical forms to evaluate the performance of textual entailment models (e.g., BERT). (Geiger et al., 2019) use synthetic data to create fair evaluation sets for natural language inference. (Geva et al., 2020) show the importance of injecting numerical reasoning via generated data into the model to solve reading comprehension tasks. They propose different templates for generating synthetic numerical examples."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[235898880 | Yoran et al. | 2021 | Citations: 54]",
                "snippets": "In this work, we propose to leverage semi-structured tables, and automatically generate at scale question-paragraph pairs, where answering the question requires reasoning over multiple facts in the paragraph. We add a pre-training step over this synthetic data, which includes examples that require 16 different reasoning skills such as number comparison, conjunction, and fact composition. To improve data efficiency, we sample examples from reasoning skills where the model currently errs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this work, we propose to leverage semi-structured tables, and automatically generate at scale question-paragraph pairs, where answering the question requires reasoning over multiple facts in the paragraph. We add a pre-training step over this synthetic data, which includes examples that require 16 different reasoning skills such as number comparison, conjunction, and fact composition. To improve data efficiency, we sample examples from reasoning skills where the model currently errs.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[250390686 | Maharana et al. | 2022 | Citations: 96]",
                "snippets": "Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[253107930 | Mekala et al. | 2022 | Citations: 18]",
                "snippets": "In this work, we propose CONDA, an approach to further improve GLM's ability to generate synthetic data by reformulating data generation as context generation for a given question-answer (QA) pair and leveraging QA datasets for training context generators. Then, we cast downstream tasks into the same question answering format and adapt the fine-tuned context generators to the target task domain. Finally, we use the fine-tuned GLM to generate relevant contexts, which are in turn used as synthetic training data for their corresponding tasks...Our analysis reveals that QA datasets that require high-level reasoning abilities (e.g., abstractive and common-sense QA datasets) tend to give the best boost in performance in both few-shot and zero-shot settings.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 139,
                        "end": 683,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In this work, we propose CONDA, an approach to further improve GLM's ability to generate synthetic data by reformulating data generation as context generation for a given question-answer (QA) pair and leveraging QA datasets for training context generators. Then, we cast downstream tasks into the same question answering format and adapt the fine-tuned context generators to the target task domain. Finally, we use the fine-tuned GLM to generate relevant contexts, which are in turn used as synthetic training data for their corresponding tasks"
                    },
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 877,
                        "end": 1092,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Our analysis reveals that QA datasets that require high-level reasoning abilities (e.g., abstractive and common-sense QA datasets) tend to give the best boost in performance in both few-shot and zero-shot settings."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[253370208 | Trivedi et al. | 2022 | Citations: 11]",
                "snippets": "In this work, we show how to use question decompositions to teach language models these broad reasoning skills in a robust fashion. Specifically, we use widely available QDMR representations to programmatically create hard-to-cheat synthetic contexts for real questions in six multi-step reasoning datasets. These contexts are carefully designed to avoid common reasoning shortcuts prevalent in real contexts that prevent models from learning the right skills. This results in a pretraining dataset, named TeaBReaC, containing 525K multi-step questions (with associated formal programs) covering about 900 reasoning patterns.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this work, we show how to use question decompositions to teach language models these broad reasoning skills in a robust fashion. Specifically, we use widely available QDMR representations to programmatically create hard-to-cheat synthetic contexts for real questions in six multi-step reasoning datasets. These contexts are carefully designed to avoid common reasoning shortcuts prevalent in real contexts that prevent models from learning the right skills. This results in a pretraining dataset, named TeaBReaC, containing 525K multi-step questions (with associated formal programs) covering about 900 reasoning patterns.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[253384598 | Sadat et al. | 2022 | Citations: 2]",
                "snippets": "With the advancement of text generation models in recent years (Radford et al., 2019)Brown et al., 2020;(Lewis et al., 2019), researchers have started to adopt synthetic text generation as a data augmentation strategy for various tasks such as commonsense reasoning (Maharana et al., 2022), sentence classification (Anaby-Tavor et al., 2020) and question answering (Puri et al., 2020). More recently, synthetic text generation methods have also been employed to remove spurious correlations in human annotated data for fact verification (Lee et al., 2021) and NLI (Wu et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[250390686 | Maharana et al. | 2022 | Citations: 96]": "Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions. Our approach improves performance for SocialIQA, CODAH, HellaSwag and CommonsenseQA, and works well for generative tasks like ProtoQA. We show improvement in robustness to semantic adversaries after training with GraDA and provide human evaluation of the quality of synthetic datasets in terms of factuality and answerability. Our work provides evidence and encourages future research into graph-based generative data augmentation.",
                    "[204960716 | Lewis et al. | 2019 | Citations: 10856]": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
                    "[211258652 | Puri et al. | 2020 | Citations: 159]": "Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the SQuAD1.1 question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the SQuAD1.1 training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic corpus generated by an 8.3 billion parameter GPT-2 model. With no access to human supervision and only access to other models, we are able to train state of the art question answering networks on entirely model-generated data that achieve 88.4 Exact Match (EM) and 93.9 F1 score on the SQuAD1.1 dev set. We further apply our methodology to SQuAD2.0 and show a 2.8 absolute gain on EM score compared to prior work using synthetic data.",
                    "[238227205 | Lee et al. | 2021 | Citations: 30]": "Fact verification datasets are typically constructed using crowdsourcing techniques due to the lack of text sources with veracity labels. However, the crowdsourcing process often produces undesired biases in data that cause models to learn spurious patterns. In this paper, we propose CrossAug, a contrastive data augmentation method for debiasing fact verification models. Specifically, we employ a two-stage augmentation pipeline to generate new claims and evidences from existing samples. The generated samples are then paired cross-wise with the original pair, forming contrastive samples that facilitate the model to rely less on spurious patterns and learn more robust representations. Experimental results show that our method outperforms the previous state-of-the-art debiasing technique by 3.6% on the debiased extension of the FEVER dataset, with a total performance boost of 10.13% from the baseline. Furthermore, we evaluate our approach in data-scarce settings, where models can be more susceptible to biases due to the lack of training data. Experimental results demonstrate that our approach is also effective at debiasing in these low-resource conditions, exceeding the baseline performance on the Symmetric dataset with just 1% of the original data.",
                    "[247628095 | Wu et al. | 2022 | Citations: 68]": "Natural language processing models often exploit spurious correlations between task-independent features and labels in datasets to perform well only within the distributions they are trained on, while not generalising to different task distributions. We propose to tackle this problem by generating a debiased version of a dataset, which can then be used to train a debiased, off-the-shelf model, by simply replacing its training data. Our approach consists of 1) a method for training data generators to generate high-quality, label-consistent data samples; and 2) a filtering mechanism for removing data points that contribute to spurious correlations, measured in terms of z-statistics. We generate debiased versions of the SNLI and MNLI datasets, and we evaluate on a large suite of debiased, out-of-distribution, and adversarial test sets. Results show that models trained on our debiased datasets generalise better than those trained on the original datasets in all settings. On the majority of the datasets, our method outperforms or performs comparably to previous state-of-the-art debiasing strategies, and when combined with an orthogonal technique, product-of-experts, it improves further and outperforms previous best results of SNLI-hard and MNLI-hard."
                },
                "metadata": [
                    {
                        "section_title": "Synthetic Data Generation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 577,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "160025533",
                            "204960716",
                            "250390686",
                            "211258652",
                            "238227205",
                            "247628095"
                        ],
                        "quote": "With the advancement of text generation models in recent years (Radford et al., 2019)Brown et al., 2020;(Lewis et al., 2019), researchers have started to adopt synthetic text generation as a data augmentation strategy for various tasks such as commonsense reasoning (Maharana et al., 2022), sentence classification (Anaby-Tavor et al., 2020) and question answering (Puri et al., 2020). More recently, synthetic text generation methods have also been employed to remove spurious correlations in human annotated data for fact verification (Lee et al., 2021) and NLI (Wu et al., 2022)."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[254408974 | Dua et al. | 2022 | Citations: 118]",
                "snippets": "To mitigate these data issues, we propose a way to synthetically generate complex questions and their decompositions using semi-structured data which is easy to parse. We show that we can bootstrap model learning with this out-of-domain, synthetically generated data so it can adapt better when fine-tuned with limited in-domain supervision.\n\nGeneration Process: Inspired by Yoran et al. (2021), we use semi-structured data from tables in English Wikipedia which are available in plenty. We employ curated templates to convert the rows in the tables into paragraphs. We use single column headers to create first order simple questions and a combination of columns for higher order complex questions. We synthesize data for 10 simple operations: COUNT, TOP(k), BOTTOM(k), FILTER, SUM, COMPARISON, DIFFERENCE, NEGATION, GATHER, and INTERSECTION.\n\nWe generate higher order combinations of firstorder operations, wherever possible.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Synthetic Dataset",
                        "pdf_hash": "",
                        "start": 389,
                        "end": 1316,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "To mitigate these data issues, we propose a way to synthetically generate complex questions and their decompositions using semi-structured data which is easy to parse. We show that we can bootstrap model learning with this out-of-domain, synthetically generated data so it can adapt better when fine-tuned with limited in-domain supervision.\n\nGeneration Process: Inspired by Yoran et al. (2021), we use semi-structured data from tables in English Wikipedia which are available in plenty. We employ curated templates to convert the rows in the tables into paragraphs. We use single column headers to create first order simple questions and a combination of columns for higher order complex questions. We synthesize data for 10 simple operations: COUNT, TOP(k), BOTTOM(k), FILTER, SUM, COMPARISON, DIFFERENCE, NEGATION, GATHER, and INTERSECTION.\n\nWe generate higher order combinations of firstorder operations, wherever possible."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[254877131 | Li et al. | 2022 | Citations: 6]",
                "snippets": "To address the insufficient annotation challenge, we present a self-training framework for unsupervised complex tabular reasoning (UCTR-ST) by generating diverse synthetic data with complex logic. Specifically, UCTR-ST incorporates several essential techniques: we aggregate diverse programs and execute them on tables based on a \"Program-Management\" component, and we bridge the gap between programs and text with a powerful \"Program-Transformation\" module that generates natural language sentences with complex logic.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "To address the insufficient annotation challenge, we present a self-training framework for unsupervised complex tabular reasoning (UCTR-ST) by generating diverse synthetic data with complex logic. Specifically, UCTR-ST incorporates several essential techniques: we aggregate diverse programs and execute them on tables based on a \"Program-Management\" component, and we bridge the gap between programs and text with a powerful \"Program-Transformation\" module that generates natural language sentences with complex logic.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[258461502 | Rose et al. | 2023 | Citations: 47]",
                "snippets": "We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evaluation that VCoT offers novel and consistent synthetic data augmentation beating chain-of-thought baselines, which can be used to enhance downstream performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 888,
                        "end": 1159,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evaluation that VCoT offers novel and consistent synthetic data augmentation beating chain-of-thought baselines, which can be used to enhance downstream performance."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[258558899 | Niu et al. | 2023 | Citations: 6]",
                "snippets": "Data synthesis plays an important role in semantic parsing, especially in dataset construction and model training. Many datasets are constructed by first generating synthetic data by rules and then manually rewriting the questions for diverse expressions (Wang et al., 2015)(Cao et al., 2020)(Gu et al., 2020). Since the generation of synthetic data is controllable, researchers can restrict the distribution of logical forms in order to examine or improve various generalization capabilities of models, such as compositional generalization and scalability (Lake and (Lake et al., 2017)(Gu et al., 2020)(Oren et al., 2021). In many scenarios, where manually rewriting is laborious or unavailable, training on synthetic data can also alleviate the problems of cold-start and privacy (Yang et al., 2022)(Gu et al., 2020). Limited by stereotyped expressions, semantic parsers trained with only synthetic data are difficult to generalize to diverse natural questions. (Wu et al., 2021) and (Xu et al., 2020) automatically rewrite synthetic questions with paraphrasing models, which introduces various phrase-level expressions but change little in sentence structures. Instead of rule-based synthesis, (Zhong et al., 2020) and Yang et al. (2021) train a neural data synthesizer with natural data from other domains, and then generate training data for the target domain in a way of domain transfer.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[14472576 | Wang et al. | 2015 | Citations: 364]": "How do we build a semantic parser in a new domain starting with zero training examples? We introduce a new methodology for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canonical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours.",
                    "[221739233 | Zhong et al. | 2020 | Citations: 100]": "We propose Grounded Adaptation for Zero-shot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas). GAZP combines a forward semantic parser with a backward utterance generator to synthesize data (e.g. utterances and SQL queries) in the new environment, then selects cycle-consistent examples to adapt the parser. Unlike data-augmentation, which typically synthesizes unverified examples in the training environment, GAZP synthesizes examples in the new environment whose input-output consistency are verified. On the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks, GAZP improves logical form and execution accuracy of the baseline parser. Our analyses show that GAZP outperforms data-augmentation in the training environment, performance increases with the amount of GAZP-synthesized data, and cycle-consistency is central to successful adaptation.",
                    "[221819674 | Xu et al. | 2020 | Citations: 52]": "We propose AutoQA, a methodology and toolkit to generate semantic parsers that answer questions on databases, with no manual effort. Given a database schema and its data, AutoQA automatically generates a large set of high-quality questions for training that covers different database operations. It uses automatic paraphrasing combined with template-based parsing to find alternative expressions of an attribute in different parts of speech. It also uses a novel filtered auto-paraphraser to generate correct paraphrases of entire sentences. We apply AutoQA to the Schema2QA dataset and obtain an average logical form accuracy of 62.9% when tested on natural questions, which is only 6.4% lower than a model trained with expert natural language annotations and paraphrase data collected from crowdworkers. To demonstrate the generality of AutoQA, we also apply it to the Overnight dataset. AutoQA achieves 69.8% answer accuracy, 16.4% higher than the state-of-the-art zero-shot models and only 5.2% lower than the same model trained with human data.",
                    "[226965153 | Gu et al. | 2020 | Citations: 215]": "Existing studies on question answering on knowledge bases (KBQA) mainly operate with the standard i.i.d. assumption, i.e., training distribution over questions is the same as the test distribution. However, i.i.d. may be neither achievable nor desirable on large-scale KBs because 1) true user distribution is hard to capture and 2) randomly sampling training examples from the enormous space would be data-inefficient. Instead, we suggest that KBQA models should have three levels of built-in generalization: i.i.d., compositional, and zero-shot. To facilitate the development of KBQA models with stronger generalization, we construct and release a new large-scale, high-quality dataset with 64,331 questions, GrailQA, and provide evaluation settings for all three levels of generalization. In addition, we propose a novel BERT-based KBQA model. The combination of our dataset and model enables us to thoroughly examine and demonstrate, for the first time, the key role of pre-trained contextual embeddings like BERT in the generalization of KBQA.1",
                    "[235417352 | Wu et al. | 2021 | Citations: 24]": "Semantic parsing is challenging due to the structure gap and the semantic gap between utterances and logical forms. In this paper, we propose an unsupervised semantic parsing method - Synchronous Semantic Decoding (SSD), which can simultaneously resolve the semantic gap and the structure gap by jointly leveraging paraphrasing and grammar-constrained decoding. Specifically, we reformulate semantic parsing as a constrained paraphrasing problem: given an utterance, our model synchronously generates its canonical utterancel and meaning representation. During synchronously decoding: the utterance paraphrasing is constrained by the structure of the logical form, therefore the canonical utterance can be paraphrased controlledly; the semantic decoding is guided by the semantics of the canonical utterance, therefore its logical form can be generated unsupervisedly. Experimental results show that SSD is a promising approach and can achieve state-of-the-art unsupervised semantic parsing performance on multiple datasets.",
                    "[237420426 | Oren et al. | 2021 | Citations: 31]": "Modern semantic parsers suffer from two principal limitations. First, training requires expensive collection of utterance-program pairs. Second, semantic parsers fail to generalize at test time to new compositions/structures that have not been observed during training. Recent research has shown that automatic generation of synthetic utterance-program pairs can alleviate the first problem, but its potential for the second has thus far been under-explored. In this work, we investigate automatic generation of synthetic utterance-program pairs for improving compositional generalization in semantic parsing. Given a small training set of annotated examples and an \u201cinfinite\u201d pool of synthetic examples, we select a subset of synthetic examples that are structurally-diverse and use them to improve compositional generalization. We evaluate our approach on a new split of the schema2QA dataset, and show that it leads to dramatic improvements in compositional generalization as well as moderate improvements in the traditional i.i.d setup. Moreover, structurally-diverse sampling achieves these improvements with as few as 5K examples, compared to 1M examples when sampling uniformly at random \u2013 a 200x improvement in data efficiency.",
                    "[247362971 | Cao et al. | 2020 | Citations: 102]": "Complex question answering over knowledge base (Complex KBQA) is challenging because it requires various compositional reasoning capabilities, such as multi-hop inference, attribute comparison, set operation, etc. Existing benchmarks have some shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are poor in diversity or scale. To this end, we introduce KQA Pro, a dataset for Complex KBQA including around 120K diverse natural language questions. We introduce a compositional and interpretable programming language KoPL to represent the reasoning process of complex questions. For each question, we provide the corresponding KoPL program and SPARQL query, so that KQA Pro can serve for both KBQA and semantic parsing tasks. Experimental results show that state-of-the-art KBQA methods cannot achieve promising results on KQA Pro as on current datasets, which suggests that KQA Pro is challenging and Complex KBQA requires further research efforts. We also treat KQA Pro as a diagnostic dataset for testing multiple reasoning skills, conduct a thorough evaluation of existing models and discuss further directions for Complex KBQA. Our codes and datasets can be obtained from https://github.com/shijx12/KQAPro_Baselines.",
                    "[248779994 | Yang et al. | 2022 | Citations: 10]": "We introduce a novel setup for low-resource task-oriented semantic parsing which incorporates several constraints that may arise in real-world scenarios: (1) lack of similar datasets/models from a related domain, (2) inability to sample useful logical forms directly from a grammar, and (3) privacy requirements for unlabeled natural utterances. Our goal is to improve a low-resource semantic parser using utterances collected through user interactions. In this highly challenging but realistic setting, we investigate data augmentation approaches involving generating a set of structured canonical utterances corresponding to logical forms, before simulating corresponding natural language and filtering the resulting pairs. We find that such approaches are effective despite our restrictive setup: in a low-resource setting on the complex SMCalFlow calendaring dataset (Andreas et al. 2020), we observe 33% relative improvement over a non-data-augmented baseline in top-1 match."
                },
                "metadata": [
                    {
                        "section_title": "Data Synthesis for Semantic Parsing",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1382,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 114
                            },
                            {
                                "start": 115,
                                "end": 308
                            },
                            {
                                "start": 309,
                                "end": 613
                            },
                            {
                                "start": 614,
                                "end": 809
                            },
                            {
                                "start": 810,
                                "end": 953
                            },
                            {
                                "start": 954,
                                "end": 1153
                            },
                            {
                                "start": 1154,
                                "end": 1382
                            }
                        ],
                        "ref_mentions": [
                            "14472576",
                            "247362971",
                            "226965153",
                            "46761158",
                            "226965153",
                            "237420426",
                            "248779994",
                            "226965153",
                            "235417352",
                            "221819674",
                            "221739233"
                        ],
                        "quote": "Data synthesis plays an important role in semantic parsing, especially in dataset construction and model training. Many datasets are constructed by first generating synthetic data by rules and then manually rewriting the questions for diverse expressions (Wang et al., 2015)(Cao et al., 2020)(Gu et al., 2020). Since the generation of synthetic data is controllable, researchers can restrict the distribution of logical forms in order to examine or improve various generalization capabilities of models, such as compositional generalization and scalability (Lake and (Lake et al., 2017)(Gu et al., 2020)(Oren et al., 2021). In many scenarios, where manually rewriting is laborious or unavailable, training on synthetic data can also alleviate the problems of cold-start and privacy (Yang et al., 2022)(Gu et al., 2020). Limited by stereotyped expressions, semantic parsers trained with only synthetic data are difficult to generalize to diverse natural questions. (Wu et al., 2021) and (Xu et al., 2020) automatically rewrite synthetic questions with paraphrasing models, which introduces various phrase-level expressions but change little in sentence structures. Instead of rule-based synthesis, (Zhong et al., 2020) and Yang et al. (2021) train a neural data synthesizer with natural data from other domains, and then generate training data for the target domain in a way of domain transfer."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[258833579 | Bao et al. | 2023 | Citations: 4]",
                "snippets": "In this paper, we performed contrastive learning for discriminative large language models on sentences augmented from a synthetic dataset. This dataset contains 14,962 sentences with different combinations of 23 entities, 2 relations and 40 attributes. Synthetic data was used to generate more controllable logical sentences.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Datasets",
                        "pdf_hash": "",
                        "start": 801,
                        "end": 1126,
                        "sentence_offsets": [
                            {
                                "start": 761,
                                "end": 939
                            },
                            {
                                "start": 940,
                                "end": 1053
                            },
                            {
                                "start": 1054,
                                "end": 1126
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this paper, we performed contrastive learning for discriminative large language models on sentences augmented from a synthetic dataset. This dataset contains 14,962 sentences with different combinations of 23 entities, 2 relations and 40 attributes. Synthetic data was used to generate more controllable logical sentences."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[260846474 | Morishita et al. | 2023 | Citations: 25]",
                "snippets": "RuleTaker (Clark et al., 2021) proposed a deduction corpus composed of synthetically generated multistep deductive proofs written in natural languages. Each deductive proof (dis-)proves a hypothesis by applying deduction rules multiple times to a given set of facts. As the deduction rules, rules of the implication kind were used.\n\nArtificial Argument Corpus (Betz et al., 2021) is another corpus composed of synthetically generated single-step deductive proofs. As the deduction rules, hand-selected rules useful for critical thinking were used.\n\nGontier et al. (2020) investigated the deductive reasoning ability of LMs on a corpus, which is composed of a specific type of multistep inference: kinship relationship on synthetic kinship graphs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "RuleTaker (Clark et al., 2021) proposed a deduction corpus composed of synthetically generated multistep deductive proofs written in natural languages. Each deductive proof (dis-)proves a hypothesis by applying deduction rules multiple times to a given set of facts. As the deduction rules, rules of the implication kind were used.\n\nArtificial Argument Corpus (Betz et al., 2021) is another corpus composed of synthetically generated single-step deductive proofs. As the deduction rules, hand-selected rules useful for critical thinking were used.\n\nGontier et al. (2020) investigated the deductive reasoning ability of LMs on a corpus, which is composed of a specific type of multistep inference: kinship relationship on synthetic kinship graphs.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[263909600 | Gholami et al. | 2023 | Citations: 15]",
                "snippets": "One challenge persistently confronting researchers is the scarcity of high-quality, annotated datasets that drive these models. This paper explores the nuances of synthetic data generation in NLP, with a focal point on template-based question generation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "One challenge persistently confronting researchers is the scarcity of high-quality, annotated datasets that drive these models. This paper explores the nuances of synthetic data generation in NLP, with a focal point on template-based question generation.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[264439655 | Sprague et al. | 2023 | Citations: 78]",
                "snippets": "We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 388,
                        "end": 856,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[265150344 | Koksal et al. | 2023 | Citations: 5]",
                "snippets": "Synthetic Data Generation: Synthetic question answering dataset generation without counterfactuals has shown limited improvement in out-ofdomain generalization (Bartolo et al., 2021)(Lewis et al., 2021). However, recent advancements in large language models (LLMs) have led to growing interest in synthetic data generation with LLMs, such as in more generalized instruction tuning datasets from scratch (Wang et al., 2022) or by restructuring existing corpora (K\u00f6ksal et al., 2023). Synthetic data generation with LLMs has also been applied to existing datasets for specific tasks to improve model quality, such as natural language inference (Liu et al., 2022) and sentiment analysis (Meng et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[231924957 | Lewis et al. | 2021 | Citations: 234]": "Abstract Open-domain Question Answering models that directly leverage question-answer (QA) pairs, such as closed-book QA (CBQA) models and QA-pair retrievers, show promise in terms of speed and memory compared with conventional models which retrieve and read from text corpora. QA-pair retrievers also offer interpretable answers, a high degree of control, and are trivial to update at test time with new knowledge. However, these models fall short of the accuracy of retrieve-and-read systems, as substantially less knowledge is covered by the available QA-pairs relative to text corpora like Wikipedia. To facilitate improved QA-pair models, we introduce Probably Asked Questions (PAQ), a very large resource of 65M automatically generated QA-pairs. We introduce a new QA-pair retriever, RePAQ, to complement PAQ. We find that PAQ preempts and caches test questions, enabling RePAQ to match the accuracy of recent retrieve-and-read models, whilst being significantly faster. Using PAQ, we train CBQA models which outperform comparable baselines by 5%, but trail RePAQ by over 15%, indicating the effectiveness of explicit retrieval. RePAQ can be configured for size (under 500MB) or speed (over 1K questions per second) while retaining high accuracy. Lastly, we demonstrate RePAQ\u2019s strength at selective QA, abstaining from answering when it is likely to be incorrect. This enables RePAQ to \u201cback-off\u201d to a more expensive state-of-the-art model, leading to a combined system which is both more accurate and 2x faster than the state-of-the-art model alone.",
                    "[233296924 | Bartolo et al. | 2021 | Citations: 104]": "Despite recent progress, state-of-the-art question answering models remain vulnerable to a variety of adversarial attacks. While dynamic adversarial data collection, in which a human annotator tries to write examples that fool a model-in-the-loop, can improve model robustness, this process is expensive which limits the scale of the collected data. In this work, we are the first to use synthetic adversarial data generation to make question answering models more robust to human adversaries. We develop a data generation pipeline that selects source passages, identifies candidate answers, generates questions, then finally filters or re-labels them to improve quality. Using this approach, we amplify a smaller human-written adversarial dataset to a much larger set of synthetic question-answer pairs. By incorporating our synthetic data, we improve the state-of-the-art on the AdversarialQA dataset by 3.7F1 and improve model generalisation on nine of the twelve MRQA datasets. We further conduct a novel human-in-the-loop evaluation and show that our models are considerably more robust to new human-written adversarial examples: crowdworkers can fool our model only 8.8% of the time on average, compared to 17.6% for a model trained without synthetic data.",
                    "[246016339 | Liu et al. | 2022 | Citations: 221]": "A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel approach for dataset creation based on worker and AI collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI for natural language inference (NLI), our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers. The resulting dataset, WANLI, consists of 107,885 NLI examples and presents unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI improves performance on eight out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI, compared to training on the 4x larger MultiNLI. Moreover, it continues to be more effective than MultiNLI augmented with other NLI datasets. Our results demonstrate the promise of leveraging natural language generation techniques and re-imagining the role of humans in the dataset creation process.",
                    "[253384628 | Meng et al. | 2022 | Citations: 49]": "Recent studies have revealed the intriguing few-shot learning ability of pretrained language models (PLMs): They can quickly adapt to a new task when fine-tuned on a small amount of labeled data formulated as prompts, without requiring abundant task-specific annotations. Despite their promising performance, most existing few-shot approaches that only learn from the small training set still underperform fully supervised training by nontrivial margins. In this work, we study few-shot learning with PLMs from a different perspective: We first tune an autoregressive PLM on the few-shot samples and then use it as a generator to synthesize a large amount of novel training samples which augment the original training set. To encourage the generator to produce label-discriminative samples, we train it via weighted maximum likelihood where the weight of each token is automatically adjusted based on a discriminative meta-learning objective. A classification PLM can then be fine-tuned on both the few-shot and the synthetic samples with regularization for better generalization and stability. Our approach FewGen achieves an overall better result across seven classification tasks of the GLUE benchmark than existing few-shot learning methods, improving no-augmentation methods by 5+ average points, and outperforming augmentation methods by 3+ average points.",
                    "[254877310 | Wang et al. | 2022 | Citations: 2251]": "Large \u201cinstruction-tuned\u201d language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 116,
                        "end": 819,
                        "sentence_offsets": [
                            {
                                "start": 116,
                                "end": 318
                            },
                            {
                                "start": 319,
                                "end": 597
                            },
                            {
                                "start": 598,
                                "end": 819
                            }
                        ],
                        "ref_mentions": [
                            "233296924",
                            "231924957",
                            "254877310",
                            "246016339",
                            "253384628"
                        ],
                        "quote": "Synthetic Data Generation: Synthetic question answering dataset generation without counterfactuals has shown limited improvement in out-ofdomain generalization (Bartolo et al., 2021)(Lewis et al., 2021). However, recent advancements in large language models (LLMs) have led to growing interest in synthetic data generation with LLMs, such as in more generalized instruction tuning datasets from scratch (Wang et al., 2022) or by restructuring existing corpora (K\u00f6ksal et al., 2023). Synthetic data generation with LLMs has also been applied to existing datasets for specific tasks to improve model quality, such as natural language inference (Liu et al., 2022) and sentiment analysis (Meng et al., 2022)."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[267311830 | Zhou et al. | 2024 | Citations: 27]",
                "snippets": "Liu et al. (2022) introduced a fresh method to create datasets by combining the efforts of human workers and AI. It began with an established dataset called MultiNLI, which is designed to deal with natural language inference. The proposed approach involves employing dataset cartography to automatically identify instances that exhibit complex reasoning patterns. These patterns serve as guidelines for instructing GPT-3 to generate new examples with similar characteristics. The generated examples are then filtered automatically and subsequently reviewed and labeled by human crowd workers.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Generated content-based approaches",
                        "pdf_hash": "",
                        "start": 1704,
                        "end": 2296,
                        "sentence_offsets": [
                            {
                                "start": 1704,
                                "end": 1816
                            },
                            {
                                "start": 1817,
                                "end": 1929
                            },
                            {
                                "start": 1930,
                                "end": 2067
                            },
                            {
                                "start": 2068,
                                "end": 2179
                            },
                            {
                                "start": 2180,
                                "end": 2296
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Liu et al. (2022) introduced a fresh method to create datasets by combining the efforts of human workers and AI. It began with an established dataset called MultiNLI, which is designed to deal with natural language inference. The proposed approach involves employing dataset cartography to automatically identify instances that exhibit complex reasoning patterns. These patterns serve as guidelines for instructing GPT-3 to generate new examples with similar characteristics. The generated examples are then filtered automatically and subsequently reviewed and labeled by human crowd workers."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[267759981 | Li et al. | 2024 | Citations: 63]",
                "snippets": "We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines...With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate diverse instructions with a broad coverage across the entire spectrum of human knowledge and skills. Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "quote": "With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate diverse instructions with a broad coverage across the entire spectrum of human knowledge and skills. Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[268249187 | Ding et al. | 2024 | Citations: 20]",
                "snippets": "Automate-CoT (Shum et al., 2023) proposes a fully automatic pipeline for finding better chainof-thought prompts to solve commonsense reasoning question answering tasks. This pipeline leverages powerful LLMs like GPT-3 to augment reasoning paths, prune incorrect paths, and select optimal combinations of exemplars.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "\u2022 Commonsense Reasoning:",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 314,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 168
                            },
                            {
                                "start": 169,
                                "end": 314
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Automate-CoT (Shum et al., 2023) proposes a fully automatic pipeline for finding better chainof-thought prompts to solve commonsense reasoning question answering tasks. This pipeline leverages powerful LLMs like GPT-3 to augment reasoning paths, prune incorrect paths, and select optimal combinations of exemplars."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[268363344 | Fang et al. | 2024 | Citations: 14]",
                "snippets": "we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 368,
                        "end": 738,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[269042851 | Liu et al. | 2024 | Citations: 96]",
                "snippets": "WizardMath (Luo et al., 2023a) leverages a series of operations to increase the complexity of questions and answers using GPT-3.5, while MetaMath (Yu et al., 2023) bootstraps the questions in MATH and GSM8K by rewriting them in different ways, such as semantic rephrasing, self-verification, and backward reasoning...Wei et al. (2023a) augmented existing natural language datasets by replacing natural language labels with arbitrary symbols, generating over 500k synthetic examples. Using these synthetic data for supervised finetuning significantly improved model performance on unseen in-context learning and algorithmic-reasoning tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Reasoning",
                        "pdf_hash": "",
                        "start": 499,
                        "end": 813,
                        "sentence_offsets": [
                            {
                                "start": 485,
                                "end": 814
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "WizardMath (Luo et al., 2023a) leverages a series of operations to increase the complexity of questions and answers using GPT-3.5, while MetaMath (Yu et al., 2023) bootstraps the questions in MATH and GSM8K by rewriting them in different ways, such as semantic rephrasing, self-verification, and backward reasoning"
                    },
                    {
                        "section_title": "Reasoning",
                        "pdf_hash": "",
                        "start": 1632,
                        "end": 1955,
                        "sentence_offsets": [
                            {
                                "start": 1618,
                                "end": 1797
                            },
                            {
                                "start": 1798,
                                "end": 1954
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Wei et al. (2023a) augmented existing natural language datasets by replacing natural language labels with arbitrary symbols, generating over 500k synthetic examples. Using these synthetic data for supervised finetuning significantly improved model performance on unseen in-context learning and algorithmic-reasoning tasks."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[270870071 | He et al. | 2024 | Citations: 7]",
                "snippets": "To further enhance the accuracy of these intermediate steps, recent studies leverage extensive synthetic datasets, which are either distilled from cutting-edge models (Yu et al., 2023b;Luo et al., 2023) or composed of selfgenerated rationales (Zelikman et al., 2022;Yuan et al., 2023;Ni et al., 2022), to fine-tune the LLMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A Related Work",
                        "pdf_hash": "",
                        "start": 558,
                        "end": 882,
                        "sentence_offsets": [
                            {
                                "start": 558,
                                "end": 882
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To further enhance the accuracy of these intermediate steps, recent studies leverage extensive synthetic datasets, which are either distilled from cutting-edge models (Yu et al., 2023b;Luo et al., 2023) or composed of selfgenerated rationales (Zelikman et al., 2022;Yuan et al., 2023;Ni et al., 2022), to fine-tune the LLMs."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[271270817 | Vatsal et al. | 2024 | Citations: 33]",
                "snippets": "The authors of (Shao et al., 2023) come up with Synthetic prompting using LLMs to generate synthetic examples which are augmented to the existing hand-crafted examples as seen in a conventional few-shot setting.This prompting method involves two steps: (1) the backward step, where the LLM synthesizes a query based on a self-generated reasoning chain; and (2) the forward step, where the LLM generates a reasoning chain for the synthesized query, making the reasoning chain to be more accurate.Finally, to choose the best examples, this work uses an in-cluster complexity and the most complex examples with the longest reasoning chains are used during inference.The results show Synthetic prompting achieving up to 15.6% absolute gains when experimented with different Mathematical Problem Solving, Commonsense Reasoning and Logical Reasoning task datasets.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[256459681 | Shao et al. | 2023 | Citations: 70]": "Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through step-by-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce Synthetic prompting, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process generates a question that match a sampled reasoning chain, so that the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. We evaluate our method on numerical, symbolic, and algorithmic reasoning tasks, and show that it outperforms existing prompting techniques."
                },
                "metadata": [
                    {
                        "section_title": "SYNTHETIC PROMPTING",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 857,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 210
                            },
                            {
                                "start": 210,
                                "end": 494
                            },
                            {
                                "start": 494,
                                "end": 662
                            },
                            {
                                "start": 662,
                                "end": 857
                            }
                        ],
                        "ref_mentions": [
                            "256459681"
                        ],
                        "quote": "The authors of (Shao et al., 2023) come up with Synthetic prompting using LLMs to generate synthetic examples which are augmented to the existing hand-crafted examples as seen in a conventional few-shot setting.This prompting method involves two steps: (1) the backward step, where the LLM synthesizes a query based on a self-generated reasoning chain; and (2) the forward step, where the LLM generates a reasoning chain for the synthesized query, making the reasoning chain to be more accurate.Finally, to choose the best examples, this work uses an in-cluster complexity and the most complex examples with the longest reasoning chains are used during inference.The results show Synthetic prompting achieving up to 15.6% absolute gains when experimented with different Mathematical Problem Solving, Commonsense Reasoning and Logical Reasoning task datasets."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[272600015 | Lupidi et al. | 2024 | Citations: 8]",
                "snippets": "Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[272753147 | Zhou et al. | 2024 | Citations: 4]",
                "snippets": "Despite recent advances in training and prompting strategies for Large Language Models (LLMs), these models continue to face challenges with complex logical reasoning tasks that involve long reasoning chains. In this work, we explore the potential and limitations of using graph-based synthetic reasoning data as training signals to enhance LLMs' reasoning capabilities.\n\nMotivated by the fact that natural language reasoning tasks can be represented as structured data with finite nodes and edges (Jin et al., 2024), and inspired by existing works on constructing reasoning benchmarks (Fatemi et al., 2024;Kazemi et al., 2023;Agrawal et al., 2024), we propose to leverage synthetic graph-based data for task-specific posttraining adaptation to improve the correctness of the generated reasoning questions and labels.\n\nIn this paper, we carefully design a random walk sampling algorithm on graphs and introduce a new prompting strategy that first extracts a reasoning chain and then derives the answer.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Despite recent advances in training and prompting strategies for Large Language Models (LLMs), these models continue to face challenges with complex logical reasoning tasks that involve long reasoning chains. In this work, we explore the potential and limitations of using graph-based synthetic reasoning data as training signals to enhance LLMs' reasoning capabilities.\n\nMotivated by the fact that natural language reasoning tasks can be represented as structured data with finite nodes and edges (Jin et al., 2024), and inspired by existing works on constructing reasoning benchmarks (Fatemi et al., 2024;Kazemi et al., 2023;Agrawal et al., 2024), we propose to leverage synthetic graph-based data for task-specific posttraining adaptation to improve the correctness of the generated reasoning questions and labels.\n\nIn this paper, we carefully design a random walk sampling algorithm on graphs and introduce a new prompting strategy that first extracts a reasoning chain and then derives the answer.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[272753174 | Jiang et al. | 2024 | Citations: 7]",
                "snippets": "In this paper, we propose a new data synthesis method called \\textbf{LogicPro}, which leverages LeetCode-style algorithm \\underline{Pro}blems and their corresponding \\underline{Pro}gram solutions to synthesize Complex \\underline{Logic}al Reasoning data in text format. First, we synthesize complex reasoning problems through source algorithm problems and test cases. Then, standard answers and intermediate variable outputs are obtained for each problem based on standard python solutions and test cases. Finally, with the guidance of code intermediate variables, we synthesize the text reasoning process for each reasoning problems. Through this method, we can synthesize data that is difficult, scalable, effective, and comes with golden standard answers and high-quality reasoning processes.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this paper, we propose a new data synthesis method called \\textbf{LogicPro}, which leverages LeetCode-style algorithm \\underline{Pro}blems and their corresponding \\underline{Pro}gram solutions to synthesize Complex \\underline{Logic}al Reasoning data in text format. First, we synthesize complex reasoning problems through source algorithm problems and test cases. Then, standard answers and intermediate variable outputs are obtained for each problem based on standard python solutions and test cases. Finally, with the guidance of code intermediate variables, we synthesize the text reasoning process for each reasoning problems. Through this method, we can synthesize data that is difficult, scalable, effective, and comes with golden standard answers and high-quality reasoning processes.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[272826678 | Irawan et al. | 2024 | Citations: 0]",
                "snippets": "In the realm of multi-modal learning, particularly in the visionlanguage domain, the potential of synthetic data generation has been extensively explored. Li et al. (2023) discuss the application of synthetic data across various tasks and modalities. In computer vision, GAN-based models (Kayser et al., 2021) and diffusion-based approaches (Nichol et al., 2022) are utilized for image synthesis. Within natural language processing domain, several studies (Kumar et al., 2021;(Chung et al., 2023)Schmidt et al., 2024)",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[234338081 | Kayser et al. | 2021 | Citations: 101]": "Recently, there has been an increasing number of efforts to introduce models capable of generating natural language explanations (NLEs) for their predictions on vision-language (VL) tasks. Such models are appealing, because they can provide human-friendly and comprehensive explanations. However, there is a lack of comparison between existing methods, which is due to a lack of re-usable evaluation frameworks and a scarcity of datasets. In this work, we introduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable vision-language tasks that establishes a unified evaluation framework and provides the first comprehensive comparison of existing approaches that generate NLEs for VL tasks. It spans four models and three datasets and both automatic metrics and human evaluation are used to assess model-generated explanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs (over 430k instances). We also propose a new model that combines UNITER [15], which learns joint embeddings of images and text, and GPT-2 [38], a pre-trained language model that is well-suited for text generation. It surpasses the previous state of the art by a large margin across all datasets. Code and data are available here: https://github.com/maximek3/e-ViL.",
                    "[259096160 | Chung et al. | 2023 | Citations: 121]": "Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging. In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation. We first examine two approaches to diversify text generation: 1) logit suppression, which minimizes the generation of languages that have already been frequently generated, and 2) temperature sampling, which flattens the token sampling probability. We found that diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain). To address this issue, we examined two human interventions, 1) label replacement (LR), correcting misaligned labels, and 2) out-of-scope filtering (OOSF), removing instances that are out of the user\u2019s domain of interest or to which no considered label applies. With oracle studies, we found that LR increases the absolute accuracy of models trained with diversified datasets by 14.4%. Moreover, we found that some models trained with data generated with LR interventions outperformed LLM-based few-shot classification. In contrast, OOSF was not effective in increasing model accuracy, implying the need for future work in human-in-the-loop text data generation."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 796,
                        "end": 1312,
                        "sentence_offsets": [
                            {
                                "start": 763,
                                "end": 950
                            },
                            {
                                "start": 951,
                                "end": 1046
                            },
                            {
                                "start": 1047,
                                "end": 1192
                            },
                            {
                                "start": 1193,
                                "end": 1312
                            }
                        ],
                        "ref_mentions": [
                            "234338081",
                            "259096160"
                        ],
                        "quote": "In the realm of multi-modal learning, particularly in the visionlanguage domain, the potential of synthetic data generation has been extensively explored. Li et al. (2023) discuss the application of synthetic data across various tasks and modalities. In computer vision, GAN-based models (Kayser et al., 2021) and diffusion-based approaches (Nichol et al., 2022) are utilized for image synthesis. Within natural language processing domain, several studies (Kumar et al., 2021;(Chung et al., 2023)Schmidt et al., 2024)"
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[273662200 | Braga et al. | 2024 | Citations: 1]",
                "snippets": "Puri et al. [39] explore question and answer generation as a data augmentation method to enhance QA models, addressing the challenge of limited human-labelled data. The authors point out that the synthetic data alone overcome the performance of the original training set in the Exact Match (EM) and F1 scores. Li et al. [40] investigate the effectiveness of synthetic data generated by LLMs in training text classification models across tasks with varying levels of subjectivity. They compare models trained on real-world data with those trained on LLMgenerated data under zero-shot and few-shot settings. Their findings reveal that while models trained on synthetic data perform comparably on low-subjectivity tasks, e.g. news and topic classification, there is a notable performance drop on high-subjectivity tasks, such as sarcasm detection. Regarding the generation of synthetic data for IR tasks, LLMs can be used to generate synthetic queries (Bonifacio et al., 2022) or documents [17]. In (Wang et al., 2023) the authors propose to generate documents by few-shot prompting LLMs and concatenate them with the original query to form a new query.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[250340449 | Bonifacio et al. | 2022 | Citations: 114]": "The Information Retrieval (IR) community has recently witnessed a revolution due to large pretrained transformer models. Another key ingredient for this revolution was the MS MARCO dataset, whose scale and diversity has enabled zero-shot transfer learning to various tasks. However, not all IR tasks and domains can benefit from one single dataset equally. Extensive research in various NLP tasks has shown that using domain-specific training data, as opposed to a general-purpose one, improves the performance of neural models. In this work, we harness the few-shot capabilities of large pretrained language models as synthetic data generators for IR tasks. We show that models finetuned solely on our synthetic datasets outperform strong baselines such as BM25 as well as recently proposed self-supervised dense retrieval methods. Code, models, and data are available at https://github.com/zetaalphavector/inpars.",
                    "[257505063 | Wang et al. | 2023 | Citations: 112]": "This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo-documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results."
                },
                "metadata": [
                    {
                        "section_title": "B. Synthetic Data Generation",
                        "pdf_hash": "",
                        "start": 476,
                        "end": 1591,
                        "sentence_offsets": [
                            {
                                "start": 476,
                                "end": 640
                            },
                            {
                                "start": 641,
                                "end": 785
                            },
                            {
                                "start": 786,
                                "end": 955
                            },
                            {
                                "start": 956,
                                "end": 1081
                            },
                            {
                                "start": 1082,
                                "end": 1198
                            },
                            {
                                "start": 1199,
                                "end": 1320
                            },
                            {
                                "start": 1321,
                                "end": 1448
                            },
                            {
                                "start": 1449,
                                "end": 1591
                            }
                        ],
                        "ref_mentions": [
                            "250340449",
                            "257505063"
                        ],
                        "quote": "Puri et al. [39] explore question and answer generation as a data augmentation method to enhance QA models, addressing the challenge of limited human-labelled data. The authors point out that the synthetic data alone overcome the performance of the original training set in the Exact Match (EM) and F1 scores. Li et al. [40] investigate the effectiveness of synthetic data generated by LLMs in training text classification models across tasks with varying levels of subjectivity. They compare models trained on real-world data with those trained on LLMgenerated data under zero-shot and few-shot settings. Their findings reveal that while models trained on synthetic data perform comparably on low-subjectivity tasks, e.g. news and topic classification, there is a notable performance drop on high-subjectivity tasks, such as sarcasm detection. Regarding the generation of synthetic data for IR tasks, LLMs can be used to generate synthetic queries (Bonifacio et al., 2022) or documents [17]. In (Wang et al., 2023) the authors propose to generate documents by few-shot prompting LLMs and concatenate them with the original query to form a new query."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[273695832 | Xie et al. | 2024 | Citations: 41]",
                "snippets": "Logical reasoning benchmarks. To evaluate logical reasoning capabilities in LLMs, synthetic benchmarks have been developed to enable scalable generation of samples with varying configurations and difficulty levels (Clark et al., 2020)(Giadikiaroglou et al., 2024)(Parmar et al., 2024). For instance, DyVal (Zhu et al., 2024) uses directed acyclic graphs to dynamically generate samples on reasoning tasks including deductive, Boolean, and abductive reasoning. (Chen et al., 2024) focus on propositional logical problems involving definite clauses, and synthetically generate variations with different premise orders, such as forward, backward, and shuffled. (Dziri et al., 2024) explore the limitations of LLMs in tasks requiring compositional reasoning, including multiplication, logic grid puzzles, and dynamic programming problems. ZebraLogic (Lin et al., 2024) is an extended benchmark that systematically tests logical reasoning capabilities. BoardgameQA (Kazemi et al., 2023) presents a question-answering dataset characterized by contradictory facts and rules in the questions. PRONTOQA (Saparov & He, 2023) is a synthetic question-answering dataset where each example is generated from a synthetic world model represented in first-order logic. This dataset enables parsing the generated chain of thought into symbolic proofs, facilitating formal analysis. TruthQuest (Mondorf & Plank, 2024) is the most similar task to our work, which provides evaluation samples based on K&K-type of puzzles involving 3-6 person.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[211126663 | Clark et al. | 2020 | Citations: 361]": "Beginning with McCarthy's Advice Taker (1959), AI has pursued the goal of providing a system with explicit, general knowledge and having the system reason over that knowledge. However, expressing the knowledge in a formal (logical or probabilistic) representation has been a major obstacle to this research. This paper investigates a modern approach to this problem where the facts and rules are provided as natural language sentences, thus bypassing a formal representation. We train transformers to reason (or emulate reasoning) over these sentences using synthetically generated data. Our models, that we call RuleTakers, provide the first empirical demonstration that this kind of soft reasoning over language is learnable, can achieve high (99%) accuracy, and generalizes to test data requiring substantially deeper chaining than seen during training (95%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as limited \"soft theorem provers\" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering.",
                    "[259144942 | Kazemi et al. | 2023 | Citations: 50]": "Automated reasoning with unstructured natural text is a key requirement for many potential applications of NLP and for developing robust AI systems. Recently, Language Models (LMs) have demonstrated complex reasoning capacities even without any finetuning. However, existing evaluation for automated reasoning assumes access to a consistent and coherent set of information over which models reason. When reasoning in the real-world, the available information is frequently inconsistent or contradictory, and therefore models need to be equipped with a strategy to resolve such conflicts when they arise. One widely-applicable way of resolving conflicts is to impose preferences over information sources (e.g., based on source credibility or information recency) and adopt the source with higher preference. In this paper, we formulate the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of defeasible reasoning, and develop a dataset called BoardgameQA for measuring the reasoning capacity of LMs in this setting. BoardgameQA also incorporates reasoning with implicit background knowledge, to better reflect reasoning problems in downstream applications. We benchmark various LMs on BoardgameQA and the results reveal a significant gap in the reasoning capacity of state-of-the-art LMs on this problem, showing that reasoning with conflicting information does not surface out-of-the-box in LMs. While performance can be improved with finetuning, it nevertheless remains poor.",
                    "[267657940 | Chen et al. | 2024 | Citations: 32]": "Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark.",
                    "[267751102 | Giadikiaroglou et al. | 2024 | Citations: 34]": "Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in AI, marking a significant step towards understanding their applicability in complex reasoning tasks. This survey leverages a unique taxonomy\u2014dividing puzzles into rule-based and rule-less categories\u2014to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning. Through a critical review of relevant datasets and benchmarks, we assess LLMs\u2019 performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance LLMs\u2019 puzzle-solving proficiency and contribute to AI\u2019s logical reasoning and creative problem-solving advancements.",
                    "[269330143 | Parmar et al. | 2024 | Citations: 31]": "Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really\"reason\"over the natural language? This question has been receiving significant research attention and many reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to 'logical reasoning' has remained underexplored. Existing work investigating this reasoning ability of LLMs has focused only on a couple of inference rules (such as modus ponens and modus tollens) of propositional and first-order logic. Addressing the above limitation, we comprehensively evaluate the logical reasoning ability of LLMs on 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics. To enable systematic evaluation, we introduce LogicBench, a natural language question-answering dataset focusing on the use of a single inference rule. We conduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini, Llama-2, and Mistral using chain-of-thought prompting. Experimental results show that existing LLMs do not fare well on LogicBench; especially, they struggle with instances involving complex reasoning and negations. Furthermore, they sometimes overlook contextual information necessary for reasoning to arrive at the correct conclusion. We believe that our work and findings facilitate future research for evaluating and enhancing the logical reasoning ability of LLMs. Data and code are available at https://github.com/Mihir3009/LogicBench."
                },
                "metadata": [
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1519,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 29
                            },
                            {
                                "start": 30,
                                "end": 283
                            },
                            {
                                "start": 284,
                                "end": 457
                            },
                            {
                                "start": 458,
                                "end": 655
                            },
                            {
                                "start": 656,
                                "end": 832
                            },
                            {
                                "start": 833,
                                "end": 945
                            },
                            {
                                "start": 946,
                                "end": 1082
                            },
                            {
                                "start": 1083,
                                "end": 1249
                            },
                            {
                                "start": 1250,
                                "end": 1361
                            },
                            {
                                "start": 1362,
                                "end": 1519
                            }
                        ],
                        "ref_mentions": [
                            "211126663",
                            "267751102",
                            "269330143",
                            "267657940",
                            "259144942"
                        ],
                        "quote": "Logical reasoning benchmarks. To evaluate logical reasoning capabilities in LLMs, synthetic benchmarks have been developed to enable scalable generation of samples with varying configurations and difficulty levels (Clark et al., 2020)(Giadikiaroglou et al., 2024)(Parmar et al., 2024). For instance, DyVal (Zhu et al., 2024) uses directed acyclic graphs to dynamically generate samples on reasoning tasks including deductive, Boolean, and abductive reasoning. (Chen et al., 2024) focus on propositional logical problems involving definite clauses, and synthetically generate variations with different premise orders, such as forward, backward, and shuffled. (Dziri et al., 2024) explore the limitations of LLMs in tasks requiring compositional reasoning, including multiplication, logic grid puzzles, and dynamic programming problems. ZebraLogic (Lin et al., 2024) is an extended benchmark that systematically tests logical reasoning capabilities. BoardgameQA (Kazemi et al., 2023) presents a question-answering dataset characterized by contradictory facts and rules in the questions. PRONTOQA (Saparov & He, 2023) is a synthetic question-answering dataset where each example is generated from a synthetic world model represented in first-order logic. This dataset enables parsing the generated chain of thought into symbolic proofs, facilitating formal analysis. TruthQuest (Mondorf & Plank, 2024) is the most similar task to our work, which provides evaluation samples based on K&K-type of puzzles involving 3-6 person."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[276250232 | Zeng et al. | 2025 | Citations: 4]",
                "snippets": "In order to obtain step-wise reasoning data for non-Math domains, we devise a pipeline, as outlined in Figure 2, to generate synthetic reasoning CoTs from existing questionanswering data. These CoTs are then given step-wise labels based on reasoning correctness. We detail the synthetic data generation process in Sections 5.1 and 5.2, including methods to create and annotate reasoning steps.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Automatic Generation of Multi-Domain",
                        "pdf_hash": "",
                        "start": 29,
                        "end": 422,
                        "sentence_offsets": [
                            {
                                "start": 29,
                                "end": 216
                            },
                            {
                                "start": 217,
                                "end": 291
                            },
                            {
                                "start": 292,
                                "end": 422
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In order to obtain step-wise reasoning data for non-Math domains, we devise a pipeline, as outlined in Figure 2, to generate synthetic reasoning CoTs from existing questionanswering data. These CoTs are then given step-wise labels based on reasoning correctness. We detail the synthetic data generation process in Sections 5.1 and 5.2, including methods to create and annotate reasoning steps."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[276318012 | You et al. | 2025 | Citations: 0]",
                "snippets": "Recent studies have emphasized the effectiveness of synthetic datasets in mitigating inconsistent hallucinations related to specific tasks. For instance, synthetic mathematical datasets (Toshniwal et al., 2024;Huang et al., 2024) have shown reduced contradictions in the calculation process and significant improvements in solving mathematical problems. Additionally, some research has synthesized reasoning data in the logical reasoning (Nie et al., 2019)(Saeed et al., 2021) field while providing the reasoning process (Dalvi et al., 2021) during synthesis. LOGIC-LM and SymbCOT (Pan et al., 2023)Xu et al., 2024) integrate LLMs with symbolic solvers to enhance logical problem-solving.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207756753 | Nie et al. | 2019 | Citations: 1011]": "We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.",
                    "[233297051 | Dalvi et al. | 2021 | Citations: 185]": "Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a \u201crationale\u201d). If this could be done, new opportunities for understanding and debugging the system\u2019s reasoning become possible. Our approach is to generate explanations in the form of entailment trees, namely a tree of multipremise entailment steps from facts that are known, through intermediate conclusions, to the hypothesis of interest (namely the question + answer). To train a model with this skill, we created ENTAILMENTBANK, the first dataset to contain multistep entailment trees. Given a hypothesis (question + answer), we define three increasingly difficult explanation tasks: generate a valid entailment tree given (a) all relevant sentences (b) all relevant and some irrelevant sentences, or (c) a corpus. We show that a strong language model can partially solve these tasks, in particular when the relevant sentences are included in the input (e.g., 35% of trees for (a) are perfect), and with indications of generalization to other domains. This work is significant as it provides a new type of dataset (multistep entailments) and baselines, offering a new avenue for the community to generate richer, more systematic explanations.",
                    "[237940562 | Saeed et al. | 2021 | Citations: 33]": "While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems, they are still very limited in their ability to capture and to use common-sense knowledge. In fact, even if information is available in the form of approximate (soft) logical rules, it is not clear how to transfer it to a PLM in order to improve its performance for deductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how to reason with soft Horn rules. We introduce a classification task where, given facts and soft rules, the PLM should return a prediction with a probability for a given hypothesis. We release the first dataset for this task, and we propose a revised loss function that enables the PLM to learn how to predict precise probabilities for the task. Our evaluation results show that the resulting fine-tuned models achieve very high performance, even on logical rules that were unseen at training. Moreover, we demonstrate that logical notions expressed by the rules are transferred to the fine-tuned model, yielding state-of-the-art results on external datasets.",
                    "[258833332 | Pan et al. | 2023 | Citations: 270]": "Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver's error messages to revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant performance boost of 39.2% over using LLM alone with standard prompting and 18.4% over LLM with chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning. Code and data are publicly available at https://github.com/teacherpeterpan/Logic-LLM."
                },
                "metadata": [
                    {
                        "section_title": "Inconsistent Hallucinations Explanation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 687,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 139
                            },
                            {
                                "start": 140,
                                "end": 353
                            },
                            {
                                "start": 354,
                                "end": 558
                            },
                            {
                                "start": 559,
                                "end": 687
                            }
                        ],
                        "ref_mentions": [
                            "207756753",
                            "237940562",
                            "233297051",
                            "258833332"
                        ],
                        "quote": "Recent studies have emphasized the effectiveness of synthetic datasets in mitigating inconsistent hallucinations related to specific tasks. For instance, synthetic mathematical datasets (Toshniwal et al., 2024;Huang et al., 2024) have shown reduced contradictions in the calculation process and significant improvements in solving mathematical problems. Additionally, some research has synthesized reasoning data in the logical reasoning (Nie et al., 2019)(Saeed et al., 2021) field while providing the reasoning process (Dalvi et al., 2021) during synthesis. LOGIC-LM and SymbCOT (Pan et al., 2023)Xu et al., 2024) integrate LLMs with symbolic solvers to enhance logical problem-solving."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[276768281 | Zhou et al. | 2025 | Citations: 6]",
                "snippets": "We also introduce an automated data synthesis framework for generating MAS benchmarks without any human annotations. Experimental results show that ReSo matches or outperforms existing methods, achieving 33.7 percent accuracy on Math-MAS and 32.3 percent accuracy on SciBench-MAS, where other approaches completely fail.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We also introduce an automated data synthesis framework for generating MAS benchmarks without any human annotations. Experimental results show that ReSo matches or outperforms existing methods, achieving 33.7 percent accuracy on Math-MAS and 32.3 percent accuracy on SciBench-MAS, where other approaches completely fail.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[276774742 | Li et al. | 2025 | Citations: 9]",
                "snippets": "To overcome these challenges, we propose a novel and scalable text-to-SQL data synthesis framework for automatically synthesizing large-scale, high-quality, and diverse datasets without extensive human intervention. Using this framework, we introduce SynSQL-2.5M, the first million-scale text-to-SQL dataset, containing 2.5 million samples spanning over 16,000 synthetic databases. Each sample includes a database, SQL query, natural language question, and chain-of-thought (CoT) solution.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "To overcome these challenges, we propose a novel and scalable text-to-SQL data synthesis framework for automatically synthesizing large-scale, high-quality, and diverse datasets without extensive human intervention. Using this framework, we introduce SynSQL-2.5M, the first million-scale text-to-SQL dataset, containing 2.5 million samples spanning over 16,000 synthetic databases. Each sample includes a database, SQL query, natural language question, and chain-of-thought (CoT) solution.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[277065796 | Luo et al. | 2025 | Citations: 3]",
                "snippets": "Recently, the use of LLMs for data synthesis or data augmentation has become increasingly prevalent. Many studies leverage LLMgenerated data for training models (Hartvigsen et al., 2022)36,53,56,59], as well as for evaluating the performance of other trained models (Ribeiro et al., 2022). In the NLP domain, researchers have utilized LLMs to generate synthetic data for tasks like text classification [1,3,(Yoo et al., 2021). These works showcase that LLM-generated data can enhance data diversity, thereby improving model generalization and robustness...By leveraging LLMs to generate multi-step reasoning data, the performance of models on long-chain and complex reasoning tasks can be further improved. As demonstrated by Hunter et al. [18], process supervision via multi-step reasoning significantly enhances model reliability on tasks such as mathematical problem-solving. Similarly, Step-DPO [14] shows that generating step-wise reasoning data enables models to better capture intermediate steps, resulting in improved accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[233296100 | Yoo et al. | 2021 | Citations: 242]": "Large-scale language models such as GPT-3 are excellent few-shot learners, allowing them to be controlled via natural text prompts. Recent studies report that prompt-based direct classification eliminates the need for fine-tuning but lacks data and inference scalability. This paper proposes a novel data augmentation technique that leverages large-scale language models to generate realistic text samples from a mixture of real samples. We also propose utilizing soft-labels predicted by the language models, effectively distilling knowledge from the large-scale language models and creating textual perturbations simultaneously. We perform data augmentation experiments on diverse classification tasks and show that our method hugely outperforms existing text augmentation methods. Ablation studies and a qualitative analysis provide more insights into our approach.",
                    "[247519233 | Hartvigsen et al. | 2022 | Citations: 388]": "Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language.To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model. Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset.",
                    "[248779886 | Ribeiro et al. | 2022 | Citations: 85]": "Current approaches to testing and debugging NLP models rely on highly variable human creativity and extensive labor, or only work for a very restrictive class of bugs. We present AdaTest, a process which uses large scale language models (LMs) in partnership with human feedback to automatically write unit tests highlighting bugs in a target model. Such bugs are then addressed through an iterative text-fix-retest loop, inspired by traditional software development. In experiments with expert and non-expert users and commercial / research models for 8 different tasks, AdaTest makes users 5-10x more effective at finding bugs than current approaches, and helps users effectively fix bugs without adding new bugs."
                },
                "metadata": [
                    {
                        "section_title": "LLMs Data Synthesis",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 498,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 100
                            },
                            {
                                "start": 101,
                                "end": 249
                            },
                            {
                                "start": 250,
                                "end": 371
                            },
                            {
                                "start": 372,
                                "end": 499
                            }
                        ],
                        "ref_mentions": [
                            "247519233",
                            "248779886",
                            "233296100"
                        ],
                        "quote": "Recently, the use of LLMs for data synthesis or data augmentation has become increasingly prevalent. Many studies leverage LLMgenerated data for training models (Hartvigsen et al., 2022)36,53,56,59], as well as for evaluating the performance of other trained models (Ribeiro et al., 2022). In the NLP domain, researchers have utilized LLMs to generate synthetic data for tasks like text classification [1,3,(Yoo et al., 2021). These works showcase that LLM-generated data can enhance data diversity, thereby improving model generalization and robustness"
                    },
                    {
                        "section_title": "LLMs Data Synthesis",
                        "pdf_hash": "",
                        "start": 1473,
                        "end": 1953,
                        "sentence_offsets": [
                            {
                                "start": 1473,
                                "end": 1623
                            },
                            {
                                "start": 1624,
                                "end": 1795
                            },
                            {
                                "start": 1796,
                                "end": 1952
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "By leveraging LLMs to generate multi-step reasoning data, the performance of models on long-chain and complex reasoning tasks can be further improved. As demonstrated by Hunter et al. [18], process supervision via multi-step reasoning significantly enhances model reliability on tasks such as mathematical problem-solving. Similarly, Step-DPO [14] shows that generating step-wise reasoning data enables models to better capture intermediate steps, resulting in improved accuracy."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[277104955 | Nadas et al. | 2025 | Citations: 3]",
                "snippets": "Self-Ask [47] Multi-hop Question Answering (QA), Reasoning \n\nIntroduced an LLM-based self-asking strategy where models recursively break down complex questions into simpler sub-questions. Demonstrated improved reasoning performance on multi-step inference tasks by leveraging self-generated sub-questions for enhanced accuracy.\n\nCoT [13] Scientific QA, Logical Reasoning \n\nExamined how Chain-of-Thought (CoT) prompting improves LLM reasoning capabilities. Proposed Sci-CoT, a two-stage framework where intermediate rationales are generated separately from answer inference, improving efficiency and model performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[264288838 | Fan et al. | 2023 | Citations: 4]": "Chain-of-Thought (CoT) is a technique that guides Large Language Models (LLMs) to decompose complex tasks into multi-step reasoning through intermediate steps in natural language form. Briefly, CoT enables LLMs to think step by step. However, although many Natural Language Understanding (NLU) tasks also require thinking step by step, LLMs perform less well than small-scale Masked Language Models (MLMs). To migrate CoT from LLMs to MLMs, we propose Chain-of-Thought Tuning (CoTT), a two-step reasoning framework based on prompt tuning, to implement step-by-step thinking for MLMs on NLU tasks. From the perspective of CoT, CoTT's two-step framework enables MLMs to implement task decomposition; CoTT's prompt tuning allows intermediate steps to be used in natural language form. Thereby, the success of CoT can be extended to NLU tasks through MLMs. To verify the effectiveness of CoTT, we conduct experiments on two NLU tasks: hierarchical classification and relation extraction, and the results show that CoTT outperforms baselines and achieves state-of-the-art performance."
                },
                "metadata": [
                    {
                        "section_title": "Various approaches for",
                        "pdf_hash": "",
                        "start": 1220,
                        "end": 1837,
                        "sentence_offsets": [
                            {
                                "start": 1155,
                                "end": 1308
                            },
                            {
                                "start": 1309,
                                "end": 1480
                            },
                            {
                                "start": 1481,
                                "end": 1539
                            },
                            {
                                "start": 1542,
                                "end": 1668
                            },
                            {
                                "start": 1669,
                                "end": 1808
                            },
                            {
                                "start": 1809,
                                "end": 1850
                            }
                        ],
                        "ref_mentions": [
                            "264288838"
                        ],
                        "quote": "Self-Ask [47] Multi-hop Question Answering (QA), Reasoning \n\nIntroduced an LLM-based self-asking strategy where models recursively break down complex questions into simpler sub-questions. Demonstrated improved reasoning performance on multi-step inference tasks by leveraging self-generated sub-questions for enhanced accuracy.\n\nCoT [13] Scientific QA, Logical Reasoning \n\nExamined how Chain-of-Thought (CoT) prompting improves LLM reasoning capabilities. Proposed Sci-CoT, a two-stage framework where intermediate rationales are generated separately from answer inference, improving efficiency and model performance."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[277452727 | Zhao et al. | 2025 | Citations: 0]",
                "snippets": "(Wei et al., 2019) adopts synonym replacement, random insertion, random swap, and random deletion to augment the original sentences...(Anaby-Tavor et al., 2019) and [18] fine-tune multiple generative models for each relation type to generate augmentations. (Bayer et al., 2021) proposes a sophisticated generation-based method that generates augmented data by incorporating new linguistic patterns. Beyond that, [19] introduced ChatGPT to make great progress in nearly all kinds of NLP tasks. It has proved to be effective utilizing ChatGPT as a data augmentation technique to enrich the data instances (Nooten et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[212821571 | Anaby-Tavor et al. | 2019 | Citations: 370]": "Based on recent advances in natural language modeling and those in text generation capabilities, we propose a novel data augmentation method for text classification tasks. We use a powerful pre-trained neural network model to artificially synthesize new labeled data for supervised learning. We mainly focus on cases with scarce labeled data. Our method, referred to as language-model-based data augmentation (LAMBADA), involves fine-tuning a state-of-the-art language generator to a specific task through an initial training phase on the existing (usually small) labeled data. Using the fine-tuned model and given a class label, new sentences for the class are generated. Our process then filters these new sentences by using a classifier trained on the original data. In a series of experiments, we show that LAMBADA improves classifiers' performance on a variety of datasets. Moreover, LAMBADA significantly improves upon the state-of-the-art techniques for data augmentation, specifically those applicable to text classification tasks with little data.",
                    "[232379879 | Bayer et al. | 2021 | Citations: 118]": "In many cases of machine learning, research suggests that the development of training data might have a higher relevance than the choice and modelling of classifiers themselves. Thus, data augmentation methods have been developed to improve classifiers by artificially created training data. In NLP, there is the challenge of establishing universal rules for text transformations which provide new linguistic patterns. In this paper, we present and evaluate a text generation method suitable to increase the performance of classifiers for long and short texts. We achieved promising improvements when evaluating short as well as long text tasks with the enhancement by our text generation method. Especially with regard to small data analytics, additive accuracy gains of up to 15.53% and 3.56% are achieved within a constructed low data regime, compared to the no augmentation baseline and another data augmentation technique. As the current track of these constructed regimes is not universally applicable, we also show major improvements in several real world low data tasks (up to +4.84 F1-score). Since we are evaluating the method from many perspectives (in total 11 datasets), we also observe situations where the method might not be suitable. We discuss implications and patterns for the successful application of our approach on different types of datasets.",
                    "[260063065 | Nooten et al. | 2023 | Citations: 9]": "In this paper, we leverage the GPT-3.5 language model both using the Chat-GPT API interface and the GPT-3.5 API interface to generate realistic examples of anti-vaccination tweets in Dutch with the aim of augmenting an imbalanced multi-label vaccine hesitancy argumentation classification dataset. In line with previous research, we devise a prompt that, on the one hand, instructs the model to generate realistic examples based on the gold standard dataset and, on the other hand, to assign multiple pseudo-labels (or a single pseudo-label) to the generated instances. We then augment our gold standard data with the generated examples and evaluate the impact thereof in a cross-validation setting with several state-of-the-art Dutch large language models. This augmentation technique predominantly shows improvements in F1 for classifying underrepresented classes while increasing the overall recall, paired with a slight decrease in precision for more common classes. Furthermore, we examine how well the synthetic data generalises to human data in the classification task. To our knowledge, we are the first to utilise Chat-GPT and GPT-3.5 for augmenting a Dutch multi-label dataset classification task.",
                    "[59523656 | Wei et al. | 2019 | Citations: 1962]": "We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use."
                },
                "metadata": [
                    {
                        "section_title": "B. Synthetic Data Augmentation in BioNLP",
                        "pdf_hash": "",
                        "start": 358,
                        "end": 474,
                        "sentence_offsets": [
                            {
                                "start": 358,
                                "end": 475
                            }
                        ],
                        "ref_mentions": [
                            "59523656"
                        ],
                        "quote": "(Wei et al., 2019) adopts synonym replacement, random insertion, random swap, and random deletion to augment the original sentences"
                    },
                    {
                        "section_title": "B. Synthetic Data Augmentation in BioNLP",
                        "pdf_hash": "",
                        "start": 735,
                        "end": 1172,
                        "sentence_offsets": [
                            {
                                "start": 735,
                                "end": 835
                            },
                            {
                                "start": 836,
                                "end": 961
                            },
                            {
                                "start": 962,
                                "end": 1055
                            },
                            {
                                "start": 1056,
                                "end": 1171
                            }
                        ],
                        "ref_mentions": [
                            "212821571",
                            "232379879",
                            "260063065"
                        ],
                        "quote": "(Anaby-Tavor et al., 2019) and [18] fine-tune multiple generative models for each relation type to generate augmentations. (Bayer et al., 2021) proposes a sophisticated generation-based method that generates augmented data by incorporating new linguistic patterns. Beyond that, [19] introduced ChatGPT to make great progress in nearly all kinds of NLP tasks. It has proved to be effective utilizing ChatGPT as a data augmentation technique to enrich the data instances (Nooten et al., 2023)."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[277622127 | Goldie et al. | 2025 | Citations: 11]",
                "snippets": "We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 370,
                        "end": 1181,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[278171321 | Wang et al. | 2025 | Citations: 0]",
                "snippets": "To address this scarcity, researchers have explored various synthesizing methods, particularly in the mathematics domain. The mainstream methods involve data augmentation based on existing annotated training sets, such as GSM8K (Cobbe et al., 2021) and MATH (Huang et al., 2024), ranging from self-evolving instructions (Xu et al., 2024;(Zeng et al., 2024) and question paraphrasing (Yu et al., 2024), to solution augmentation (Lu et al., 2024). However, these methods are limited by the available training data, constraining the synthesis diversity (Li et al., 2024c(Li et al., , 2023)). To enhance diversity, recent approaches enable LLMs to generate a large scale of questions from various mathematicsrelated sources, including web pages (Yue et al., 2024) and knowledge points (Tang et al., 2024) from web corpora or textbooks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[268247488 | Huang et al. | 2024 | Citations: 43]": "Large language models have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality and reasoning-focused training datasets. Addressing this challenge, we propose Key-PointDriven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar practices from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, an extensive synthetic dataset tailored for mathematical reasoning, comprising over 800K questionanswer pairs. Utilizing KPMath and augmenting it with additional reasoning-intensive corpora, we create the comprehensive KPMath-Plus dataset. Our experiments demonstrate that this dataset can enhance the mathematical reasoning performance of models across various architectures and sizes. The Qwen1.5-72B model, fine-tuned on KPMath-Plus, achieves 87.0% accuracy on GSM8K and 58.3% on MATH, surpassing competitors in the 7B to 72B range and best commercial models like GPT-4 across multiple math reasoning datasets.",
                    "[268247902 | Tang et al. | 2024 | Citations: 82]": "Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., {\\tt GPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct {\\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten datasets (including GSM8K and MATH) covering K-12, college, and competition level math problems. We apply MathScaleQA to fine-tune open-source LLMs (e.g., LLaMA-2 and Mistral), resulting in significantly improved capabilities in mathematical reasoning. Evaluated on {\\sc MwpBench}, MathScale-7B achieves state-of-the-art performance across all datasets, surpassing its best peers of equivalent size by 42.9\\% in micro average accuracy and 43.7\\% in macro average accuracy, respectively.",
                    "[270213007 | Zeng et al. | 2024 | Citations: 10]": "Fine-tuning large pre-trained language models with Evol-Instruct has achieved encouraging results across a wide range of tasks. However, designing effective evolving methods for instruction evolution requires substantial human expertise. This paper proposes Auto Evol-Instruct, an end-to-end framework that evolves instruction datasets using large language models without any human effort. The framework automatically analyzes and summarizes suitable evolutionary strategies for the given instruction data and iteratively improves the evolving method based on issues exposed during the instruction evolution process. Our extensive experiments demonstrate that the best method optimized by Auto Evol-Instruct outperforms human-designed methods on various benchmarks, including MT-Bench, AlpacaEval, GSM8K, and HumanEval."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 430,
                        "end": 1264,
                        "sentence_offsets": [
                            {
                                "start": 430,
                                "end": 551
                            },
                            {
                                "start": 552,
                                "end": 878
                            },
                            {
                                "start": 879,
                                "end": 1021
                            },
                            {
                                "start": 1022,
                                "end": 1264
                            }
                        ],
                        "ref_mentions": [
                            "268247488",
                            "270213007",
                            "268247902"
                        ],
                        "quote": "To address this scarcity, researchers have explored various synthesizing methods, particularly in the mathematics domain. The mainstream methods involve data augmentation based on existing annotated training sets, such as GSM8K (Cobbe et al., 2021) and MATH (Huang et al., 2024), ranging from self-evolving instructions (Xu et al., 2024;(Zeng et al., 2024) and question paraphrasing (Yu et al., 2024), to solution augmentation (Lu et al., 2024). However, these methods are limited by the available training data, constraining the synthesis diversity (Li et al., 2024c(Li et al., , 2023)). To enhance diversity, recent approaches enable LLMs to generate a large scale of questions from various mathematicsrelated sources, including web pages (Yue et al., 2024) and knowledge points (Tang et al., 2024) from web corpora or textbooks."
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[44096233 | Goyal et al. | 2018 | Citations: 2]",
                "snippets": "In this paper, we study the problem of geometric reasoning (a form of visual reasoning) in the context of question-answering. We introduce Dynamic Spatial Memory Network (DSMN), a new deep network architecture that specializes in answering questions that admit latent visual representations, and learns to generate and reason over such representations. Further, we propose two synthetic benchmarks, FloorPlanQA and ShapeIntersection, to evaluate the geometric reasoning capability of QA systems.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this paper, we study the problem of geometric reasoning (a form of visual reasoning) in the context of question-answering. We introduce Dynamic Spatial Memory Network (DSMN), a new deep network architecture that specializes in answering questions that admit latent visual representations, and learns to generate and reason over such representations. Further, we propose two synthetic benchmarks, FloorPlanQA and ShapeIntersection, to evaluate the geometric reasoning capability of QA systems.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 39,
                "key": "[49865372 | Ding et al. | 2018 | Citations: 7]",
                "snippets": "We introduce several generative models for producing synthetic keyword queries from natural language questions (Sect. 3.1). (3) We develop two filtering mechanisms, which are essential for ensuring that the synthetic training data we feed into the neural network is of high-quality (Sect. 3.2).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Did you mean instead ? (Diverse Questions)",
                        "pdf_hash": "",
                        "start": 14,
                        "end": 308,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We introduce several generative models for producing synthetic keyword queries from natural language questions (Sect. 3.1). (3) We develop two filtering mechanisms, which are essential for ensuring that the synthetic training data we feed into the neural network is of high-quality (Sect. 3.2)."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.034461,
        "cot": "After analyzing the query and the quotes, I'll organize the information into several key dimensions to provide a comprehensive overview of synthetic data generation methods for complex reasoning tasks in NLP.\n\n1. Introduction: This section will provide a general overview of synthetic data generation in NLP and its importance for complex reasoning tasks. This will help the user understand the context and significance of the research being discussed. This should be in synthesis format to provide a coherent explanation of the topic.\n\n2. Logical Reasoning Methods: Several quotes discuss synthetic data generation specifically for logical reasoning tasks, including deductive reasoning, propositional logic, and other formal reasoning approaches. This dimension should be in synthesis format to effectively explain and connect the various methods.\n\n3. Multi-Step Question Answering Approaches: Many papers focus on generating synthetic data for multi-hop or multi-step question answering, which requires complex reasoning across multiple facts or steps. This will be presented as a synthesis to show connections between different approaches.\n\n4. Mathematical Reasoning Techniques: A significant subset of the quotes discusses synthetic data generation for mathematical reasoning tasks, which often involve complex, multi-step reasoning processes. This will be a synthesis to show the evolution and comparison of different techniques.\n\n5. Commonsense Reasoning Data Generation: Several papers specifically address generating synthetic data for commonsense reasoning tasks, which require different approaches than formal logical reasoning. This will be presented as a synthesis to explain the unique challenges and solutions.\n\n6. Generation Methodologies and Frameworks: This dimension will focus on specific methodologies and frameworks used across different reasoning tasks, such as template-based generation, graph-based generation, and LLM-based synthesis. This will be a synthesis to provide a comprehensive understanding of the technical approaches.\n\n7. Evaluation and Benchmarks: Some quotes mention benchmark datasets and evaluation methods for synthetic data in complex reasoning tasks. This will be a synthesis to explain how researchers evaluate the effectiveness of synthetic data.\n\nEach dimension addresses a specific aspect of synthetic data generation for complex reasoning tasks, and together they provide a comprehensive answer to the user's query. The synthesis format is appropriate for all dimensions as they require explanation and connection of concepts rather than simple listing.",
        "plan": {
            "Introduction (synthesis)": [],
            "Logical Reasoning Methods (synthesis)": [
                11,
                12,
                24,
                25,
                28,
                30
            ],
            "Multi-Step Question Answering Approaches (synthesis)": [
                0,
                1,
                2,
                4,
                5,
                7,
                8,
                14,
                18,
                19,
                23,
                29,
                34,
                36,
                38
            ],
            "Mathematical Reasoning Techniques (synthesis)": [
                20,
                21,
                22,
                31,
                37
            ],
            "Commonsense Reasoning Data Generation (synthesis)": [
                3,
                6,
                9,
                17
            ],
            "Generation Methodologies and Frameworks (synthesis)": [
                10,
                13,
                15,
                16,
                26,
                27,
                32,
                33,
                35,
                39
            ],
            "Evaluation and Benchmarks (synthesis)": [
                18,
                28
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction",
                "tldr": "Synthetic data generation methods have become essential for advancing complex reasoning tasks in NLP research. These methods address data scarcity while enabling controlled experimentation with specific reasoning patterns across logical inference, multi-step reasoning, and mathematical problem-solving domains. (LLM Memory)",
                "text": "\nSynthetic data generation has emerged as a critical approach for developing and evaluating natural language processing (NLP) models capable of complex reasoning. While large language models are trained on vast amounts of text, they often struggle with specialized reasoning tasks that require logical consistency, multi-step inference chains, or mathematical operations. Research papers addressing this challenge have introduced various methodologies to generate synthetic datasets that can isolate and test specific reasoning capabilities. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThese synthetic data generation methods serve multiple important purposes: they compensate for the scarcity of naturally occurring examples of complex reasoning, provide controlled experimental environments where specific reasoning patterns can be isolated and evaluated, and create benchmarks with known ground truth answers against which model performance can be measured objectively. The approaches typically involve either programmatic generation using templates and rules, bootstrapping from existing models to create more complex examples, or hybrid methods that combine human annotation with algorithmic data generation. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe research in this area spans across multiple reasoning domains including formal logic, mathematics, commonsense reasoning, and natural language inference, with each domain requiring specialized techniques for generating realistic yet challenging examples. The following sections will explore the specific methodologies employed across these different reasoning domains, highlighting key papers and their approaches to synthetic data generation. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Logical Reasoning Methods",
                "tldr": "Research in synthetic data generation for logical reasoning focuses on creating datasets that test deductive inference, propositional logic, and complex reasoning chains. Approaches range from template-based generation to graph-based synthetic data and specialized frameworks that transform logic problems into natural language. (13 sources)",
                "text": "\nSynthetic data generation for logical reasoning tasks has emerged as a crucial methodology for assessing and enhancing language models' capacity for formal reasoning. Several approaches have been developed to create controlled datasets that isolate specific aspects of logical reasoning. The RuleTaker corpus presents a pioneering approach, generating multistep deductive proofs in natural language where models must apply implication rules to a given set of facts to prove or disprove hypotheses <Paper corpusId=\"260846474\" paperTitle=\"(Morishita et al., 2023)\" isShortName></Paper> <Paper corpusId=\"211126663\" paperTitle=\"(Clark et al., 2020)\" isShortName></Paper>. Similarly, the Artificial Argument Corpus focuses on single-step deductive proofs using hand-selected rules for critical thinking <Paper corpusId=\"260846474\" paperTitle=\"(Morishita et al., 2023)\" isShortName></Paper>.\n\nGraph-based approaches have proven effective for generating complex reasoning chains. Zhou et al. developed a method that leverages random walk sampling algorithms on graphs to create reasoning questions with explicit reasoning chains <Paper corpusId=\"272753147\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper>. This approach is motivated by the insight that natural language reasoning tasks can be represented as structured data with finite nodes and edges, allowing for controlled generation of reasoning patterns <Paper corpusId=\"272753147\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper>. Similarly, BoardgameQA presents a challenging dataset with contradictory facts and rules, requiring models to resolve conflicts through preference-based reasoning <Paper corpusId=\"259144942\" paperTitle=\"(Kazemi et al., 2023)\" isShortName></Paper>.\n\nSeveral innovative frameworks have been introduced to generate high-quality logical reasoning data. LogicPro transforms LeetCode-style algorithm problems and their program solutions into complex logical reasoning data in text format, creating difficult, scalable examples with golden standard answers and high-quality reasoning processes <Paper corpusId=\"272753174\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>. PRONTOQA generates question-answering examples from synthetic world models represented in first-order logic, enabling the parsing of reasoning chains into symbolic proofs for formal analysis <Paper corpusId=\"273695832\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\nRecent research has also focused on mitigating limitations in logical reasoning capabilities. Studies have revealed that large language models are surprisingly brittle to the ordering of premises in logical reasoning tasks, with performance drops of over 30% when premise order is changed <Paper corpusId=\"267657940\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>. This has led to the development of comprehensive evaluation benchmarks like LogicBench, which tests 25 different reasoning patterns across propositional, first-order, and non-monotonic logics <Paper corpusId=\"269330143\" paperTitle=\"(Parmar et al., 2024)\" isShortName></Paper>.\n\nAdversarial data collection methods have also proven valuable, as demonstrated in the development of datasets like ANLI, where human annotators iteratively find weaknesses in current models <Paper corpusId=\"207756753\" paperTitle=\"(Nie et al., 2019)\" isShortName></Paper>. This creates a \"moving target\" that continuously challenges model capabilities. Other approaches combine language models with symbolic solvers, as in Logic-LM, which shows significant performance improvements on logical reasoning tasks by translating natural language problems into symbolic formulations and then using deterministic solvers <Paper corpusId=\"258833332\" paperTitle=\"(Pan et al., 2023)\" isShortName></Paper>.\n\nSome methods focus on generating data with explicit reasoning paths. ENTAILMENTBANK contains multistep entailment trees that show how intermediate conclusions are derived from known facts to reach the final answer <Paper corpusId=\"233297051\" paperTitle=\"(Dalvi et al., 2021)\" isShortName></Paper>. Similarly, Saeed et al. developed a dataset for teaching language models to reason with soft Horn rules, enabling predictions with precise probabilities <Paper corpusId=\"237940562\" paperTitle=\"(Saeed et al., 2021)\" isShortName></Paper>. Bao et al. used contrastive learning on a synthetic dataset containing different combinations of entities, relations, and attributes to generate controllable logical sentences <Paper corpusId=\"258833579\" paperTitle=\"(Bao et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Morishita et al., 2023)",
                        "snippets": [
                            "RuleTaker (Clark et al., 2021) proposed a deduction corpus composed of synthetically generated multistep deductive proofs written in natural languages. Each deductive proof (dis-)proves a hypothesis by applying deduction rules multiple times to a given set of facts. As the deduction rules, rules of the implication kind were used.\n\nArtificial Argument Corpus (Betz et al., 2021) is another corpus composed of synthetically generated single-step deductive proofs. As the deduction rules, hand-selected rules useful for critical thinking were used.\n\nGontier et al. (2020) investigated the deductive reasoning ability of LMs on a corpus, which is composed of a specific type of multistep inference: kinship relationship on synthetic kinship graphs."
                        ],
                        "paper": {
                            "corpus_id": 260846474,
                            "title": "Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic",
                            "authors": [
                                {
                                    "authorId": "1379579811",
                                    "name": "Terufumi Morishita"
                                },
                                {
                                    "authorId": "29347584",
                                    "name": "Gaku Morio"
                                },
                                {
                                    "authorId": "145412147",
                                    "name": "Atsuki Yamaguchi"
                                },
                                {
                                    "authorId": "2106369",
                                    "name": "Yasuhiro Sogawa"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 25
                        },
                        "score": 0.8564453125
                    },
                    {
                        "id": "(Clark et al., 2020)",
                        "snippets": [
                            "Beginning with McCarthy's Advice Taker (1959), AI has pursued the goal of providing a system with explicit, general knowledge and having the system reason over that knowledge. However, expressing the knowledge in a formal (logical or probabilistic) representation has been a major obstacle to this research. This paper investigates a modern approach to this problem where the facts and rules are provided as natural language sentences, thus bypassing a formal representation. We train transformers to reason (or emulate reasoning) over these sentences using synthetically generated data. Our models, that we call RuleTakers, provide the first empirical demonstration that this kind of soft reasoning over language is learnable, can achieve high (99%) accuracy, and generalizes to test data requiring substantially deeper chaining than seen during training (95%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as limited \"soft theorem provers\" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering."
                        ],
                        "paper": {
                            "corpus_id": 211126663,
                            "title": "Transformers as Soft Reasoners over Language",
                            "authors": [
                                {
                                    "authorId": "48323507",
                                    "name": "Peter Clark"
                                },
                                {
                                    "authorId": "3385516",
                                    "name": "Oyvind Tafjord"
                                },
                                {
                                    "authorId": "46666605",
                                    "name": "Kyle Richardson"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Joint Conference on Artificial Intelligence",
                            "n_citations": 361
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhou et al., 2024)",
                        "snippets": [
                            "Despite recent advances in training and prompting strategies for Large Language Models (LLMs), these models continue to face challenges with complex logical reasoning tasks that involve long reasoning chains. In this work, we explore the potential and limitations of using graph-based synthetic reasoning data as training signals to enhance LLMs' reasoning capabilities.\n\nMotivated by the fact that natural language reasoning tasks can be represented as structured data with finite nodes and edges (Jin et al., 2024), and inspired by existing works on constructing reasoning benchmarks (Fatemi et al., 2024;Kazemi et al., 2023;Agrawal et al., 2024), we propose to leverage synthetic graph-based data for task-specific posttraining adaptation to improve the correctness of the generated reasoning questions and labels.\n\nIn this paper, we carefully design a random walk sampling algorithm on graphs and introduce a new prompting strategy that first extracts a reasoning chain and then derives the answer."
                        ],
                        "paper": {
                            "corpus_id": 272753147,
                            "title": "Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data",
                            "authors": [
                                {
                                    "authorId": "2297828274",
                                    "name": "Jiaming Zhou"
                                },
                                {
                                    "authorId": "2321867953",
                                    "name": "Abbas Ghaddar"
                                },
                                {
                                    "authorId": "2321875142",
                                    "name": "Ge Zhang"
                                },
                                {
                                    "authorId": "1892081076",
                                    "name": "Liheng Ma"
                                },
                                {
                                    "authorId": "2288403553",
                                    "name": "Yaochen Hu"
                                },
                                {
                                    "authorId": "38939190",
                                    "name": "Soumyasundar Pal"
                                },
                                {
                                    "authorId": "2287938359",
                                    "name": "Mark Coates"
                                },
                                {
                                    "authorId": "2321911323",
                                    "name": "Bin Wang"
                                },
                                {
                                    "authorId": "2275529643",
                                    "name": "Yingxue Zhang"
                                },
                                {
                                    "authorId": "2307072418",
                                    "name": "Jianye Hao"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.9267578125
                    },
                    {
                        "id": "(Kazemi et al., 2023)",
                        "snippets": [
                            "Automated reasoning with unstructured natural text is a key requirement for many potential applications of NLP and for developing robust AI systems. Recently, Language Models (LMs) have demonstrated complex reasoning capacities even without any finetuning. However, existing evaluation for automated reasoning assumes access to a consistent and coherent set of information over which models reason. When reasoning in the real-world, the available information is frequently inconsistent or contradictory, and therefore models need to be equipped with a strategy to resolve such conflicts when they arise. One widely-applicable way of resolving conflicts is to impose preferences over information sources (e.g., based on source credibility or information recency) and adopt the source with higher preference. In this paper, we formulate the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of defeasible reasoning, and develop a dataset called BoardgameQA for measuring the reasoning capacity of LMs in this setting. BoardgameQA also incorporates reasoning with implicit background knowledge, to better reflect reasoning problems in downstream applications. We benchmark various LMs on BoardgameQA and the results reveal a significant gap in the reasoning capacity of state-of-the-art LMs on this problem, showing that reasoning with conflicting information does not surface out-of-the-box in LMs. While performance can be improved with finetuning, it nevertheless remains poor."
                        ],
                        "paper": {
                            "corpus_id": 259144942,
                            "title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information",
                            "authors": [
                                {
                                    "authorId": "2173102324",
                                    "name": "Mehran Kazemi"
                                },
                                {
                                    "authorId": "2117782275",
                                    "name": "Quan Yuan"
                                },
                                {
                                    "authorId": "2055505691",
                                    "name": "Deepti Bhatia"
                                },
                                {
                                    "authorId": "8756748",
                                    "name": "Najoung Kim"
                                },
                                {
                                    "authorId": null,
                                    "name": "Xin Xu"
                                },
                                {
                                    "authorId": "2410945",
                                    "name": "Vaiva Imbrasaite"
                                },
                                {
                                    "authorId": "143812128",
                                    "name": "Deepak Ramachandran"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 50
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jiang et al., 2024)",
                        "snippets": [
                            "In this paper, we propose a new data synthesis method called \\textbf{LogicPro}, which leverages LeetCode-style algorithm \\underline{Pro}blems and their corresponding \\underline{Pro}gram solutions to synthesize Complex \\underline{Logic}al Reasoning data in text format. First, we synthesize complex reasoning problems through source algorithm problems and test cases. Then, standard answers and intermediate variable outputs are obtained for each problem based on standard python solutions and test cases. Finally, with the guidance of code intermediate variables, we synthesize the text reasoning process for each reasoning problems. Through this method, we can synthesize data that is difficult, scalable, effective, and comes with golden standard answers and high-quality reasoning processes."
                        ],
                        "paper": {
                            "corpus_id": 272753174,
                            "title": "LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning",
                            "authors": [
                                {
                                    "authorId": "2319385003",
                                    "name": "Jin Jiang"
                                },
                                {
                                    "authorId": "2284984220",
                                    "name": "Yuchen Yan"
                                },
                                {
                                    "authorId": "2316670312",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "2321879179",
                                    "name": "Yonggang Jin"
                                },
                                {
                                    "authorId": "2072715089",
                                    "name": "Shuai Peng"
                                },
                                {
                                    "authorId": "2284132141",
                                    "name": "Mengdi Zhang"
                                },
                                {
                                    "authorId": "2317059694",
                                    "name": "Xunliang Cai"
                                },
                                {
                                    "authorId": "2319459835",
                                    "name": "Yixin Cao"
                                },
                                {
                                    "authorId": "2165982472",
                                    "name": "Liangcai Gao"
                                },
                                {
                                    "authorId": "2268046262",
                                    "name": "Zhi Tang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.8291015625
                    },
                    {
                        "id": "(Xie et al., 2024)",
                        "snippets": [
                            "Logical reasoning benchmarks. To evaluate logical reasoning capabilities in LLMs, synthetic benchmarks have been developed to enable scalable generation of samples with varying configurations and difficulty levels (Clark et al., 2020)(Giadikiaroglou et al., 2024)(Parmar et al., 2024). For instance, DyVal (Zhu et al., 2024) uses directed acyclic graphs to dynamically generate samples on reasoning tasks including deductive, Boolean, and abductive reasoning. (Chen et al., 2024) focus on propositional logical problems involving definite clauses, and synthetically generate variations with different premise orders, such as forward, backward, and shuffled. (Dziri et al., 2024) explore the limitations of LLMs in tasks requiring compositional reasoning, including multiplication, logic grid puzzles, and dynamic programming problems. ZebraLogic (Lin et al., 2024) is an extended benchmark that systematically tests logical reasoning capabilities. BoardgameQA (Kazemi et al., 2023) presents a question-answering dataset characterized by contradictory facts and rules in the questions. PRONTOQA (Saparov & He, 2023) is a synthetic question-answering dataset where each example is generated from a synthetic world model represented in first-order logic. This dataset enables parsing the generated chain of thought into symbolic proofs, facilitating formal analysis. TruthQuest (Mondorf & Plank, 2024) is the most similar task to our work, which provides evaluation samples based on K&K-type of puzzles involving 3-6 person."
                        ],
                        "paper": {
                            "corpus_id": 273695832,
                            "title": "On Memorization of Large Language Models in Logical Reasoning",
                            "authors": [
                                {
                                    "authorId": "2308424593",
                                    "name": "Chulin Xie"
                                },
                                {
                                    "authorId": "2283305597",
                                    "name": "Yangsibo Huang"
                                },
                                {
                                    "authorId": "2309481623",
                                    "name": "Chiyuan Zhang"
                                },
                                {
                                    "authorId": "2290010050",
                                    "name": "Da Yu"
                                },
                                {
                                    "authorId": "2328442132",
                                    "name": "Xinyun Chen"
                                },
                                {
                                    "authorId": "2328296250",
                                    "name": "Bill Yuchen Lin"
                                },
                                {
                                    "authorId": "2290141407",
                                    "name": "Bo Li"
                                },
                                {
                                    "authorId": "2529354",
                                    "name": "Badih Ghazi"
                                },
                                {
                                    "authorId": "2237408905",
                                    "name": "Ravi Kumar"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 41
                        },
                        "score": 0.82080078125
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark."
                        ],
                        "paper": {
                            "corpus_id": 267657940,
                            "title": "Premise Order Matters in Reasoning with Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2238263119",
                                    "name": "Xinyun Chen"
                                },
                                {
                                    "authorId": "2284066085",
                                    "name": "Ryan A. Chi"
                                },
                                {
                                    "authorId": "2238394232",
                                    "name": "Xuezhi Wang"
                                },
                                {
                                    "authorId": "2256313467",
                                    "name": "Denny Zhou"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 32
                        },
                        "score": 0
                    },
                    {
                        "id": "(Parmar et al., 2024)",
                        "snippets": [
                            "Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really\"reason\"over the natural language? This question has been receiving significant research attention and many reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to 'logical reasoning' has remained underexplored. Existing work investigating this reasoning ability of LLMs has focused only on a couple of inference rules (such as modus ponens and modus tollens) of propositional and first-order logic. Addressing the above limitation, we comprehensively evaluate the logical reasoning ability of LLMs on 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics. To enable systematic evaluation, we introduce LogicBench, a natural language question-answering dataset focusing on the use of a single inference rule. We conduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini, Llama-2, and Mistral using chain-of-thought prompting. Experimental results show that existing LLMs do not fare well on LogicBench; especially, they struggle with instances involving complex reasoning and negations. Furthermore, they sometimes overlook contextual information necessary for reasoning to arrive at the correct conclusion. We believe that our work and findings facilitate future research for evaluating and enhancing the logical reasoning ability of LLMs. Data and code are available at https://github.com/Mihir3009/LogicBench."
                        ],
                        "paper": {
                            "corpus_id": 269330143,
                            "title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "1423660254",
                                    "name": "Mihir Parmar"
                                },
                                {
                                    "authorId": "2218094729",
                                    "name": "Nisarg Patel"
                                },
                                {
                                    "authorId": "2067056655",
                                    "name": "Neeraj Varshney"
                                },
                                {
                                    "authorId": "2287764",
                                    "name": "Mutsumi Nakamura"
                                },
                                {
                                    "authorId": "145779426",
                                    "name": "Man Luo"
                                },
                                {
                                    "authorId": "2219861482",
                                    "name": "Santosh Mashetty"
                                },
                                {
                                    "authorId": "2146720788",
                                    "name": "Arindam Mitra"
                                },
                                {
                                    "authorId": "2064619864",
                                    "name": "Chitta Baral"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 31
                        },
                        "score": 0
                    },
                    {
                        "id": "(Nie et al., 2019)",
                        "snippets": [
                            "We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate."
                        ],
                        "paper": {
                            "corpus_id": 207756753,
                            "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding",
                            "authors": [
                                {
                                    "authorId": "40383658",
                                    "name": "Yixin Nie"
                                },
                                {
                                    "authorId": "81840293",
                                    "name": "Adina Williams"
                                },
                                {
                                    "authorId": "31461304",
                                    "name": "Emily Dinan"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                },
                                {
                                    "authorId": "145183709",
                                    "name": "J. Weston"
                                },
                                {
                                    "authorId": "1743722",
                                    "name": "Douwe Kiela"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 1011
                        },
                        "score": 0
                    },
                    {
                        "id": "(Pan et al., 2023)",
                        "snippets": [
                            "Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver's error messages to revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant performance boost of 39.2% over using LLM alone with standard prompting and 18.4% over LLM with chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning. Code and data are publicly available at https://github.com/teacherpeterpan/Logic-LLM."
                        ],
                        "paper": {
                            "corpus_id": 258833332,
                            "title": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning",
                            "authors": [
                                {
                                    "authorId": "3470231",
                                    "name": "Liangming Pan"
                                },
                                {
                                    "authorId": "2044198106",
                                    "name": "Alon Albalak"
                                },
                                {
                                    "authorId": "2115553132",
                                    "name": "Xinyi Wang"
                                },
                                {
                                    "authorId": "1682479",
                                    "name": "William Yang Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 270
                        },
                        "score": 0
                    },
                    {
                        "id": "(Dalvi et al., 2021)",
                        "snippets": [
                            "Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a \"rationale\"). If this could be done, new opportunities for understanding and debugging the system\u2019s reasoning become possible. Our approach is to generate explanations in the form of entailment trees, namely a tree of multipremise entailment steps from facts that are known, through intermediate conclusions, to the hypothesis of interest (namely the question + answer). To train a model with this skill, we created ENTAILMENTBANK, the first dataset to contain multistep entailment trees. Given a hypothesis (question + answer), we define three increasingly difficult explanation tasks: generate a valid entailment tree given (a) all relevant sentences (b) all relevant and some irrelevant sentences, or (c) a corpus. We show that a strong language model can partially solve these tasks, in particular when the relevant sentences are included in the input (e.g., 35% of trees for (a) are perfect), and with indications of generalization to other domains. This work is significant as it provides a new type of dataset (multistep entailments) and baselines, offering a new avenue for the community to generate richer, more systematic explanations."
                        ],
                        "paper": {
                            "corpus_id": 233297051,
                            "title": "Explaining Answers with Entailment Trees",
                            "authors": [
                                {
                                    "authorId": "40135250",
                                    "name": "Bhavana Dalvi"
                                },
                                {
                                    "authorId": "144949918",
                                    "name": "Peter Alexander Jansen"
                                },
                                {
                                    "authorId": "3385516",
                                    "name": "Oyvind Tafjord"
                                },
                                {
                                    "authorId": "9746804",
                                    "name": "Zhengnan Xie"
                                },
                                {
                                    "authorId": "2110645880",
                                    "name": "Hannah Smith"
                                },
                                {
                                    "authorId": "2078502192",
                                    "name": "Leighanna Pipatanangkura"
                                },
                                {
                                    "authorId": "48323507",
                                    "name": "Peter Clark"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 185
                        },
                        "score": 0
                    },
                    {
                        "id": "(Saeed et al., 2021)",
                        "snippets": [
                            "While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems, they are still very limited in their ability to capture and to use common-sense knowledge. In fact, even if information is available in the form of approximate (soft) logical rules, it is not clear how to transfer it to a PLM in order to improve its performance for deductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how to reason with soft Horn rules. We introduce a classification task where, given facts and soft rules, the PLM should return a prediction with a probability for a given hypothesis. We release the first dataset for this task, and we propose a revised loss function that enables the PLM to learn how to predict precise probabilities for the task. Our evaluation results show that the resulting fine-tuned models achieve very high performance, even on logical rules that were unseen at training. Moreover, we demonstrate that logical notions expressed by the rules are transferred to the fine-tuned model, yielding state-of-the-art results on external datasets."
                        ],
                        "paper": {
                            "corpus_id": 237940562,
                            "title": "RuleBERT: Teaching Soft Rules to Pre-Trained Language Models",
                            "authors": [
                                {
                                    "authorId": "2073358417",
                                    "name": "Mohammed Saeed"
                                },
                                {
                                    "authorId": "84069926",
                                    "name": "N. Ahmadi"
                                },
                                {
                                    "authorId": "1683562",
                                    "name": "Preslav Nakov"
                                },
                                {
                                    "authorId": "1802817",
                                    "name": "Paolo Papotti"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 33
                        },
                        "score": 0
                    },
                    {
                        "id": "(Bao et al., 2023)",
                        "snippets": [
                            "In this paper, we performed contrastive learning for discriminative large language models on sentences augmented from a synthetic dataset. This dataset contains 14,962 sentences with different combinations of 23 entities, 2 relations and 40 attributes. Synthetic data was used to generate more controllable logical sentences."
                        ],
                        "paper": {
                            "corpus_id": 258833579,
                            "title": "Abstract Meaning Representation-Based Logic-Driven Data Augmentation for Logical Reasoning",
                            "authors": [
                                {
                                    "authorId": "1491516930",
                                    "name": "Qiming Bao"
                                },
                                {
                                    "authorId": "2064347311",
                                    "name": "A. Peng"
                                },
                                {
                                    "authorId": "2244794",
                                    "name": "Zhenyun Deng"
                                },
                                {
                                    "authorId": "81970097",
                                    "name": "Wanjun Zhong"
                                },
                                {
                                    "authorId": "2151063121",
                                    "name": "N. Tan"
                                },
                                {
                                    "authorId": "2159676415",
                                    "name": "Nathan Young"
                                },
                                {
                                    "authorId": "2144353312",
                                    "name": "Yang Chen"
                                },
                                {
                                    "authorId": "2116513959",
                                    "name": "Yonghua Zhu"
                                },
                                {
                                    "authorId": "2582677",
                                    "name": "Michael Witbrock"
                                },
                                {
                                    "authorId": "2108326935",
                                    "name": "Jiamou Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 4
                        },
                        "score": 0.86328125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Multi-Step Question Answering Approaches",
                "tldr": "Researchers have developed diverse synthetic data generation methods for multi-step question answering, including template-based approaches, neurosymbolic techniques, and self-training frameworks that leverage semi-structured data to create examples requiring complex reasoning chains. (17 sources)",
                "text": "\nMulti-step question answering presents a significant challenge in NLP, requiring models to perform complex reasoning across multiple facts or inferences. To address this challenge, researchers have developed various synthetic data generation approaches. Early pioneering efforts include the bAbI dataset, which uses random simulation to generate facts, questions, and answers for training models on simple logical reasoning tasks <Paper corpusId=\"3178759\" paperTitle=\"(Weston et al., 2015)\" isShortName></Paper>. Similarly, Hermann et al. developed methods to automatically generate reading comprehension questions from news-summarization pairs <Paper corpusId=\"6203757\" paperTitle=\"(Hermann et al., 2015)\" isShortName></Paper>, while Serban et al. used RNNs to generate 30 million factoid questions about Freebase for training question-answering systems <Paper corpusId=\"12241221\" paperTitle=\"(Serban et al., 2016)\" isShortName></Paper> <Paper corpusId=\"10817864\" paperTitle=\"(Wang et al., 2016)\" isShortName></Paper>.\n\nSemi-structured data has emerged as a valuable resource for generating complex reasoning examples. Yoran et al. proposed leveraging semi-structured tables to automatically generate question-paragraph pairs at scale, where answering requires reasoning over multiple facts <Paper corpusId=\"235898880\" paperTitle=\"(Yoran et al., 2021)\" isShortName></Paper>. Building on this approach, Dua et al. developed a method to synthetically generate complex questions and their decompositions using Wikipedia tables, creating data for ten simple operations (COUNT, FILTER, SUM, etc.) and their higher-order combinations <Paper corpusId=\"254408974\" paperTitle=\"(Dua et al., 2022)\" isShortName></Paper>. Li et al. presented a self-training framework for unsupervised complex tabular reasoning that generates diverse synthetic data with complex logic through program execution on tables and transformation into natural language sentences <Paper corpusId=\"254877131\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>.\n\nRecent approaches have focused on generating synthetic reasoning chains. Trivedi et al. developed TeaBReaC, a pretraining dataset containing 525K multi-step questions with associated formal programs covering about 900 reasoning patterns, carefully designed to avoid common reasoning shortcuts <Paper corpusId=\"253370208\" paperTitle=\"(Trivedi et al., 2022)\" isShortName></Paper>. Similarly, MuSR introduced a neurosymbolic synthetic-to-natural generation algorithm for creating complex reasoning instances, such as murder mysteries approximately 1,000 words in length that challenge even advanced models like GPT-4 <Paper corpusId=\"264439655\" paperTitle=\"(Sprague et al., 2023)\" isShortName></Paper>.\n\nSeveral frameworks leverage existing models to generate synthetic data. CONDA reformulates data generation as context generation for given question-answer pairs and uses QA datasets for training context generators, finding that datasets requiring high-level reasoning abilities tend to provide the best performance boost <Paper corpusId=\"253107930\" paperTitle=\"(Mekala et al., 2022)\" isShortName></Paper>. Automate-CoT proposes a fully automatic pipeline that uses powerful language models to augment reasoning paths, prune incorrect paths, and select optimal combinations of exemplars <Paper corpusId=\"268249187\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>. Similarly, Source2Synth produces synthetic data points with intermediate reasoning steps grounded in real-world sources, improving performance by 22.57% for multi-hop question answering on HotPotQA compared to fine-tuned baselines <Paper corpusId=\"272600015\" paperTitle=\"(Lupidi et al., 2024)\" isShortName></Paper>.\n\nSpecialized reasoning strategies have also been explored. Self-Ask introduced an LLM-based self-asking strategy where models recursively break down complex questions into simpler sub-questions, demonstrating improved reasoning performance on multi-step inference tasks <Paper corpusId=\"277104955\" paperTitle=\"(Nadas et al., 2025)\" isShortName></Paper>. Step-Wise Reinforcement Learning (SWiRL) iteratively generates multi-step reasoning data and learns from it by breaking each multi-step trajectory into multiple sub-trajectories, showing significant performance improvements on various multi-step tasks including HotPotQA and MuSiQue <Paper corpusId=\"277622127\" paperTitle=\"(Goldie et al., 2025)\" isShortName></Paper>. For non-mathematical domains, Zeng et al. devised a pipeline to generate synthetic reasoning chains-of-thought from existing question-answering data, with step-wise labels based on reasoning correctness <Paper corpusId=\"276250232\" paperTitle=\"(Zeng et al., 2025)\" isShortName></Paper>.\n\nDomain-specific applications have also been explored, such as geometric reasoning in the context of question-answering. Goyal et al. introduced the Dynamic Spatial Memory Network and proposed two synthetic benchmarks, FloorPlanQA and ShapeIntersection, to evaluate geometric reasoning capabilities <Paper corpusId=\"44096233\" paperTitle=\"(Goyal et al., 2018)\" isShortName></Paper>. Similarly, COM2 created a dataset by sampling multi-hop logical queries from an existing commonsense knowledge graph and verbalizing them using handcrafted rules and large language models <Paper corpusId=\"268363344\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Weston et al., 2015)",
                        "snippets": [
                            "One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks."
                        ],
                        "paper": {
                            "corpus_id": 3178759,
                            "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks",
                            "authors": [
                                {
                                    "authorId": "145183709",
                                    "name": "J. Weston"
                                },
                                {
                                    "authorId": "1713934",
                                    "name": "Antoine Bordes"
                                },
                                {
                                    "authorId": "3295092",
                                    "name": "S. Chopra"
                                },
                                {
                                    "authorId": "2047446108",
                                    "name": "Tomas Mikolov"
                                }
                            ],
                            "year": 2015,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 1182
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hermann et al., 2015)",
                        "snippets": [
                            "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure."
                        ],
                        "paper": {
                            "corpus_id": 6203757,
                            "title": "Teaching Machines to Read and Comprehend",
                            "authors": [
                                {
                                    "authorId": "2910877",
                                    "name": "Karl Moritz Hermann"
                                },
                                {
                                    "authorId": "2367821",
                                    "name": "Tom\u00e1s Kocisk\u00fd"
                                },
                                {
                                    "authorId": "1864353",
                                    "name": "Edward Grefenstette"
                                },
                                {
                                    "authorId": "2311318",
                                    "name": "L. Espeholt"
                                },
                                {
                                    "authorId": "2062879616",
                                    "name": "W. Kay"
                                },
                                {
                                    "authorId": "2573615",
                                    "name": "Mustafa Suleyman"
                                },
                                {
                                    "authorId": "1685771",
                                    "name": "Phil Blunsom"
                                }
                            ],
                            "year": 2015,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 3553
                        },
                        "score": 0
                    },
                    {
                        "id": "(Serban et al., 2016)",
                        "snippets": [
                            "Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale question-answer corpora available. In this paper we present the 30M Factoid Question-Answer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the question-generation model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear comparable in quality to real human-generated questions."
                        ],
                        "paper": {
                            "corpus_id": 12241221,
                            "title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus",
                            "authors": [
                                {
                                    "authorId": "35224828",
                                    "name": "Iulian Serban"
                                },
                                {
                                    "authorId": "1405061488",
                                    "name": "Alberto Garc\u00eda-Dur\u00e1n"
                                },
                                {
                                    "authorId": "1854385",
                                    "name": "\u00c7aglar G\u00fcl\u00e7ehre"
                                },
                                {
                                    "authorId": "3103594",
                                    "name": "Sungjin Ahn"
                                },
                                {
                                    "authorId": "144631588",
                                    "name": "A. Chandar"
                                },
                                {
                                    "authorId": "1760871",
                                    "name": "Aaron C. Courville"
                                },
                                {
                                    "authorId": "1751762",
                                    "name": "Yoshua Bengio"
                                }
                            ],
                            "year": 2016,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 288
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2016)",
                        "snippets": [
                            "Synthetic datasets have also arisen recently for semantic tasks in natural language processing. bAbI is a dataset of facts, questions, and answers, generated by random simulation, for training machines to do simple logic (Weston et al., 2015)(Hermann et al., 2015) generate reading comprehension questions and their answers, based on a large set of newssummarization pairs, for training machine readers. (Serban et al., 2016) used RNNs to generate 30 million factoid questions about Freebase, with answers, for training question-answering systems. (Wang et al., 2015) obtain data to train semantic parsers in a new domain by first generating synthetic (utterance, logical form) pairs and then asking human annotators to paraphrase the synthetic utterances into more natural human language."
                        ],
                        "paper": {
                            "corpus_id": 10817864,
                            "title": "The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages",
                            "authors": [
                                {
                                    "authorId": "2099597",
                                    "name": "D. Wang"
                                },
                                {
                                    "authorId": "145043214",
                                    "name": "Jason Eisner"
                                }
                            ],
                            "year": 2016,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 58
                        },
                        "score": 0.81884765625
                    },
                    {
                        "id": "(Yoran et al., 2021)",
                        "snippets": [
                            "In this work, we propose to leverage semi-structured tables, and automatically generate at scale question-paragraph pairs, where answering the question requires reasoning over multiple facts in the paragraph. We add a pre-training step over this synthetic data, which includes examples that require 16 different reasoning skills such as number comparison, conjunction, and fact composition. To improve data efficiency, we sample examples from reasoning skills where the model currently errs."
                        ],
                        "paper": {
                            "corpus_id": 235898880,
                            "title": "Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills",
                            "authors": [
                                {
                                    "authorId": "2074100429",
                                    "name": "Ori Yoran"
                                },
                                {
                                    "authorId": "12371246",
                                    "name": "Alon Talmor"
                                },
                                {
                                    "authorId": "1750652",
                                    "name": "Jonathan Berant"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 54
                        },
                        "score": 0.85302734375
                    },
                    {
                        "id": "(Dua et al., 2022)",
                        "snippets": [
                            "To mitigate these data issues, we propose a way to synthetically generate complex questions and their decompositions using semi-structured data which is easy to parse. We show that we can bootstrap model learning with this out-of-domain, synthetically generated data so it can adapt better when fine-tuned with limited in-domain supervision.\n\nGeneration Process: Inspired by Yoran et al. (2021), we use semi-structured data from tables in English Wikipedia which are available in plenty. We employ curated templates to convert the rows in the tables into paragraphs. We use single column headers to create first order simple questions and a combination of columns for higher order complex questions. We synthesize data for 10 simple operations: COUNT, TOP(k), BOTTOM(k), FILTER, SUM, COMPARISON, DIFFERENCE, NEGATION, GATHER, and INTERSECTION.\n\nWe generate higher order combinations of firstorder operations, wherever possible."
                        ],
                        "paper": {
                            "corpus_id": 254408974,
                            "title": "Successive Prompting for Decomposing Complex Questions",
                            "authors": [
                                {
                                    "authorId": "33546336",
                                    "name": "Dheeru Dua"
                                },
                                {
                                    "authorId": "1698760333",
                                    "name": "Shivanshu Gupta"
                                },
                                {
                                    "authorId": "34650964",
                                    "name": "Sameer Singh"
                                },
                                {
                                    "authorId": "40642935",
                                    "name": "Matt Gardner"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 118
                        },
                        "score": 0.861328125
                    },
                    {
                        "id": "(Li et al., 2022)",
                        "snippets": [
                            "To address the insufficient annotation challenge, we present a self-training framework for unsupervised complex tabular reasoning (UCTR-ST) by generating diverse synthetic data with complex logic. Specifically, UCTR-ST incorporates several essential techniques: we aggregate diverse programs and execute them on tables based on a \"Program-Management\" component, and we bridge the gap between programs and text with a powerful \"Program-Transformation\" module that generates natural language sentences with complex logic."
                        ],
                        "paper": {
                            "corpus_id": 254877131,
                            "title": "Optimization Techniques for Unsupervised Complex Table Reasoning via Self-Training Framework",
                            "authors": [
                                {
                                    "authorId": "2155354022",
                                    "name": "Zhenyu Li"
                                },
                                {
                                    "authorId": "2116521868",
                                    "name": "Xiuxing Li"
                                },
                                {
                                    "authorId": "2272781120",
                                    "name": "Sunqi Fan"
                                },
                                {
                                    "authorId": "2115642141",
                                    "name": "Jianyong Wang"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Transactions on Knowledge and Data Engineering",
                            "n_citations": 6
                        },
                        "score": 0.88037109375
                    },
                    {
                        "id": "(Trivedi et al., 2022)",
                        "snippets": [
                            "In this work, we show how to use question decompositions to teach language models these broad reasoning skills in a robust fashion. Specifically, we use widely available QDMR representations to programmatically create hard-to-cheat synthetic contexts for real questions in six multi-step reasoning datasets. These contexts are carefully designed to avoid common reasoning shortcuts prevalent in real contexts that prevent models from learning the right skills. This results in a pretraining dataset, named TeaBReaC, containing 525K multi-step questions (with associated formal programs) covering about 900 reasoning patterns."
                        ],
                        "paper": {
                            "corpus_id": 253370208,
                            "title": "Teaching Broad Reasoning Skills for Multi-Step QA by Generating Hard Contexts",
                            "authors": [
                                {
                                    "authorId": "6365809",
                                    "name": "H. Trivedi"
                                },
                                {
                                    "authorId": "35217367",
                                    "name": "Niranjan Balasubramanian"
                                },
                                {
                                    "authorId": "2236429",
                                    "name": "Tushar Khot"
                                },
                                {
                                    "authorId": "48229640",
                                    "name": "Ashish Sabharwal"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 11
                        },
                        "score": 0.93310546875
                    },
                    {
                        "id": "(Sprague et al., 2023)",
                        "snippets": [
                            "We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released."
                        ],
                        "paper": {
                            "corpus_id": 264439655,
                            "title": "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning",
                            "authors": [
                                {
                                    "authorId": "2150345078",
                                    "name": "Zayne Sprague"
                                },
                                {
                                    "authorId": "50183897",
                                    "name": "Xi Ye"
                                },
                                {
                                    "authorId": "1620502132",
                                    "name": "Kaj Bostrom"
                                },
                                {
                                    "authorId": "35865989",
                                    "name": "Swarat Chaudhuri"
                                },
                                {
                                    "authorId": "1814094",
                                    "name": "Greg Durrett"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 78
                        },
                        "score": 0.810546875
                    },
                    {
                        "id": "(Mekala et al., 2022)",
                        "snippets": [
                            "In this work, we propose CONDA, an approach to further improve GLM's ability to generate synthetic data by reformulating data generation as context generation for a given question-answer (QA) pair and leveraging QA datasets for training context generators. Then, we cast downstream tasks into the same question answering format and adapt the fine-tuned context generators to the target task domain. Finally, we use the fine-tuned GLM to generate relevant contexts, which are in turn used as synthetic training data for their corresponding tasks",
                            "Our analysis reveals that QA datasets that require high-level reasoning abilities (e.g., abstractive and common-sense QA datasets) tend to give the best boost in performance in both few-shot and zero-shot settings."
                        ],
                        "paper": {
                            "corpus_id": 253107930,
                            "title": "Leveraging QA Datasets to Improve Generative Data Augmentation",
                            "authors": [
                                {
                                    "authorId": "7565696",
                                    "name": "Dheeraj Mekala"
                                },
                                {
                                    "authorId": "144244743",
                                    "name": "Tu Vu"
                                },
                                {
                                    "authorId": "32246932",
                                    "name": "Timo Schick"
                                },
                                {
                                    "authorId": "2884976",
                                    "name": "Jingbo Shang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 18
                        },
                        "score": 0.95458984375
                    },
                    {
                        "id": "(Ding et al., 2024)",
                        "snippets": [
                            "Automate-CoT (Shum et al., 2023) proposes a fully automatic pipeline for finding better chainof-thought prompts to solve commonsense reasoning question answering tasks. This pipeline leverages powerful LLMs like GPT-3 to augment reasoning paths, prune incorrect paths, and select optimal combinations of exemplars."
                        ],
                        "paper": {
                            "corpus_id": 268249187,
                            "title": "Data Augmentation using Large Language Models: Data Perspectives, Learning Paradigms and Challenges",
                            "authors": [
                                {
                                    "authorId": "2064493724",
                                    "name": "Bosheng Ding"
                                },
                                {
                                    "authorId": "2084609980",
                                    "name": "Chengwei Qin"
                                },
                                {
                                    "authorId": "2091437375",
                                    "name": "Ruochen Zhao"
                                },
                                {
                                    "authorId": "2290030532",
                                    "name": "Tianze Luo"
                                },
                                {
                                    "authorId": "2290023354",
                                    "name": "Xinze Li"
                                },
                                {
                                    "authorId": "2290026474",
                                    "name": "Guizhen Chen"
                                },
                                {
                                    "authorId": "2276610995",
                                    "name": "Wenhan Xia"
                                },
                                {
                                    "authorId": "2290145274",
                                    "name": "Junjie Hu"
                                },
                                {
                                    "authorId": "1755919",
                                    "name": "A. Luu"
                                },
                                {
                                    "authorId": "2708940",
                                    "name": "Shafiq R. Joty"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 20
                        },
                        "score": 0.77392578125
                    },
                    {
                        "id": "(Lupidi et al., 2024)",
                        "snippets": [
                            "Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines."
                        ],
                        "paper": {
                            "corpus_id": 272600015,
                            "title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources",
                            "authors": [
                                {
                                    "authorId": "2223643390",
                                    "name": "A. Lupidi"
                                },
                                {
                                    "authorId": "2320807392",
                                    "name": "Carlos Gemmell"
                                },
                                {
                                    "authorId": "2313189469",
                                    "name": "Nicola Cancedda"
                                },
                                {
                                    "authorId": "2173509991",
                                    "name": "Jane Dwivedi-Yu"
                                },
                                {
                                    "authorId": "2267341626",
                                    "name": "Jason E Weston"
                                },
                                {
                                    "authorId": "2320805323",
                                    "name": "Jakob Foerster"
                                },
                                {
                                    "authorId": "48647153",
                                    "name": "R. Raileanu"
                                },
                                {
                                    "authorId": "2253400960",
                                    "name": "Maria Lomeli"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.9638671875
                    },
                    {
                        "id": "(Nadas et al., 2025)",
                        "snippets": [
                            "Self-Ask [47] Multi-hop Question Answering (QA), Reasoning \n\nIntroduced an LLM-based self-asking strategy where models recursively break down complex questions into simpler sub-questions. Demonstrated improved reasoning performance on multi-step inference tasks by leveraging self-generated sub-questions for enhanced accuracy.\n\nCoT [13] Scientific QA, Logical Reasoning \n\nExamined how Chain-of-Thought (CoT) prompting improves LLM reasoning capabilities. Proposed Sci-CoT, a two-stage framework where intermediate rationales are generated separately from answer inference, improving efficiency and model performance."
                        ],
                        "paper": {
                            "corpus_id": 277104955,
                            "title": "Synthetic Data Generation Using Large Language Models: Advances in Text and Code",
                            "authors": [
                                {
                                    "authorId": "2350754914",
                                    "name": "Mihai Nadas"
                                },
                                {
                                    "authorId": "2306585020",
                                    "name": "Laura Dio\u015fan"
                                },
                                {
                                    "authorId": "2350756350",
                                    "name": "Andreea Tomescu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.94775390625
                    },
                    {
                        "id": "(Goldie et al., 2025)",
                        "snippets": [
                            "We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively."
                        ],
                        "paper": {
                            "corpus_id": 277622127,
                            "title": "Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use",
                            "authors": [
                                {
                                    "authorId": "46684455",
                                    "name": "Anna Goldie"
                                },
                                {
                                    "authorId": "1861312",
                                    "name": "Azalia Mirhoseini"
                                },
                                {
                                    "authorId": "2354343696",
                                    "name": "Hao Zhou"
                                },
                                {
                                    "authorId": "2354178736",
                                    "name": "Irene Cai"
                                },
                                {
                                    "authorId": "2262643057",
                                    "name": "Christopher D. Manning"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 11
                        },
                        "score": 0.91943359375
                    },
                    {
                        "id": "(Zeng et al., 2025)",
                        "snippets": [
                            "In order to obtain step-wise reasoning data for non-Math domains, we devise a pipeline, as outlined in Figure 2, to generate synthetic reasoning CoTs from existing questionanswering data. These CoTs are then given step-wise labels based on reasoning correctness. We detail the synthetic data generation process in Sections 5.1 and 5.2, including methods to create and annotate reasoning steps."
                        ],
                        "paper": {
                            "corpus_id": 276250232,
                            "title": "VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data",
                            "authors": [
                                {
                                    "authorId": "2323820473",
                                    "name": "Thomas Zeng"
                                },
                                {
                                    "authorId": "2190935184",
                                    "name": "Shuibai Zhang"
                                },
                                {
                                    "authorId": "2344794414",
                                    "name": "Shutong Wu"
                                },
                                {
                                    "authorId": "2344760610",
                                    "name": "Christian Classen"
                                },
                                {
                                    "authorId": "2253659910",
                                    "name": "Daewon Chae"
                                },
                                {
                                    "authorId": "2323781863",
                                    "name": "Ethan Ewer"
                                },
                                {
                                    "authorId": "2345324748",
                                    "name": "Minjae Lee"
                                },
                                {
                                    "authorId": "2344831673",
                                    "name": "Heeju Kim"
                                },
                                {
                                    "authorId": "2282472476",
                                    "name": "Wonjun Kang"
                                },
                                {
                                    "authorId": "2334358539",
                                    "name": "Jackson Kunde"
                                },
                                {
                                    "authorId": "2344807639",
                                    "name": "Ying Fan"
                                },
                                {
                                    "authorId": "2344788904",
                                    "name": "Jungtaek Kim"
                                },
                                {
                                    "authorId": "2282471620",
                                    "name": "Hyung Il Koo"
                                },
                                {
                                    "authorId": "144161012",
                                    "name": "K. Ramchandran"
                                },
                                {
                                    "authorId": "107579958",
                                    "name": "Dimitris Papailiopoulos"
                                },
                                {
                                    "authorId": "2323790154",
                                    "name": "Kangwook Lee"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.90185546875
                    },
                    {
                        "id": "(Goyal et al., 2018)",
                        "snippets": [
                            "In this paper, we study the problem of geometric reasoning (a form of visual reasoning) in the context of question-answering. We introduce Dynamic Spatial Memory Network (DSMN), a new deep network architecture that specializes in answering questions that admit latent visual representations, and learns to generate and reason over such representations. Further, we propose two synthetic benchmarks, FloorPlanQA and ShapeIntersection, to evaluate the geometric reasoning capability of QA systems."
                        ],
                        "paper": {
                            "corpus_id": 44096233,
                            "title": "Think Visually: Question Answering through Virtual Imagery",
                            "authors": [
                                {
                                    "authorId": "47989608",
                                    "name": "Ankit Goyal"
                                },
                                {
                                    "authorId": "2152769770",
                                    "name": "Jian Wang"
                                },
                                {
                                    "authorId": "153302678",
                                    "name": "Jia Deng"
                                }
                            ],
                            "year": 2018,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 2
                        },
                        "score": 0.80419921875
                    },
                    {
                        "id": "(Fang et al., 2024)",
                        "snippets": [
                            "we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions."
                        ],
                        "paper": {
                            "corpus_id": 268363344,
                            "title": "Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs",
                            "authors": [
                                {
                                    "authorId": "2044202073",
                                    "name": "Tianqing Fang"
                                },
                                {
                                    "authorId": "2111435018",
                                    "name": "Zeming Chen"
                                },
                                {
                                    "authorId": "2241325169",
                                    "name": "Yangqiu Song"
                                },
                                {
                                    "authorId": "2691021",
                                    "name": "Antoine Bosselut"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 14
                        },
                        "score": 0.81787109375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Mathematical Reasoning Techniques",
                "tldr": "Synthetic data generation for mathematical reasoning employs diverse approaches including complexity augmentation, symbolic replacement, and knowledge-driven synthesis. These techniques leverage existing datasets through bootstrapping, self-verification, and backward-forward generation processes to create high-quality training examples with explicit reasoning steps. (9 sources)",
                "text": "\nMathematical reasoning represents a particularly challenging domain for language models, requiring precise calculations, logical deductions, and procedural problem-solving. To address these challenges, researchers have developed specialized synthetic data generation techniques. One prominent approach involves augmenting the complexity of existing mathematical problems. WizardMath leverages GPT-3.5 to increase the complexity of questions and answers, while MetaMath bootstraps questions from established datasets like MATH and GSM8K by rewriting them through semantic rephrasing, self-verification, and backward reasoning <Paper corpusId=\"269042851\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. Similarly, Wei et al. enhanced natural language datasets by replacing natural language labels with arbitrary symbols, generating over 500,000 synthetic examples that significantly improved model performance on unseen in-context learning and algorithmic reasoning tasks <Paper corpusId=\"269042851\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nSeveral studies have focused on generating high-quality intermediate reasoning steps to improve mathematical problem-solving. Recent approaches leverage extensive synthetic datasets either distilled from cutting-edge models or composed of self-generated rationales to fine-tune language models <Paper corpusId=\"270870071\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>. One notable technique is Synthetic prompting, which uses language models to generate synthetic examples that augment hand-crafted examples in few-shot settings. This method employs a two-step process: (1) a backward step where the model synthesizes a query based on a self-generated reasoning chain, and (2) a forward step where it generates a more accurate reasoning chain for the synthesized query <Paper corpusId=\"271270817\" paperTitle=\"(Vatsal et al., 2024)\" isShortName></Paper> <Paper corpusId=\"256459681\" paperTitle=\"(Shao et al., 2023)\" isShortName></Paper>. This approach has achieved up to 15.6% absolute gains on mathematical problem-solving, commonsense reasoning, and logical reasoning tasks <Paper corpusId=\"271270817\" paperTitle=\"(Vatsal et al., 2024)\" isShortName></Paper>.\n\nAutomated data synthesis frameworks have emerged to generate mathematical benchmarks without human annotations. ReSo, for example, has demonstrated impressive results, achieving 33.7% accuracy on Math-MAS and 32.3% accuracy on SciBench-MAS, outperforming existing methods that struggled with these challenging datasets <Paper corpusId=\"276768281\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper>. To address the scarcity of high-quality mathematical reasoning data, researchers have explored various synthesis methods. Mainstream approaches involve data augmentation based on existing annotated training sets like GSM8K and MATH <Paper corpusId=\"278171321\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"268247488\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>.\n\nRecent innovation has focused on enhancing data diversity beyond traditional augmentation techniques. The Key-Point-Driven Data Synthesis (KPDDS) framework synthesizes question-answer pairs by leveraging key points and exemplar practices from authentic data sources, resulting in KPMath, an extensive synthetic dataset with over 800,000 question-answer pairs <Paper corpusId=\"268247488\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>. Similarly, MathScale creates high-quality mathematical reasoning data using frontier language models by first extracting topics and knowledge points from seed math questions to build a concept graph, which is then used to generate new math questions. This approach has demonstrated effective scalability, resulting in MathScaleQA, a dataset containing two million math question-answer pairs <Paper corpusId=\"268247902\" paperTitle=\"(Tang et al., 2024)\" isShortName></Paper>.\n\nAnother promising direction involves evolutionary approaches to data generation. Auto Evol-Instruct proposes an end-to-end framework that evolves instruction datasets using large language models without human effort. The framework automatically analyzes and summarizes suitable evolutionary strategies for given instruction data and iteratively improves the evolving method based on issues exposed during the instruction evolution process <Paper corpusId=\"270213007\" paperTitle=\"(Zeng et al., 2024)\" isShortName></Paper>. Experiments demonstrate that methods optimized by Auto Evol-Instruct outperform human-designed methods on various benchmarks, including GSM8K <Paper corpusId=\"270213007\" paperTitle=\"(Zeng et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "WizardMath (Luo et al., 2023a) leverages a series of operations to increase the complexity of questions and answers using GPT-3.5, while MetaMath (Yu et al., 2023) bootstraps the questions in MATH and GSM8K by rewriting them in different ways, such as semantic rephrasing, self-verification, and backward reasoning",
                            "Wei et al. (2023a) augmented existing natural language datasets by replacing natural language labels with arbitrary symbols, generating over 500k synthetic examples. Using these synthetic data for supervised finetuning significantly improved model performance on unseen in-context learning and algorithmic-reasoning tasks."
                        ],
                        "paper": {
                            "corpus_id": 269042851,
                            "title": "Best Practices and Lessons Learned on Synthetic Data for Language Models",
                            "authors": [
                                {
                                    "authorId": "7247867",
                                    "name": "Ruibo Liu"
                                },
                                {
                                    "authorId": "2254934700",
                                    "name": "Jerry Wei"
                                },
                                {
                                    "authorId": "144097210",
                                    "name": "Fangyu Liu"
                                },
                                {
                                    "authorId": "2290019998",
                                    "name": "Chenglei Si"
                                },
                                {
                                    "authorId": "2121290295",
                                    "name": "Yanzhe Zhang"
                                },
                                {
                                    "authorId": "2238635148",
                                    "name": "Jinmeng Rao"
                                },
                                {
                                    "authorId": "2275574241",
                                    "name": "Steven Zheng"
                                },
                                {
                                    "authorId": "2283933546",
                                    "name": "Daiyi Peng"
                                },
                                {
                                    "authorId": "2254124342",
                                    "name": "Diyi Yang"
                                },
                                {
                                    "authorId": "2263221363",
                                    "name": "Denny Zhou"
                                },
                                {
                                    "authorId": "2253757717",
                                    "name": "Andrew M. Dai"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 96
                        },
                        "score": 0.89013671875
                    },
                    {
                        "id": "(He et al., 2024)",
                        "snippets": [
                            "To further enhance the accuracy of these intermediate steps, recent studies leverage extensive synthetic datasets, which are either distilled from cutting-edge models (Yu et al., 2023b;Luo et al., 2023) or composed of selfgenerated rationales (Zelikman et al., 2022;Yuan et al., 2023;Ni et al., 2022), to fine-tune the LLMs."
                        ],
                        "paper": {
                            "corpus_id": 270870071,
                            "title": "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning",
                            "authors": [
                                {
                                    "authorId": "2309218600",
                                    "name": "Mingqian He"
                                },
                                {
                                    "authorId": "1471660296",
                                    "name": "Yongliang Shen"
                                },
                                {
                                    "authorId": "2135282890",
                                    "name": "Wenqi Zhang"
                                },
                                {
                                    "authorId": "2092670555",
                                    "name": "Zeqi Tan"
                                },
                                {
                                    "authorId": "1776903",
                                    "name": "Weiming Lu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 7
                        },
                        "score": 0.7666015625
                    },
                    {
                        "id": "(Vatsal et al., 2024)",
                        "snippets": [
                            "The authors of (Shao et al., 2023) come up with Synthetic prompting using LLMs to generate synthetic examples which are augmented to the existing hand-crafted examples as seen in a conventional few-shot setting.This prompting method involves two steps: (1) the backward step, where the LLM synthesizes a query based on a self-generated reasoning chain; and (2) the forward step, where the LLM generates a reasoning chain for the synthesized query, making the reasoning chain to be more accurate.Finally, to choose the best examples, this work uses an in-cluster complexity and the most complex examples with the longest reasoning chains are used during inference.The results show Synthetic prompting achieving up to 15.6% absolute gains when experimented with different Mathematical Problem Solving, Commonsense Reasoning and Logical Reasoning task datasets."
                        ],
                        "paper": {
                            "corpus_id": 271270817,
                            "title": "A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks",
                            "authors": [
                                {
                                    "authorId": "1491705917",
                                    "name": "Shubham Vatsal"
                                },
                                {
                                    "authorId": "2311890970",
                                    "name": "Harsh Dubey"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 33
                        },
                        "score": 0.78369140625
                    },
                    {
                        "id": "(Shao et al., 2023)",
                        "snippets": [
                            "Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through step-by-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce Synthetic prompting, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process generates a question that match a sampled reasoning chain, so that the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. We evaluate our method on numerical, symbolic, and algorithmic reasoning tasks, and show that it outperforms existing prompting techniques."
                        ],
                        "paper": {
                            "corpus_id": 256459681,
                            "title": "Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "144485528",
                                    "name": "Zhihong Shao"
                                },
                                {
                                    "authorId": "2171182",
                                    "name": "Yeyun Gong"
                                },
                                {
                                    "authorId": "1752875",
                                    "name": "Yelong Shen"
                                },
                                {
                                    "authorId": "1730108",
                                    "name": "Minlie Huang"
                                },
                                {
                                    "authorId": "46429989",
                                    "name": "Nan Duan"
                                },
                                {
                                    "authorId": "2109136147",
                                    "name": "Weizhu Chen"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 70
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhou et al., 2025)",
                        "snippets": [
                            "We also introduce an automated data synthesis framework for generating MAS benchmarks without any human annotations. Experimental results show that ReSo matches or outperforms existing methods, achieving 33.7 percent accuracy on Math-MAS and 32.3 percent accuracy on SciBench-MAS, where other approaches completely fail."
                        ],
                        "paper": {
                            "corpus_id": 276768281,
                            "title": "ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for Reasoning Tasks",
                            "authors": [
                                {
                                    "authorId": "2348427354",
                                    "name": "Heng Zhou"
                                },
                                {
                                    "authorId": "2232782866",
                                    "name": "Hejia Geng"
                                },
                                {
                                    "authorId": "2320221475",
                                    "name": "Xiangyuan Xue"
                                },
                                {
                                    "authorId": "2348879787",
                                    "name": "Zhenfei Yin"
                                },
                                {
                                    "authorId": "2350152497",
                                    "name": "Lei Bai"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.87646484375
                    },
                    {
                        "id": "(Wang et al., 2025)",
                        "snippets": [
                            "To address this scarcity, researchers have explored various synthesizing methods, particularly in the mathematics domain. The mainstream methods involve data augmentation based on existing annotated training sets, such as GSM8K (Cobbe et al., 2021) and MATH (Huang et al., 2024), ranging from self-evolving instructions (Xu et al., 2024;(Zeng et al., 2024) and question paraphrasing (Yu et al., 2024), to solution augmentation (Lu et al., 2024). However, these methods are limited by the available training data, constraining the synthesis diversity (Li et al., 2024c(Li et al., , 2023)). To enhance diversity, recent approaches enable LLMs to generate a large scale of questions from various mathematicsrelated sources, including web pages (Yue et al., 2024) and knowledge points (Tang et al., 2024) from web corpora or textbooks."
                        ],
                        "paper": {
                            "corpus_id": 278171321,
                            "title": "RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library",
                            "authors": [
                                {
                                    "authorId": "2302813110",
                                    "name": "Jiapeng Wang"
                                },
                                {
                                    "authorId": "2118240359",
                                    "name": "Jinhao Jiang"
                                },
                                {
                                    "authorId": "2358106383",
                                    "name": "Zhiqiang Zhang"
                                },
                                {
                                    "authorId": "2279870653",
                                    "name": "Jun Zhou"
                                },
                                {
                                    "authorId": "2294811281",
                                    "name": "Wayne Xin Zhao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.87451171875
                    },
                    {
                        "id": "(Huang et al., 2024)",
                        "snippets": [
                            "Large language models have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality and reasoning-focused training datasets. Addressing this challenge, we propose Key-PointDriven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar practices from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, an extensive synthetic dataset tailored for mathematical reasoning, comprising over 800K questionanswer pairs. Utilizing KPMath and augmenting it with additional reasoning-intensive corpora, we create the comprehensive KPMath-Plus dataset. Our experiments demonstrate that this dataset can enhance the mathematical reasoning performance of models across various architectures and sizes. The Qwen1.5-72B model, fine-tuned on KPMath-Plus, achieves 87.0% accuracy on GSM8K and 58.3% on MATH, surpassing competitors in the 7B to 72B range and best commercial models like GPT-4 across multiple math reasoning datasets."
                        ],
                        "paper": {
                            "corpus_id": 268247488,
                            "title": "Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning",
                            "authors": [
                                {
                                    "authorId": "2261394934",
                                    "name": "Yiming Huang"
                                },
                                {
                                    "authorId": "49544272",
                                    "name": "Xiao Liu"
                                },
                                {
                                    "authorId": "2254121650",
                                    "name": "Yeyun Gong"
                                },
                                {
                                    "authorId": "1797090",
                                    "name": "Zhibin Gou"
                                },
                                {
                                    "authorId": "2237948786",
                                    "name": "Yelong Shen"
                                },
                                {
                                    "authorId": "2269471632",
                                    "name": "Nan Duan"
                                },
                                {
                                    "authorId": "2249538838",
                                    "name": "Weizhu Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 43
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tang et al., 2024)",
                        "snippets": [
                            "Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., {\\tt GPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct {\\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten datasets (including GSM8K and MATH) covering K-12, college, and competition level math problems. We apply MathScaleQA to fine-tune open-source LLMs (e.g., LLaMA-2 and Mistral), resulting in significantly improved capabilities in mathematical reasoning. Evaluated on {\\sc MwpBench}, MathScale-7B achieves state-of-the-art performance across all datasets, surpassing its best peers of equivalent size by 42.9\\% in micro average accuracy and 43.7\\% in macro average accuracy, respectively."
                        ],
                        "paper": {
                            "corpus_id": 268247902,
                            "title": "MathScale: Scaling Instruction Tuning for Mathematical Reasoning",
                            "authors": [
                                {
                                    "authorId": "2284834012",
                                    "name": "Zhengyang Tang"
                                },
                                {
                                    "authorId": "2284863493",
                                    "name": "Xingxing Zhang"
                                },
                                {
                                    "authorId": "2284827140",
                                    "name": "Benyou Wang"
                                },
                                {
                                    "authorId": "2290016262",
                                    "name": "Furu Wei"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 82
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zeng et al., 2024)",
                        "snippets": [
                            "Fine-tuning large pre-trained language models with Evol-Instruct has achieved encouraging results across a wide range of tasks. However, designing effective evolving methods for instruction evolution requires substantial human expertise. This paper proposes Auto Evol-Instruct, an end-to-end framework that evolves instruction datasets using large language models without any human effort. The framework automatically analyzes and summarizes suitable evolutionary strategies for the given instruction data and iteratively improves the evolving method based on issues exposed during the instruction evolution process. Our extensive experiments demonstrate that the best method optimized by Auto Evol-Instruct outperforms human-designed methods on various benchmarks, including MT-Bench, AlpacaEval, GSM8K, and HumanEval."
                        ],
                        "paper": {
                            "corpus_id": 270213007,
                            "title": "Automatic Instruction Evolving for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2069640466",
                                    "name": "Weihao Zeng"
                                },
                                {
                                    "authorId": "2304522809",
                                    "name": "Can Xu"
                                },
                                {
                                    "authorId": "2290476555",
                                    "name": "Yingxiu Zhao"
                                },
                                {
                                    "authorId": "2304469600",
                                    "name": "Jian-Guang Lou"
                                },
                                {
                                    "authorId": "2264439430",
                                    "name": "Weizhu Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 10
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Commonsense Reasoning Data Generation",
                "tldr": "Synthetic data generation for commonsense reasoning addresses the challenge of creating realistic scenarios that align with real-world knowledge. Researchers have developed graph-based approaches, taxonomy-driven frameworks, and contrastive generation techniques to create high-quality commonsense reasoning datasets. (7 sources)",
                "text": "\nGenerating synthetic data for commonsense reasoning presents unique challenges compared to other reasoning domains, as the data must accurately reflect real-world knowledge and scenarios. To address these challenges, researchers have developed specialized approaches that leverage knowledge graphs and generative models. GraDA stands out as a graph-generative data augmentation framework that synthesizes factual data samples from knowledge graphs for commonsense reasoning. This approach employs a two-stage process: first training a graph-to-text model to generate questions from graph entities and relations, then training a generator with GAN loss to create plausible distractors for these questions. GraDA has demonstrated performance improvements across multiple commonsense reasoning datasets, including SocialIQA, CODAH, HellaSwag, and CommonsenseQA <Paper corpusId=\"250390686\" paperTitle=\"(Maharana et al., 2022)\" isShortName></Paper>.\n\nThe adoption of synthetic text generation as a data augmentation strategy has gained momentum with the advancement of powerful text generation models. Researchers have applied these techniques to various tasks including commonsense reasoning, sentence classification, and question answering <Paper corpusId=\"253384598\" paperTitle=\"(Sadat et al., 2022)\" isShortName></Paper> <Paper corpusId=\"250390686\" paperTitle=\"(Maharana et al., 2022)\" isShortName></Paper> <Paper corpusId=\"211258652\" paperTitle=\"(Puri et al., 2020)\" isShortName></Paper>. Additionally, synthetic text generation methods have been employed to address spurious correlations in human-annotated data for fact verification and natural language inference tasks <Paper corpusId=\"253384598\" paperTitle=\"(Sadat et al., 2022)\" isShortName></Paper> <Paper corpusId=\"238227205\" paperTitle=\"(Lee et al., 2021)\" isShortName></Paper> <Paper corpusId=\"247628095\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper>.\n\nRecent innovations include visual chain-of-thought (VCoT) approaches that generate novel and consistent synthetic data for tasks requiring visual reasoning and storytelling. When applied to the Visual Storytelling and WikiHow summarization datasets, VCoT has demonstrated superior performance compared to standard chain-of-thought baselines through human evaluation <Paper corpusId=\"258461502\" paperTitle=\"(Rose et al., 2023)\" isShortName></Paper>. This suggests that explicitly modeling reasoning processes can enhance the quality of synthetic data for commonsense tasks that involve visual elements.\n\nA more comprehensive approach to synthetic data generation for commonsense reasoning is the Generalized Instruction Tuning (GLAN) method, which leverages a pre-curated taxonomy of human knowledge and capabilities. Unlike approaches that rely on seed examples or existing datasets, GLAN generates diverse instructions with broad coverage across multiple domains of human knowledge and skills. By using fine-grained key concepts from academic syllabi, GLAN creates synthetic instruction data that encompasses commonsense reasoning along with other capabilities such as mathematical reasoning, coding, and logical reasoning. Experiments with large language models like Mistral demonstrate GLAN's effectiveness across these diverse reasoning domains without requiring task-specific training data <Paper corpusId=\"267759981\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Maharana et al., 2022)",
                        "snippets": [
                            "Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions."
                        ],
                        "paper": {
                            "corpus_id": 250390686,
                            "title": "GraDA: Graph Generative Data Augmentation for Commonsense Reasoning",
                            "authors": [
                                {
                                    "authorId": "8785371",
                                    "name": "Adyasha Maharana"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 96
                        },
                        "score": 0.91162109375
                    },
                    {
                        "id": "(Sadat et al., 2022)",
                        "snippets": [
                            "With the advancement of text generation models in recent years (Radford et al., 2019)Brown et al., 2020;(Lewis et al., 2019), researchers have started to adopt synthetic text generation as a data augmentation strategy for various tasks such as commonsense reasoning (Maharana et al., 2022), sentence classification (Anaby-Tavor et al., 2020) and question answering (Puri et al., 2020). More recently, synthetic text generation methods have also been employed to remove spurious correlations in human annotated data for fact verification (Lee et al., 2021) and NLI (Wu et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 253384598,
                            "title": "Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference",
                            "authors": [
                                {
                                    "authorId": "2140791766",
                                    "name": "Mobashir Sadat"
                                },
                                {
                                    "authorId": "2140493460",
                                    "name": "Cornelia Caragea"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 2
                        },
                        "score": 0.90380859375
                    },
                    {
                        "id": "(Puri et al., 2020)",
                        "snippets": [
                            "Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the SQuAD1.1 question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the SQuAD1.1 training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic corpus generated by an 8.3 billion parameter GPT-2 model. With no access to human supervision and only access to other models, we are able to train state of the art question answering networks on entirely model-generated data that achieve 88.4 Exact Match (EM) and 93.9 F1 score on the SQuAD1.1 dev set. We further apply our methodology to SQuAD2.0 and show a 2.8 absolute gain on EM score compared to prior work using synthetic data."
                        ],
                        "paper": {
                            "corpus_id": 211258652,
                            "title": "Training Question Answering Models from Synthetic Data",
                            "authors": [
                                {
                                    "authorId": "41158993",
                                    "name": "Raul Puri"
                                },
                                {
                                    "authorId": "145297046",
                                    "name": "Ryan Spring"
                                },
                                {
                                    "authorId": "66870756",
                                    "name": "M. Patwary"
                                },
                                {
                                    "authorId": "1911755",
                                    "name": "M. Shoeybi"
                                },
                                {
                                    "authorId": "2301680",
                                    "name": "Bryan Catanzaro"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 159
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2021)",
                        "snippets": [
                            "Fact verification datasets are typically constructed using crowdsourcing techniques due to the lack of text sources with veracity labels. However, the crowdsourcing process often produces undesired biases in data that cause models to learn spurious patterns. In this paper, we propose CrossAug, a contrastive data augmentation method for debiasing fact verification models. Specifically, we employ a two-stage augmentation pipeline to generate new claims and evidences from existing samples. The generated samples are then paired cross-wise with the original pair, forming contrastive samples that facilitate the model to rely less on spurious patterns and learn more robust representations. Experimental results show that our method outperforms the previous state-of-the-art debiasing technique by 3.6% on the debiased extension of the FEVER dataset, with a total performance boost of 10.13% from the baseline. Furthermore, we evaluate our approach in data-scarce settings, where models can be more susceptible to biases due to the lack of training data. Experimental results demonstrate that our approach is also effective at debiasing in these low-resource conditions, exceeding the baseline performance on the Symmetric dataset with just 1% of the original data."
                        ],
                        "paper": {
                            "corpus_id": 238227205,
                            "title": "CrossAug: A Contrastive Data Augmentation Method for Debiasing Fact Verification Models",
                            "authors": [
                                {
                                    "authorId": "2125213401",
                                    "name": "Minwoo Lee"
                                },
                                {
                                    "authorId": "40981970",
                                    "name": "Seungpil Won"
                                },
                                {
                                    "authorId": "2109974526",
                                    "name": "Juae Kim"
                                },
                                {
                                    "authorId": "2143341775",
                                    "name": "Hwanhee Lee"
                                },
                                {
                                    "authorId": "48976447",
                                    "name": "Cheoneum Park"
                                },
                                {
                                    "authorId": "1731707",
                                    "name": "Kyomin Jung"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Information and Knowledge Management",
                            "n_citations": 30
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wu et al., 2022)",
                        "snippets": [
                            "Natural language processing models often exploit spurious correlations between task-independent features and labels in datasets to perform well only within the distributions they are trained on, while not generalising to different task distributions. We propose to tackle this problem by generating a debiased version of a dataset, which can then be used to train a debiased, off-the-shelf model, by simply replacing its training data. Our approach consists of 1) a method for training data generators to generate high-quality, label-consistent data samples; and 2) a filtering mechanism for removing data points that contribute to spurious correlations, measured in terms of z-statistics. We generate debiased versions of the SNLI and MNLI datasets, and we evaluate on a large suite of debiased, out-of-distribution, and adversarial test sets. Results show that models trained on our debiased datasets generalise better than those trained on the original datasets in all settings. On the majority of the datasets, our method outperforms or performs comparably to previous state-of-the-art debiasing strategies, and when combined with an orthogonal technique, product-of-experts, it improves further and outperforms previous best results of SNLI-hard and MNLI-hard."
                        ],
                        "paper": {
                            "corpus_id": 247628095,
                            "title": "Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets",
                            "authors": [
                                {
                                    "authorId": "39417610",
                                    "name": "Yuxiang Wu"
                                },
                                {
                                    "authorId": "40642935",
                                    "name": "Matt Gardner"
                                },
                                {
                                    "authorId": "1918552",
                                    "name": "Pontus Stenetorp"
                                },
                                {
                                    "authorId": "2697425",
                                    "name": "Pradeep Dasigi"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 68
                        },
                        "score": 0
                    },
                    {
                        "id": "(Rose et al., 2023)",
                        "snippets": [
                            "We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evaluation that VCoT offers novel and consistent synthetic data augmentation beating chain-of-thought baselines, which can be used to enhance downstream performance."
                        ],
                        "paper": {
                            "corpus_id": 258461502,
                            "title": "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings",
                            "authors": [
                                {
                                    "authorId": "2215927057",
                                    "name": "Daniel Philip Rose"
                                },
                                {
                                    "authorId": "2215926792",
                                    "name": "Vaishnavi Himakunthala"
                                },
                                {
                                    "authorId": "2215914015",
                                    "name": "Andy Ouyang"
                                },
                                {
                                    "authorId": "2215915076",
                                    "name": "Ryan He"
                                },
                                {
                                    "authorId": "2185480449",
                                    "name": "Alex Mei"
                                },
                                {
                                    "authorId": "47006228",
                                    "name": "Yujie Lu"
                                },
                                {
                                    "authorId": "48227633",
                                    "name": "Michael Stephen Saxon"
                                },
                                {
                                    "authorId": "26961225",
                                    "name": "Chinmay Sonar"
                                },
                                {
                                    "authorId": "1705929",
                                    "name": "Diba Mirza"
                                },
                                {
                                    "authorId": "1682479",
                                    "name": "William Yang Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 47
                        },
                        "score": 0.8818359375
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines",
                            "With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate diverse instructions with a broad coverage across the entire spectrum of human knowledge and skills. Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks."
                        ],
                        "paper": {
                            "corpus_id": 267759981,
                            "title": "Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models",
                            "authors": [
                                {
                                    "authorId": "47893214",
                                    "name": "Haoran Li"
                                },
                                {
                                    "authorId": "2047143813",
                                    "name": "Qingxiu Dong"
                                },
                                {
                                    "authorId": "2284834012",
                                    "name": "Zhengyang Tang"
                                },
                                {
                                    "authorId": "2144522700",
                                    "name": "Chaojun Wang"
                                },
                                {
                                    "authorId": "2284863493",
                                    "name": "Xingxing Zhang"
                                },
                                {
                                    "authorId": "15086992",
                                    "name": "Haoyang Huang"
                                },
                                {
                                    "authorId": "3110003",
                                    "name": "Shaohan Huang"
                                },
                                {
                                    "authorId": "2116768132",
                                    "name": "Xiaolong Huang"
                                },
                                {
                                    "authorId": "2284830389",
                                    "name": "Zeqiang Huang"
                                },
                                {
                                    "authorId": "2273919921",
                                    "name": "Dongdong Zhang"
                                },
                                {
                                    "authorId": "2116405624",
                                    "name": "Yuxian Gu"
                                },
                                {
                                    "authorId": "2193630544",
                                    "name": "Xin Cheng"
                                },
                                {
                                    "authorId": "2193104542",
                                    "name": "Xun Wang"
                                },
                                {
                                    "authorId": "2263708536",
                                    "name": "Si-Qing Chen"
                                },
                                {
                                    "authorId": "145307652",
                                    "name": "Li Dong"
                                },
                                {
                                    "authorId": "2261087197",
                                    "name": "Wei Lu"
                                },
                                {
                                    "authorId": "3335836",
                                    "name": "Zhifang Sui"
                                },
                                {
                                    "authorId": "2284827140",
                                    "name": "Benyou Wang"
                                },
                                {
                                    "authorId": "2273358252",
                                    "name": "Wai Lam"
                                },
                                {
                                    "authorId": "2253471545",
                                    "name": "Furu Wei"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 63
                        },
                        "score": 0.94140625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Generation Methodologies and Frameworks",
                "tldr": "Synthetic data generation frameworks employ diverse approaches including template-based methods, neural generation, and human-AI collaboration pipelines. Modern techniques leverage large language models to create high-quality synthetic datasets with filtering mechanisms that ensure data quality while balancing diversity and accuracy. (18 sources)",
                "text": "\nSynthetic data generation for complex reasoning tasks has evolved significantly, with researchers developing diverse methodologies to address the challenge of creating high-quality training data. Template-based generation represents one of the fundamental approaches, where structured templates are used to create controlled datasets with specific reasoning patterns <Paper corpusId=\"263909600\" paperTitle=\"(Gholami et al., 2023)\" isShortName></Paper>. This approach is particularly prevalent in semantic parsing tasks, where researchers first generate synthetic data using rules and templates before manually rewriting questions to improve diversity <Paper corpusId=\"258558899\" paperTitle=\"(Niu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"247362971\" paperTitle=\"(Cao et al., 2020)\" isShortName></Paper> <Paper corpusId=\"226965153\" paperTitle=\"(Gu et al., 2020)\" isShortName></Paper>.\n\nRecent advances have enabled more sophisticated generation methods that leverage large language models (LLMs). Liu et al. introduced a collaborative approach that combines human workers and AI, using dataset cartography to identify challenging reasoning patterns in existing datasets and instructing GPT-3 to compose new examples with similar patterns <Paper corpusId=\"267311830\" paperTitle=\"(Zhou et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"246016339\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>. This human-AI collaboration has proven particularly effective, with generated examples automatically filtered and subsequently reviewed and labeled by human crowdworkers, resulting in datasets with greater linguistic diversity.\n\nData quality is a critical concern in synthetic data generation, leading researchers to develop specialized filtering mechanisms. Ding et al. introduced essential filtering techniques to ensure that synthetic keyword queries generated from natural language questions meet high-quality standards before being used for neural network training <Paper corpusId=\"49865372\" paperTitle=\"(Ding et al., 2018)\" isShortName></Paper>. Similarly, modern approaches incorporate techniques such as logit suppression and temperature sampling to increase data diversity, though often at the cost of accuracy <Paper corpusId=\"259096160\" paperTitle=\"(Chung et al., 2023)\" isShortName></Paper>. To address this trade-off, researchers have explored human interventions such as label replacement, which can increase the absolute accuracy of models trained with diversified datasets by up to 14.4% <Paper corpusId=\"259096160\" paperTitle=\"(Chung et al., 2023)\" isShortName></Paper>.\n\nThe rise of LLMs has revolutionized synthetic data generation across various NLP tasks. Self-Instruct introduced a framework for improving instruction-following capabilities of pretrained language models by bootstrapping off their own generations, showing a 33% absolute improvement over the original GPT-3 model <Paper corpusId=\"254877310\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. In the realm of text classification, researchers have demonstrated that LLM-generated data can enhance diversity and improve model generalization <Paper corpusId=\"233296100\" paperTitle=\"(Yoo et al., 2021)\" isShortName></Paper>. For information retrieval tasks, LLMs have been used to generate synthetic queries <Paper corpusId=\"250340449\" paperTitle=\"(Bonifacio et al., 2022)\" isShortName></Paper> or documents <Paper corpusId=\"257505063\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>, with approaches like query2doc boosting the performance of retrieval systems by 3% to 15% without model fine-tuning <Paper corpusId=\"257505063\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>.\n\nData augmentation techniques have also proven effective for enhancing existing datasets. EDA (Easy Data Augmentation) presents simple but powerful operations including synonym replacement, random insertion, random swap, and random deletion to boost performance on text classification tasks <Paper corpusId=\"59523656\" paperTitle=\"(Wei et al., 2019)\" isShortName></Paper>. More sophisticated methods like LAMBADA (Language-Model-Based Data Augmentation) involve fine-tuning state-of-the-art language generators on small labeled datasets to synthesize new labeled data for supervised learning <Paper corpusId=\"212821571\" paperTitle=\"(Anaby-Tavor et al., 2019)\" isShortName></Paper>. Recent studies have shown that synthetic data can be particularly valuable in low-resource settings, with experiments demonstrating that training with augmented data while using only 50% of available training data can achieve the same accuracy as normal training with all available data <Paper corpusId=\"59523656\" paperTitle=\"(Wei et al., 2019)\" isShortName></Paper>.\n\nSpecialized generation frameworks have been developed for complex reasoning tasks. Li et al. introduced a novel and scalable text-to-SQL data synthesis framework that automatically produces large-scale, high-quality, and diverse datasets without extensive human intervention, resulting in SynSQL-2.5M, a dataset containing 2.5 million samples across 16,000 synthetic databases <Paper corpusId=\"276774742\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. In the visual-language domain, researchers have explored GAN-based models <Paper corpusId=\"234338081\" paperTitle=\"(Kayser et al., 2021)\" isShortName></Paper> and diffusion-based approaches for image synthesis, while LLMs have been leveraged to generate synthetic data for tasks like text classification <Paper corpusId=\"277065796\" paperTitle=\"(Luo et al., 2025)\" isShortName></Paper>.\n\nThe effectiveness of synthetic data generation depends heavily on the specific task characteristics. Studies have shown that while models trained on synthetic data perform comparably to those trained on real-world data for low-subjectivity tasks like news and topic classification, there is a notable performance drop on high-subjectivity tasks such as sarcasm detection <Paper corpusId=\"273662200\" paperTitle=\"(Braga et al., 2024)\" isShortName></Paper>. This highlights the importance of considering task specificity when designing synthetic data generation approaches for complex reasoning tasks.",
                "citations": [
                    {
                        "id": "(Gholami et al., 2023)",
                        "snippets": [
                            "One challenge persistently confronting researchers is the scarcity of high-quality, annotated datasets that drive these models. This paper explores the nuances of synthetic data generation in NLP, with a focal point on template-based question generation."
                        ],
                        "paper": {
                            "corpus_id": 263909600,
                            "title": "Does Synthetic Data Make Large Language Models More Efficient?",
                            "authors": [
                                {
                                    "authorId": "2141639835",
                                    "name": "Sia Gholami"
                                },
                                {
                                    "authorId": "2239197098",
                                    "name": "Marwan Omar"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 15
                        },
                        "score": 0.8623046875
                    },
                    {
                        "id": "(Niu et al., 2023)",
                        "snippets": [
                            "Data synthesis plays an important role in semantic parsing, especially in dataset construction and model training. Many datasets are constructed by first generating synthetic data by rules and then manually rewriting the questions for diverse expressions (Wang et al., 2015)(Cao et al., 2020)(Gu et al., 2020). Since the generation of synthetic data is controllable, researchers can restrict the distribution of logical forms in order to examine or improve various generalization capabilities of models, such as compositional generalization and scalability (Lake and (Lake et al., 2017)(Gu et al., 2020)(Oren et al., 2021). In many scenarios, where manually rewriting is laborious or unavailable, training on synthetic data can also alleviate the problems of cold-start and privacy (Yang et al., 2022)(Gu et al., 2020). Limited by stereotyped expressions, semantic parsers trained with only synthetic data are difficult to generalize to diverse natural questions. (Wu et al., 2021) and (Xu et al., 2020) automatically rewrite synthetic questions with paraphrasing models, which introduces various phrase-level expressions but change little in sentence structures. Instead of rule-based synthesis, (Zhong et al., 2020) and Yang et al. (2021) train a neural data synthesizer with natural data from other domains, and then generate training data for the target domain in a way of domain transfer."
                        ],
                        "paper": {
                            "corpus_id": 258558899,
                            "title": "Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing",
                            "authors": [
                                {
                                    "authorId": "10680347",
                                    "name": "Yilin Niu"
                                },
                                {
                                    "authorId": "152159016",
                                    "name": "Fei Huang"
                                },
                                {
                                    "authorId": "46641573",
                                    "name": "W. Liu"
                                },
                                {
                                    "authorId": "3061725",
                                    "name": "Jianwei Cui"
                                },
                                {
                                    "authorId": "2152593593",
                                    "name": "Bin Wang"
                                },
                                {
                                    "authorId": "1730108",
                                    "name": "Minlie Huang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 6
                        },
                        "score": 0.89404296875
                    },
                    {
                        "id": "(Cao et al., 2020)",
                        "snippets": [
                            "Complex question answering over knowledge base (Complex KBQA) is challenging because it requires various compositional reasoning capabilities, such as multi-hop inference, attribute comparison, set operation, etc. Existing benchmarks have some shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are poor in diversity or scale. To this end, we introduce KQA Pro, a dataset for Complex KBQA including around 120K diverse natural language questions. We introduce a compositional and interpretable programming language KoPL to represent the reasoning process of complex questions. For each question, we provide the corresponding KoPL program and SPARQL query, so that KQA Pro can serve for both KBQA and semantic parsing tasks. Experimental results show that state-of-the-art KBQA methods cannot achieve promising results on KQA Pro as on current datasets, which suggests that KQA Pro is challenging and Complex KBQA requires further research efforts. We also treat KQA Pro as a diagnostic dataset for testing multiple reasoning skills, conduct a thorough evaluation of existing models and discuss further directions for Complex KBQA. Our codes and datasets can be obtained from https://github.com/shijx12/KQAPro_Baselines."
                        ],
                        "paper": {
                            "corpus_id": 247362971,
                            "title": "KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base",
                            "authors": [
                                {
                                    "authorId": "1712738522",
                                    "name": "S. Cao"
                                },
                                {
                                    "authorId": "2522647",
                                    "name": "Jiaxin Shi"
                                },
                                {
                                    "authorId": "3470231",
                                    "name": "Liangming Pan"
                                },
                                {
                                    "authorId": "115361209",
                                    "name": "L. Nie"
                                },
                                {
                                    "authorId": "2068340987",
                                    "name": "Yutong Xiang"
                                },
                                {
                                    "authorId": "145779862",
                                    "name": "Lei Hou"
                                },
                                {
                                    "authorId": "2133353675",
                                    "name": "Juanzi Li"
                                },
                                {
                                    "authorId": "1631386300",
                                    "name": "Bin He"
                                },
                                {
                                    "authorId": "2119078220",
                                    "name": "Hanwang Zhang"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 102
                        },
                        "score": 0
                    },
                    {
                        "id": "(Gu et al., 2020)",
                        "snippets": [
                            "Existing studies on question answering on knowledge bases (KBQA) mainly operate with the standard i.i.d. assumption, i.e., training distribution over questions is the same as the test distribution. However, i.i.d. may be neither achievable nor desirable on large-scale KBs because 1) true user distribution is hard to capture and 2) randomly sampling training examples from the enormous space would be data-inefficient. Instead, we suggest that KBQA models should have three levels of built-in generalization: i.i.d., compositional, and zero-shot. To facilitate the development of KBQA models with stronger generalization, we construct and release a new large-scale, high-quality dataset with 64,331 questions, GrailQA, and provide evaluation settings for all three levels of generalization. In addition, we propose a novel BERT-based KBQA model. The combination of our dataset and model enables us to thoroughly examine and demonstrate, for the first time, the key role of pre-trained contextual embeddings like BERT in the generalization of KBQA.1"
                        ],
                        "paper": {
                            "corpus_id": 226965153,
                            "title": "Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases",
                            "authors": [
                                {
                                    "authorId": "2022231256",
                                    "name": "Yu Gu"
                                },
                                {
                                    "authorId": "145030211",
                                    "name": "Sue E. Kase"
                                },
                                {
                                    "authorId": "36349209",
                                    "name": "M. Vanni"
                                },
                                {
                                    "authorId": "1709722",
                                    "name": "Brian M. Sadler"
                                },
                                {
                                    "authorId": "145419642",
                                    "name": "Percy Liang"
                                },
                                {
                                    "authorId": "1740249",
                                    "name": "Xifeng Yan"
                                },
                                {
                                    "authorId": "1758652",
                                    "name": "Yu Su"
                                }
                            ],
                            "year": 2020,
                            "venue": "The Web Conference",
                            "n_citations": 215
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhou et al._1, 2024)",
                        "snippets": [
                            "Liu et al. (2022) introduced a fresh method to create datasets by combining the efforts of human workers and AI. It began with an established dataset called MultiNLI, which is designed to deal with natural language inference. The proposed approach involves employing dataset cartography to automatically identify instances that exhibit complex reasoning patterns. These patterns serve as guidelines for instructing GPT-3 to generate new examples with similar characteristics. The generated examples are then filtered automatically and subsequently reviewed and labeled by human crowd workers."
                        ],
                        "paper": {
                            "corpus_id": 267311830,
                            "title": "A Survey on Data Augmentation in Large Model Era",
                            "authors": [
                                {
                                    "authorId": "2281788432",
                                    "name": "Yue Zhou"
                                },
                                {
                                    "authorId": "2282250059",
                                    "name": "Chenlu Guo"
                                },
                                {
                                    "authorId": "2281791317",
                                    "name": "Xu Wang"
                                },
                                {
                                    "authorId": "2281762754",
                                    "name": "Yi Chang"
                                },
                                {
                                    "authorId": "2280899184",
                                    "name": "Yuan Wu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 27
                        },
                        "score": 0.814453125
                    },
                    {
                        "id": "(Liu et al., 2022)",
                        "snippets": [
                            "A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel approach for dataset creation based on worker and AI collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI for natural language inference (NLI), our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers. The resulting dataset, WANLI, consists of 107,885 NLI examples and presents unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI improves performance on eight out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI, compared to training on the 4x larger MultiNLI. Moreover, it continues to be more effective than MultiNLI augmented with other NLI datasets. Our results demonstrate the promise of leveraging natural language generation techniques and re-imagining the role of humans in the dataset creation process."
                        ],
                        "paper": {
                            "corpus_id": 246016339,
                            "title": "WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation",
                            "authors": [
                                {
                                    "authorId": "94500147",
                                    "name": "Alisa Liu"
                                },
                                {
                                    "authorId": "2133324514",
                                    "name": "Swabha Swayamdipta"
                                },
                                {
                                    "authorId": "144365875",
                                    "name": "Noah A. Smith"
                                },
                                {
                                    "authorId": "1699545",
                                    "name": "Yejin Choi"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 221
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ding et al., 2018)",
                        "snippets": [
                            "We introduce several generative models for producing synthetic keyword queries from natural language questions (Sect. 3.1). (3) We develop two filtering mechanisms, which are essential for ensuring that the synthetic training data we feed into the neural network is of high-quality (Sect. 3.2)."
                        ],
                        "paper": {
                            "corpus_id": 49865372,
                            "title": "Generating Synthetic Data for Neural Keyword-to-Question Models",
                            "authors": [
                                {
                                    "authorId": "2113453574",
                                    "name": "Heng Ding"
                                },
                                {
                                    "authorId": "1680484",
                                    "name": "K. Balog"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on the Theory of Information Retrieval",
                            "n_citations": 7
                        },
                        "score": 0.8349609375
                    },
                    {
                        "id": "(Chung et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging. In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation. We first examine two approaches to diversify text generation: 1) logit suppression, which minimizes the generation of languages that have already been frequently generated, and 2) temperature sampling, which flattens the token sampling probability. We found that diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain). To address this issue, we examined two human interventions, 1) label replacement (LR), correcting misaligned labels, and 2) out-of-scope filtering (OOSF), removing instances that are out of the user\u2019s domain of interest or to which no considered label applies. With oracle studies, we found that LR increases the absolute accuracy of models trained with diversified datasets by 14.4%. Moreover, we found that some models trained with data generated with LR interventions outperformed LLM-based few-shot classification. In contrast, OOSF was not effective in increasing model accuracy, implying the need for future work in human-in-the-loop text data generation."
                        ],
                        "paper": {
                            "corpus_id": 259096160,
                            "title": "Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions",
                            "authors": [
                                {
                                    "authorId": "152836325",
                                    "name": "John Joon Young Chung"
                                },
                                {
                                    "authorId": "1783184",
                                    "name": "Ece Kamar"
                                },
                                {
                                    "authorId": "1719124",
                                    "name": "Saleema Amershi"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 121
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "Large \"instruction-tuned\" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning."
                        ],
                        "paper": {
                            "corpus_id": 254877310,
                            "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
                            "authors": [
                                {
                                    "authorId": "1705260",
                                    "name": "Yizhong Wang"
                                },
                                {
                                    "authorId": "2156538832",
                                    "name": "Yeganeh Kordi"
                                },
                                {
                                    "authorId": "1817207",
                                    "name": "Swaroop Mishra"
                                },
                                {
                                    "authorId": "94500147",
                                    "name": "Alisa Liu"
                                },
                                {
                                    "authorId": "144365875",
                                    "name": "Noah A. Smith"
                                },
                                {
                                    "authorId": "1783281",
                                    "name": "Daniel Khashabi"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 2251
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yoo et al., 2021)",
                        "snippets": [
                            "Large-scale language models such as GPT-3 are excellent few-shot learners, allowing them to be controlled via natural text prompts. Recent studies report that prompt-based direct classification eliminates the need for fine-tuning but lacks data and inference scalability. This paper proposes a novel data augmentation technique that leverages large-scale language models to generate realistic text samples from a mixture of real samples. We also propose utilizing soft-labels predicted by the language models, effectively distilling knowledge from the large-scale language models and creating textual perturbations simultaneously. We perform data augmentation experiments on diverse classification tasks and show that our method hugely outperforms existing text augmentation methods. Ablation studies and a qualitative analysis provide more insights into our approach."
                        ],
                        "paper": {
                            "corpus_id": 233296100,
                            "title": "GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation",
                            "authors": [
                                {
                                    "authorId": "31760501",
                                    "name": "Kang Min Yoo"
                                },
                                {
                                    "authorId": "13453892",
                                    "name": "Dongju Park"
                                },
                                {
                                    "authorId": "35518563",
                                    "name": "Jaewook Kang"
                                },
                                {
                                    "authorId": "3226948",
                                    "name": "Sang-Woo Lee"
                                },
                                {
                                    "authorId": "2087289230",
                                    "name": "Woomyeong Park"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 242
                        },
                        "score": 0
                    },
                    {
                        "id": "(Bonifacio et al., 2022)",
                        "snippets": [
                            "The Information Retrieval (IR) community has recently witnessed a revolution due to large pretrained transformer models. Another key ingredient for this revolution was the MS MARCO dataset, whose scale and diversity has enabled zero-shot transfer learning to various tasks. However, not all IR tasks and domains can benefit from one single dataset equally. Extensive research in various NLP tasks has shown that using domain-specific training data, as opposed to a general-purpose one, improves the performance of neural models. In this work, we harness the few-shot capabilities of large pretrained language models as synthetic data generators for IR tasks. We show that models finetuned solely on our synthetic datasets outperform strong baselines such as BM25 as well as recently proposed self-supervised dense retrieval methods. Code, models, and data are available at https://github.com/zetaalphavector/inpars."
                        ],
                        "paper": {
                            "corpus_id": 250340449,
                            "title": "InPars: Unsupervised Dataset Generation for Information Retrieval",
                            "authors": [
                                {
                                    "authorId": "2003019597",
                                    "name": "L. Bonifacio"
                                },
                                {
                                    "authorId": "1394470211",
                                    "name": "H. Abonizio"
                                },
                                {
                                    "authorId": "2818759",
                                    "name": "Marzieh Fadaee"
                                },
                                {
                                    "authorId": "143744603",
                                    "name": "Rodrigo Nogueira"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 114
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2023)",
                        "snippets": [
                            "This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo-documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results."
                        ],
                        "paper": {
                            "corpus_id": 257505063,
                            "title": "Query2doc: Query Expansion with Large Language Models",
                            "authors": [
                                {
                                    "authorId": "145769448",
                                    "name": "Liang Wang"
                                },
                                {
                                    "authorId": "144610884",
                                    "name": "Nan Yang"
                                },
                                {
                                    "authorId": "49807919",
                                    "name": "Furu Wei"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 112
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wei et al., 2019)",
                        "snippets": [
                            "We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use."
                        ],
                        "paper": {
                            "corpus_id": 59523656,
                            "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks",
                            "authors": [
                                {
                                    "authorId": "144026731",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "2132971481",
                                    "name": "Kai Zou"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 1962
                        },
                        "score": 0
                    },
                    {
                        "id": "(Anaby-Tavor et al., 2019)",
                        "snippets": [
                            "Based on recent advances in natural language modeling and those in text generation capabilities, we propose a novel data augmentation method for text classification tasks. We use a powerful pre-trained neural network model to artificially synthesize new labeled data for supervised learning. We mainly focus on cases with scarce labeled data. Our method, referred to as language-model-based data augmentation (LAMBADA), involves fine-tuning a state-of-the-art language generator to a specific task through an initial training phase on the existing (usually small) labeled data. Using the fine-tuned model and given a class label, new sentences for the class are generated. Our process then filters these new sentences by using a classifier trained on the original data. In a series of experiments, we show that LAMBADA improves classifiers' performance on a variety of datasets. Moreover, LAMBADA significantly improves upon the state-of-the-art techniques for data augmentation, specifically those applicable to text classification tasks with little data."
                        ],
                        "paper": {
                            "corpus_id": 212821571,
                            "title": "Do Not Have Enough Data? Deep Learning to the Rescue!",
                            "authors": [
                                {
                                    "authorId": "1400347538",
                                    "name": "Ateret Anaby-Tavor"
                                },
                                {
                                    "authorId": "1719299",
                                    "name": "Boaz Carmeli"
                                },
                                {
                                    "authorId": "1734231",
                                    "name": "Esther Goldbraich"
                                },
                                {
                                    "authorId": "1703796",
                                    "name": "Amir Kantor"
                                },
                                {
                                    "authorId": "2497491",
                                    "name": "George Kour"
                                },
                                {
                                    "authorId": "20513655",
                                    "name": "Segev Shlomov"
                                },
                                {
                                    "authorId": "3166348",
                                    "name": "N. Tepper"
                                },
                                {
                                    "authorId": "2652616",
                                    "name": "Naama Zwerdling"
                                }
                            ],
                            "year": 2019,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 370
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2025)",
                        "snippets": [
                            "To overcome these challenges, we propose a novel and scalable text-to-SQL data synthesis framework for automatically synthesizing large-scale, high-quality, and diverse datasets without extensive human intervention. Using this framework, we introduce SynSQL-2.5M, the first million-scale text-to-SQL dataset, containing 2.5 million samples spanning over 16,000 synthetic databases. Each sample includes a database, SQL query, natural language question, and chain-of-thought (CoT) solution."
                        ],
                        "paper": {
                            "corpus_id": 276774742,
                            "title": "OmniSQL: Synthesizing High-quality Text-to-SQL Data at Scale",
                            "authors": [
                                {
                                    "authorId": "2274084217",
                                    "name": "Haoyang Li"
                                },
                                {
                                    "authorId": "2348412920",
                                    "name": "Shang Wu"
                                },
                                {
                                    "authorId": "2108046717",
                                    "name": "Xiaokang Zhang"
                                },
                                {
                                    "authorId": "2198199708",
                                    "name": "Xinmei Huang"
                                },
                                {
                                    "authorId": "2345829238",
                                    "name": "Jing Zhang"
                                },
                                {
                                    "authorId": "2295705983",
                                    "name": "Fuxin Jiang"
                                },
                                {
                                    "authorId": "2346968040",
                                    "name": "Shuai Wang"
                                },
                                {
                                    "authorId": "2297140223",
                                    "name": "Tieying Zhang"
                                },
                                {
                                    "authorId": "2225061614",
                                    "name": "Jianjun Chen"
                                },
                                {
                                    "authorId": "2349071083",
                                    "name": "Rui Shi"
                                },
                                {
                                    "authorId": "2191043357",
                                    "name": "Hong Chen"
                                },
                                {
                                    "authorId": "2287979930",
                                    "name": "Cuiping Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.76171875
                    },
                    {
                        "id": "(Kayser et al., 2021)",
                        "snippets": [
                            "Recently, there has been an increasing number of efforts to introduce models capable of generating natural language explanations (NLEs) for their predictions on vision-language (VL) tasks. Such models are appealing, because they can provide human-friendly and comprehensive explanations. However, there is a lack of comparison between existing methods, which is due to a lack of re-usable evaluation frameworks and a scarcity of datasets. In this work, we introduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable vision-language tasks that establishes a unified evaluation framework and provides the first comprehensive comparison of existing approaches that generate NLEs for VL tasks. It spans four models and three datasets and both automatic metrics and human evaluation are used to assess model-generated explanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs (over 430k instances). We also propose a new model that combines UNITER [15], which learns joint embeddings of images and text, and GPT-2 [38], a pre-trained language model that is well-suited for text generation. It surpasses the previous state of the art by a large margin across all datasets. Code and data are available here: https://github.com/maximek3/e-ViL."
                        ],
                        "paper": {
                            "corpus_id": 234338081,
                            "title": "e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks",
                            "authors": [
                                {
                                    "authorId": "1491707097",
                                    "name": "Maxime Kayser"
                                },
                                {
                                    "authorId": "3317152",
                                    "name": "Oana-Maria Camburu"
                                },
                                {
                                    "authorId": "151097297",
                                    "name": "Leonard Salewski"
                                },
                                {
                                    "authorId": "153438331",
                                    "name": "Cornelius Emde"
                                },
                                {
                                    "authorId": "2086828576",
                                    "name": "Virginie Do"
                                },
                                {
                                    "authorId": "2893664",
                                    "name": "Zeynep Akata"
                                },
                                {
                                    "authorId": "1690572",
                                    "name": "Thomas Lukasiewicz"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 101
                        },
                        "score": 0
                    },
                    {
                        "id": "(Luo et al., 2025)",
                        "snippets": [
                            "Recently, the use of LLMs for data synthesis or data augmentation has become increasingly prevalent. Many studies leverage LLMgenerated data for training models (Hartvigsen et al., 2022)36,53,56,59], as well as for evaluating the performance of other trained models (Ribeiro et al., 2022). In the NLP domain, researchers have utilized LLMs to generate synthetic data for tasks like text classification [1,3,(Yoo et al., 2021). These works showcase that LLM-generated data can enhance data diversity, thereby improving model generalization and robustness",
                            "By leveraging LLMs to generate multi-step reasoning data, the performance of models on long-chain and complex reasoning tasks can be further improved. As demonstrated by Hunter et al. [18], process supervision via multi-step reasoning significantly enhances model reliability on tasks such as mathematical problem-solving. Similarly, Step-DPO [14] shows that generating step-wise reasoning data enables models to better capture intermediate steps, resulting in improved accuracy."
                        ],
                        "paper": {
                            "corpus_id": 277065796,
                            "title": "nvBench 2.0: A Benchmark for Natural Language to Visualization under Ambiguity",
                            "authors": [
                                {
                                    "authorId": "2324719591",
                                    "name": "Tianqi Luo"
                                },
                                {
                                    "authorId": "2351107046",
                                    "name": "Chuhan Huang"
                                },
                                {
                                    "authorId": "2314809440",
                                    "name": "Leixian Shen"
                                },
                                {
                                    "authorId": "2304516904",
                                    "name": "Boyan Li"
                                },
                                {
                                    "authorId": "2316124858",
                                    "name": "Shuyu Shen"
                                },
                                {
                                    "authorId": "2351038617",
                                    "name": "Wei Zeng"
                                },
                                {
                                    "authorId": "2301154561",
                                    "name": "Nan Tang"
                                },
                                {
                                    "authorId": "2324528358",
                                    "name": "Yuyu Luo"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.91552734375
                    },
                    {
                        "id": "(Braga et al., 2024)",
                        "snippets": [
                            "Puri et al. [39] explore question and answer generation as a data augmentation method to enhance QA models, addressing the challenge of limited human-labelled data. The authors point out that the synthetic data alone overcome the performance of the original training set in the Exact Match (EM) and F1 scores. Li et al. [40] investigate the effectiveness of synthetic data generated by LLMs in training text classification models across tasks with varying levels of subjectivity. They compare models trained on real-world data with those trained on LLMgenerated data under zero-shot and few-shot settings. Their findings reveal that while models trained on synthetic data perform comparably on low-subjectivity tasks, e.g. news and topic classification, there is a notable performance drop on high-subjectivity tasks, such as sarcasm detection. Regarding the generation of synthetic data for IR tasks, LLMs can be used to generate synthetic queries (Bonifacio et al., 2022) or documents [17]. In (Wang et al., 2023) the authors propose to generate documents by few-shot prompting LLMs and concatenate them with the original query to form a new query."
                        ],
                        "paper": {
                            "corpus_id": 273662200,
                            "title": "Synthetic Data Generation with Large Language Models for Personalized Community Question Answering",
                            "authors": [
                                {
                                    "authorId": "2227483106",
                                    "name": "Marco Braga"
                                },
                                {
                                    "authorId": "2187856026",
                                    "name": "Pranav Kasela"
                                },
                                {
                                    "authorId": "3106437",
                                    "name": "Alessandro Raganato"
                                },
                                {
                                    "authorId": "2243333911",
                                    "name": "Gabriella Pasi"
                                }
                            ],
                            "year": 2024,
                            "venue": "2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)",
                            "n_citations": 1
                        },
                        "score": 0.90771484375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Evaluation and Benchmarks",
                "tldr": "Evaluation frameworks for synthetic reasoning data have evolved from simple accuracy measures to comprehensive benchmarks targeting specific reasoning capabilities. These benchmarks use controlled synthetic data generation to systematically test model performance across various reasoning patterns, premise orders, and difficulty levels. (5 sources)",
                "text": "\nThe development of specialized benchmarks has become crucial for assessing language models' reasoning capabilities on synthetic data. Researchers have created comprehensive evaluation frameworks that systematically test specific reasoning patterns across multiple domains. LogicBench stands out as a notable example, evaluating 25 different reasoning patterns spanning propositional, first-order, and non-monotonic logics . This benchmark enables systematic assessment through natural language question-answering focused on single inference rules, revealing that even advanced models struggle with complex reasoning patterns and negations.\n\nTo facilitate systematic evaluation, researchers have developed frameworks for dynamically generating evaluation samples with varying configurations and difficulty levels <Paper corpusId=\"273695832\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"211126663\" paperTitle=\"(Clark et al., 2020)\" isShortName></Paper>. DyVal represents a notable approach, using directed acyclic graphs to dynamically generate samples for various reasoning tasks including deductive, Boolean, and abductive reasoning <Paper corpusId=\"273695832\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>. Similarly, ZebraLogic provides an extended benchmark specifically designed to test logical reasoning capabilities systematically <Paper corpusId=\"273695832\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\nRecent research has revealed important insights about model sensitivity to evaluation data presentation. Chen et al. discovered that language models demonstrate surprising brittleness to premise ordering in logical reasoning tasks, with performance drops exceeding 30% when premises are reordered <Paper corpusId=\"267657940\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>. This finding suggests that models perform best when premise order aligns with the steps required for intermediate reasoning, highlighting a significant limitation in current reasoning capabilities. To further examine this phenomenon, researchers released R-GSM, a benchmark based on GSM8K that tests the ordering effect in mathematical problem-solving <Paper corpusId=\"267657940\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>.\n\nSeveral specialized benchmarks focus on specific reasoning challenges. BoardgameQA presents a dataset characterized by contradictory facts and rules, requiring models to resolve conflicts through preference-based reasoning <Paper corpusId=\"259144942\" paperTitle=\"(Kazemi et al., 2023)\" isShortName></Paper>. Similarly, PRONTOQA offers a synthetic question-answering dataset generated from world models represented in first-order logic, enabling the parsing of reasoning chains into symbolic proofs for formal analysis <Paper corpusId=\"273695832\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>. TruthQuest provides evaluation samples based on K&K-type puzzles involving 3-6 persons, testing complex logical deduction <Paper corpusId=\"273695832\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\nBeyond traditional benchmarks, researchers have also developed automated evaluation pipelines. Automate-CoT introduces a fully automatic pipeline for finding optimal chain-of-thought prompts for commonsense reasoning tasks, leveraging powerful language models to augment reasoning paths, prune incorrect paths, and select optimal exemplar combinations <Paper corpusId=\"268249187\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>. This approach demonstrates how synthetic data can be used not just for training but also for optimizing prompting strategies.\n\nThe growing landscape of specialized benchmarks underscores the importance of synthetic data in evaluating specific reasoning capabilities. Each benchmark targets particular aspects of reasoning, from premise ordering sensitivity to conflict resolution and multi-step inference. As models continue to advance, these targeted evaluation frameworks provide crucial insights into their strengths and limitations across different reasoning domains.",
                "citations": [
                    {
                        "id": "(Xie et al., 2024)",
                        "snippets": [
                            "Logical reasoning benchmarks. To evaluate logical reasoning capabilities in LLMs, synthetic benchmarks have been developed to enable scalable generation of samples with varying configurations and difficulty levels (Clark et al., 2020)(Giadikiaroglou et al., 2024)(Parmar et al., 2024). For instance, DyVal (Zhu et al., 2024) uses directed acyclic graphs to dynamically generate samples on reasoning tasks including deductive, Boolean, and abductive reasoning. (Chen et al., 2024) focus on propositional logical problems involving definite clauses, and synthetically generate variations with different premise orders, such as forward, backward, and shuffled. (Dziri et al., 2024) explore the limitations of LLMs in tasks requiring compositional reasoning, including multiplication, logic grid puzzles, and dynamic programming problems. ZebraLogic (Lin et al., 2024) is an extended benchmark that systematically tests logical reasoning capabilities. BoardgameQA (Kazemi et al., 2023) presents a question-answering dataset characterized by contradictory facts and rules in the questions. PRONTOQA (Saparov & He, 2023) is a synthetic question-answering dataset where each example is generated from a synthetic world model represented in first-order logic. This dataset enables parsing the generated chain of thought into symbolic proofs, facilitating formal analysis. TruthQuest (Mondorf & Plank, 2024) is the most similar task to our work, which provides evaluation samples based on K&K-type of puzzles involving 3-6 person."
                        ],
                        "paper": {
                            "corpus_id": 273695832,
                            "title": "On Memorization of Large Language Models in Logical Reasoning",
                            "authors": [
                                {
                                    "authorId": "2308424593",
                                    "name": "Chulin Xie"
                                },
                                {
                                    "authorId": "2283305597",
                                    "name": "Yangsibo Huang"
                                },
                                {
                                    "authorId": "2309481623",
                                    "name": "Chiyuan Zhang"
                                },
                                {
                                    "authorId": "2290010050",
                                    "name": "Da Yu"
                                },
                                {
                                    "authorId": "2328442132",
                                    "name": "Xinyun Chen"
                                },
                                {
                                    "authorId": "2328296250",
                                    "name": "Bill Yuchen Lin"
                                },
                                {
                                    "authorId": "2290141407",
                                    "name": "Bo Li"
                                },
                                {
                                    "authorId": "2529354",
                                    "name": "Badih Ghazi"
                                },
                                {
                                    "authorId": "2237408905",
                                    "name": "Ravi Kumar"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 41
                        },
                        "score": 0.82080078125
                    },
                    {
                        "id": "(Clark et al., 2020)",
                        "snippets": [
                            "Beginning with McCarthy's Advice Taker (1959), AI has pursued the goal of providing a system with explicit, general knowledge and having the system reason over that knowledge. However, expressing the knowledge in a formal (logical or probabilistic) representation has been a major obstacle to this research. This paper investigates a modern approach to this problem where the facts and rules are provided as natural language sentences, thus bypassing a formal representation. We train transformers to reason (or emulate reasoning) over these sentences using synthetically generated data. Our models, that we call RuleTakers, provide the first empirical demonstration that this kind of soft reasoning over language is learnable, can achieve high (99%) accuracy, and generalizes to test data requiring substantially deeper chaining than seen during training (95%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as limited \"soft theorem provers\" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering."
                        ],
                        "paper": {
                            "corpus_id": 211126663,
                            "title": "Transformers as Soft Reasoners over Language",
                            "authors": [
                                {
                                    "authorId": "48323507",
                                    "name": "Peter Clark"
                                },
                                {
                                    "authorId": "3385516",
                                    "name": "Oyvind Tafjord"
                                },
                                {
                                    "authorId": "46666605",
                                    "name": "Kyle Richardson"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Joint Conference on Artificial Intelligence",
                            "n_citations": 361
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark."
                        ],
                        "paper": {
                            "corpus_id": 267657940,
                            "title": "Premise Order Matters in Reasoning with Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2238263119",
                                    "name": "Xinyun Chen"
                                },
                                {
                                    "authorId": "2284066085",
                                    "name": "Ryan A. Chi"
                                },
                                {
                                    "authorId": "2238394232",
                                    "name": "Xuezhi Wang"
                                },
                                {
                                    "authorId": "2256313467",
                                    "name": "Denny Zhou"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 32
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kazemi et al., 2023)",
                        "snippets": [
                            "Automated reasoning with unstructured natural text is a key requirement for many potential applications of NLP and for developing robust AI systems. Recently, Language Models (LMs) have demonstrated complex reasoning capacities even without any finetuning. However, existing evaluation for automated reasoning assumes access to a consistent and coherent set of information over which models reason. When reasoning in the real-world, the available information is frequently inconsistent or contradictory, and therefore models need to be equipped with a strategy to resolve such conflicts when they arise. One widely-applicable way of resolving conflicts is to impose preferences over information sources (e.g., based on source credibility or information recency) and adopt the source with higher preference. In this paper, we formulate the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of defeasible reasoning, and develop a dataset called BoardgameQA for measuring the reasoning capacity of LMs in this setting. BoardgameQA also incorporates reasoning with implicit background knowledge, to better reflect reasoning problems in downstream applications. We benchmark various LMs on BoardgameQA and the results reveal a significant gap in the reasoning capacity of state-of-the-art LMs on this problem, showing that reasoning with conflicting information does not surface out-of-the-box in LMs. While performance can be improved with finetuning, it nevertheless remains poor."
                        ],
                        "paper": {
                            "corpus_id": 259144942,
                            "title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information",
                            "authors": [
                                {
                                    "authorId": "2173102324",
                                    "name": "Mehran Kazemi"
                                },
                                {
                                    "authorId": "2117782275",
                                    "name": "Quan Yuan"
                                },
                                {
                                    "authorId": "2055505691",
                                    "name": "Deepti Bhatia"
                                },
                                {
                                    "authorId": "8756748",
                                    "name": "Najoung Kim"
                                },
                                {
                                    "authorId": null,
                                    "name": "Xin Xu"
                                },
                                {
                                    "authorId": "2410945",
                                    "name": "Vaiva Imbrasaite"
                                },
                                {
                                    "authorId": "143812128",
                                    "name": "Deepak Ramachandran"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 50
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ding et al., 2024)",
                        "snippets": [
                            "Automate-CoT (Shum et al., 2023) proposes a fully automatic pipeline for finding better chainof-thought prompts to solve commonsense reasoning question answering tasks. This pipeline leverages powerful LLMs like GPT-3 to augment reasoning paths, prune incorrect paths, and select optimal combinations of exemplars."
                        ],
                        "paper": {
                            "corpus_id": 268249187,
                            "title": "Data Augmentation using Large Language Models: Data Perspectives, Learning Paradigms and Challenges",
                            "authors": [
                                {
                                    "authorId": "2064493724",
                                    "name": "Bosheng Ding"
                                },
                                {
                                    "authorId": "2084609980",
                                    "name": "Chengwei Qin"
                                },
                                {
                                    "authorId": "2091437375",
                                    "name": "Ruochen Zhao"
                                },
                                {
                                    "authorId": "2290030532",
                                    "name": "Tianze Luo"
                                },
                                {
                                    "authorId": "2290023354",
                                    "name": "Xinze Li"
                                },
                                {
                                    "authorId": "2290026474",
                                    "name": "Guizhen Chen"
                                },
                                {
                                    "authorId": "2276610995",
                                    "name": "Wenhan Xia"
                                },
                                {
                                    "authorId": "2290145274",
                                    "name": "Junjie Hu"
                                },
                                {
                                    "authorId": "1755919",
                                    "name": "A. Luu"
                                },
                                {
                                    "authorId": "2708940",
                                    "name": "Shafiq R. Joty"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 20
                        },
                        "score": 0.77392578125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.230481
    }
}