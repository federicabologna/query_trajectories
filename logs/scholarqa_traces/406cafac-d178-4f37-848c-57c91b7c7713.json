{
    "query": "What is the impact of 4-bit quantization on neural network inference accuracy compared to 8-bit and 16-bit quantization?",
    "user_id": "lib_user",
    "task_id": "406cafac-d178-4f37-848c-57c91b7c7713",
    "timestamp": "2025-06-23T21:51:42.956367",
    "n_retrieval": 256,
    "n_retrieved": 275,
    "n_candidates": 47,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.504324,
    "decomposed_query": {
        "rewritten_query": "Impact of 4-bit quantization on neural network inference accuracy compared to 8-bit and 16-bit quantization.",
        "keyword_query": "4-bit quantization neural network inference accuracy 8-bit 16-bit",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009891,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
            "venue": "arXiv.org",
            "year": 2018,
            "reference_count": 42,
            "citation_count": 1021,
            "influential_citation_count": 115,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.08342, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2065915235",
                    "name": "Raghuraman Krishnamoorthi"
                }
            ],
            "abstract": "We present an overview of techniques for quantizing convolutional neural networks for inference with integer weights and activations. Per-channel quantization of weights and per-layer quantization of activations to 8-bits of precision post-training produces classification accuracies within 2% of floating point networks for a wide variety of CNN architectures. Model sizes can be reduced by a factor of 4 by quantizing weights to 8-bits, even when 8-bit arithmetic is not supported. This can be achieved with simple, post training quantization of weights.We benchmark latencies of quantized networks on CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations compared to floating point on CPUs. Speedups of up to 10x are observed on specialized processors with fixed point SIMD capabilities, like the Qualcomm QDSPs with HVX. \nQuantization-aware training can provide further improvements, reducing the gap to floating point to 1% at 8-bit precision. Quantization-aware training also allows for reducing the precision of weights to four bits with accuracy losses ranging from 2% to 10%, with higher accuracy drop for smaller networks.We introduce tools in TensorFlow and TensorFlowLite for quantizing convolutional networks and review best practices for quantization-aware training to obtain high accuracy with quantized weights and activations. We recommend that per-channel quantization of weights and per-layer quantization of activations be the preferred quantization scheme for hardware acceleration and kernel optimization. We also propose that future processors and hardware accelerators for optimized inference support precisions of 4, 8 and 16 bits.",
            "corpus_id": 49356451,
            "sentences": [
                {
                    "corpus_id": "49356451",
                    "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
                    "text": "We note that at 8-bits of precision, post training quantization schemes provide close to floating point accuracy. In order to better understand the benefits of quantization aware training, we perform experiments to assess performance at 4 bit quantization for weights and activations. \n\nWe perform the following experiments: \n\n\u2022 Experiment 1: Per-channel quantization is significantly better than perlayer quantization at 4 bits. We show that per-channel quantization provides big gains over per-layer quantization for all networks. At 8-bits, the gains were not significant as there were sufficient levels to represent the weights with high fidelity. At four bits, the benefits of per-channel quantization are apparent, even for post training quantization (columns 2 and 3 of Table 5). \n\n\u2022 Experiment 2: Fine tuning can provide substantial accuracy improvements at lower bitwidths. It is interesting to see that for most networks, one can obtain accuracies within 5% of 8-bit quantization with fine tuning 4 bit weights (column 4 of Table 5). The improvements due to fine tuning are also more apparent at 4 bits. Note that activations are quantized to 8-bits in these experiments. \n\n\u2022 Experiment 3: Lower precision activations: We investigate the accuracies obtained with 4-bit activations for all layers with and without fine tuning. Note that activations are quantized on a per-layer basis. The weights are quantized at 8bits of precision with per-channel granularity. We note that fine tuning improves accuracy in this case also. The losses due to activation quantization are more severe than that of weight quantization (see Table 6). Note that the quantization granularity is different for activations and weights, so this is not a fair comparison of the impact of quantization. We hypothesize that quantizing activations We experiment with several configurations for training quantized models: Our first experiment compares stochastic quantization with deterministic quantization. Subse-quently, we study if training a quantized model from scratch provides higher accuracies than fine tuning from a floating point model.",
                    "score": 0.6842441086209085,
                    "section_title": "Lower Precision Networks",
                    "char_start_offset": 19489,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 114,
                            "end": 284
                        },
                        {
                            "start": 287,
                            "end": 324
                        },
                        {
                            "start": 327,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 532
                        },
                        {
                            "start": 533,
                            "end": 651
                        },
                        {
                            "start": 652,
                            "end": 786
                        },
                        {
                            "start": 789,
                            "end": 882
                        },
                        {
                            "start": 883,
                            "end": 1043
                        },
                        {
                            "start": 1044,
                            "end": 1113
                        },
                        {
                            "start": 1114,
                            "end": 1181
                        },
                        {
                            "start": 1184,
                            "end": 1335
                        },
                        {
                            "start": 1336,
                            "end": 1393
                        },
                        {
                            "start": 1394,
                            "end": 1471
                        },
                        {
                            "start": 1472,
                            "end": 1533
                        },
                        {
                            "start": 1534,
                            "end": 1639
                        },
                        {
                            "start": 1640,
                            "end": 1784
                        },
                        {
                            "start": 1785,
                            "end": 1987
                        },
                        {
                            "start": 1988,
                            "end": 2127
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98876953125
                },
                {
                    "corpus_id": "49356451",
                    "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
                    "text": "We present an overview of techniques for quantizing convolutional neural networks for inference with integer weights and activations. Per-channel quantization of weights and per-layer quantization of activations to 8-bits of precision post-training produces classification accuracies within 2% of floating point networks for a wide variety of CNN architectures. Model sizes can be reduced by a factor of 4 by quantizing weights to 8-bits, even when 8-bit arithmetic is not supported. This can be achieved with simple, post training quantization of weights.We benchmark latencies of quantized networks on CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations compared to floating point on CPUs. Speedups of up to 10x are observed on specialized processors with fixed point SIMD capabilities, like the Qualcomm QDSPs with HVX. \nQuantization-aware training can provide further improvements, reducing the gap to floating point to 1% at 8-bit precision. Quantization-aware training also allows for reducing the precision of weights to four bits with accuracy losses ranging from 2% to 10%, with higher accuracy drop for smaller networks.We introduce tools in TensorFlow and TensorFlowLite for quantizing convolutional networks and review best practices for quantization-aware training to obtain high accuracy with quantized weights and activations. We recommend that per-channel quantization of weights and per-layer quantization of activations be the preferred quantization scheme for hardware acceleration and kernel optimization. We also propose that future processors and hardware accelerators for optimized inference support precisions of 4, 8 and 16 bits.",
                    "score": 0.6317779514440182,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96826171875
                }
            ],
            "relevance_judgement": 0.98876953125,
            "relevance_judgment_input_expanded": "# Title: Quantizing deep convolutional networks for efficient inference: A whitepaper\n# Venue: arXiv.org\n# Authors: Raghuraman Krishnamoorthi\n## Abstract\nWe present an overview of techniques for quantizing convolutional neural networks for inference with integer weights and activations. Per-channel quantization of weights and per-layer quantization of activations to 8-bits of precision post-training produces classification accuracies within 2% of floating point networks for a wide variety of CNN architectures. Model sizes can be reduced by a factor of 4 by quantizing weights to 8-bits, even when 8-bit arithmetic is not supported. This can be achieved with simple, post training quantization of weights.We benchmark latencies of quantized networks on CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations compared to floating point on CPUs. Speedups of up to 10x are observed on specialized processors with fixed point SIMD capabilities, like the Qualcomm QDSPs with HVX. \nQuantization-aware training can provide further improvements, reducing the gap to floating point to 1% at 8-bit precision. Quantization-aware training also allows for reducing the precision of weights to four bits with accuracy losses ranging from 2% to 10%, with higher accuracy drop for smaller networks.We introduce tools in TensorFlow and TensorFlowLite for quantizing convolutional networks and review best practices for quantization-aware training to obtain high accuracy with quantized weights and activations. We recommend that per-channel quantization of weights and per-layer quantization of activations be the preferred quantization scheme for hardware acceleration and kernel optimization. We also propose that future processors and hardware accelerators for optimized inference support precisions of 4, 8 and 16 bits.\n## Lower Precision Networks\nWe note that at 8-bits of precision, post training quantization schemes provide close to floating point accuracy. In order to better understand the benefits of quantization aware training, we perform experiments to assess performance at 4 bit quantization for weights and activations. \n\nWe perform the following experiments: \n\n\u2022 Experiment 1: Per-channel quantization is significantly better than perlayer quantization at 4 bits. We show that per-channel quantization provides big gains over per-layer quantization for all networks. At 8-bits, the gains were not significant as there were sufficient levels to represent the weights with high fidelity. At four bits, the benefits of per-channel quantization are apparent, even for post training quantization (columns 2 and 3 of Table 5). \n\n\u2022 Experiment 2: Fine tuning can provide substantial accuracy improvements at lower bitwidths. It is interesting to see that for most networks, one can obtain accuracies within 5% of 8-bit quantization with fine tuning 4 bit weights (column 4 of Table 5). The improvements due to fine tuning are also more apparent at 4 bits. Note that activations are quantized to 8-bits in these experiments. \n\n\u2022 Experiment 3: Lower precision activations: We investigate the accuracies obtained with 4-bit activations for all layers with and without fine tuning. Note that activations are quantized on a per-layer basis. The weights are quantized at 8bits of precision with per-channel granularity. We note that fine tuning improves accuracy in this case also. The losses due to activation quantization are more severe than that of weight quantization (see Table 6). Note that the quantization granularity is different for activations and weights, so this is not a fair comparison of the impact of quantization. We hypothesize that quantizing activations We experiment with several configurations for training quantized models: Our first experiment compares stochastic quantization with deterministic quantization. Subse-quently, we study if training a quantized model from scratch provides higher accuracies than fine tuning from a floating point model.",
            "reference_string": "[49356451 | Krishnamoorthi | 2018 | Citations: 1021]"
        },
        {
            "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit",
            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "year": 2021,
            "reference_count": 35,
            "citation_count": 36,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2105.03536",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.03536, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1938271",
                    "name": "AmirAli Abdolrashidi"
                },
                {
                    "authorId": "2108552747",
                    "name": "Lisa Wang"
                },
                {
                    "authorId": "3504647",
                    "name": "Shivani Agrawal"
                },
                {
                    "authorId": "3274291",
                    "name": "J. Malmaud"
                },
                {
                    "authorId": "145573927",
                    "name": "Oleg Rybakov"
                },
                {
                    "authorId": "108381331",
                    "name": "Chas Leichner"
                },
                {
                    "authorId": "2065594000",
                    "name": "Lukasz Lew"
                }
            ],
            "abstract": "Quantization has become a popular technique to compress neural networks and reduce compute cost, but most prior work focuses on studying quantization without changing the network size. Many real-world applications of neural networks have compute cost and memory budgets, which can be traded off with model quality by changing the number of parameters. In this work, we use ResNet as a case study to systematically investigate the effects of quantization on inference compute cost-quality tradeoff curves. Our results suggest that for each bfloat16 ResNet model, there are quantized models with lower cost and higher ac-curacy; in other words, the bfloat16 compute cost-quality tradeoff curve is Pareto-dominated by the 4-bit and 8-bit curves, with models primarily quantized to 4-bit yielding the best Pareto curve. Furthermore, we achieve state-of-the-art results on ImageNet for 4-bit ResNet-50 with quantization-aware training, obtaining a top-1 eval accuracy of 77.09%. We demonstrate the regularizing effect of quantization by measuring the generalization gap. The quantization method we used is optimized for practicality: It requires little tuning and is designed with hardware capabilities in mind. Our work motivates further research into optimal numeric formats for quantization, as well as the development of machine learning accelerators supporting these formats. As part of this work, we contribute a quantization library written in JAX, which is open-sourced at https://github.com/google-research/google-research/tree/master/aqt.",
            "corpus_id": 234337594,
            "sentences": [
                {
                    "corpus_id": "234337594",
                    "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit",
                    "text": "In this work, we analyzed how quantization at different precisions influences the compute cost-quality Pareto curves on ResNet models. We found that quantization consistently improves the tradeoff regardless of where we are on the compute cost-quality tradeoff graph. In other words, for each bfloat16 model, we found quantized models with lower compute cost and higher accuracy. Additionally, models in 4-bit (with first and last layers in 8-bit) offer a consistent advantage over 8-bit, using both linear and quadratic cost models. This observation suggests that 4-bit may be a preferred numeric format for quantizing neural networks. Based on our results, we proposed a simple practical approach to compress models given compute cost and quality constraints, consisting of two steps: quantize the model to 4-bit, then multiply the number of parameters in each layer by a global factor to achieve the desired tradeoff. We invite further research into comparing different numeric formats for quantization, and hope that this line of work will inform the development of future hardware. We also encourage more works in model compression to compare methods on cost-quality tradeoff graphs, as this provides a more nuanced and thorough analysis. Lastly, we open-sourced our quantization library, in the hopes that it will accelerate quantization research and deployment with JAX.",
                    "score": 0.6394526637336789,
                    "section_title": "Conclusion",
                    "char_start_offset": 23412,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98583984375
                },
                {
                    "corpus_id": "234337594",
                    "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit",
                    "text": "Quantization has become a popular technique to compress neural networks and reduce compute cost, but most prior work focuses on studying quantization without changing the network size. Many real-world applications of neural networks have compute cost and memory budgets, which can be traded off with model quality by changing the number of parameters. In this work, we use ResNet as a case study to systematically investigate the effects of quantization on inference compute cost-quality tradeoff curves. Our results suggest that for each bfloat16 ResNet model, there are quantized models with lower cost and higher ac-curacy; in other words, the bfloat16 compute cost-quality tradeoff curve is Pareto-dominated by the 4-bit and 8-bit curves, with models primarily quantized to 4-bit yielding the best Pareto curve. Furthermore, we achieve state-of-the-art results on ImageNet for 4-bit ResNet-50 with quantization-aware training, obtaining a top-1 eval accuracy of 77.09%. We demonstrate the regularizing effect of quantization by measuring the generalization gap. The quantization method we used is optimized for practicality: It requires little tuning and is designed with hardware capabilities in mind. Our work motivates further research into optimal numeric formats for quantization, as well as the development of machine learning accelerators supporting these formats. As part of this work, we contribute a quantization library written in JAX, which is open-sourced at https://github.com/google-research/google-research/tree/master/aqt.",
                    "score": 0.6450294116604723,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97705078125
                }
            ],
            "relevance_judgement": 0.98583984375,
            "relevance_judgment_input_expanded": "# Title: Pareto-Optimal Quantized ResNet Is Mostly 4-bit\n# Venue: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\n# Authors: AmirAli Abdolrashidi, Lisa Wang, Shivani Agrawal, J. Malmaud, Oleg Rybakov, Chas Leichner, Lukasz Lew\n## Abstract\nQuantization has become a popular technique to compress neural networks and reduce compute cost, but most prior work focuses on studying quantization without changing the network size. Many real-world applications of neural networks have compute cost and memory budgets, which can be traded off with model quality by changing the number of parameters. In this work, we use ResNet as a case study to systematically investigate the effects of quantization on inference compute cost-quality tradeoff curves. Our results suggest that for each bfloat16 ResNet model, there are quantized models with lower cost and higher ac-curacy; in other words, the bfloat16 compute cost-quality tradeoff curve is Pareto-dominated by the 4-bit and 8-bit curves, with models primarily quantized to 4-bit yielding the best Pareto curve. Furthermore, we achieve state-of-the-art results on ImageNet for 4-bit ResNet-50 with quantization-aware training, obtaining a top-1 eval accuracy of 77.09%. We demonstrate the regularizing effect of quantization by measuring the generalization gap. The quantization method we used is optimized for practicality: It requires little tuning and is designed with hardware capabilities in mind. Our work motivates further research into optimal numeric formats for quantization, as well as the development of machine learning accelerators supporting these formats. As part of this work, we contribute a quantization library written in JAX, which is open-sourced at https://github.com/google-research/google-research/tree/master/aqt.\n## Conclusion\nIn this work, we analyzed how quantization at different precisions influences the compute cost-quality Pareto curves on ResNet models. We found that quantization consistently improves the tradeoff regardless of where we are on the compute cost-quality tradeoff graph. In other words, for each bfloat16 model, we found quantized models with lower compute cost and higher accuracy. Additionally, models in 4-bit (with first and last layers in 8-bit) offer a consistent advantage over 8-bit, using both linear and quadratic cost models. This observation suggests that 4-bit may be a preferred numeric format for quantizing neural networks. Based on our results, we proposed a simple practical approach to compress models given compute cost and quality constraints, consisting of two steps: quantize the model to 4-bit, then multiply the number of parameters in each layer by a global factor to achieve the desired tradeoff. We invite further research into comparing different numeric formats for quantization, and hope that this line of work will inform the development of future hardware. We also encourage more works in model compression to compare methods on cost-quality tradeoff graphs, as this provides a more nuanced and thorough analysis. Lastly, we open-sourced our quantization library, in the hopes that it will accelerate quantization research and deployment with JAX.",
            "reference_string": "[234337594 | Abdolrashidi et al. | 2021 | Citations: 36]"
        },
        {
            "title": "Accurate Inference With Inaccurate RRAM Devices: A Joint Algorithm-Design Solution",
            "venue": "IEEE Journal on Exploratory Solid-State Computational Devices and Circuits",
            "year": 2020,
            "reference_count": 30,
            "citation_count": 30,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6570653/9133209/09069242.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JXCDC.2020.2987605?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JXCDC.2020.2987605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "91781917",
                    "name": "Gouranga Charan"
                },
                {
                    "authorId": "2942785",
                    "name": "Abinash Mohanty"
                },
                {
                    "authorId": "3457252",
                    "name": "Xiaocong Du"
                },
                {
                    "authorId": "144828251",
                    "name": "Gokul Krishnan"
                },
                {
                    "authorId": "145553390",
                    "name": "R. Joshi"
                },
                {
                    "authorId": "1965873861",
                    "name": "Yu Cao"
                }
            ],
            "abstract": "Resistive random access memory (RRAM) is a promising technology for energy-efficient neuromorphic accelerators. However, when a pretrained deep neural network (DNN) model is programmed to an RRAM array for inference, the model suffers from accuracy degradation due to RRAM nonidealities, such as device variations, quantization error, and stuck-at-faults. Previous solutions involving multiple read\u2013verify\u2013write (R-V-W) to the RRAM cells require cell-by-cell compensation and, thus, an excessive amount of processing time. In this article, we propose a joint algorithm-design solution to mitigate the accuracy degradation. We first leverage knowledge distillation (KD), where the model is trained with the RRAM nonidealities to increase the robustness of the model under device variations. Furthermore, we propose random sparse adaptation (RSA), which integrates a small on-chip memory with the main RRAM array for postmapping adaptation. Only the on-chip memory is updated to recover the inference accuracy. The joint algorithm-design solution achieves the state-of-the-art accuracy of 99.41% for MNIST (LeNet-5) and 91.86% for CIFAR-10 (VGG-16) with up to 5% parameters as overhead while providing a 15\u2013 $150\\times $ speedup compared with R-V-W.",
            "corpus_id": 218788236,
            "sentences": [
                {
                    "corpus_id": "218788236",
                    "title": "Accurate Inference With Inaccurate RRAM Devices: A Joint Algorithm-Design Solution",
                    "text": "In deep learning, a 32-bit floating-point (FP32) format is used for training and inference purposes. Recent works [26] have achieved an 8-bit fixed-point precision of weights and activations without incurring any inference accuracy loss. However, unlike GPUs and CPUs, RRAM devices are limited by finite quantization levels [13]. Hence, an 8-bit quantization of parameters is not a viable solution for RRAM-based crossbars. In this article, we perform a more conservative 4-bit quantization for the model parameters, i.e., weights and biases. We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization.",
                    "score": 0.7807466345707892,
                    "section_title": "B. DEVICE QUANTIZATION",
                    "char_start_offset": 7268,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 100
                        },
                        {
                            "start": 101,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 329
                        },
                        {
                            "start": 330,
                            "end": 423
                        },
                        {
                            "start": 424,
                            "end": 542
                        },
                        {
                            "start": 543,
                            "end": 894
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 114,
                            "end": 118,
                            "matchedPaperCorpusId": "44071489"
                        },
                        {
                            "start": 324,
                            "end": 328,
                            "matchedPaperCorpusId": "16877209"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9853515625
                }
            ],
            "relevance_judgement": 0.9853515625,
            "relevance_judgment_input_expanded": "# Title: Accurate Inference With Inaccurate RRAM Devices: A Joint Algorithm-Design Solution\n# Venue: IEEE Journal on Exploratory Solid-State Computational Devices and Circuits\n# Authors: Gouranga Charan, Abinash Mohanty, Xiaocong Du, Gokul Krishnan, R. Joshi, Yu Cao\n## Abstract\nResistive random access memory (RRAM) is a promising technology for energy-efficient neuromorphic accelerators. However, when a pretrained deep neural network (DNN) model is programmed to an RRAM array for inference, the model suffers from accuracy degradation due to RRAM nonidealities, such as device variations, quantization error, and stuck-at-faults. Previous solutions involving multiple read\u2013verify\u2013write (R-V-W) to the RRAM cells require cell-by-cell compensation and, thus, an excessive amount of processing time. In this article, we propose a joint algorithm-design solution to mitigate the accuracy degradation. We first leverage knowledge distillation (KD), where the model is trained with the RRAM nonidealities to increase the robustness of the model under device variations. Furthermore, we propose random sparse adaptation (RSA), which integrates a small on-chip memory with the main RRAM array for postmapping adaptation. Only the on-chip memory is updated to recover the inference accuracy. The joint algorithm-design solution achieves the state-of-the-art accuracy of 99.41% for MNIST (LeNet-5) and 91.86% for CIFAR-10 (VGG-16) with up to 5% parameters as overhead while providing a 15\u2013 $150\\times $ speedup compared with R-V-W.\n## B. DEVICE QUANTIZATION\nIn deep learning, a 32-bit floating-point (FP32) format is used for training and inference purposes. Recent works [26] have achieved an 8-bit fixed-point precision of weights and activations without incurring any inference accuracy loss. However, unlike GPUs and CPUs, RRAM devices are limited by finite quantization levels [13]. Hence, an 8-bit quantization of parameters is not a viable solution for RRAM-based crossbars. In this article, we perform a more conservative 4-bit quantization for the model parameters, i.e., weights and biases. We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization.",
            "reference_string": "[218788236 | Charan et al. | 2020 | Citations: 30]"
        },
        {
            "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study",
            "venue": "International Conference on Language Resources and Evaluation",
            "year": 2023,
            "reference_count": 49,
            "citation_count": 35,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.08072",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.08072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108129670",
                    "name": "Peiyu Liu"
                },
                {
                    "authorId": "2119618242",
                    "name": "Zikang Liu"
                },
                {
                    "authorId": "9136116",
                    "name": "Ze-Feng Gao"
                },
                {
                    "authorId": "2162036220",
                    "name": "Dawei Gao"
                },
                {
                    "authorId": "2542603",
                    "name": "Wayne Xin Zhao"
                },
                {
                    "authorId": "2110479359",
                    "name": "Yaliang Li"
                },
                {
                    "authorId": "1696332",
                    "name": "Bolin Ding"
                },
                {
                    "authorId": "153693432",
                    "name": "Ji-rong Wen"
                }
            ],
            "abstract": "Despite the superior performance, Large Language Models (LLMs) require significant computational resources for deployment and use. To overcome this issue, quantization methods have been widely applied to reduce the memory footprint of LLMs as well as increase the inference rate. However, a major challenge is that low-bit quantization methods often lead to performance degradation. It is important to understand how quantization impacts the capacity of LLMs. Different from previous studies focused on overall performance, this work aims to investigate the impact of quantization on emergent abilities, which are important characteristics that distinguish LLMs from small language models. Specifically, we examine the abilities of in-context learning, chain-of-thought reasoning, and instruction-following in quantized LLMs. Our empirical experiments show that these emergent abilities still exist in 4-bit quantization models, while 2-bit models encounter severe performance degradation on the test of these abilities. To improve the performance of low-bit models, we conduct two special experiments: (1) fine-gained impact analysis that studies which components (or substructures) are more sensitive to quantization, and (2) performance compensation through model fine-tuning. Our work derives a series of important findings to understand the impact of quantization on emergent abilities and sheds light on the possibilities of extremely low-bit quantization for LLMs.",
            "corpus_id": 259937594,
            "sentences": [
                {
                    "corpus_id": "259937594",
                    "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study",
                    "text": "In this part, we present the experimental results and the corresponding analysis. \n\nOverall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K. It indicates that 4-bit quantization can effectively retain emergent abilities on these test datasets. \n\n4-bit precision exhibits a favorable trade-off in terms of both total bits and performance. As shown in Table 1, it can be observed that 4-bit quantization offers a notable reduction in memory cost. \n\nTo further examine the relation between model performance and resource usage, we follow Dettmers and Zettlemoyer (2022) to introduce the measure of total bits by multiplying the number of the parameters and the bits, and report the test results in Figure 1 by varying the number of total bits. s From the four accuracy curves corresponding to different bit precision, we can see that 4-bit preci-sion consistently exhibits higher model accuracy under the same amount of total model bits. Thus, 4-bit quantization is recommended to be used for a favorable balance between memory cost and model performance in practice. \n\nThe scaling effect depends on specific tasks, and increasing the model scale benefits the CoT task the most. We conducted an investigation, as depicted in Figure 1, to examine the impact of scaling the total number of bits on the performance of a low-bit model across multiple tasks.",
                    "score": 0.7644134163808378,
                    "section_title": "Results and Analysis",
                    "char_start_offset": 10205,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 81
                        },
                        {
                            "start": 84,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 415
                        },
                        {
                            "start": 416,
                            "end": 570
                        },
                        {
                            "start": 571,
                            "end": 780
                        },
                        {
                            "start": 781,
                            "end": 883
                        },
                        {
                            "start": 886,
                            "end": 977
                        },
                        {
                            "start": 978,
                            "end": 1084
                        },
                        {
                            "start": 1087,
                            "end": 1380
                        },
                        {
                            "start": 1381,
                            "end": 1574
                        },
                        {
                            "start": 1575,
                            "end": 1704
                        },
                        {
                            "start": 1707,
                            "end": 1815
                        },
                        {
                            "start": 1816,
                            "end": 1990
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98291015625
                }
            ],
            "relevance_judgement": 0.98291015625,
            "relevance_judgment_input_expanded": "# Title: Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study\n# Venue: International Conference on Language Resources and Evaluation\n# Authors: Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin Zhao, Yaliang Li, Bolin Ding, Ji-rong Wen\n## Abstract\nDespite the superior performance, Large Language Models (LLMs) require significant computational resources for deployment and use. To overcome this issue, quantization methods have been widely applied to reduce the memory footprint of LLMs as well as increase the inference rate. However, a major challenge is that low-bit quantization methods often lead to performance degradation. It is important to understand how quantization impacts the capacity of LLMs. Different from previous studies focused on overall performance, this work aims to investigate the impact of quantization on emergent abilities, which are important characteristics that distinguish LLMs from small language models. Specifically, we examine the abilities of in-context learning, chain-of-thought reasoning, and instruction-following in quantized LLMs. Our empirical experiments show that these emergent abilities still exist in 4-bit quantization models, while 2-bit models encounter severe performance degradation on the test of these abilities. To improve the performance of low-bit models, we conduct two special experiments: (1) fine-gained impact analysis that studies which components (or substructures) are more sensitive to quantization, and (2) performance compensation through model fine-tuning. Our work derives a series of important findings to understand the impact of quantization on emergent abilities and sheds light on the possibilities of extremely low-bit quantization for LLMs.\n## Results and Analysis\nIn this part, we present the experimental results and the corresponding analysis. \n\nOverall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K. It indicates that 4-bit quantization can effectively retain emergent abilities on these test datasets. \n\n4-bit precision exhibits a favorable trade-off in terms of both total bits and performance. As shown in Table 1, it can be observed that 4-bit quantization offers a notable reduction in memory cost. \n\nTo further examine the relation between model performance and resource usage, we follow Dettmers and Zettlemoyer (2022) to introduce the measure of total bits by multiplying the number of the parameters and the bits, and report the test results in Figure 1 by varying the number of total bits. s From the four accuracy curves corresponding to different bit precision, we can see that 4-bit preci-sion consistently exhibits higher model accuracy under the same amount of total model bits. Thus, 4-bit quantization is recommended to be used for a favorable balance between memory cost and model performance in practice. \n\nThe scaling effect depends on specific tasks, and increasing the model scale benefits the CoT task the most. We conducted an investigation, as depicted in Figure 1, to examine the impact of scaling the total number of bits on the performance of a low-bit model across multiple tasks.",
            "reference_string": "[259937594 | Liu et al. | 2023 | Citations: 35]"
        },
        {
            "title": "Fast Implementation of 4-bit Convolutional Neural Networks for Mobile Devices",
            "venue": "International Conference on Pattern Recognition",
            "year": 2020,
            "reference_count": 35,
            "citation_count": 17,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2009.06488",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2009.06488, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1441088616",
                    "name": "A. Trusov"
                },
                {
                    "authorId": "37239110",
                    "name": "E. Limonova"
                },
                {
                    "authorId": "89361905",
                    "name": "Dmitry Slugin"
                },
                {
                    "authorId": "35331830",
                    "name": "D. Nikolaev"
                },
                {
                    "authorId": "35343060",
                    "name": "V. Arlazarov"
                }
            ],
            "abstract": "Quantized low-precision neural networks are very popular because they require less computational resources for inference and can provide high performance, which is vital for real-time and embedded recognition systems. However, their advantages are apparent for FPGA and ASIC devices, while general-purpose processor architectures are not always able to perform low-bit integer computations efficiently. The most frequently used low-precision neural network model for mobile central processors is an 8-bit quantized network. However, in a number of cases, it is possible to use fewer bits for weights and activations, and the only problem is the difficulty of efficient implementation. We introduce an efficient implementation of 4-bit matrix multiplication for quantized neural networks and perform time measurements on a mobile ARM processor. It shows 2.9 times speedup compared to standard floating-point multiplication and is 1.5 times faster than 8-bit quantized one. We also demonstrate a 4-bit quantized neural network for OCR recognition on the MIDV-500 dataset. 4-bit quantization gives 95.0% accuracy and 48% overall inference speedup, while an 8-bit quantized network gives 95.4% accuracy and 39% speedup. The results show that 4-bit quantization perfectly suits mobile devices, yielding good enough accuracy and low inference time.",
            "corpus_id": 221655691,
            "sentences": [
                {
                    "corpus_id": "221655691",
                    "title": "Fast Implementation of 4-bit Convolutional Neural Networks for Mobile Devices",
                    "text": "Quantized low-precision neural networks are very popular because they require less computational resources for inference and can provide high performance, which is vital for real-time and embedded recognition systems. However, their advantages are apparent for FPGA and ASIC devices, while general-purpose processor architectures are not always able to perform low-bit integer computations efficiently. The most frequently used low-precision neural network model for mobile central processors is an 8-bit quantized network. However, in a number of cases, it is possible to use fewer bits for weights and activations, and the only problem is the difficulty of efficient implementation. We introduce an efficient implementation of 4-bit matrix multiplication for quantized neural networks and perform time measurements on a mobile ARM processor. It shows 2.9 times speedup compared to standard floating-point multiplication and is 1.5 times faster than 8-bit quantized one. We also demonstrate a 4-bit quantized neural network for OCR recognition on the MIDV-500 dataset. 4-bit quantization gives 95.0% accuracy and 48% overall inference speedup, while an 8-bit quantized network gives 95.4% accuracy and 39% speedup. The results show that 4-bit quantization perfectly suits mobile devices, yielding good enough accuracy and low inference time.",
                    "score": 0.6669654331213666,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98291015625
                }
            ],
            "relevance_judgement": 0.98291015625,
            "relevance_judgment_input_expanded": "# Title: Fast Implementation of 4-bit Convolutional Neural Networks for Mobile Devices\n# Venue: International Conference on Pattern Recognition\n# Authors: A. Trusov, E. Limonova, Dmitry Slugin, D. Nikolaev, V. Arlazarov\n## Abstract\nQuantized low-precision neural networks are very popular because they require less computational resources for inference and can provide high performance, which is vital for real-time and embedded recognition systems. However, their advantages are apparent for FPGA and ASIC devices, while general-purpose processor architectures are not always able to perform low-bit integer computations efficiently. The most frequently used low-precision neural network model for mobile central processors is an 8-bit quantized network. However, in a number of cases, it is possible to use fewer bits for weights and activations, and the only problem is the difficulty of efficient implementation. We introduce an efficient implementation of 4-bit matrix multiplication for quantized neural networks and perform time measurements on a mobile ARM processor. It shows 2.9 times speedup compared to standard floating-point multiplication and is 1.5 times faster than 8-bit quantized one. We also demonstrate a 4-bit quantized neural network for OCR recognition on the MIDV-500 dataset. 4-bit quantization gives 95.0% accuracy and 48% overall inference speedup, while an 8-bit quantized network gives 95.4% accuracy and 39% speedup. The results show that 4-bit quantization perfectly suits mobile devices, yielding good enough accuracy and low inference time.\n",
            "reference_string": "[221655691 | Trusov et al. | 2020 | Citations: 17]"
        },
        {
            "title": "The case for 4-bit precision: k-bit Inference Scaling Laws",
            "venue": "International Conference on Machine Learning",
            "year": 2022,
            "reference_count": 55,
            "citation_count": 234,
            "influential_citation_count": 20,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2212.09720",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.09720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3239480",
                    "name": "Tim Dettmers"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                }
            ],
            "abstract": "Quantization methods reduce the number of bits required to represent each parameter in a model, trading accuracy for smaller memory footprints and inference latencies. However, the final model size depends on both the number of parameters of the original model and the rate of compression. For example, a 30B 8-bit model and a 60B 4-bit model have the same number of bits but may have very different zero-shot accuracies. In this work, we study this trade-off by developing inference scaling laws of zero-shot performance in Large Language Models (LLMs) to determine the bit-precision and model size that maximizes zero-shot performance. We run more than 35,000 experiments with 16-bit inputs and k-bit parameters to examine which zero-shot quantization methods improve scaling for 3 to 8-bit precision at scales of 19M to 176B parameters across the LLM families BLOOM, OPT, NeoX/Pythia, and GPT-2. We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float). Overall, our findings show that {4-bit} precision is almost universally optimal for total model bits and zero-shot accuracy.",
            "corpus_id": 254853733,
            "sentences": [
                {
                    "corpus_id": "254853733",
                    "title": "The case for 4-bit precision: k-bit Inference Scaling Laws",
                    "text": "Quantization methods reduce the number of bits required to represent each parameter in a model, trading accuracy for smaller memory footprints and inference latencies. However, the final model size depends on both the number of parameters of the original model and the rate of compression. For example, a 30B 8-bit model and a 60B 4-bit model have the same number of bits but may have very different zero-shot accuracies. In this work, we study this trade-off by developing inference scaling laws of zero-shot performance in Large Language Models (LLMs) to determine the bit-precision and model size that maximizes zero-shot performance. We run more than 35,000 experiments with 16-bit inputs and k-bit parameters to examine which zero-shot quantization methods improve scaling for 3 to 8-bit precision at scales of 19M to 176B parameters across the LLM families BLOOM, OPT, NeoX/Pythia, and GPT-2. We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float). Overall, our findings show that {4-bit} precision is almost universally optimal for total model bits and zero-shot accuracy.",
                    "score": 0.6211987533973806,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.982421875
                }
            ],
            "relevance_judgement": 0.982421875,
            "relevance_judgment_input_expanded": "# Title: The case for 4-bit precision: k-bit Inference Scaling Laws\n# Venue: International Conference on Machine Learning\n# Authors: Tim Dettmers, Luke Zettlemoyer\n## Abstract\nQuantization methods reduce the number of bits required to represent each parameter in a model, trading accuracy for smaller memory footprints and inference latencies. However, the final model size depends on both the number of parameters of the original model and the rate of compression. For example, a 30B 8-bit model and a 60B 4-bit model have the same number of bits but may have very different zero-shot accuracies. In this work, we study this trade-off by developing inference scaling laws of zero-shot performance in Large Language Models (LLMs) to determine the bit-precision and model size that maximizes zero-shot performance. We run more than 35,000 experiments with 16-bit inputs and k-bit parameters to examine which zero-shot quantization methods improve scaling for 3 to 8-bit precision at scales of 19M to 176B parameters across the LLM families BLOOM, OPT, NeoX/Pythia, and GPT-2. We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float). Overall, our findings show that {4-bit} precision is almost universally optimal for total model bits and zero-shot accuracy.\n",
            "reference_string": "[254853733 | Dettmers et al. | 2022 | Citations: 234]"
        },
        {
            "title": "Face Recognition and Machine Learning at the Edge",
            "venue": "IOP Conference Series: Materials Science and Engineering",
            "year": 2020,
            "reference_count": 24,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1088/1757-899x/884/1/012084",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1088/1757-899X/884/1/012084?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1088/1757-899X/884/1/012084, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2004954858",
                    "name": "Joanne Ling Sin Yee"
                },
                {
                    "authorId": "2412102",
                    "name": "U. U. Sheikh"
                },
                {
                    "authorId": "1951977",
                    "name": "M. Mokji"
                },
                {
                    "authorId": "2111988471",
                    "name": "S. Rahman"
                }
            ],
            "abstract": "The number of IoT is expected to reach 20 billion by year 2020. This is due to data that log in the sensors or cameras are all send to the cloud for further processing. Cloud computing is not able to support big data analytic anymore due to network bandwidth. Face recognition is chosen as a case study to demonstrate the challenges to shift the application to the edge. The objective of this project is to develop a face recognition system that is suitable to be used at the edge using a deep neural network. Secondly, investigate the performance in terms of model size, speed and inference time after different bit-width fixed point quantization on the weights of the network. Lastly, deploy the model to Raspberry Pi 3 and test the performance. The chosen data set is AT&T. MATLAB is used to train the network in laptop with i5-7300 CPU while OpenCV-python is used to load and test the network in Raspberry Pi3 and laptop. The proposed system is designed by doing transfer learning on SqueezeNet to classify face. Fixed-point quantization is being applied to the weights of the layers to reduce the size of the model. From the experiment result, it is recommended to use 8-bit fixed-point quantization to the weights in all the layers in the model to compress the size of the network up to 2.5 times while maintaining the original accuracy 90%. That is only 1.1\u00d7 speed up of the model on Raspberry Pi 3 after different bit-width weight quantization.",
            "corpus_id": 225583435,
            "sentences": [
                {
                    "corpus_id": "225583435",
                    "title": "Face Recognition and Machine Learning at the Edge",
                    "text": "Figure 1 compares the difference in model size after different bit-width quantization are applied on the weights in a single layer. Model size reduced most after apply 4-bit fixed point quantization in all of the layers compared with 8-bit and 16bit because the weights are stored in lower precision format which means it consumes less memory storage. For 4-bit fixed-point quantization, the accuracy drops to 72.5% and 85% from the original 90% after quantization on the weights in the layer 'fire8-expand3x3' and 'fire9expand3x3'. The accuracy drops the to the lowest which is 42.5% after 4-bit fixed point quantization is being applied on the weights in the layer 'fire6-expand3x3'. This might due to the large value changes in total weight after quantization. Although the model size is compressed the most after applying weight quantization in the layer 'fire8-expand3x3', but there is 17.5% accuracy drop. Hence, it is recommended to apply 4-bit quantization on weights in layer 'fire-9-expand3x3' when doing 4-bit fixed point quantization on the weights in a single layer of the network as it causes a slight drop in accuracy while reducing the model size. For 8-bit and 16-bit fixed-point quantization, there is no change even when weight quantization is applied on any of the layers. Hence, it is recommended to apply 8-bit or 16-bit fixed point quantization on the weights in layer 'fire-8-expand3x3' when implementing weight quantization in a single layer as they can help to reduce model size efficiency.",
                    "score": 0.6981499634369914,
                    "section_title": "RESULTS AND DISCUSSION",
                    "char_start_offset": 15210,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 351
                        },
                        {
                            "start": 352,
                            "end": 532
                        },
                        {
                            "start": 533,
                            "end": 685
                        },
                        {
                            "start": 686,
                            "end": 763
                        },
                        {
                            "start": 764,
                            "end": 911
                        },
                        {
                            "start": 912,
                            "end": 1163
                        },
                        {
                            "start": 1164,
                            "end": 1292
                        },
                        {
                            "start": 1293,
                            "end": 1516
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98193359375
                }
            ],
            "relevance_judgement": 0.98193359375,
            "relevance_judgment_input_expanded": "# Title: Face Recognition and Machine Learning at the Edge\n# Venue: IOP Conference Series: Materials Science and Engineering\n# Authors: Joanne Ling Sin Yee, U. U. Sheikh, M. Mokji, S. Rahman\n## Abstract\nThe number of IoT is expected to reach 20 billion by year 2020. This is due to data that log in the sensors or cameras are all send to the cloud for further processing. Cloud computing is not able to support big data analytic anymore due to network bandwidth. Face recognition is chosen as a case study to demonstrate the challenges to shift the application to the edge. The objective of this project is to develop a face recognition system that is suitable to be used at the edge using a deep neural network. Secondly, investigate the performance in terms of model size, speed and inference time after different bit-width fixed point quantization on the weights of the network. Lastly, deploy the model to Raspberry Pi 3 and test the performance. The chosen data set is AT&T. MATLAB is used to train the network in laptop with i5-7300 CPU while OpenCV-python is used to load and test the network in Raspberry Pi3 and laptop. The proposed system is designed by doing transfer learning on SqueezeNet to classify face. Fixed-point quantization is being applied to the weights of the layers to reduce the size of the model. From the experiment result, it is recommended to use 8-bit fixed-point quantization to the weights in all the layers in the model to compress the size of the network up to 2.5 times while maintaining the original accuracy 90%. That is only 1.1\u00d7 speed up of the model on Raspberry Pi 3 after different bit-width weight quantization.\n## RESULTS AND DISCUSSION\nFigure 1 compares the difference in model size after different bit-width quantization are applied on the weights in a single layer. Model size reduced most after apply 4-bit fixed point quantization in all of the layers compared with 8-bit and 16bit because the weights are stored in lower precision format which means it consumes less memory storage. For 4-bit fixed-point quantization, the accuracy drops to 72.5% and 85% from the original 90% after quantization on the weights in the layer 'fire8-expand3x3' and 'fire9expand3x3'. The accuracy drops the to the lowest which is 42.5% after 4-bit fixed point quantization is being applied on the weights in the layer 'fire6-expand3x3'. This might due to the large value changes in total weight after quantization. Although the model size is compressed the most after applying weight quantization in the layer 'fire8-expand3x3', but there is 17.5% accuracy drop. Hence, it is recommended to apply 4-bit quantization on weights in layer 'fire-9-expand3x3' when doing 4-bit fixed point quantization on the weights in a single layer of the network as it causes a slight drop in accuracy while reducing the model size. For 8-bit and 16-bit fixed-point quantization, there is no change even when weight quantization is applied on any of the layers. Hence, it is recommended to apply 8-bit or 16-bit fixed point quantization on the weights in layer 'fire-8-expand3x3' when implementing weight quantization in a single layer as they can help to reduce model size efficiency.",
            "reference_string": "[225583435 | Yee et al. | 2020 | Citations: 2]"
        },
        {
            "title": "Integer-Only Neural Network Quantization Scheme Based on Shift-Batch-Normalization",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 18,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.00127, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2892749",
                    "name": "Qingyu Guo"
                },
                {
                    "authorId": "2156166711",
                    "name": "Yuan Wang"
                },
                {
                    "authorId": "39658679",
                    "name": "Xiaoxin Cui"
                }
            ],
            "abstract": "Neural networks are very popular in many areas, but great computing complexity makes it hard to run neural networks on devices with limited resources. To address this problem, quantization methods are used to reduce model size and computation cost, making it possible to use neural networks on embedded platforms or mobile devices. In this paper, an integer-only-quantization scheme is introduced. This scheme uses one layer that combines shift-based batch normalization and uniform quantization to implement 4-bit integer-only inference. Without big integer multiplication(which is used in previous integer-only-quantization methods), this scheme can achieve good power and latency efficiency, and is especially suitable to be deployed on co-designed hardware platforms. Tests have proved that this scheme works very well for easy tasks. And for tough tasks, performance loss can be tolerated for its inference efficiency. Our work is available on github: https://github.com/hguq/IntegerNet.",
            "corpus_id": 235266137,
            "sentences": [
                {
                    "corpus_id": "235266137",
                    "title": "Integer-Only Neural Network Quantization Scheme Based on Shift-Batch-Normalization",
                    "text": "We proposed a quantization scheme that can avoid big integer multiplication, which can be used in low precision neural network training and inference. In the whole integer-only inference process, only these integer operations are used: \n\n\u2022 Multiplication of 4 bit integer, producing 8 bit integer. \u2022 Addition of 8 bit and 32 bit integer, producing 32 bit integer. \u2022 Right shifting of 32 bit integer, producing 4 bit integer. We tried 4-bit quantization on some simple datasets(MNIST, CIFAR10, CIFAR100), and there was no quantization loss. We also tried 4-bit quantization on big dataset(ImageNet). The experiment result shows that there is about 12% performance loss, but due to the inference efficiency of 4-bit quantization, this loss may be tolerable in some application scenarios.",
                    "score": 0.7683589669550788,
                    "section_title": "IV. DISCUSSION",
                    "char_start_offset": 14046,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 235
                        },
                        {
                            "start": 238,
                            "end": 297
                        },
                        {
                            "start": 298,
                            "end": 363
                        },
                        {
                            "start": 364,
                            "end": 424
                        },
                        {
                            "start": 425,
                            "end": 539
                        },
                        {
                            "start": 540,
                            "end": 598
                        },
                        {
                            "start": 599,
                            "end": 785
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9814453125
                }
            ],
            "relevance_judgement": 0.9814453125,
            "relevance_judgment_input_expanded": "# Title: Integer-Only Neural Network Quantization Scheme Based on Shift-Batch-Normalization\n# Venue: arXiv.org\n# Authors: Qingyu Guo, Yuan Wang, Xiaoxin Cui\n## Abstract\nNeural networks are very popular in many areas, but great computing complexity makes it hard to run neural networks on devices with limited resources. To address this problem, quantization methods are used to reduce model size and computation cost, making it possible to use neural networks on embedded platforms or mobile devices. In this paper, an integer-only-quantization scheme is introduced. This scheme uses one layer that combines shift-based batch normalization and uniform quantization to implement 4-bit integer-only inference. Without big integer multiplication(which is used in previous integer-only-quantization methods), this scheme can achieve good power and latency efficiency, and is especially suitable to be deployed on co-designed hardware platforms. Tests have proved that this scheme works very well for easy tasks. And for tough tasks, performance loss can be tolerated for its inference efficiency. Our work is available on github: https://github.com/hguq/IntegerNet.\n## IV. DISCUSSION\nWe proposed a quantization scheme that can avoid big integer multiplication, which can be used in low precision neural network training and inference. In the whole integer-only inference process, only these integer operations are used: \n\n\u2022 Multiplication of 4 bit integer, producing 8 bit integer. \u2022 Addition of 8 bit and 32 bit integer, producing 32 bit integer. \u2022 Right shifting of 32 bit integer, producing 4 bit integer. We tried 4-bit quantization on some simple datasets(MNIST, CIFAR10, CIFAR100), and there was no quantization loss. We also tried 4-bit quantization on big dataset(ImageNet). The experiment result shows that there is about 12% performance loss, but due to the inference efficiency of 4-bit quantization, this loss may be tolerable in some application scenarios.",
            "reference_string": "[235266137 | Guo et al. | 2021 | Citations: 2]"
        },
        {
            "title": "Hessian Aware Quantization of Spiking Neural Networks",
            "venue": "International Conference on Systems",
            "year": 2021,
            "reference_count": 19,
            "citation_count": 15,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3477145.3477158",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.14117, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "80511619",
                    "name": "H. Lui"
                },
                {
                    "authorId": "1734355",
                    "name": "E. Neftci"
                }
            ],
            "abstract": "To achieve the low latency, high throughput, and energy efficiency benefits of Spiking Neural Networks (SNNs), reducing the memory and compute requirements when running on a neuromorphic hardware is an important step. Neuromorphic architecture allows massively parallel computation with variable and local bit-precisions. However, how different bit-precisions should be allocated to different layers or connections of the network is not trivial. In this work, we demonstrate how a layer-wise Hessian trace analysis can measure the sensitivity of the loss to any perturbation of the layer\u2019s weights, and this can be used to guide the allocation of a layer-specific bit-precision when quantizing an SNN. In addition, current gradient based methods of SNN training use a complex neuron model with multiple state variables, which is not ideal for compute and memory efficiency. To address this challenge, we present a simplified neuron model that reduces the number of state variables by 4-fold while still being compatible with gradient based training. We find that the impact on model accuracy when using a layer-wise bit-precision correlated well with that layer\u2019s Hessian trace. The accuracy of the optimal quantized network only dropped by 0.3%, yet the network size was reduced by 58%. This reduces memory usage and allows fixed-point arithmetic with simpler digital circuits to be used, increasing the overall throughput and energy efficiency.",
            "corpus_id": 233443785,
            "sentences": [
                {
                    "corpus_id": "233443785",
                    "title": "Hessian Aware Quantization of Spiking Neural Networks",
                    "text": "each layer did not lower the accuracy at all, while reducing the network size by 50%. This shows that full precision arithmetic is not needed. As expected, the accuracy dropped when lowering the bit-precision of all 3 layers from 16 to 8 and 4 bits, from 98.1% to 96.3% and 81.1% respectively. We also compared the accuracy when using a layer-wise bit-precision, with a (16,8) and (8, 4) bit combination. As indicated by the Hessian trace shown on table 1, we expected assigning L1 with a low bit-precision to have the least impact to accuracy, followed by L2 and L3. This was confirmed by the results, with the accuracy dropping from 98.0% to 97.9% and 97.6% as we moved an 8-bit quantizer from L1 to L3 in a 16-bit network. The same observation was found when moving a 4-bit quantizer from L1 to L3 in an 8-bit network, with the accuracy dropping from 95.0% to 94.0% and 93.6%. Finally we quantized the first two layers of the network with lower bit-precision, and found that the (8,8,16) combination provided a good balance between accuracy and network size, with the accuracy dropping by only 0.3% while the network size was reduced by 58%.",
                    "score": 0.7077559717050941,
                    "section_title": "EXPERIMENTS AND RESULTS",
                    "char_start_offset": 13037,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 370,
                            "end": 374,
                            "matchedPaperCorpusId": "225041435"
                        },
                        {
                            "start": 987,
                            "end": 990,
                            "matchedPaperCorpusId": "225041435"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9814453125
                }
            ],
            "relevance_judgement": 0.9814453125,
            "relevance_judgment_input_expanded": "# Title: Hessian Aware Quantization of Spiking Neural Networks\n# Venue: International Conference on Systems\n# Authors: H. Lui, E. Neftci\n## Abstract\nTo achieve the low latency, high throughput, and energy efficiency benefits of Spiking Neural Networks (SNNs), reducing the memory and compute requirements when running on a neuromorphic hardware is an important step. Neuromorphic architecture allows massively parallel computation with variable and local bit-precisions. However, how different bit-precisions should be allocated to different layers or connections of the network is not trivial. In this work, we demonstrate how a layer-wise Hessian trace analysis can measure the sensitivity of the loss to any perturbation of the layer\u2019s weights, and this can be used to guide the allocation of a layer-specific bit-precision when quantizing an SNN. In addition, current gradient based methods of SNN training use a complex neuron model with multiple state variables, which is not ideal for compute and memory efficiency. To address this challenge, we present a simplified neuron model that reduces the number of state variables by 4-fold while still being compatible with gradient based training. We find that the impact on model accuracy when using a layer-wise bit-precision correlated well with that layer\u2019s Hessian trace. The accuracy of the optimal quantized network only dropped by 0.3%, yet the network size was reduced by 58%. This reduces memory usage and allows fixed-point arithmetic with simpler digital circuits to be used, increasing the overall throughput and energy efficiency.\n## EXPERIMENTS AND RESULTS\neach layer did not lower the accuracy at all, while reducing the network size by 50%. This shows that full precision arithmetic is not needed. As expected, the accuracy dropped when lowering the bit-precision of all 3 layers from 16 to 8 and 4 bits, from 98.1% to 96.3% and 81.1% respectively. We also compared the accuracy when using a layer-wise bit-precision, with a (16,8) and (8, 4) bit combination. As indicated by the Hessian trace shown on table 1, we expected assigning L1 with a low bit-precision to have the least impact to accuracy, followed by L2 and L3. This was confirmed by the results, with the accuracy dropping from 98.0% to 97.9% and 97.6% as we moved an 8-bit quantizer from L1 to L3 in a 16-bit network. The same observation was found when moving a 4-bit quantizer from L1 to L3 in an 8-bit network, with the accuracy dropping from 95.0% to 94.0% and 93.6%. Finally we quantized the first two layers of the network with lower bit-precision, and found that the (8,8,16) combination provided a good balance between accuracy and network size, with the accuracy dropping by only 0.3% while the network size was reduced by 58%.",
            "reference_string": "[233443785 | Lui et al. | 2021 | Citations: 15]"
        },
        {
            "title": "Position-based Scaled Gradient for Model Quantization and Sparse Training",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 35,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.11035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "49476045",
                    "name": "Jangho Kim"
                },
                {
                    "authorId": "1713608836",
                    "name": "Kiyoon Yoo"
                },
                {
                    "authorId": "3160425",
                    "name": "Nojun Kwak"
                }
            ],
            "abstract": "We propose the position-based scaled gradient (PSG) that scales the gradient depending on the position of a weight vector to make it more compression-friendly. First, we theoretically show that applying PSG to the standard gradient descent (GD), which is called PSGD, is equivalent to the GD in the warped weight space, a space made by warping the original weight space via an appropriately designed invertible function. Second, we empirically show that PSG acting as a regularizer to a weight vector is very useful in model compression domains such as quantization and sparse training. PSG reduces the gap between the weight distributions of a full-precision model and its compressed counterpart. This enables the versatile deployment of a model either as an uncompressed mode or as a compressed mode depending on the availability of resources. The experimental results on CIFAR-10/100 and Imagenet datasets show the effectiveness of the proposed PSG in both domains of sparse training and quantization even for extremely low bits.",
            "corpus_id": 218862856,
            "sentences": [
                {
                    "corpus_id": "218862856",
                    "title": "Position-based Scaled Gradient for Model Quantization and Sparse Training",
                    "text": "On the other hand, ImageNet experiment generally shows reasonable results until 6-bits but the accuracy drastically drops at 4-bits. PSGD targeting 8-bits and 6-bits marginally improves on all bits, yet also experiences drastic accuracy drop at 4-bits. In contrast, Gradient L1 (\u03bb = 0.05) and PSGD @ W4 maintain the performance of the quantized models even at 4-bits. Comparing with the second best method Gradient L1 (\u03bb = 0.05) [1], PSGD outperforms it at all bit-widths. At full precision (FP), 8-, 6-and 4-bits, the gap of performance between [1] and ours are about 4.2%, 3.8%, 1.8% and 8%, respectively. From Table 2, while the quantization noise may slightly degrade the accuracy in some cases, a general trend that using more bits leads to higher accuracy is demonstrated. Consequently, as our 4-bit targeted model (PSGD@W4) achieves comparable accuracy for all higher bits of the other methods, PSGD outperforms conventional methods in all bit-widths. Compared to other regularization methods, PSGD is able to maintain reasonable performance across all bits by constraining the distribution of the full precision weight to resemble that of the quantized weight. This quantization-friendliness is achieved by the appropriately designed scaling function. Also, unlike [1], PSGD does not need additional overhead of calculating the double-backpropagation. \n\nPost-training methods Table 3 shows that OCS, state-of-the-art post-training method, has a drastic accuracy drop at 4-bits. For OCS, following the original paper, we chose the best clipping method for both weights and activation. DFQ also has a similar tendency of showing drastic accuracy drop under the 6 bit-widths depicted in Fig. 1 at the original paper of DFQ [26]. This is due to the fundamental discrepancy between FP and quantized weight distributions as stated in Sec 1 and Fig. 1.",
                    "score": 0.6524877117583656,
                    "section_title": "Method",
                    "char_start_offset": 20351,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 132
                        },
                        {
                            "start": 133,
                            "end": 252
                        },
                        {
                            "start": 253,
                            "end": 367
                        },
                        {
                            "start": 368,
                            "end": 472
                        },
                        {
                            "start": 473,
                            "end": 607
                        },
                        {
                            "start": 608,
                            "end": 778
                        },
                        {
                            "start": 779,
                            "end": 958
                        },
                        {
                            "start": 959,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1259
                        },
                        {
                            "start": 1260,
                            "end": 1359
                        },
                        {
                            "start": 1362,
                            "end": 1485
                        },
                        {
                            "start": 1486,
                            "end": 1591
                        },
                        {
                            "start": 1592,
                            "end": 1733
                        },
                        {
                            "start": 1734,
                            "end": 1853
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 429,
                            "end": 432,
                            "matchedPaperCorpusId": "211724357"
                        },
                        {
                            "start": 546,
                            "end": 549,
                            "matchedPaperCorpusId": "211724357"
                        },
                        {
                            "start": 1273,
                            "end": 1276,
                            "matchedPaperCorpusId": "211724357"
                        },
                        {
                            "start": 1728,
                            "end": 1732,
                            "matchedPaperCorpusId": "184487878"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98046875
                }
            ],
            "relevance_judgement": 0.98046875,
            "relevance_judgment_input_expanded": "# Title: Position-based Scaled Gradient for Model Quantization and Sparse Training\n# Venue: arXiv.org\n# Authors: Jangho Kim, Kiyoon Yoo, Nojun Kwak\n## Abstract\nWe propose the position-based scaled gradient (PSG) that scales the gradient depending on the position of a weight vector to make it more compression-friendly. First, we theoretically show that applying PSG to the standard gradient descent (GD), which is called PSGD, is equivalent to the GD in the warped weight space, a space made by warping the original weight space via an appropriately designed invertible function. Second, we empirically show that PSG acting as a regularizer to a weight vector is very useful in model compression domains such as quantization and sparse training. PSG reduces the gap between the weight distributions of a full-precision model and its compressed counterpart. This enables the versatile deployment of a model either as an uncompressed mode or as a compressed mode depending on the availability of resources. The experimental results on CIFAR-10/100 and Imagenet datasets show the effectiveness of the proposed PSG in both domains of sparse training and quantization even for extremely low bits.\n## Method\nOn the other hand, ImageNet experiment generally shows reasonable results until 6-bits but the accuracy drastically drops at 4-bits. PSGD targeting 8-bits and 6-bits marginally improves on all bits, yet also experiences drastic accuracy drop at 4-bits. In contrast, Gradient L1 (\u03bb = 0.05) and PSGD @ W4 maintain the performance of the quantized models even at 4-bits. Comparing with the second best method Gradient L1 (\u03bb = 0.05) [1], PSGD outperforms it at all bit-widths. At full precision (FP), 8-, 6-and 4-bits, the gap of performance between [1] and ours are about 4.2%, 3.8%, 1.8% and 8%, respectively. From Table 2, while the quantization noise may slightly degrade the accuracy in some cases, a general trend that using more bits leads to higher accuracy is demonstrated. Consequently, as our 4-bit targeted model (PSGD@W4) achieves comparable accuracy for all higher bits of the other methods, PSGD outperforms conventional methods in all bit-widths. Compared to other regularization methods, PSGD is able to maintain reasonable performance across all bits by constraining the distribution of the full precision weight to resemble that of the quantized weight. This quantization-friendliness is achieved by the appropriately designed scaling function. Also, unlike [1], PSGD does not need additional overhead of calculating the double-backpropagation. \n\nPost-training methods Table 3 shows that OCS, state-of-the-art post-training method, has a drastic accuracy drop at 4-bits. For OCS, following the original paper, we chose the best clipping method for both weights and activation. DFQ also has a similar tendency of showing drastic accuracy drop under the 6 bit-widths depicted in Fig. 1 at the original paper of DFQ [26]. This is due to the fundamental discrepancy between FP and quantized weight distributions as stated in Sec 1 and Fig. 1.",
            "reference_string": "[218862856 | Kim et al. | 2020 | Citations: 7]"
        },
        {
            "title": "Low-bit Shift Network for End-to-End Spoken Language Understanding",
            "venue": "Interspeech",
            "year": 2022,
            "reference_count": 22,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2207.07497",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.07497, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "36941802",
                    "name": "Anderson R. Avila"
                },
                {
                    "authorId": "2337172863",
                    "name": "Khalil Bibi"
                },
                {
                    "authorId": "2115428434",
                    "name": "Ruizhi Yang"
                },
                {
                    "authorId": "2108184905",
                    "name": "Xinlin Li"
                },
                {
                    "authorId": "2064677272",
                    "name": "Chao Xing"
                },
                {
                    "authorId": "2135917026",
                    "name": "Xiao Chen"
                }
            ],
            "abstract": "Deep neural networks (DNN) have achieved impressive suc-cess in multiple domains. Over the years, the accuracy of these models has increased with the proliferation of deeper and more complex architectures. Thus, state-of-the-art solutions are often computationally expensive, which makes them unfit to be deployed on edge computing platforms. In order to mitigate the high computation, memory, and power requirements of inferring convolutional neural networks (CNNs), we propose the use of power-of-two quantization, which quantizes continuous parameters into low-bit power-of-two values. This reduces computational complexity by removing expensive multiplication operations and with the use of low-bit weights. ResNet is adopted as the building block of our solution and the proposed model is evaluated on a spoken language understanding (SLU) task. Experimental results show improved performance for shift neural network architectures, with our low-bit quantization achieving 98.76 % on the test set which is comparable performance to its full-precision counterpart and state-of-the-art solutions.",
            "corpus_id": 250607637,
            "sentences": [
                {
                    "corpus_id": "250607637",
                    "title": "Low-bit Shift Network for End-to-End Spoken Language Understanding",
                    "text": "In Table 1, we present the performance in terms of accuracy and other quantization settings are explored. Results are based on the best model and not on the final model. For D3, for instance, we chose a model around epoch 100 which gave the best accuracy on the validation set. The same was done for the other experiments in Table 1. Results are compatible to the baseline results presented in [14], with the full-precision (FP32) even outperforming it with accuracy as high as 92.34 % and 98.97 % for the validation and test set, respectively, followed by our proposed solution quantized with 4 bits, S4, which provides 92.24 % and 98.76 %, respectively, for the validation and test sets as well. \n\nQuantization had a mild impact on the overall performance. For integer quantization, for example, quantizing with 16 bits showed to be slightly benefitial compared to quantizatizing with 2 bits with latter dropping accuracy only 0.06 % on the test set. Similar trend was found for the ternary shift network. While quantizing with 4 bits gives 98.76 % accuracy, quantizing with 2 bits provides 98.41 % accuracy, representing a decay of only 0.35 %. These results show that compressing a model with low-bit representation is feasible for the SLU task and with the ternary shift network fast inference will be possible with the absence of expensive multiplication operations.",
                    "score": 0.6511918439500938,
                    "section_title": "Results",
                    "char_start_offset": 13655,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 105
                        },
                        {
                            "start": 106,
                            "end": 169
                        },
                        {
                            "start": 170,
                            "end": 277
                        },
                        {
                            "start": 278,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 697
                        },
                        {
                            "start": 700,
                            "end": 758
                        },
                        {
                            "start": 759,
                            "end": 952
                        },
                        {
                            "start": 953,
                            "end": 1007
                        },
                        {
                            "start": 1008,
                            "end": 1147
                        },
                        {
                            "start": 1148,
                            "end": 1372
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97998046875
                }
            ],
            "relevance_judgement": 0.97998046875,
            "relevance_judgment_input_expanded": "# Title: Low-bit Shift Network for End-to-End Spoken Language Understanding\n# Venue: Interspeech\n# Authors: Anderson R. Avila, Khalil Bibi, Ruizhi Yang, Xinlin Li, Chao Xing, Xiao Chen\n## Abstract\nDeep neural networks (DNN) have achieved impressive suc-cess in multiple domains. Over the years, the accuracy of these models has increased with the proliferation of deeper and more complex architectures. Thus, state-of-the-art solutions are often computationally expensive, which makes them unfit to be deployed on edge computing platforms. In order to mitigate the high computation, memory, and power requirements of inferring convolutional neural networks (CNNs), we propose the use of power-of-two quantization, which quantizes continuous parameters into low-bit power-of-two values. This reduces computational complexity by removing expensive multiplication operations and with the use of low-bit weights. ResNet is adopted as the building block of our solution and the proposed model is evaluated on a spoken language understanding (SLU) task. Experimental results show improved performance for shift neural network architectures, with our low-bit quantization achieving 98.76 % on the test set which is comparable performance to its full-precision counterpart and state-of-the-art solutions.\n## Results\nIn Table 1, we present the performance in terms of accuracy and other quantization settings are explored. Results are based on the best model and not on the final model. For D3, for instance, we chose a model around epoch 100 which gave the best accuracy on the validation set. The same was done for the other experiments in Table 1. Results are compatible to the baseline results presented in [14], with the full-precision (FP32) even outperforming it with accuracy as high as 92.34 % and 98.97 % for the validation and test set, respectively, followed by our proposed solution quantized with 4 bits, S4, which provides 92.24 % and 98.76 %, respectively, for the validation and test sets as well. \n\nQuantization had a mild impact on the overall performance. For integer quantization, for example, quantizing with 16 bits showed to be slightly benefitial compared to quantizatizing with 2 bits with latter dropping accuracy only 0.06 % on the test set. Similar trend was found for the ternary shift network. While quantizing with 4 bits gives 98.76 % accuracy, quantizing with 2 bits provides 98.41 % accuracy, representing a decay of only 0.35 %. These results show that compressing a model with low-bit representation is feasible for the SLU task and with the ternary shift network fast inference will be possible with the absence of expensive multiplication operations.",
            "reference_string": "[250607637 | Avila et al. | 2022 | Citations: 4]"
        },
        {
            "title": "Accelerating Inference and Language Model Fusion of Recurrent Neural Network Transducers via End-to-End 4-bit Quantization",
            "venue": "Interspeech",
            "year": 2022,
            "reference_count": 29,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2206.07882",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.07882, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "152971821",
                    "name": "A. Fasoli"
                },
                {
                    "authorId": "48240124",
                    "name": "Chia-Yu Chen"
                },
                {
                    "authorId": "2140484359",
                    "name": "Mauricio Serrano"
                },
                {
                    "authorId": "1778215",
                    "name": "Swagath Venkataramani"
                },
                {
                    "authorId": "1698208",
                    "name": "G. Saon"
                },
                {
                    "authorId": "2357983",
                    "name": "Xiaodong Cui"
                },
                {
                    "authorId": "144707379",
                    "name": "Brian Kingsbury"
                },
                {
                    "authorId": "33678523",
                    "name": "K. Gopalakrishnan"
                }
            ],
            "abstract": "We report on aggressive quantization strategies that greatly accelerate inference of Recurrent Neural Network Transducers (RNN-T). We use a 4 bit integer representation for both weights and activations and apply Quantization Aware Training (QAT) to retrain the full model (acoustic encoder and language model) and achieve near-iso-accuracy. We show that customized quantization schemes that are tailored to the local properties of the network are essential to achieve good performance while limiting the computational overhead of QAT. Density ratio Language Model fusion has shown remarkable accuracy gains on RNN-T workloads but it severely increases the computational cost of inference. We show that our quantization strategies enable using large beam widths for hypothesis search while achieving streaming-compatible runtimes and a full model compression ratio of 7.6$\\times$ compared to the full precision model. Via hardware simulations, we estimate a 3.4$\\times$ acceleration from FP16 to INT4 for the end-to-end quantized RNN-T inclusive of LM fusion, resulting in a Real Time Factor (RTF) of 0.06. On the NIST Hub5 2000, Hub5 2001, and RT-03 test sets, we retain most of the gains associated with LM fusion, improving the average WER by $>$1.5%.",
            "corpus_id": 249712097,
            "sentences": [],
            "relevance_judgement": 0.97998046875,
            "relevance_judgment_input_expanded": "# Title: Accelerating Inference and Language Model Fusion of Recurrent Neural Network Transducers via End-to-End 4-bit Quantization\n# Venue: Interspeech\n# Authors: A. Fasoli, Chia-Yu Chen, Mauricio Serrano, Swagath Venkataramani, G. Saon, Xiaodong Cui, Brian Kingsbury, K. Gopalakrishnan\n## Abstract\nWe report on aggressive quantization strategies that greatly accelerate inference of Recurrent Neural Network Transducers (RNN-T). We use a 4 bit integer representation for both weights and activations and apply Quantization Aware Training (QAT) to retrain the full model (acoustic encoder and language model) and achieve near-iso-accuracy. We show that customized quantization schemes that are tailored to the local properties of the network are essential to achieve good performance while limiting the computational overhead of QAT. Density ratio Language Model fusion has shown remarkable accuracy gains on RNN-T workloads but it severely increases the computational cost of inference. We show that our quantization strategies enable using large beam widths for hypothesis search while achieving streaming-compatible runtimes and a full model compression ratio of 7.6$\\times$ compared to the full precision model. Via hardware simulations, we estimate a 3.4$\\times$ acceleration from FP16 to INT4 for the end-to-end quantized RNN-T inclusive of LM fusion, resulting in a Real Time Factor (RTF) of 0.06. On the NIST Hub5 2000, Hub5 2001, and RT-03 test sets, we retain most of the gains associated with LM fusion, improving the average WER by $>$1.5%.\n",
            "reference_string": "[249712097 | Fasoli et al. | 2022 | Citations: 6]"
        },
        {
            "title": "DSConv: Efficient Convolution Operator",
            "venue": "IEEE International Conference on Computer Vision",
            "year": 2019,
            "reference_count": 48,
            "citation_count": 67,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1901.01928",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.01928, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2065676996",
                    "name": "Marcelo Gennari"
                },
                {
                    "authorId": "144924636",
                    "name": "Roger Fawcett"
                },
                {
                    "authorId": "2824784",
                    "name": "V. Prisacariu"
                }
            ],
            "abstract": "Quantization is a popular way of increasing the speed and lowering the memory usage of Convolution Neural Networks (CNNs). When labelled training data is available, network weights and activations have successfully been quantized down to 1-bit. The same cannot be said about the scenario when labelled training data is not available, e.g. when quantizing a pre-trained model, where current approaches show, at best, no loss of accuracy at 8-bit quantizations. We introduce DSConv, a flexible quantized convolution operator that replaces single-precision operations with their far less expensive integer counterparts, while maintaining the probability distributions over both the kernel weights and the outputs. We test our model as a plug-and-play replacement for standard convolution on most popular neural network architectures, ResNet, DenseNet, GoogLeNet, AlexNet and VGG-Net and demonstrate state-of-the-art results, with less than 1% loss of accuracy, without retraining, using only 4-bit quantization. We also show how a distillation-based adaptation stage with unlabelled data can improve results even further.",
            "corpus_id": 57573783,
            "sentences": [
                {
                    "corpus_id": "57573783",
                    "title": "DSConv: Efficient Convolution Operator",
                    "text": "Our method is designed to produce accurate results even when training data is not available, by quantizing from a pre-trained network. The second and fifth rows of Table 3 show that for both the compression and fast inference problems, no loss of accuracy can be achieved with 8-bit networks even with very high block sizes, as already demonstrated by previous papers and real-life applications [26,16]. The results also shows that compression down to 4-bits (which in convolutions with channel size input of 256 would yield a 5x compression rate) results in an accuracy drop of only 1% to 2% depending on the architecture. It can also be seen that very low-bit quantizations become noticeably unstable, varying greatly with architecture. At the extreme, using 2bits, losses vary by as much as -40% for GoogLeNet and only -11% for ResNet50. \n\nThe last four rows show the results for the fast inference problem. Also as known in previous research papers [26,16], models of 8/8 bits lose only around 0.1% accuracy. For models of 5/5 and 4/4, we get a drop of 1% to 3% in accuracy. To our knowledge, this is the smallest bit-width for fast inference that has been reported when models are neither retrained nor adapted. \n\nThe variance with respect to architecture suggests that quantization for 5 or less bits is unstable. However, even for fast-inference with 8-bit accuracy, it can achieve stable and satisfactory results within 1% of the full precision model. Accuracy with respect to Block Size Table 4 shows the accuracy with respect to block size. The table shows the results of quantizing the weights only, where the number in parenthesis represents the bit-width of the weights. Naturally, this represents a trade-off between memory and computational load against precision of the network. The largest discrepancy in accuracy can be seen in models that use 3 or 2 bit weights. For example, the GoogLeNet model with 3bits improves its Top1 accuracy from 5.7% to 56.8% when changing from a block-size of 256 to 8.",
                    "score": 0.6935088675822614,
                    "section_title": "Accuracy Before Retraining or Adaptation",
                    "char_start_offset": 21747,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 135,
                            "end": 403
                        },
                        {
                            "start": 404,
                            "end": 623
                        },
                        {
                            "start": 624,
                            "end": 738
                        },
                        {
                            "start": 739,
                            "end": 840
                        },
                        {
                            "start": 843,
                            "end": 910
                        },
                        {
                            "start": 911,
                            "end": 1012
                        },
                        {
                            "start": 1013,
                            "end": 1078
                        },
                        {
                            "start": 1079,
                            "end": 1216
                        },
                        {
                            "start": 1219,
                            "end": 1319
                        },
                        {
                            "start": 1320,
                            "end": 1459
                        },
                        {
                            "start": 1460,
                            "end": 1550
                        },
                        {
                            "start": 1551,
                            "end": 1683
                        },
                        {
                            "start": 1684,
                            "end": 1794
                        },
                        {
                            "start": 1795,
                            "end": 1881
                        },
                        {
                            "start": 1882,
                            "end": 2016
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 399,
                            "end": 402,
                            "matchedPaperCorpusId": "51610353"
                        },
                        {
                            "start": 957,
                            "end": 960,
                            "matchedPaperCorpusId": "51610353"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9765625
                }
            ],
            "relevance_judgement": 0.9765625,
            "relevance_judgment_input_expanded": "# Title: DSConv: Efficient Convolution Operator\n# Venue: IEEE International Conference on Computer Vision\n# Authors: Marcelo Gennari, Roger Fawcett, V. Prisacariu\n## Abstract\nQuantization is a popular way of increasing the speed and lowering the memory usage of Convolution Neural Networks (CNNs). When labelled training data is available, network weights and activations have successfully been quantized down to 1-bit. The same cannot be said about the scenario when labelled training data is not available, e.g. when quantizing a pre-trained model, where current approaches show, at best, no loss of accuracy at 8-bit quantizations. We introduce DSConv, a flexible quantized convolution operator that replaces single-precision operations with their far less expensive integer counterparts, while maintaining the probability distributions over both the kernel weights and the outputs. We test our model as a plug-and-play replacement for standard convolution on most popular neural network architectures, ResNet, DenseNet, GoogLeNet, AlexNet and VGG-Net and demonstrate state-of-the-art results, with less than 1% loss of accuracy, without retraining, using only 4-bit quantization. We also show how a distillation-based adaptation stage with unlabelled data can improve results even further.\n## Accuracy Before Retraining or Adaptation\nOur method is designed to produce accurate results even when training data is not available, by quantizing from a pre-trained network. The second and fifth rows of Table 3 show that for both the compression and fast inference problems, no loss of accuracy can be achieved with 8-bit networks even with very high block sizes, as already demonstrated by previous papers and real-life applications [26,16]. The results also shows that compression down to 4-bits (which in convolutions with channel size input of 256 would yield a 5x compression rate) results in an accuracy drop of only 1% to 2% depending on the architecture. It can also be seen that very low-bit quantizations become noticeably unstable, varying greatly with architecture. At the extreme, using 2bits, losses vary by as much as -40% for GoogLeNet and only -11% for ResNet50. \n\nThe last four rows show the results for the fast inference problem. Also as known in previous research papers [26,16], models of 8/8 bits lose only around 0.1% accuracy. For models of 5/5 and 4/4, we get a drop of 1% to 3% in accuracy. To our knowledge, this is the smallest bit-width for fast inference that has been reported when models are neither retrained nor adapted. \n\nThe variance with respect to architecture suggests that quantization for 5 or less bits is unstable. However, even for fast-inference with 8-bit accuracy, it can achieve stable and satisfactory results within 1% of the full precision model. Accuracy with respect to Block Size Table 4 shows the accuracy with respect to block size. The table shows the results of quantizing the weights only, where the number in parenthesis represents the bit-width of the weights. Naturally, this represents a trade-off between memory and computational load against precision of the network. The largest discrepancy in accuracy can be seen in models that use 3 or 2 bit weights. For example, the GoogLeNet model with 3bits improves its Top1 accuracy from 5.7% to 56.8% when changing from a block-size of 256 to 8.",
            "reference_string": "[57573783 | Gennari et al. | 2019 | Citations: 67]"
        },
        {
            "title": "Model Compression Techniques in Biometrics Applications: A Survey",
            "venue": "Information Fusion",
            "year": 2024,
            "reference_count": 101,
            "citation_count": 10,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.10139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2219025387",
                    "name": "Eduarda Caldeira"
                },
                {
                    "authorId": "2044499858",
                    "name": "Pedro C. Neto"
                },
                {
                    "authorId": "1516862195",
                    "name": "Marco Huber"
                },
                {
                    "authorId": "2265721",
                    "name": "N. Damer"
                },
                {
                    "authorId": "145279873",
                    "name": "Ana F. Sequeira"
                }
            ],
            "abstract": "The development of deep learning algorithms has extensively empowered humanity's task automatization capacity. However, the huge improvement in the performance of these models is highly correlated with their increasing level of complexity, limiting their usefulness in human-oriented applications, which are usually deployed in resource-constrained devices. This led to the development of compression techniques that drastically reduce the computational and memory costs of deep learning models without significant performance degradation. This paper aims to systematize the current literature on this topic by presenting a comprehensive survey of model compression techniques in biometrics applications, namely quantization, knowledge distillation and pruning. We conduct a critical analysis of the comparative value of these techniques, focusing on their advantages and disadvantages and presenting suggestions for future work directions that can potentially improve the current methods. Additionally, we discuss and analyze the link between model bias and model compression, highlighting the need to direct compression research toward model fairness in future works.",
            "corpus_id": 267035013,
            "sentences": [
                {
                    "corpus_id": "267035013",
                    "title": "Model Compression Techniques in Biometrics Applications: A Survey",
                    "text": "Using a lower number of bits to represent the weights and activations of the layers that do not have a big negative impact on the performance can highly reduce the computational resources needed at inference time without significant performance degradation. The authors prove that model performance suffers a bigger impact when the first layers are quantized due to the forward propagation of the performed approximations, allowing for stronger quantization when progressing toward the last layers of the network. The final MPQ framework comprised 8, 4 and 2-bit quantization for the initial, middle and final layers, respectively, resulting in a network with only 10% of the computational footprint of the original model and a performance drop of 0.68 percentual points on LFW. Thus, MPQ can be extremely useful in establishing a compromise between achieving good performances and having computationally efficient models. However, determining the MPQ configuration manually will most likely lead to a suboptimal quantization, especially considering the exponential growth of possible configurations with the network size [14], revealing the need to develop methods that automatize the MPQ configuration selection during quantization. \n\nKolf et al. [3] focused on the compression of models trained on the periocular recognition (PR) task through a QAT procedure that performed WAQ at 8, 6 and 4-bit precision. The achieved results show that the more the number of bits of the final representation is reduced, the bigger the verified performance drop, as expected. When b = 8 or b = 6, the performance degradation was small in the identification and verification tasks and, in some cases, the quantized models even slightly surpassed the baseline. For b = 4, the performance suffered an extreme drop. As an example, a MobileFaceNet trained in the identification task suffered a reduction in accuracy from 99.80% in 6-bit quantization to 5.53% in 4-bit quantization. Hence, in this experimental setup, 4-bit quantization is not enough to achieve good performance, showing that this precision level does not allow the model to learn how to encode the relevant features properly. \n\nKolf et al. [41] also compressed PR models in a WAQ framework at 8, 6 and 4-bit precision. The FP models were trained on visible spectrum images, which are widely available and labeled.",
                    "score": 0.5965343538980915,
                    "section_title": "A. Quantization",
                    "char_start_offset": 37214,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 257
                        },
                        {
                            "start": 258,
                            "end": 513
                        },
                        {
                            "start": 514,
                            "end": 778
                        },
                        {
                            "start": 779,
                            "end": 922
                        },
                        {
                            "start": 923,
                            "end": 1234
                        },
                        {
                            "start": 1237,
                            "end": 1409
                        },
                        {
                            "start": 1410,
                            "end": 1563
                        },
                        {
                            "start": 1564,
                            "end": 1746
                        },
                        {
                            "start": 1747,
                            "end": 1799
                        },
                        {
                            "start": 1800,
                            "end": 1964
                        },
                        {
                            "start": 1965,
                            "end": 2175
                        },
                        {
                            "start": 2178,
                            "end": 2268
                        },
                        {
                            "start": 2269,
                            "end": 2363
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1122,
                            "end": 1126,
                            "matchedPaperCorpusId": "232352683"
                        },
                        {
                            "start": 1249,
                            "end": 1252,
                            "matchedPaperCorpusId": "255997000"
                        },
                        {
                            "start": 2190,
                            "end": 2194,
                            "matchedPaperCorpusId": "258493730"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97607421875
                }
            ],
            "relevance_judgement": 0.97607421875,
            "relevance_judgment_input_expanded": "# Title: Model Compression Techniques in Biometrics Applications: A Survey\n# Venue: Information Fusion\n# Authors: Eduarda Caldeira, Pedro C. Neto, Marco Huber, N. Damer, Ana F. Sequeira\n## Abstract\nThe development of deep learning algorithms has extensively empowered humanity's task automatization capacity. However, the huge improvement in the performance of these models is highly correlated with their increasing level of complexity, limiting their usefulness in human-oriented applications, which are usually deployed in resource-constrained devices. This led to the development of compression techniques that drastically reduce the computational and memory costs of deep learning models without significant performance degradation. This paper aims to systematize the current literature on this topic by presenting a comprehensive survey of model compression techniques in biometrics applications, namely quantization, knowledge distillation and pruning. We conduct a critical analysis of the comparative value of these techniques, focusing on their advantages and disadvantages and presenting suggestions for future work directions that can potentially improve the current methods. Additionally, we discuss and analyze the link between model bias and model compression, highlighting the need to direct compression research toward model fairness in future works.\n## A. Quantization\nUsing a lower number of bits to represent the weights and activations of the layers that do not have a big negative impact on the performance can highly reduce the computational resources needed at inference time without significant performance degradation. The authors prove that model performance suffers a bigger impact when the first layers are quantized due to the forward propagation of the performed approximations, allowing for stronger quantization when progressing toward the last layers of the network. The final MPQ framework comprised 8, 4 and 2-bit quantization for the initial, middle and final layers, respectively, resulting in a network with only 10% of the computational footprint of the original model and a performance drop of 0.68 percentual points on LFW. Thus, MPQ can be extremely useful in establishing a compromise between achieving good performances and having computationally efficient models. However, determining the MPQ configuration manually will most likely lead to a suboptimal quantization, especially considering the exponential growth of possible configurations with the network size [14], revealing the need to develop methods that automatize the MPQ configuration selection during quantization. \n\nKolf et al. [3] focused on the compression of models trained on the periocular recognition (PR) task through a QAT procedure that performed WAQ at 8, 6 and 4-bit precision. The achieved results show that the more the number of bits of the final representation is reduced, the bigger the verified performance drop, as expected. When b = 8 or b = 6, the performance degradation was small in the identification and verification tasks and, in some cases, the quantized models even slightly surpassed the baseline. For b = 4, the performance suffered an extreme drop. As an example, a MobileFaceNet trained in the identification task suffered a reduction in accuracy from 99.80% in 6-bit quantization to 5.53% in 4-bit quantization. Hence, in this experimental setup, 4-bit quantization is not enough to achieve good performance, showing that this precision level does not allow the model to learn how to encode the relevant features properly. \n\nKolf et al. [41] also compressed PR models in a WAQ framework at 8, 6 and 4-bit precision. The FP models were trained on visible spectrum images, which are widely available and labeled.",
            "reference_string": "[267035013 | Caldeira et al. | 2024 | Citations: 10]"
        },
        {
            "title": "FPGA-Based Hybrid-Type Implementation of Quantized Neural Networks for Remote Sensing Applications",
            "venue": "Italian National Conference on Sensors",
            "year": 2019,
            "reference_count": 37,
            "citation_count": 28,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/1424-8220/19/4/924/pdf?version=1550822096",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC6412419, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1382778048",
                    "name": "Xin Wei"
                },
                {
                    "authorId": "2109188137",
                    "name": "Wenchao Liu"
                },
                {
                    "authorId": "2146072742",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145044878",
                    "name": "Long Ma"
                },
                {
                    "authorId": "92779203",
                    "name": "He Chen"
                },
                {
                    "authorId": "2199301922",
                    "name": "Zhuang Yin"
                }
            ],
            "abstract": "Recently, extensive convolutional neural network (CNN)-based methods have been used in remote sensing applications, such as object detection and classification, and have achieved significant improvements in performance. Furthermore, there are a lot of hardware implementation demands for remote sensing real-time processing applications. However, the operation and storage processes in floating-point models hinder the deployment of networks in hardware implements with limited resource and power budgets, such as field-programmable gate arrays (FPGAs) and application-specific integrated circuits (ASICs). To solve this problem, this paper focuses on optimizing the hardware design of CNN with low bit-width integers by quantization. First, a symmetric quantization scheme-based hybrid-type inference method was proposed, which uses the low bit-width integer to replace floating-point precision. Then, a training approach for the quantized network is introduced to reduce accuracy degradation. Finally, a processing engine (PE) with a low bit-width is proposed to optimize the hardware design of FPGA for remote sensing image classification. Besides, a fused-layer PE is also presented for state-of-the-art CNNs equipped with Batch-Normalization and LeakyRelu. The experiments performed on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset using a graphics processing unit (GPU) demonstrate that the accuracy of 8-bit quantized model drops by about 1%, which is an acceptable accuracy loss. The accuracy result tested on FPGA is consistent with that of GPU. As for the resource consumptions of FPGA, the Look Up Table (LUT), Flip-flop (FF), Digital Signal Processor (DSP), and Block Random Access Memory (BRAM) are reduced by 46.21%, 43.84%, 45%, and 51%, respectively, compared with that of floating-point implementation.",
            "corpus_id": 73479694,
            "sentences": [
                {
                    "corpus_id": "73479694",
                    "title": "FPGA-Based Hybrid-Type Implementation of Quantized Neural Networks for Remote Sensing Applications",
                    "text": "Table 4 shows the classification accuracies of the fundamental model and quantized model in five data formats. The model with the floating-point data type obtains the greatest result of 98.43%. The classification accuracy of the 4-bit quantized model has the maximum loss of 4.0%. The other quantized models obtain satisfying performance and the average results are all around 97.2%, with a 1.2% accuracy degradation, compared to floating-point types. It can be seen that the classification accuracy degradation is reduced as the bit-width increases, which may be due to the increased quantization resolution resulting in a computing error reduction. However, the performance can hardly improve after reaching the maximum as the inherent redundancy within the network is limited. The standards deviations of 4-bit and 6-bit quantized models are higher than the others. This means that the stability of training is not good when adopting a low bit-width quantization scheme and multiple trainings are required to obtain a satisfying performance. The memory requirement for the weight storage of quantized models is shown in Table 5. These results indicate that the compression rate of 4, 6, 8, 10, and 12-bit quantization is 8.0\u00d7, 5.3\u00d7, 4.0\u00d7, 3.2\u00d7 and 2.7\u00d7, respectively. Since the memory requirement of scaling factors is negligible, the compression rate \u03b7 can be approximately calculated by the following: \n\nThe image classification performance will degrade when the network is quantized. However fortunately, it is within an acceptable range. More importantly, the requirement of on-chip Static Random-Access Memory (SRAM) and memory bandwidth sharply decreases. The 8-bit quantized model achieves a trade-off between classification accuracy degradation and this requirement; therefore, the 8-bit quantization scheme is used for the hardware optimization design in this paper. Figure 9 shows the test results for each epoch for all the aforementioned models. To clearly reflect the test results, the formula, described in the figure, is used to smooth the curve, and the smoothing rate is set to 0.65. As can be seen from the results, floating-point networks achieve nearly best performance at about 50 epochs, while all the quantized networks require about 130 epochs.",
                    "score": 0.6101221161592826,
                    "section_title": "Dataset Description and Data Preprocessing",
                    "char_start_offset": 40382,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 110
                        },
                        {
                            "start": 111,
                            "end": 193
                        },
                        {
                            "start": 194,
                            "end": 280
                        },
                        {
                            "start": 281,
                            "end": 451
                        },
                        {
                            "start": 452,
                            "end": 650
                        },
                        {
                            "start": 651,
                            "end": 779
                        },
                        {
                            "start": 780,
                            "end": 868
                        },
                        {
                            "start": 869,
                            "end": 1044
                        },
                        {
                            "start": 1045,
                            "end": 1131
                        },
                        {
                            "start": 1132,
                            "end": 1270
                        },
                        {
                            "start": 1271,
                            "end": 1406
                        },
                        {
                            "start": 1409,
                            "end": 1489
                        },
                        {
                            "start": 1490,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1664
                        },
                        {
                            "start": 1665,
                            "end": 1878
                        },
                        {
                            "start": 1879,
                            "end": 1960
                        },
                        {
                            "start": 1961,
                            "end": 2103
                        },
                        {
                            "start": 2104,
                            "end": 2271
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97509765625
                }
            ],
            "relevance_judgement": 0.97509765625,
            "relevance_judgment_input_expanded": "# Title: FPGA-Based Hybrid-Type Implementation of Quantized Neural Networks for Remote Sensing Applications\n# Venue: Italian National Conference on Sensors\n# Authors: Xin Wei, Wenchao Liu, Lei Chen, Long Ma, He Chen, Zhuang Yin\n## Abstract\nRecently, extensive convolutional neural network (CNN)-based methods have been used in remote sensing applications, such as object detection and classification, and have achieved significant improvements in performance. Furthermore, there are a lot of hardware implementation demands for remote sensing real-time processing applications. However, the operation and storage processes in floating-point models hinder the deployment of networks in hardware implements with limited resource and power budgets, such as field-programmable gate arrays (FPGAs) and application-specific integrated circuits (ASICs). To solve this problem, this paper focuses on optimizing the hardware design of CNN with low bit-width integers by quantization. First, a symmetric quantization scheme-based hybrid-type inference method was proposed, which uses the low bit-width integer to replace floating-point precision. Then, a training approach for the quantized network is introduced to reduce accuracy degradation. Finally, a processing engine (PE) with a low bit-width is proposed to optimize the hardware design of FPGA for remote sensing image classification. Besides, a fused-layer PE is also presented for state-of-the-art CNNs equipped with Batch-Normalization and LeakyRelu. The experiments performed on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset using a graphics processing unit (GPU) demonstrate that the accuracy of 8-bit quantized model drops by about 1%, which is an acceptable accuracy loss. The accuracy result tested on FPGA is consistent with that of GPU. As for the resource consumptions of FPGA, the Look Up Table (LUT), Flip-flop (FF), Digital Signal Processor (DSP), and Block Random Access Memory (BRAM) are reduced by 46.21%, 43.84%, 45%, and 51%, respectively, compared with that of floating-point implementation.\n## Dataset Description and Data Preprocessing\nTable 4 shows the classification accuracies of the fundamental model and quantized model in five data formats. The model with the floating-point data type obtains the greatest result of 98.43%. The classification accuracy of the 4-bit quantized model has the maximum loss of 4.0%. The other quantized models obtain satisfying performance and the average results are all around 97.2%, with a 1.2% accuracy degradation, compared to floating-point types. It can be seen that the classification accuracy degradation is reduced as the bit-width increases, which may be due to the increased quantization resolution resulting in a computing error reduction. However, the performance can hardly improve after reaching the maximum as the inherent redundancy within the network is limited. The standards deviations of 4-bit and 6-bit quantized models are higher than the others. This means that the stability of training is not good when adopting a low bit-width quantization scheme and multiple trainings are required to obtain a satisfying performance. The memory requirement for the weight storage of quantized models is shown in Table 5. These results indicate that the compression rate of 4, 6, 8, 10, and 12-bit quantization is 8.0\u00d7, 5.3\u00d7, 4.0\u00d7, 3.2\u00d7 and 2.7\u00d7, respectively. Since the memory requirement of scaling factors is negligible, the compression rate \u03b7 can be approximately calculated by the following: \n\nThe image classification performance will degrade when the network is quantized. However fortunately, it is within an acceptable range. More importantly, the requirement of on-chip Static Random-Access Memory (SRAM) and memory bandwidth sharply decreases. The 8-bit quantized model achieves a trade-off between classification accuracy degradation and this requirement; therefore, the 8-bit quantization scheme is used for the hardware optimization design in this paper. Figure 9 shows the test results for each epoch for all the aforementioned models. To clearly reflect the test results, the formula, described in the figure, is used to smooth the curve, and the smoothing rate is set to 0.65. As can be seen from the results, floating-point networks achieve nearly best performance at about 50 epochs, while all the quantized networks require about 130 epochs.",
            "reference_string": "[73479694 | Wei et al. | 2019 | Citations: 28]"
        },
        {
            "title": "On the Accuracy of Analog Neural Network Inference Accelerators [Feature]",
            "venue": "IEEE Circuits and Systems Magazine",
            "year": 2021,
            "reference_count": 70,
            "citation_count": 37,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2109.01262",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.01262, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2061135402",
                    "name": "T. Xiao"
                },
                {
                    "authorId": "153505332",
                    "name": "Ben Feinberg"
                },
                {
                    "authorId": "2464307",
                    "name": "C. Bennett"
                },
                {
                    "authorId": "35839928",
                    "name": "V. Prabhakar"
                },
                {
                    "authorId": "144956677",
                    "name": "Prashant Saxena"
                },
                {
                    "authorId": "1578156774",
                    "name": "V. Agrawal"
                },
                {
                    "authorId": "3491847",
                    "name": "S. Agarwal"
                },
                {
                    "authorId": "30782065",
                    "name": "M. Marinella"
                }
            ],
            "abstract": "Specialized accelerators have recently garnered attention as a method to reduce the power consumption of neural network inference. A promising category of accelerators utilizes nonvolatile memory arrays to both store weights and perform in situ analog computation inside the array. While prior work has explored the design space of analog accelerators to optimize performance and energy efficiency, there is seldom a rigorous evaluation of the accuracy of these accelerators. This work shows how architectural design decisions, particularly in mapping neural network parameters to analog memory cells, influence inference accuracy. When evaluated using ResNet50 on ImageNet, the resilience of the system to analog non-idealities\u2014cell programming errors, analog-to-digital converter resolution, and array parasitic resistances\u2014all improve when analog quantities in the hardware are made proportional to the numerical values in the network. Moreover, contrary to the assumptions of prior work, nearly equivalent resilience to cell imprecision can be achieved by fully storing weights as analog quantities, rather than spreading weight bits across multiple devices, often referred to as bit slicing. By exploiting proportionality, analog system designers have the freedom to match the precision of the hardware to the needs of the algorithm, rather than attempting to guarantee the same level of precision in the intermediate results as an equivalent digital accelerator. This ultimately results in an analog accelerator that is more accurate, more robust to analog errors, and more energy-efficient.",
            "corpus_id": 237416732,
            "sentences": [
                {
                    "corpus_id": "237416732",
                    "title": "On the Accuracy of Analog Neural Network Inference Accelerators [Feature]",
                    "text": "The digital software accuracy of this network is shown in Table 2. When simulating the analog accuracy of this network, these scaling steps are processed digitally between in situ MVMs. \n\nFig. 19 compares the error sensitivity of the 4-bit QAT model with the floating-point ResNet50-v1.5 model, whose weights are quantized to 8 bits after training. For a fair comparison, an 8-bit ADC is included for both cases; in the 4-bit model, this higher-resolution ADC helps minimize errors prior to the 4-bit quantization step, which is performed digitally. Fig. 19(a) shows that the 4-bit model is substantially more resilient to state-independent errors than the 8-bit network. This results entirely from activation quantization, and not weight quantization. The large separation between the 16 activation levels effectively cuts off the propagation of accumulated cell errors from one layer to the next. The same cell error results in a smaller dot product error on average. Fig. 19(b) shows that the 4-bit network is also more resilient to state-proportional error, but here the benefit is smaller. This can be explained by the different weight value distributions of the two networks. With only 16 levels, the distribution of the 4-bit weights cannot peak as sharply at zero as the 8-bit weights, which have 256 levels. As a result, the memory cells in the 4-bit network have a significantly higher average conductance (7.52% of G max ) than the cells in the 8-bit network (1.95% of G max ). Thus, deeply quantized weights can actually be harmful for analog systems, as it leads to a higher state-proportional error per cell. This is ultimately outweighed by the benefit of quantized activations, so there is a net improvement in error sensitivity. \n\nNotably, for the 8-bit network, the sensitivity to both types of error with an 8-bit ADC remains largely unchanged from the case with no ADC quantization, shown in Fig. 9(b) and Fig. 10(b). As shown in Section 6, a calibrated 8-bit ADC on its own induces very little accuracy loss for this network.",
                    "score": 0.60963431438721,
                    "section_title": "SUPPRESSING ERROR PROPAGATION",
                    "char_start_offset": 55501,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 66
                        },
                        {
                            "start": 67,
                            "end": 185
                        },
                        {
                            "start": 188,
                            "end": 348
                        },
                        {
                            "start": 349,
                            "end": 549
                        },
                        {
                            "start": 550,
                            "end": 671
                        },
                        {
                            "start": 672,
                            "end": 752
                        },
                        {
                            "start": 753,
                            "end": 898
                        },
                        {
                            "start": 899,
                            "end": 969
                        },
                        {
                            "start": 970,
                            "end": 1094
                        },
                        {
                            "start": 1095,
                            "end": 1181
                        },
                        {
                            "start": 1182,
                            "end": 1316
                        },
                        {
                            "start": 1317,
                            "end": 1488
                        },
                        {
                            "start": 1489,
                            "end": 1622
                        },
                        {
                            "start": 1623,
                            "end": 1745
                        },
                        {
                            "start": 1748,
                            "end": 1937
                        },
                        {
                            "start": 1938,
                            "end": 2046
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.974609375
                },
                {
                    "corpus_id": "237416732",
                    "title": "On the Accuracy of Analog Neural Network Inference Accelerators [Feature]",
                    "text": "As shown in Section 5, the accuracy loss due to accumulated cell errors can be minmized by using sufficiently precise memory cells and exploiting state-proportional errors. With less precise cells, some prior work has relied on ADC quantization to cut off the propagation of cell errors from layer to layer in a DNN [30,66,71]. Yang et al. [66] activated only a few rows per MVM, such that on average, the accumulated errors on a bit line fall below the separation of levels in an  ADC, as shown in Fig. 18(b). While this approach succeeds in suppressing error propagation, it reduces energy efficiency since many more analog MVMs (and ADC operations) are needed to process each layer. A coarse ADC can provide the same benefit without reducing the number of rows, as shown in Fig. 18(c), but the accuracy would suffer due to quantization errors, as discussed in Section 6. A purely hardware solution cannot solve this problem, but it is possible to eliminate the quantization errors by training a DNN to tolerate low-precision activations during inference. This would combine the benefits of low quantization errors, greater resilience to cell errors, and high energy efficiency. \n\nUnlike training techniques that are specialized for analog systems (see Section 2.5), quantization-aware training (QAT) benefits digital accelerators by reducing the computational load at inference time. Therefore, there has been much recent work on 4-bit or lower resolution networks with nearly no accuracy loss relative to floating-point networks [12,59,70]. Importantly, the broad applicability of low-precision networks increases the likelihood that QAT methods can be integrated into state-of-the-art training workflows. \n\nThis section evaluates a 4-bit QAT network with the ResNet50-v1.5 topology, submitted by Nvidia to the MLPerf Inference Benchmark [1]. The network uses 4-bit weights and activations in all layers except the first and last, which use 8-bit weights. Each ReLU output is multiplied by 16-bit scaling factors before quantizing to 4 bits. The digital software accuracy of this network is shown in Table 2.",
                    "score": 0.6484805681660575,
                    "section_title": "SUPPRESSING ERROR PROPAGATION",
                    "char_start_offset": 53455,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 510
                        },
                        {
                            "start": 511,
                            "end": 685
                        },
                        {
                            "start": 686,
                            "end": 873
                        },
                        {
                            "start": 874,
                            "end": 1057
                        },
                        {
                            "start": 1058,
                            "end": 1180
                        },
                        {
                            "start": 1183,
                            "end": 1386
                        },
                        {
                            "start": 1387,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1709
                        },
                        {
                            "start": 1712,
                            "end": 1846
                        },
                        {
                            "start": 1847,
                            "end": 1959
                        },
                        {
                            "start": 1960,
                            "end": 2045
                        },
                        {
                            "start": 2046,
                            "end": 2112
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 316,
                            "end": 320,
                            "matchedPaperCorpusId": "1589163"
                        },
                        {
                            "start": 320,
                            "end": 323,
                            "matchedPaperCorpusId": "189818963"
                        },
                        {
                            "start": 323,
                            "end": 326,
                            "matchedPaperCorpusId": "209405390"
                        },
                        {
                            "start": 340,
                            "end": 344,
                            "matchedPaperCorpusId": "189818963"
                        },
                        {
                            "start": 1540,
                            "end": 1543,
                            "matchedPaperCorpusId": "50784025"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96484375
                }
            ],
            "relevance_judgement": 0.974609375,
            "relevance_judgment_input_expanded": "# Title: On the Accuracy of Analog Neural Network Inference Accelerators [Feature]\n# Venue: IEEE Circuits and Systems Magazine\n# Authors: T. Xiao, Ben Feinberg, C. Bennett, V. Prabhakar, Prashant Saxena, V. Agrawal, S. Agarwal, M. Marinella\n## Abstract\nSpecialized accelerators have recently garnered attention as a method to reduce the power consumption of neural network inference. A promising category of accelerators utilizes nonvolatile memory arrays to both store weights and perform in situ analog computation inside the array. While prior work has explored the design space of analog accelerators to optimize performance and energy efficiency, there is seldom a rigorous evaluation of the accuracy of these accelerators. This work shows how architectural design decisions, particularly in mapping neural network parameters to analog memory cells, influence inference accuracy. When evaluated using ResNet50 on ImageNet, the resilience of the system to analog non-idealities\u2014cell programming errors, analog-to-digital converter resolution, and array parasitic resistances\u2014all improve when analog quantities in the hardware are made proportional to the numerical values in the network. Moreover, contrary to the assumptions of prior work, nearly equivalent resilience to cell imprecision can be achieved by fully storing weights as analog quantities, rather than spreading weight bits across multiple devices, often referred to as bit slicing. By exploiting proportionality, analog system designers have the freedom to match the precision of the hardware to the needs of the algorithm, rather than attempting to guarantee the same level of precision in the intermediate results as an equivalent digital accelerator. This ultimately results in an analog accelerator that is more accurate, more robust to analog errors, and more energy-efficient.\n## SUPPRESSING ERROR PROPAGATION\nAs shown in Section 5, the accuracy loss due to accumulated cell errors can be minmized by using sufficiently precise memory cells and exploiting state-proportional errors. With less precise cells, some prior work has relied on ADC quantization to cut off the propagation of cell errors from layer to layer in a DNN [30,66,71]. Yang et al. [66] activated only a few rows per MVM, such that on average, the accumulated errors on a bit line fall below the separation of levels in an  ADC, as shown in Fig. 18(b). While this approach succeeds in suppressing error propagation, it reduces energy efficiency since many more analog MVMs (and ADC operations) are needed to process each layer. A coarse ADC can provide the same benefit without reducing the number of rows, as shown in Fig. 18(c), but the accuracy would suffer due to quantization errors, as discussed in Section 6. A purely hardware solution cannot solve this problem, but it is possible to eliminate the quantization errors by training a DNN to tolerate low-precision activations during inference. This would combine the benefits of low quantization errors, greater resilience to cell errors, and high energy efficiency. \n\nUnlike training techniques that are specialized for analog systems (see Section 2.5), quantization-aware training (QAT) benefits digital accelerators by reducing the computational load at inference time. Therefore, there has been much recent work on 4-bit or lower resolution networks with nearly no accuracy loss relative to floating-point networks [12,59,70]. Importantly, the broad applicability of low-precision networks increases the likelihood that QAT methods can be integrated into state-of-the-art training workflows. \n\nThis section evaluates a 4-bit QAT network with the ResNet50-v1.5 topology, submitted by Nvidia to the MLPerf Inference Benchmark [1]. The network uses 4-bit weights and activations in all layers except the first and last, which use 8-bit weights. Each ReLU output is multiplied by 16-bit scaling factors before quantizing to 4 bits. The digital software accuracy of this network is shown in Table 2.\n...\nThe digital software accuracy of this network is shown in Table 2. When simulating the analog accuracy of this network, these scaling steps are processed digitally between in situ MVMs. \n\nFig. 19 compares the error sensitivity of the 4-bit QAT model with the floating-point ResNet50-v1.5 model, whose weights are quantized to 8 bits after training. For a fair comparison, an 8-bit ADC is included for both cases; in the 4-bit model, this higher-resolution ADC helps minimize errors prior to the 4-bit quantization step, which is performed digitally. Fig. 19(a) shows that the 4-bit model is substantially more resilient to state-independent errors than the 8-bit network. This results entirely from activation quantization, and not weight quantization. The large separation between the 16 activation levels effectively cuts off the propagation of accumulated cell errors from one layer to the next. The same cell error results in a smaller dot product error on average. Fig. 19(b) shows that the 4-bit network is also more resilient to state-proportional error, but here the benefit is smaller. This can be explained by the different weight value distributions of the two networks. With only 16 levels, the distribution of the 4-bit weights cannot peak as sharply at zero as the 8-bit weights, which have 256 levels. As a result, the memory cells in the 4-bit network have a significantly higher average conductance (7.52% of G max ) than the cells in the 8-bit network (1.95% of G max ). Thus, deeply quantized weights can actually be harmful for analog systems, as it leads to a higher state-proportional error per cell. This is ultimately outweighed by the benefit of quantized activations, so there is a net improvement in error sensitivity. \n\nNotably, for the 8-bit network, the sensitivity to both types of error with an 8-bit ADC remains largely unchanged from the case with no ADC quantization, shown in Fig. 9(b) and Fig. 10(b). As shown in Section 6, a calibrated 8-bit ADC on its own induces very little accuracy loss for this network.",
            "reference_string": "[237416732 | Xiao et al. | 2021 | Citations: 37]"
        },
        {
            "title": "Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference",
            "venue": "arXiv.org",
            "year": 2018,
            "reference_count": 31,
            "citation_count": 94,
            "influential_citation_count": 8,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1809.04191, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46571359",
                    "name": "J. McKinstry"
                },
                {
                    "authorId": "2357931",
                    "name": "S. K. Esser"
                },
                {
                    "authorId": "2730753",
                    "name": "R. Appuswamy"
                },
                {
                    "authorId": "2064431971",
                    "name": "Deepika Bablani"
                },
                {
                    "authorId": "2248110488",
                    "name": "John V. Arthur"
                },
                {
                    "authorId": "3121907",
                    "name": "Izzet B. Yildiz"
                },
                {
                    "authorId": "1944330",
                    "name": "D. Modha"
                }
            ],
            "abstract": "To realize the promise of ubiquitous embedded deep network inference, it is essential to seek limits of energy and area efficiency. To this end, low-precision networks offer tremendous promise because both energy and area scale down quadratically with the reduction in precision. Here we demonstrate ResNet-18, -34, -50, -152, Inception-v3, Densenet-161, and VGG-16bn networks on the ImageNet classification benchmark that, at 8-bit precision exceed the accuracy of the full-precision baseline networks after one epoch of finetuning, thereby leveraging the availability of pretrained models. We also demonstrate ResNet-18, -34, -50, -152, Densenet-161, and VGG-16bn 4-bit models that match the accuracy of the full-precision baseline networks -- the highest scores to date. Surprisingly, the weights of the low-precision networks are very close (in cosine similarity) to the weights of the corresponding baseline networks, making training from scratch unnecessary. \nWe find that gradient noise due to quantization during training increases with reduced precision, and seek ways to overcome this noise. The number of iterations required by SGD to achieve a given training error is related to the square of (a) the distance of the initial solution from the final plus (b) the maximum variance of the gradient estimates. Therefore, we (a) reduce solution distance by starting with pretrained fp32 precision baseline networks and fine-tuning, and (b) combat gradient noise introduced by quantization by training longer and reducing learning rates. Sensitivity analysis indicates that these simple techniques, coupled with proper activation function range calibration to take full advantage of the limited precision, are sufficient to discover low-precision networks, if they exist, close to fp32 precision baseline networks. The results herein provide evidence that 4-bits suffice for classification.",
            "corpus_id": 52197199,
            "sentences": [
                {
                    "corpus_id": "52197199",
                    "title": "Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference",
                    "text": "Fine-tuning matches or exceeds the accuracy of the initial high-precision network and outperforms other methods FAQ trained 8-bit networks outperform all comparable quantization methods in all but one instance and exceeded pretrained fp32 network accuracy with only one epoch of training following quantization for all networks explored (Table 1). Immediately following quantization, network accuracy was nearly at the level of the pretrained networks (data not shown) with one exception, Inception-v3, which started at 72.34% top-1. Since the networks started close to a good solution, they did not require extensive fine-tuning to return to and surpass pretrained networks. \n\nFAQ trained 4-bit network accuracy outperforms all comparable quantization methods, exceeding the next closest approach by over 0.5% for ResNet-18 (Jung et al. 2018), and matched or exceeded pretrained fp32 network accuracy. FAQ 4-bit networks required significantly longer fine-tuning -110 epochs -for networks trained, ResNet-18, ResNet-34, and ResNet-50. In constrast to the 8-bit cases, immediately following quantization, network accuracy dropped precipitously, requiring significant fine-tuning to match and surpass the pretrained networks. \n\nFAQ trained 4-bit network accuracy is sensitive to several hyperparameters (Table 2). In summary, longer training duration, initialization from a pretrained model, larger batch size, lower weight decay, and activation calibration all contribute to improved accuracy when training the 4-bit ResNet-18 network, while the exact learning rate decay schedule contributed the least. We elaborate on some of these results subsequently.",
                    "score": 0.6366464818617033,
                    "section_title": "Experiments",
                    "char_start_offset": 12749,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 533
                        },
                        {
                            "start": 534,
                            "end": 675
                        },
                        {
                            "start": 678,
                            "end": 902
                        },
                        {
                            "start": 903,
                            "end": 1035
                        },
                        {
                            "start": 1036,
                            "end": 1224
                        },
                        {
                            "start": 1227,
                            "end": 1312
                        },
                        {
                            "start": 1313,
                            "end": 1603
                        },
                        {
                            "start": 1604,
                            "end": 1655
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 825,
                            "end": 843,
                            "matchedPaperCorpusId": "195347490"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97265625
                }
            ],
            "relevance_judgement": 0.97265625,
            "relevance_judgment_input_expanded": "# Title: Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference\n# Venue: arXiv.org\n# Authors: J. McKinstry, S. K. Esser, R. Appuswamy, Deepika Bablani, John V. Arthur, Izzet B. Yildiz, D. Modha\n## Abstract\nTo realize the promise of ubiquitous embedded deep network inference, it is essential to seek limits of energy and area efficiency. To this end, low-precision networks offer tremendous promise because both energy and area scale down quadratically with the reduction in precision. Here we demonstrate ResNet-18, -34, -50, -152, Inception-v3, Densenet-161, and VGG-16bn networks on the ImageNet classification benchmark that, at 8-bit precision exceed the accuracy of the full-precision baseline networks after one epoch of finetuning, thereby leveraging the availability of pretrained models. We also demonstrate ResNet-18, -34, -50, -152, Densenet-161, and VGG-16bn 4-bit models that match the accuracy of the full-precision baseline networks -- the highest scores to date. Surprisingly, the weights of the low-precision networks are very close (in cosine similarity) to the weights of the corresponding baseline networks, making training from scratch unnecessary. \nWe find that gradient noise due to quantization during training increases with reduced precision, and seek ways to overcome this noise. The number of iterations required by SGD to achieve a given training error is related to the square of (a) the distance of the initial solution from the final plus (b) the maximum variance of the gradient estimates. Therefore, we (a) reduce solution distance by starting with pretrained fp32 precision baseline networks and fine-tuning, and (b) combat gradient noise introduced by quantization by training longer and reducing learning rates. Sensitivity analysis indicates that these simple techniques, coupled with proper activation function range calibration to take full advantage of the limited precision, are sufficient to discover low-precision networks, if they exist, close to fp32 precision baseline networks. The results herein provide evidence that 4-bits suffice for classification.\n## Experiments\nFine-tuning matches or exceeds the accuracy of the initial high-precision network and outperforms other methods FAQ trained 8-bit networks outperform all comparable quantization methods in all but one instance and exceeded pretrained fp32 network accuracy with only one epoch of training following quantization for all networks explored (Table 1). Immediately following quantization, network accuracy was nearly at the level of the pretrained networks (data not shown) with one exception, Inception-v3, which started at 72.34% top-1. Since the networks started close to a good solution, they did not require extensive fine-tuning to return to and surpass pretrained networks. \n\nFAQ trained 4-bit network accuracy outperforms all comparable quantization methods, exceeding the next closest approach by over 0.5% for ResNet-18 (Jung et al. 2018), and matched or exceeded pretrained fp32 network accuracy. FAQ 4-bit networks required significantly longer fine-tuning -110 epochs -for networks trained, ResNet-18, ResNet-34, and ResNet-50. In constrast to the 8-bit cases, immediately following quantization, network accuracy dropped precipitously, requiring significant fine-tuning to match and surpass the pretrained networks. \n\nFAQ trained 4-bit network accuracy is sensitive to several hyperparameters (Table 2). In summary, longer training duration, initialization from a pretrained model, larger batch size, lower weight decay, and activation calibration all contribute to improved accuracy when training the 4-bit ResNet-18 network, while the exact learning rate decay schedule contributed the least. We elaborate on some of these results subsequently.",
            "reference_string": "[52197199 | McKinstry et al. | 2018 | Citations: 94]"
        },
        {
            "title": "4.6-Bit Quantization for Fast and Accurate Neural Network Inference on CPUs",
            "venue": "Mathematics",
            "year": 2024,
            "reference_count": 19,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2227-7390/12/5/651/pdf?version=1708681029",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/math12050651?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/math12050651, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2209812418",
                    "name": "A. Trusov"
                },
                {
                    "authorId": "37239110",
                    "name": "E. Limonova"
                },
                {
                    "authorId": "2188704593",
                    "name": "Dmitry P. Nikolaev"
                },
                {
                    "authorId": "35343060",
                    "name": "V. Arlazarov"
                }
            ],
            "abstract": "Quantization is a widespread method for reducing the inference time of neural networks on mobile Central Processing Units (CPUs). Eight-bit quantized networks demonstrate similarly high quality as full precision models and perfectly fit the hardware architecture with one-byte coefficients and thirty-two-bit dot product accumulators. Lower precision quantizations usually suffer from noticeable quality loss and require specific computational algorithms to outperform eight-bit quantization. In this paper, we propose a novel 4.6-bit quantization scheme that allows for more efficient use of CPU resources. This scheme has more quantization bins than four-bit quantization and is more accurate while preserving the computational efficiency of the later (it runs only 4% slower). Our multiplication uses a combination of 16- and 32-bit accumulators and avoids multiplication depth limitation, which the previous 4-bit multiplication algorithm had. The experiments with different convolutional neural networks on CIFAR-10 and ImageNet datasets show that 4.6-bit quantized networks are 1.5\u20131.6 times faster than eight-bit networks on the ARMv8 CPU. Regarding the quality, the results of the 4.6-bit quantized network are close to the mean of four-bit and eight-bit networks of the same architecture. Therefore, 4.6-bit quantization may serve as an intermediate solution between fast and inaccurate low-bit network quantizations and accurate but relatively slow eight-bit ones.",
            "corpus_id": 267966328,
            "sentences": [],
            "relevance_judgement": 0.97265625,
            "relevance_judgment_input_expanded": "# Title: 4.6-Bit Quantization for Fast and Accurate Neural Network Inference on CPUs\n# Venue: Mathematics\n# Authors: A. Trusov, E. Limonova, Dmitry P. Nikolaev, V. Arlazarov\n## Abstract\nQuantization is a widespread method for reducing the inference time of neural networks on mobile Central Processing Units (CPUs). Eight-bit quantized networks demonstrate similarly high quality as full precision models and perfectly fit the hardware architecture with one-byte coefficients and thirty-two-bit dot product accumulators. Lower precision quantizations usually suffer from noticeable quality loss and require specific computational algorithms to outperform eight-bit quantization. In this paper, we propose a novel 4.6-bit quantization scheme that allows for more efficient use of CPU resources. This scheme has more quantization bins than four-bit quantization and is more accurate while preserving the computational efficiency of the later (it runs only 4% slower). Our multiplication uses a combination of 16- and 32-bit accumulators and avoids multiplication depth limitation, which the previous 4-bit multiplication algorithm had. The experiments with different convolutional neural networks on CIFAR-10 and ImageNet datasets show that 4.6-bit quantized networks are 1.5\u20131.6 times faster than eight-bit networks on the ARMv8 CPU. Regarding the quality, the results of the 4.6-bit quantized network are close to the mean of four-bit and eight-bit networks of the same architecture. Therefore, 4.6-bit quantization may serve as an intermediate solution between fast and inaccurate low-bit network quantizations and accurate but relatively slow eight-bit ones.\n",
            "reference_string": "[267966328 | Trusov et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Emergent Abilities in Large Language Models: A Survey",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 97,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.05788, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2229196235",
                    "name": "Leonardo Berti"
                },
                {
                    "authorId": "2325903265",
                    "name": "Flavio Giorgi"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                }
            ],
            "abstract": "Large Language Models (LLMs) are leading a new technological revolution as one of the most promising research streams toward artificial general intelligence. The scaling of these models, accomplished by increasing the number of parameters and the magnitude of the training datasets, has been linked to various so-called emergent abilities that were previously unobserved. These emergent abilities, ranging from advanced reasoning and in-context learning to coding and problem-solving, have sparked an intense scientific debate: Are they truly emergent, or do they simply depend on external factors, such as training dynamics, the type of problems, or the chosen metric? What underlying mechanism causes them? Despite their transformative potential, emergent abilities remain poorly understood, leading to misconceptions about their definition, nature, predictability, and implications. In this work, we shed light on emergent abilities by conducting a comprehensive review of the phenomenon, addressing both its scientific underpinnings and real-world consequences. We first critically analyze existing definitions, exposing inconsistencies in conceptualizing emergent abilities. We then explore the conditions under which these abilities appear, evaluating the role of scaling laws, task complexity, pre-training loss, quantization, and prompting strategies. Our review extends beyond traditional LLMs and includes Large Reasoning Models (LRMs), which leverage reinforcement learning and inference-time search to amplify reasoning and self-reflection. However, emergence is not inherently positive. As AI systems gain autonomous reasoning capabilities, they also develop harmful behaviors, including deception, manipulation, and reward hacking. We highlight growing concerns about safety and governance, emphasizing the need for better evaluation frameworks and regulatory oversight.",
            "corpus_id": 276903421,
            "sentences": [
                {
                    "corpus_id": "276903421",
                    "title": "Emergent Abilities in Large Language Models: A Survey",
                    "text": "Liu et al. [53] investigate a critical but often overlooked factor affecting the emergent abilities of LLMs: i.e., quantization. As LLMs grow in size, their memory and computational requirements become increasingly demanding, prompting the need for quantization techniques that reduce precision and optimize efficiency. However, a key question remains: Does quantization compromise the emergent abilities that make these models so powerful? Their study systematically examines this trade-off, particularly in the context of in-context learning, chain-of-thought reasoning, and instruction following -three hallmarks of advanced LLM capabilities. \n\nHow Quantization Affects Performance. Quantization is a widely used technique that compresses neural networks by reducing the number of bits used to represent each parameter. While this drastically lowers memory usage and inference costs, it often comes at the expense of model performance. Liu et al. [53] conduct extensive empirical evaluations on a range of LLaMA models, spanning 7B to 65B parameters, across multiple quantization levels, including 2-bit, 4-bit, 8-bit, and 16-bit precision. Their findings offer a nuanced understanding of how quantization impacts emergent abilities. At higher bit levels, such as 8-bit and 16-bit, LLMs retain much of their original performance, with minimal degradation in reasoning and instruction-following tasks. However, as precision decreases, particularly at 4-bit and below, a clear pattern emerges: while 4-bit quantization manages to preserve most emergent abilities, 2-bit quantization severely degrades performance, often reducing accuracy to near-random levels. This suggests that there exists a critical threshold beyond which the model struggles to maintain the structured reasoning and learning dynamics that enable emergent behaviors. \n\nA particularly insightful aspect of the study is its componentwise analysis, which reveals that not all parts of the model are equally affected by quantization. The feed-forward networks within transformer blocks are found to be especially crucial for retaining performance. When these layers undergo aggressive quantization, the model experiences significant losses in reasoning ability and generalization. This highlights the fact that while weight compression is beneficial, indiscriminate quantization of all components can be detrimental. Can Emergent Abilities Be Preserved?",
                    "score": 0.605274684466678,
                    "section_title": "D. The Impact of Quantization on Emergent Abilities",
                    "char_start_offset": 31198,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 440
                        },
                        {
                            "start": 441,
                            "end": 645
                        },
                        {
                            "start": 648,
                            "end": 685
                        },
                        {
                            "start": 686,
                            "end": 822
                        },
                        {
                            "start": 823,
                            "end": 938
                        },
                        {
                            "start": 939,
                            "end": 1143
                        },
                        {
                            "start": 1144,
                            "end": 1236
                        },
                        {
                            "start": 1237,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1661
                        },
                        {
                            "start": 1662,
                            "end": 1838
                        },
                        {
                            "start": 1841,
                            "end": 2001
                        },
                        {
                            "start": 2002,
                            "end": 2115
                        },
                        {
                            "start": 2116,
                            "end": 2248
                        },
                        {
                            "start": 2249,
                            "end": 2384
                        },
                        {
                            "start": 2385,
                            "end": 2421
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96875
                }
            ],
            "relevance_judgement": 0.96875,
            "relevance_judgment_input_expanded": "# Title: Emergent Abilities in Large Language Models: A Survey\n# Venue: arXiv.org\n# Authors: Leonardo Berti, Flavio Giorgi, Gjergji Kasneci\n## Abstract\nLarge Language Models (LLMs) are leading a new technological revolution as one of the most promising research streams toward artificial general intelligence. The scaling of these models, accomplished by increasing the number of parameters and the magnitude of the training datasets, has been linked to various so-called emergent abilities that were previously unobserved. These emergent abilities, ranging from advanced reasoning and in-context learning to coding and problem-solving, have sparked an intense scientific debate: Are they truly emergent, or do they simply depend on external factors, such as training dynamics, the type of problems, or the chosen metric? What underlying mechanism causes them? Despite their transformative potential, emergent abilities remain poorly understood, leading to misconceptions about their definition, nature, predictability, and implications. In this work, we shed light on emergent abilities by conducting a comprehensive review of the phenomenon, addressing both its scientific underpinnings and real-world consequences. We first critically analyze existing definitions, exposing inconsistencies in conceptualizing emergent abilities. We then explore the conditions under which these abilities appear, evaluating the role of scaling laws, task complexity, pre-training loss, quantization, and prompting strategies. Our review extends beyond traditional LLMs and includes Large Reasoning Models (LRMs), which leverage reinforcement learning and inference-time search to amplify reasoning and self-reflection. However, emergence is not inherently positive. As AI systems gain autonomous reasoning capabilities, they also develop harmful behaviors, including deception, manipulation, and reward hacking. We highlight growing concerns about safety and governance, emphasizing the need for better evaluation frameworks and regulatory oversight.\n## D. The Impact of Quantization on Emergent Abilities\nLiu et al. [53] investigate a critical but often overlooked factor affecting the emergent abilities of LLMs: i.e., quantization. As LLMs grow in size, their memory and computational requirements become increasingly demanding, prompting the need for quantization techniques that reduce precision and optimize efficiency. However, a key question remains: Does quantization compromise the emergent abilities that make these models so powerful? Their study systematically examines this trade-off, particularly in the context of in-context learning, chain-of-thought reasoning, and instruction following -three hallmarks of advanced LLM capabilities. \n\nHow Quantization Affects Performance. Quantization is a widely used technique that compresses neural networks by reducing the number of bits used to represent each parameter. While this drastically lowers memory usage and inference costs, it often comes at the expense of model performance. Liu et al. [53] conduct extensive empirical evaluations on a range of LLaMA models, spanning 7B to 65B parameters, across multiple quantization levels, including 2-bit, 4-bit, 8-bit, and 16-bit precision. Their findings offer a nuanced understanding of how quantization impacts emergent abilities. At higher bit levels, such as 8-bit and 16-bit, LLMs retain much of their original performance, with minimal degradation in reasoning and instruction-following tasks. However, as precision decreases, particularly at 4-bit and below, a clear pattern emerges: while 4-bit quantization manages to preserve most emergent abilities, 2-bit quantization severely degrades performance, often reducing accuracy to near-random levels. This suggests that there exists a critical threshold beyond which the model struggles to maintain the structured reasoning and learning dynamics that enable emergent behaviors. \n\nA particularly insightful aspect of the study is its componentwise analysis, which reveals that not all parts of the model are equally affected by quantization. The feed-forward networks within transformer blocks are found to be especially crucial for retaining performance. When these layers undergo aggressive quantization, the model experiences significant losses in reasoning ability and generalization. This highlights the fact that while weight compression is beneficial, indiscriminate quantization of all components can be detrimental. Can Emergent Abilities Be Preserved?",
            "reference_string": "[276903421 | Berti et al. | 2025 | Citations: 6]"
        },
        {
            "title": "Post-Training Sparsity-Aware Quantization",
            "venue": "Neural Information Processing Systems",
            "year": 2021,
            "reference_count": 42,
            "citation_count": 34,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.11010, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "19190632",
                    "name": "Gil Shomron"
                },
                {
                    "authorId": "3147903",
                    "name": "F. Gabbay"
                },
                {
                    "authorId": "2104448639",
                    "name": "Samer Kurzum"
                },
                {
                    "authorId": "1948330",
                    "name": "U. Weiser"
                }
            ],
            "abstract": "Quantization is a technique used in deep neural networks (DNNs) to increase execution performance and hardware efficiency. Uniform post-training quantization (PTQ) methods are common, since they can be implemented efficiently in hardware and do not require extensive hardware resources or a training set. Mapping FP32 models to INT8 using uniform PTQ yields models with negligible accuracy degradation; however, reducing precision below 8 bits with PTQ is challenging, as accuracy degradation becomes noticeable, due to the increase in quantization noise. In this paper, we propose a sparsity-aware quantization (SPARQ) method, in which the unstructured and dynamic activation sparsity is leveraged in different representation granularities. 4-bit quantization, for example, is employed by dynamically examining the bits of 8-bit values and choosing a window of 4 bits, while first skipping zero-value bits. Moreover, instead of quantizing activation-by-activation to 4 bits, we focus on pairs of 8-bit activations and examine whether one of the two is equal to zero. If one is equal to zero, the second can opportunistically use the other's 4-bit budget; if both do not equal zero, then each is dynamically quantized to 4 bits, as described. SPARQ achieves minor accuracy degradation, 2x speedup over widely used hardware architectures, and a practical hardware implementation. The code is available at this https URL.",
            "corpus_id": 235166247,
            "sentences": [
                {
                    "corpus_id": "235166247",
                    "title": "Post-Training Sparsity-Aware Quantization",
                    "text": "Quantization is a technique used in deep neural networks (DNNs) to increase execution performance and hardware efficiency. Uniform post-training quantization (PTQ) methods are common, since they can be implemented efficiently in hardware and do not require extensive hardware resources or a training set. Mapping FP32 models to INT8 using uniform PTQ yields models with negligible accuracy degradation; however, reducing precision below 8 bits with PTQ is challenging, as accuracy degradation becomes noticeable, due to the increase in quantization noise. In this paper, we propose a sparsity-aware quantization (SPARQ) method, in which the unstructured and dynamic activation sparsity is leveraged in different representation granularities. 4-bit quantization, for example, is employed by dynamically examining the bits of 8-bit values and choosing a window of 4 bits, while first skipping zero-value bits. Moreover, instead of quantizing activation-by-activation to 4 bits, we focus on pairs of 8-bit activations and examine whether one of the two is equal to zero. If one is equal to zero, the second can opportunistically use the other's 4-bit budget; if both do not equal zero, then each is dynamically quantized to 4 bits, as described. SPARQ achieves minor accuracy degradation, 2x speedup over widely used hardware architectures, and a practical hardware implementation. The code is available at this https URL.",
                    "score": 0.6166409231816529,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96826171875
                }
            ],
            "relevance_judgement": 0.96826171875,
            "relevance_judgment_input_expanded": "# Title: Post-Training Sparsity-Aware Quantization\n# Venue: Neural Information Processing Systems\n# Authors: Gil Shomron, F. Gabbay, Samer Kurzum, U. Weiser\n## Abstract\nQuantization is a technique used in deep neural networks (DNNs) to increase execution performance and hardware efficiency. Uniform post-training quantization (PTQ) methods are common, since they can be implemented efficiently in hardware and do not require extensive hardware resources or a training set. Mapping FP32 models to INT8 using uniform PTQ yields models with negligible accuracy degradation; however, reducing precision below 8 bits with PTQ is challenging, as accuracy degradation becomes noticeable, due to the increase in quantization noise. In this paper, we propose a sparsity-aware quantization (SPARQ) method, in which the unstructured and dynamic activation sparsity is leveraged in different representation granularities. 4-bit quantization, for example, is employed by dynamically examining the bits of 8-bit values and choosing a window of 4 bits, while first skipping zero-value bits. Moreover, instead of quantizing activation-by-activation to 4 bits, we focus on pairs of 8-bit activations and examine whether one of the two is equal to zero. If one is equal to zero, the second can opportunistically use the other's 4-bit budget; if both do not equal zero, then each is dynamically quantized to 4 bits, as described. SPARQ achieves minor accuracy degradation, 2x speedup over widely used hardware architectures, and a practical hardware implementation. The code is available at this https URL.\n",
            "reference_string": "[235166247 | Shomron et al. | 2021 | Citations: 34]"
        },
        {
            "title": "A High-Performance Adaptive Quantization Approach for Edge CNN Applications",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 54,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2107.08382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1597397664",
                    "name": "Hsu-Hsun Chin"
                },
                {
                    "authorId": "1769065",
                    "name": "R. Tsay"
                },
                {
                    "authorId": "120155431",
                    "name": "Hsin-I Wu"
                }
            ],
            "abstract": "Recent convolutional neural network (CNN) development continues to advance the state-of-the-art model accuracy for various applications. However, the enhanced accuracy comes at the cost of substantial memory bandwidth and storage requirements and demanding computational resources. Although in the past the quantization methods have effectively reduced the deployment cost for edge devices, it suffers from significant information loss when processing the biased activations of contemporary CNNs. In this paper, we hence introduce an adaptive high-performance quantization method to resolve the issue of biased activation by dynamically adjusting the scaling and shifting factors based on the task loss. Our proposed method has been extensively evaluated on image classification models (ResNet-18/34/50, MobileNet-V2, EfficientNet-B0) with ImageNet dataset, object detection model (YOLO-V4) with COCO dataset, and language models with PTB dataset. The results show that our 4-bit integer (INT4) quantization models achieve better accuracy than the state-of-the-art 4-bit models, and in some cases, even surpass the golden full-precision models. The final designs have been successfully deployed onto extremely resource-constrained edge devices for many practical applications.",
            "corpus_id": 236087695,
            "sentences": [
                {
                    "corpus_id": "236087695",
                    "title": "A High-Performance Adaptive Quantization Approach for Edge CNN Applications",
                    "text": "Particularly, the storage requirement is substantially reduced. The memory communication efficiency is also greatly improved due to the use of low bit-width data. In addition, the circuit complexity of the low bit-width MAC (multiplyaccumulate) unit is much lower than that of the floating-point MAC unit. Thus the quantization models save both chip area and power consumption significantly. For instance, the floating-point adder consumes 30 times more energy and 116 times more area than the 8-bit adder; the floating-point multiplier consumes 18 times more energy and 27 times more area than the 8-bit multiplier [16]. \n\nDespite the high computational efficiency of low bit-width inference, the task accuracy degrades and needs to be recovered by retraining. The experimental results from recent research [17] [18] show a comparison of model accuracy under different bit-width quantization in the image classification task, and the results support that the 4-bit quantization models strike the best balance between minimal computing/memory complexity and minimal model accuracy degradation. \n\nGenerally, existing quantization approaches achieve good results in the traditional ReLU-based models, where the activations are passed through the ReLU function [19] and then restricted activations to be non-negative values. In contrast, contemporary network architectures adopt more complex activation functions, such as Swish [20], Leaky-ReLU [19], and Mish [21], with which the range of activation distributions is relatively more irregular than the weight distribution and hence the traditional quantization methods experience a significant information loss. Nevertheless, the indispensable benefit of reducing memory overhead strongly demands a more robust quantization method to manage the dynamic distribution range. \n\nThe traditional uniform quantization methods [17][22][23] simply quantize the normalized distribution using a simple scaling, and the results often suffer significant quantization errors, particularly when processing uneven and biased distributions. For instance, we show in Fig. 2(a) a sample original data distribution with the dashed box denoting the quantization range.",
                    "score": 0.654641797487885,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 2252,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 63
                        },
                        {
                            "start": 64,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 305
                        },
                        {
                            "start": 306,
                            "end": 391
                        },
                        {
                            "start": 392,
                            "end": 621
                        },
                        {
                            "start": 624,
                            "end": 761
                        },
                        {
                            "start": 762,
                            "end": 1093
                        },
                        {
                            "start": 1096,
                            "end": 1321
                        },
                        {
                            "start": 1322,
                            "end": 1659
                        },
                        {
                            "start": 1660,
                            "end": 1820
                        },
                        {
                            "start": 1823,
                            "end": 2072
                        },
                        {
                            "start": 2073,
                            "end": 2196
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9677734375
                }
            ],
            "relevance_judgement": 0.9677734375,
            "relevance_judgment_input_expanded": "# Title: A High-Performance Adaptive Quantization Approach for Edge CNN Applications\n# Venue: arXiv.org\n# Authors: Hsu-Hsun Chin, R. Tsay, Hsin-I Wu\n## Abstract\nRecent convolutional neural network (CNN) development continues to advance the state-of-the-art model accuracy for various applications. However, the enhanced accuracy comes at the cost of substantial memory bandwidth and storage requirements and demanding computational resources. Although in the past the quantization methods have effectively reduced the deployment cost for edge devices, it suffers from significant information loss when processing the biased activations of contemporary CNNs. In this paper, we hence introduce an adaptive high-performance quantization method to resolve the issue of biased activation by dynamically adjusting the scaling and shifting factors based on the task loss. Our proposed method has been extensively evaluated on image classification models (ResNet-18/34/50, MobileNet-V2, EfficientNet-B0) with ImageNet dataset, object detection model (YOLO-V4) with COCO dataset, and language models with PTB dataset. The results show that our 4-bit integer (INT4) quantization models achieve better accuracy than the state-of-the-art 4-bit models, and in some cases, even surpass the golden full-precision models. The final designs have been successfully deployed onto extremely resource-constrained edge devices for many practical applications.\n## INTRODUCTION\nParticularly, the storage requirement is substantially reduced. The memory communication efficiency is also greatly improved due to the use of low bit-width data. In addition, the circuit complexity of the low bit-width MAC (multiplyaccumulate) unit is much lower than that of the floating-point MAC unit. Thus the quantization models save both chip area and power consumption significantly. For instance, the floating-point adder consumes 30 times more energy and 116 times more area than the 8-bit adder; the floating-point multiplier consumes 18 times more energy and 27 times more area than the 8-bit multiplier [16]. \n\nDespite the high computational efficiency of low bit-width inference, the task accuracy degrades and needs to be recovered by retraining. The experimental results from recent research [17] [18] show a comparison of model accuracy under different bit-width quantization in the image classification task, and the results support that the 4-bit quantization models strike the best balance between minimal computing/memory complexity and minimal model accuracy degradation. \n\nGenerally, existing quantization approaches achieve good results in the traditional ReLU-based models, where the activations are passed through the ReLU function [19] and then restricted activations to be non-negative values. In contrast, contemporary network architectures adopt more complex activation functions, such as Swish [20], Leaky-ReLU [19], and Mish [21], with which the range of activation distributions is relatively more irregular than the weight distribution and hence the traditional quantization methods experience a significant information loss. Nevertheless, the indispensable benefit of reducing memory overhead strongly demands a more robust quantization method to manage the dynamic distribution range. \n\nThe traditional uniform quantization methods [17][22][23] simply quantize the normalized distribution using a simple scaling, and the results often suffer significant quantization errors, particularly when processing uneven and biased distributions. For instance, we show in Fig. 2(a) a sample original data distribution with the dashed box denoting the quantization range.",
            "reference_string": "[236087695 | Chin et al. | 2021 | Citations: 5]"
        },
        {
            "title": "The Impact of 8- and 4-Bit Quantization on the Accuracy and Silicon Area Footprint of Tiny Neural Networks",
            "venue": "Electronics",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics14010014?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics14010014, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2337212061",
                    "name": "Pawe\u0142 Tumialis"
                },
                {
                    "authorId": "2337207657",
                    "name": "Marcel Skierkowski"
                },
                {
                    "authorId": "2337211715",
                    "name": "Jakub Przychodny"
                },
                {
                    "authorId": "2337209535",
                    "name": "Pawe\u0142 Obszarski"
                }
            ],
            "abstract": "In the field of embedded and edge devices, efforts have been made to make deep neural network models smaller due to the limited size of the available memory and the low computational efficiency. Typical model footprints are under 100 KB. However, for some applications, models of this size are too large. In low-voltage sensors, signals must be processed, classified or predicted with an order of magnitude smaller memory. Model downsizing can be performed by limiting the number of model parameters or quantizing their weights. These types of operations have a negative impact on the accuracy of the deep network. This study tested the effect of model downscaling techniques on accuracy. The main idea was to reduce neural network models to 3 k parameters or less. Tests were conducted on three different neural network architectures in the context of three separate research problems, modeling real tasks for small networks. The impact of the reduction in the accuracy of the network depends mainly on its initial size. For a network reduced from 40 k parameters, a decrease in accuracy of 16 percentage points was achieved, and for a network with 20 k parameters, a decrease of 8 points was achieved. To obtain the best results, knowledge distillation and quantization-aware training methods were used for training. Thanks to this, the accuracy of the 4-bit networks did not differ significantly from the 8-bit ones and their results were approximately four percentage points worse than those of the full precision networks. For the fully connected network, synthesis to ASIC (application-specific integrated circuit) was also performed to demonstrate the reduction in the silicon area occupied by the model. The 4-bit quantization limits the silicon area footprint by 90%.",
            "corpus_id": 275038383,
            "sentences": [],
            "relevance_judgement": 0.96728515625,
            "relevance_judgment_input_expanded": "# Title: The Impact of 8- and 4-Bit Quantization on the Accuracy and Silicon Area Footprint of Tiny Neural Networks\n# Venue: Electronics\n# Authors: Pawe\u0142 Tumialis, Marcel Skierkowski, Jakub Przychodny, Pawe\u0142 Obszarski\n## Abstract\nIn the field of embedded and edge devices, efforts have been made to make deep neural network models smaller due to the limited size of the available memory and the low computational efficiency. Typical model footprints are under 100 KB. However, for some applications, models of this size are too large. In low-voltage sensors, signals must be processed, classified or predicted with an order of magnitude smaller memory. Model downsizing can be performed by limiting the number of model parameters or quantizing their weights. These types of operations have a negative impact on the accuracy of the deep network. This study tested the effect of model downscaling techniques on accuracy. The main idea was to reduce neural network models to 3 k parameters or less. Tests were conducted on three different neural network architectures in the context of three separate research problems, modeling real tasks for small networks. The impact of the reduction in the accuracy of the network depends mainly on its initial size. For a network reduced from 40 k parameters, a decrease in accuracy of 16 percentage points was achieved, and for a network with 20 k parameters, a decrease of 8 points was achieved. To obtain the best results, knowledge distillation and quantization-aware training methods were used for training. Thanks to this, the accuracy of the 4-bit networks did not differ significantly from the 8-bit ones and their results were approximately four percentage points worse than those of the full precision networks. For the fully connected network, synthesis to ASIC (application-specific integrated circuit) was also performed to demonstrate the reduction in the silicon area occupied by the model. The 4-bit quantization limits the silicon area footprint by 90%.\n",
            "reference_string": "[275038383 | Tumialis et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference",
            "venue": "",
            "year": 2025,
            "reference_count": 57,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.14638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51193964",
                    "name": "Tomer Gafni"
                },
                {
                    "authorId": "2160627835",
                    "name": "A. Karnieli"
                },
                {
                    "authorId": "48967743",
                    "name": "Yair Hanani"
                }
            ],
            "abstract": "Deep neural networks have achieved state-of-the-art results in a wide range of applications, from natural language processing and computer vision to speech recognition. However, as tasks become increasingly complex, model sizes continue to grow, posing challenges in latency and memory efficiency. To meet these constraints, post-training quantization has emerged as a promising solution. In this paper, we propose a novel hardware-efficient quantization and inference scheme that exploits hardware advantages with minimal accuracy degradation. Specifically, we introduce a W4A8 scheme, where weights are quantized and stored using 4-bit integer precision, and inference computations are performed using 8-bit floating-point arithmetic, demonstrating significant speedups and improved memory utilization compared to 16-bit operations, applicable on various modern accelerators. To mitigate accuracy loss, we develop a novel quantization algorithm, dubbed Dual Precision Quantization (DPQ), that leverages the unique structure of our scheme without introducing additional inference overhead. Experimental results demonstrate improved performance (i.e., increased throughput) while maintaining tolerable accuracy degradation relative to the full-precision model.",
            "corpus_id": 278769955,
            "sentences": [],
            "relevance_judgement": 0.966796875,
            "relevance_judgment_input_expanded": "# Title: Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference\n# Venue: \n# Authors: Tomer Gafni, A. Karnieli, Yair Hanani\n## Abstract\nDeep neural networks have achieved state-of-the-art results in a wide range of applications, from natural language processing and computer vision to speech recognition. However, as tasks become increasingly complex, model sizes continue to grow, posing challenges in latency and memory efficiency. To meet these constraints, post-training quantization has emerged as a promising solution. In this paper, we propose a novel hardware-efficient quantization and inference scheme that exploits hardware advantages with minimal accuracy degradation. Specifically, we introduce a W4A8 scheme, where weights are quantized and stored using 4-bit integer precision, and inference computations are performed using 8-bit floating-point arithmetic, demonstrating significant speedups and improved memory utilization compared to 16-bit operations, applicable on various modern accelerators. To mitigate accuracy loss, we develop a novel quantization algorithm, dubbed Dual Precision Quantization (DPQ), that leverages the unique structure of our scheme without introducing additional inference overhead. Experimental results demonstrate improved performance (i.e., increased throughput) while maintaining tolerable accuracy degradation relative to the full-precision model.\n",
            "reference_string": "[278769955 | Gafni et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Quantization noise in low bit quantization and iterative adaptation to quantization noise in quantizable neural networks",
            "venue": "Journal of Physics: Conference Series",
            "year": 2021,
            "reference_count": 12,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1088/1742-6596/2134/1/012004",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1088/1742-6596/2134/1/012004?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1088/1742-6596/2134/1/012004, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2146460322",
                    "name": "D. Chudakov"
                },
                {
                    "authorId": "144749173",
                    "name": "A. Goncharenko"
                },
                {
                    "authorId": "81455739",
                    "name": "S. Alyamkin"
                },
                {
                    "authorId": "2146460282",
                    "name": "A. Densidov"
                }
            ],
            "abstract": "Quantization is one of the most popular and widely used methods of speeding up a neural network. At the moment, the standard is 8-bit uniform quantization. Nevertheless, the use of uniform low-bit quantization (4- and 6-bit quantization) has significant advantages in speed and resource requirements for inference. We present our quantization algorithm that offers advantages when using uniform low-bit quantization. It is faster than quantization-aware training from scratch and more accurate than methods aimed only at selecting thresholds and reducing noise from quantization. We also investigated quantization noise in neural networks for low-bit quantization and concluded that quantization noise is not always a good metric for quantization quality.",
            "corpus_id": 245351885,
            "sentences": [
                {
                    "corpus_id": "245351885",
                    "title": "Quantization noise in low bit quantization and iterative adaptation to quantization noise in quantizable neural networks",
                    "text": "Quantization is one of the most popular and widely used methods of speeding up a neural network. At the moment, the standard is 8-bit uniform quantization. Nevertheless, the use of uniform low-bit quantization (4- and 6-bit quantization) has significant advantages in speed and resource requirements for inference. We present our quantization algorithm that offers advantages when using uniform low-bit quantization. It is faster than quantization-aware training from scratch and more accurate than methods aimed only at selecting thresholds and reducing noise from quantization. We also investigated quantization noise in neural networks for low-bit quantization and concluded that quantization noise is not always a good metric for quantization quality.",
                    "score": 0.7705364021492916,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9658203125
                }
            ],
            "relevance_judgement": 0.9658203125,
            "relevance_judgment_input_expanded": "# Title: Quantization noise in low bit quantization and iterative adaptation to quantization noise in quantizable neural networks\n# Venue: Journal of Physics: Conference Series\n# Authors: D. Chudakov, A. Goncharenko, S. Alyamkin, A. Densidov\n## Abstract\nQuantization is one of the most popular and widely used methods of speeding up a neural network. At the moment, the standard is 8-bit uniform quantization. Nevertheless, the use of uniform low-bit quantization (4- and 6-bit quantization) has significant advantages in speed and resource requirements for inference. We present our quantization algorithm that offers advantages when using uniform low-bit quantization. It is faster than quantization-aware training from scratch and more accurate than methods aimed only at selecting thresholds and reducing noise from quantization. We also investigated quantization noise in neural networks for low-bit quantization and concluded that quantization noise is not always a good metric for quantization quality.\n",
            "reference_string": "[245351885 | Chudakov et al. | 2021 | Citations: 0]"
        },
        {
            "title": "Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers",
            "venue": "IEEE International Conference on Computer Vision",
            "year": 2023,
            "reference_count": 36,
            "citation_count": 17,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.10814, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2094690757",
                    "name": "N. Frumkin"
                },
                {
                    "authorId": "2193705",
                    "name": "Dibakar Gope"
                },
                {
                    "authorId": "92419662",
                    "name": "D. Marculescu"
                }
            ],
            "abstract": "Quantization scale and bit-width are the most important parameters when considering how to quantize a neural network. Prior work focuses on optimizing quantization scales in a global manner through gradient methods (gradient descent & Hessian analysis). Yet, when applying perturbations to quantization scales, we observe a very jagged, highly non-smooth test loss landscape. In fact, small perturbations in quantization scale can greatly affect accuracy, yielding a 0.5 \u2013 0.8% accuracy boost in 4-bit quantized vision transformers (ViTs). In this regime, gradient methods break down, since they cannot reliably reach local minima. In our work, dubbed Evol-Q, we use evolutionary search to effectively traverse the non-smooth landscape. Additionally, we propose using an infoNCE loss, which not only helps combat overfitting on the small calibration dataset (1, 000 images) but also makes traversing such a highly non-smooth surface easier. Evol-Q improves the top-1 accuracy of a fully quantized ViT-Base by 10.30%, 0.78%, and 0.15% for 3-bit, 4-bit, and 8-bit weight quantization levels. Extensive experiments on a variety of CNN and ViT architectures further demonstrate its robustness in extreme quantization scenarios. Our code is available at https: // github.com/enyac-group/evol-q .",
            "corpus_id": 261049574,
            "sentences": [
                {
                    "corpus_id": "261049574",
                    "title": "Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers",
                    "text": "Moving from 8-bit to 4-bit weight quantization, we see an accuracy degradation of about 2 \u2212 5% across all models. In Tab. 3, Evol-Q performs similarly to what is shown for 8-bit quantization. In particular, we still see improvement for DeiT-Small, DeiT-Base, and ViT-Base, but now the top-1 accuracy improvement is 0.13%, 0.16%, and 0.77%, respectively.",
                    "score": 0.6693134368504698,
                    "section_title": "4-bit Quantization",
                    "char_start_offset": 16001,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 114,
                            "end": 191
                        },
                        {
                            "start": 192,
                            "end": 353
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9658203125
                }
            ],
            "relevance_judgement": 0.9658203125,
            "relevance_judgment_input_expanded": "# Title: Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers\n# Venue: IEEE International Conference on Computer Vision\n# Authors: N. Frumkin, Dibakar Gope, D. Marculescu\n## Abstract\nQuantization scale and bit-width are the most important parameters when considering how to quantize a neural network. Prior work focuses on optimizing quantization scales in a global manner through gradient methods (gradient descent & Hessian analysis). Yet, when applying perturbations to quantization scales, we observe a very jagged, highly non-smooth test loss landscape. In fact, small perturbations in quantization scale can greatly affect accuracy, yielding a 0.5 \u2013 0.8% accuracy boost in 4-bit quantized vision transformers (ViTs). In this regime, gradient methods break down, since they cannot reliably reach local minima. In our work, dubbed Evol-Q, we use evolutionary search to effectively traverse the non-smooth landscape. Additionally, we propose using an infoNCE loss, which not only helps combat overfitting on the small calibration dataset (1, 000 images) but also makes traversing such a highly non-smooth surface easier. Evol-Q improves the top-1 accuracy of a fully quantized ViT-Base by 10.30%, 0.78%, and 0.15% for 3-bit, 4-bit, and 8-bit weight quantization levels. Extensive experiments on a variety of CNN and ViT architectures further demonstrate its robustness in extreme quantization scenarios. Our code is available at https: // github.com/enyac-group/evol-q .\n## 4-bit Quantization\nMoving from 8-bit to 4-bit weight quantization, we see an accuracy degradation of about 2 \u2212 5% across all models. In Tab. 3, Evol-Q performs similarly to what is shown for 8-bit quantization. In particular, we still see improvement for DeiT-Small, DeiT-Base, and ViT-Base, but now the top-1 accuracy improvement is 0.13%, 0.16%, and 0.77%, respectively.",
            "reference_string": "[261049574 | Frumkin et al. | 2023 | Citations: 17]"
        },
        {
            "title": "ReDy: A Novel ReRAM-centric Dynamic Quantization Approach for Energy-efficient CNN Inference",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 47,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.16298",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.16298, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2058852328",
                    "name": "Mohammad Sabri"
                },
                {
                    "authorId": "143640059",
                    "name": "Marc Riera"
                },
                {
                    "authorId": "2220761969",
                    "name": "Antonio Gonz'alez"
                }
            ],
            "abstract": "The primary operation in DNNs is the dot product of quantized input activations and weights. Prior works have proposed the design of memory-centric architectures based on the Processing-In-Memory (PIM) paradigm. Resistive RAM (ReRAM) technology is especially appealing for PIM-based DNN accelerators due to its high density to store weights, low leakage energy, low read latency, and high performance capabilities to perform the DNN dot-products massively in parallel within the ReRAM crossbars. However, the main bottleneck of these architectures is the energy-hungry analog-to-digital conversions (ADCs) required to perform analog computations in-ReRAM, which penalizes the efficiency and performance benefits of PIM. To improve energy-efficiency of in-ReRAM analog dot-product computations we present ReDy, a hardware accelerator that implements a ReRAM-centric Dynamic quantization scheme to take advantage of the bit serial streaming and processing of activations. The energy consumption of ReRAM-based DNN accelerators is directly proportional to the numerical precision of the input activations of each DNN layer. In particular, ReDy exploits that activations of CONV layers from Convolutional Neural Networks (CNNs), a subset of DNNs, are commonly grouped according to the size of their filters and the size of the ReRAM crossbars. Then, ReDy quantizes on-the-fly each group of activations with a different numerical precision based on a novel heuristic that takes into account the statistical distribution of each group. Overall, ReDy greatly reduces the activity of the ReRAM crossbars and the number of A/D conversions compared to an static 8-bit uniform quantization. We evaluate ReDy on a popular set of modern CNNs. On average, ReDy provides 13\\% energy savings over an ISAAC-like accelerator with negligible accuracy loss and area overhead.",
            "corpus_id": 259274857,
            "sentences": [
                {
                    "corpus_id": "259274857",
                    "title": "ReDy: A Novel ReRAM-centric Dynamic Quantization Approach for Energy-efficient CNN Inference",
                    "text": "Similarly, when decreasing the numerical precision of the uniform quantization to 4-bits, the RRMSE increases to 28.81% and 7.89% for groups (a) and (b) respectively, which corresponds to an increment of 17.2x and 16x respect the 8-bit uniform quantization. Based on these observations, we can potentially reduce the bitwidth of groups of activations with a more uniform distribution without affecting the accuracy of the network. Consequently, ReDy analyses the distribution of activation groups on-the-fly to determine the optimal numerical precision of the dynamic quantization process of each group.",
                    "score": 0.5975433608359267,
                    "section_title": "ANALYSIS OF CNN ACTIVATION GROUPS",
                    "char_start_offset": 19068,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 257
                        },
                        {
                            "start": 258,
                            "end": 430
                        },
                        {
                            "start": 431,
                            "end": 603
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96533203125
                }
            ],
            "relevance_judgement": 0.96533203125,
            "relevance_judgment_input_expanded": "# Title: ReDy: A Novel ReRAM-centric Dynamic Quantization Approach for Energy-efficient CNN Inference\n# Venue: arXiv.org\n# Authors: Mohammad Sabri, Marc Riera, Antonio Gonz'alez\n## Abstract\nThe primary operation in DNNs is the dot product of quantized input activations and weights. Prior works have proposed the design of memory-centric architectures based on the Processing-In-Memory (PIM) paradigm. Resistive RAM (ReRAM) technology is especially appealing for PIM-based DNN accelerators due to its high density to store weights, low leakage energy, low read latency, and high performance capabilities to perform the DNN dot-products massively in parallel within the ReRAM crossbars. However, the main bottleneck of these architectures is the energy-hungry analog-to-digital conversions (ADCs) required to perform analog computations in-ReRAM, which penalizes the efficiency and performance benefits of PIM. To improve energy-efficiency of in-ReRAM analog dot-product computations we present ReDy, a hardware accelerator that implements a ReRAM-centric Dynamic quantization scheme to take advantage of the bit serial streaming and processing of activations. The energy consumption of ReRAM-based DNN accelerators is directly proportional to the numerical precision of the input activations of each DNN layer. In particular, ReDy exploits that activations of CONV layers from Convolutional Neural Networks (CNNs), a subset of DNNs, are commonly grouped according to the size of their filters and the size of the ReRAM crossbars. Then, ReDy quantizes on-the-fly each group of activations with a different numerical precision based on a novel heuristic that takes into account the statistical distribution of each group. Overall, ReDy greatly reduces the activity of the ReRAM crossbars and the number of A/D conversions compared to an static 8-bit uniform quantization. We evaluate ReDy on a popular set of modern CNNs. On average, ReDy provides 13\\% energy savings over an ISAAC-like accelerator with negligible accuracy loss and area overhead.\n## ANALYSIS OF CNN ACTIVATION GROUPS\nSimilarly, when decreasing the numerical precision of the uniform quantization to 4-bits, the RRMSE increases to 28.81% and 7.89% for groups (a) and (b) respectively, which corresponds to an increment of 17.2x and 16x respect the 8-bit uniform quantization. Based on these observations, we can potentially reduce the bitwidth of groups of activations with a more uniform distribution without affecting the accuracy of the network. Consequently, ReDy analyses the distribution of activation groups on-the-fly to determine the optimal numerical precision of the dynamic quantization process of each group.",
            "reference_string": "[259274857 | Sabri et al. | 2023 | Citations: 2]"
        },
        {
            "title": "Optimizing Large Language Model Training Using FP4 Quantization",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 63,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.17116, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2262398543",
                    "name": "Ruizhe Wang"
                },
                {
                    "authorId": "2301575726",
                    "name": "Yeyun Gong"
                },
                {
                    "authorId": "2342190401",
                    "name": "Xiao Liu"
                },
                {
                    "authorId": "2262447706",
                    "name": "Guoshuai Zhao"
                },
                {
                    "authorId": "2254136087",
                    "name": "Ziyue Yang"
                },
                {
                    "authorId": "2292202061",
                    "name": "Baining Guo"
                },
                {
                    "authorId": "2342505593",
                    "name": "Zhengjun Zha"
                },
                {
                    "authorId": "2281748184",
                    "name": "Peng Cheng"
                }
            ],
            "abstract": "The growing computational demands of training large language models (LLMs) necessitate more efficient methods. Quantized training presents a promising solution by enabling low-bit arithmetic operations to reduce these costs. While FP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge due to significant quantization errors and limited representational capacity. This work introduces the first FP4 training framework for LLMs, addressing these challenges with two key innovations: a differentiable quantization estimator for precise weight updates and an outlier clamping and compensation strategy to prevent activation collapse. To ensure stability, the framework integrates a mixed-precision training scheme and vector-wise quantization. Experimental results demonstrate that our FP4 framework achieves accuracy comparable to BF16 and FP8, with minimal degradation, scaling effectively to 13B-parameter LLMs trained on up to 100B tokens. With the emergence of next-generation hardware supporting FP4, our framework sets a foundation for efficient ultra-low precision training.",
            "corpus_id": 275932373,
            "sentences": [
                {
                    "corpus_id": "275932373",
                    "title": "Optimizing Large Language Model Training Using FP4 Quantization",
                    "text": "Meanwhile, the Blackwell series GPUs extend this capability by supporting FP6 and FP4 formats, with FP4 offering the potential to double computational throughput over FP8. Studies like FP8-LM (Peng et al., 2023) and NVIDIA's Transformer Engine (Nvidia, 2022) have demonstrated the feasibility of FP8 tensor cores for model training. But the application of FP4 tensor cores in model training remains an open research question. \n\nHowever, leveraging 4-bit data formats for neural network training presents significant challenges due to the extremely limited bit width. Directly quantizing LLMs to such a lowbit format often results in substantial accuracy degradation, as shown in Figure 1. This is primarily because low-bit formats are constrained by a limited dynamic range, which increases the risk of overflow and underflow. Even existing methods for 8-bit quantization experience some degree of accuracy loss, underscoring the difficulties of employing a 4-bit format, which provides only 16 distinct representable values. \n\nIn this study, we pioneeringly propose a framework for training language models using the FP4 format, providing a validation of the feasibility of this ultra-low precision representation. To tackle the significant quantization errors associated with weights and activations during model training, we present a series of optimization techniques: (1) For weights, we present a differentiable quantization estimator to improve gradient updates in FP4 computations. By analyzing the impact of quantization on neural network forward and backward passes, we derive a function with correction terms for accurate gradient estimation; (2) For activations, we develop an outlier clamping and compensation strategy to address the issue of outlier values commonly observed during LLM training. By analyzing activation distributions in LLMs, we introduce a clamping method and a sparse auxiliary matrix to preserve quantization accuracy and maintain model performance. \n\nWe conduct comprehensive experiments to demonstrate that our FP4 training framework achieves accuracy comparable to models trained in BF16 or FP8 formats with the same hyperparameters.",
                    "score": 0.6671064952974478,
                    "section_title": "Introduction",
                    "char_start_offset": 1824,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 171
                        },
                        {
                            "start": 172,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 425
                        },
                        {
                            "start": 428,
                            "end": 566
                        },
                        {
                            "start": 567,
                            "end": 688
                        },
                        {
                            "start": 689,
                            "end": 826
                        },
                        {
                            "start": 827,
                            "end": 1025
                        },
                        {
                            "start": 1028,
                            "end": 1215
                        },
                        {
                            "start": 1216,
                            "end": 1489
                        },
                        {
                            "start": 1490,
                            "end": 1809
                        },
                        {
                            "start": 1810,
                            "end": 1983
                        },
                        {
                            "start": 1986,
                            "end": 2170
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96484375
                }
            ],
            "relevance_judgement": 0.96484375,
            "relevance_judgment_input_expanded": "# Title: Optimizing Large Language Model Training Using FP4 Quantization\n# Venue: arXiv.org\n# Authors: Ruizhe Wang, Yeyun Gong, Xiao Liu, Guoshuai Zhao, Ziyue Yang, Baining Guo, Zhengjun Zha, Peng Cheng\n## Abstract\nThe growing computational demands of training large language models (LLMs) necessitate more efficient methods. Quantized training presents a promising solution by enabling low-bit arithmetic operations to reduce these costs. While FP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge due to significant quantization errors and limited representational capacity. This work introduces the first FP4 training framework for LLMs, addressing these challenges with two key innovations: a differentiable quantization estimator for precise weight updates and an outlier clamping and compensation strategy to prevent activation collapse. To ensure stability, the framework integrates a mixed-precision training scheme and vector-wise quantization. Experimental results demonstrate that our FP4 framework achieves accuracy comparable to BF16 and FP8, with minimal degradation, scaling effectively to 13B-parameter LLMs trained on up to 100B tokens. With the emergence of next-generation hardware supporting FP4, our framework sets a foundation for efficient ultra-low precision training.\n## Introduction\nMeanwhile, the Blackwell series GPUs extend this capability by supporting FP6 and FP4 formats, with FP4 offering the potential to double computational throughput over FP8. Studies like FP8-LM (Peng et al., 2023) and NVIDIA's Transformer Engine (Nvidia, 2022) have demonstrated the feasibility of FP8 tensor cores for model training. But the application of FP4 tensor cores in model training remains an open research question. \n\nHowever, leveraging 4-bit data formats for neural network training presents significant challenges due to the extremely limited bit width. Directly quantizing LLMs to such a lowbit format often results in substantial accuracy degradation, as shown in Figure 1. This is primarily because low-bit formats are constrained by a limited dynamic range, which increases the risk of overflow and underflow. Even existing methods for 8-bit quantization experience some degree of accuracy loss, underscoring the difficulties of employing a 4-bit format, which provides only 16 distinct representable values. \n\nIn this study, we pioneeringly propose a framework for training language models using the FP4 format, providing a validation of the feasibility of this ultra-low precision representation. To tackle the significant quantization errors associated with weights and activations during model training, we present a series of optimization techniques: (1) For weights, we present a differentiable quantization estimator to improve gradient updates in FP4 computations. By analyzing the impact of quantization on neural network forward and backward passes, we derive a function with correction terms for accurate gradient estimation; (2) For activations, we develop an outlier clamping and compensation strategy to address the issue of outlier values commonly observed during LLM training. By analyzing activation distributions in LLMs, we introduce a clamping method and a sparse auxiliary matrix to preserve quantization accuracy and maintain model performance. \n\nWe conduct comprehensive experiments to demonstrate that our FP4 training framework achieves accuracy comparable to models trained in BF16 or FP8 formats with the same hyperparameters.",
            "reference_string": "[275932373 | Wang et al. | 2025 | Citations: 12]"
        },
        {
            "title": "Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2019,
            "reference_count": 52,
            "citation_count": 96,
            "influential_citation_count": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/17329/17136",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1909.10837, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "81622425",
                    "name": "Shibo Zhou"
                },
                {
                    "authorId": "4107514",
                    "name": "Li Xiaohua"
                },
                {
                    "authorId": "2118426969",
                    "name": "Ying Chen"
                },
                {
                    "authorId": "30908225",
                    "name": "S. T. Chandrasekaran"
                },
                {
                    "authorId": "9442413",
                    "name": "A. Sanyal"
                }
            ],
            "abstract": "Spiking neural network (SNN) is promising but the development has fallen far behind conventional deep neural networks (DNNs) because of difficult training. To resolve the training problem, we analyze the closed-form input-output response of spiking neurons and use the response expression to build abstract SNN models for training. This avoids calculating membrane potential during training and makes the direct training of SNN as efficient as DNN. We show that the nonleaky integrate-and-fire neuron with single-spike temporal-coding is the best choice for direct-train deep SNNs. We develop an energy-efficient phase-domain signal processing circuit for the neuron and propose a direct-train deep SNN framework. Thanks to easy training, we train deep SNNs under weight quantizations to study their robustness over low-cost neuromorphic hardware. Experiments show that our direct-train deep SNNs have the highest CIFAR-10 classification accuracy among SNNs, achieve ImageNet classification accuracy within 1% of the DNN of equivalent architecture, and are robust to weight quantization and noise perturbation.",
            "corpus_id": 221103917,
            "sentences": [
                {
                    "corpus_id": "221103917",
                    "title": "Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance",
                    "text": "see that 4-bit quantization, which has quantization SNR (signal to noise ratio) 6.02 \u00d7 4 \u2248 24 dB, led to output jitter of 0.957% which means SNR 20 log 10 (1/.00957) \u2248 40 dB. A 40 dB SNR means that the neuron was extremely robust to weight quantization.\n\nDeep SNN Robustness: To evaluate the robustness of deep SNNs, we experimented with both weight quantization and noise perturbation. For weight quantization, the weights were quantized to 32-bit, 8-bit, 4-bit, and 2-bit words. Thanks to our easy training models, we could retrain the deep SNNs simply following the procedure developed for conventional CNN quantization (Li, Zhang, and Liu 2016;Rastegari et al. 2016). Specifically, the forward inference used quantized weights while the backward gradient propagation used full-precision weights. We first trained with 32-bit quantization. After the training converged, we applied 8-bit quantization and retrain the SNNs. This procedure was repeated until the 2-bit quantization. Table 4 shows the testing accuracy under weight quantization. For the models over MNIST, CIFAR10 and Ima-geNet, weight quantization caused the worst accuracy loss of 0.22%, 1.75%, and 8.8%, respectively. As comparison, we listed two typical CNN weight quantization results (Cheng et al. 2018;Zhang et al. 2018), which indicated a similar performance degradation pace. The results demonstrated that the SNN models were robust to weight quantization for relatively small datasets and small networks. For larger networks, the weights would better be encoded in 4-bit or over.\n\nTo evaluate noise perturbation, we added random noise to the trained weights. Experiment results are shown in Fig. 3, together with the results of weight quantization in Table 4 (expressed in SNR). We find that 24dB quantization noise (4-bit quantization) reduced ImageNet classification accuracy to 65.2%. Noise at 24dB SNR reduced ImageNet clas-   sification accuracy to 65.43",
                    "score": 0.6041269037276213,
                    "section_title": "Robustness",
                    "char_start_offset": 27448,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 648,
                            "end": 670,
                            "matchedPaperCorpusId": "14925907"
                        },
                        {
                            "start": 1275,
                            "end": 1293,
                            "matchedPaperCorpusId": "50784025"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96435546875
                }
            ],
            "relevance_judgement": 0.96435546875,
            "relevance_judgment_input_expanded": "# Title: Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Shibo Zhou, Li Xiaohua, Ying Chen, S. T. Chandrasekaran, A. Sanyal\n## Abstract\nSpiking neural network (SNN) is promising but the development has fallen far behind conventional deep neural networks (DNNs) because of difficult training. To resolve the training problem, we analyze the closed-form input-output response of spiking neurons and use the response expression to build abstract SNN models for training. This avoids calculating membrane potential during training and makes the direct training of SNN as efficient as DNN. We show that the nonleaky integrate-and-fire neuron with single-spike temporal-coding is the best choice for direct-train deep SNNs. We develop an energy-efficient phase-domain signal processing circuit for the neuron and propose a direct-train deep SNN framework. Thanks to easy training, we train deep SNNs under weight quantizations to study their robustness over low-cost neuromorphic hardware. Experiments show that our direct-train deep SNNs have the highest CIFAR-10 classification accuracy among SNNs, achieve ImageNet classification accuracy within 1% of the DNN of equivalent architecture, and are robust to weight quantization and noise perturbation.\n## Robustness\nsee that 4-bit quantization, which has quantization SNR (signal to noise ratio) 6.02 \u00d7 4 \u2248 24 dB, led to output jitter of 0.957% which means SNR 20 log 10 (1/.00957) \u2248 40 dB. A 40 dB SNR means that the neuron was extremely robust to weight quantization.\n\nDeep SNN Robustness: To evaluate the robustness of deep SNNs, we experimented with both weight quantization and noise perturbation. For weight quantization, the weights were quantized to 32-bit, 8-bit, 4-bit, and 2-bit words. Thanks to our easy training models, we could retrain the deep SNNs simply following the procedure developed for conventional CNN quantization (Li, Zhang, and Liu 2016;Rastegari et al. 2016). Specifically, the forward inference used quantized weights while the backward gradient propagation used full-precision weights. We first trained with 32-bit quantization. After the training converged, we applied 8-bit quantization and retrain the SNNs. This procedure was repeated until the 2-bit quantization. Table 4 shows the testing accuracy under weight quantization. For the models over MNIST, CIFAR10 and Ima-geNet, weight quantization caused the worst accuracy loss of 0.22%, 1.75%, and 8.8%, respectively. As comparison, we listed two typical CNN weight quantization results (Cheng et al. 2018;Zhang et al. 2018), which indicated a similar performance degradation pace. The results demonstrated that the SNN models were robust to weight quantization for relatively small datasets and small networks. For larger networks, the weights would better be encoded in 4-bit or over.\n\nTo evaluate noise perturbation, we added random noise to the trained weights. Experiment results are shown in Fig. 3, together with the results of weight quantization in Table 4 (expressed in SNR). We find that 24dB quantization noise (4-bit quantization) reduced ImageNet classification accuracy to 65.2%. Noise at 24dB SNR reduced ImageNet clas-   sification accuracy to 65.43",
            "reference_string": "[221103917 | Zhou et al. | 2019 | Citations: 96]"
        },
        {
            "title": "DyBit: Dynamic Bit-Precision Numbers for Efficient Quantized Neural Network Inference",
            "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
            "year": 2023,
            "reference_count": 18,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2302.12510",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.12510, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Jiajun Zhou"
                },
                {
                    "authorId": "2110436131",
                    "name": "Jiajun Wu"
                },
                {
                    "authorId": "1939358",
                    "name": "Yizhao Gao"
                },
                {
                    "authorId": "40879066",
                    "name": "Yuhao Ding"
                },
                {
                    "authorId": "144259094",
                    "name": "Chaofan Tao"
                },
                {
                    "authorId": "143771569",
                    "name": "Bo Li"
                },
                {
                    "authorId": "1910352",
                    "name": "Fengbin Tu"
                },
                {
                    "authorId": "145210800",
                    "name": "Kwang-Ting Cheng"
                },
                {
                    "authorId": "1710259",
                    "name": "Hayden Kwok-Hay So"
                },
                {
                    "authorId": "1873081",
                    "name": "Ngai Wong"
                }
            ],
            "abstract": "To accelerate the inference of deep neural networks (DNNs), quantization with low-bitwidth numbers is actively researched. A prominent challenge is to quantize the DNN models into low-bitwidth numbers without significant accuracy degradation, especially at very low bitwidths <inline-formula> <tex-math notation=\"LaTeX\">$( < 8$ </tex-math></inline-formula> bits). This work targets an adaptive data representation with variable-length encoding called DyBit. DyBit can dynamically adjust the precision and range of separate bit-fields to be adapted to the DNN weights/activations distribution. We also propose a hardware-aware quantization framework with a mixed-precision accelerator to tradeoff the inference accuracy and speedup. Experimental results demonstrate that the ImageNet inference accuracy via DyBit is 1.97% higher than the state-of-the-art at 4-bit quantization, and the proposed framework can achieve up to <inline-formula> <tex-math notation=\"LaTeX\">$8.1\\times $ </tex-math></inline-formula> speedup compared with the original ResNet-50 model.",
            "corpus_id": 257205844,
            "sentences": [],
            "relevance_judgement": 0.96435546875,
            "relevance_judgment_input_expanded": "# Title: DyBit: Dynamic Bit-Precision Numbers for Efficient Quantized Neural Network Inference\n# Venue: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems\n# Authors: Jiajun Zhou, Jiajun Wu, Yizhao Gao, Yuhao Ding, Chaofan Tao, Bo Li, Fengbin Tu, Kwang-Ting Cheng, Hayden Kwok-Hay So, Ngai Wong\n## Abstract\nTo accelerate the inference of deep neural networks (DNNs), quantization with low-bitwidth numbers is actively researched. A prominent challenge is to quantize the DNN models into low-bitwidth numbers without significant accuracy degradation, especially at very low bitwidths <inline-formula> <tex-math notation=\"LaTeX\">$( < 8$ </tex-math></inline-formula> bits). This work targets an adaptive data representation with variable-length encoding called DyBit. DyBit can dynamically adjust the precision and range of separate bit-fields to be adapted to the DNN weights/activations distribution. We also propose a hardware-aware quantization framework with a mixed-precision accelerator to tradeoff the inference accuracy and speedup. Experimental results demonstrate that the ImageNet inference accuracy via DyBit is 1.97% higher than the state-of-the-art at 4-bit quantization, and the proposed framework can achieve up to <inline-formula> <tex-math notation=\"LaTeX\">$8.1\\times $ </tex-math></inline-formula> speedup compared with the original ResNet-50 model.\n",
            "reference_string": "[257205844 | Zhou et al. | 2023 | Citations: 7]"
        },
        {
            "title": "Accurate Neural Training with 4-bit Matrix Multiplications at Standard Formats",
            "venue": "International Conference on Learning Representations",
            "year": 2021,
            "reference_count": 31,
            "citation_count": 23,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.10769, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "104387774",
                    "name": "Brian Chmiel"
                },
                {
                    "authorId": "2607278",
                    "name": "Ron Banner"
                },
                {
                    "authorId": "40555034",
                    "name": "Elad Hoffer"
                },
                {
                    "authorId": "2146540625",
                    "name": "Hilla Ben Yaacov"
                },
                {
                    "authorId": "1912398",
                    "name": "Daniel Soudry"
                }
            ],
            "abstract": "Quantization of the weights and activations is one of the main methods to reduce the computational footprint of Deep Neural Networks (DNNs) training. Current methods enable 4-bit quantization of the forward phase. However, this constitutes only a third of the training process. Reducing the computational footprint of the entire training process requires the quantization of the neural gradients, i.e., the loss gradients with respect to the outputs of intermediate neural layers. Previous works separately showed that accurate 4-bit quantization of the neural gradients needs to (1) be unbiased and (2) have a log scale. However, no previous work aimed to combine both ideas, as we do in this work. Specifically, we examine the importance of having unbiased quantization in quantized neural network training, where to maintain it, and how to combine it with logarithmic quantization. Based on this, we suggest a $\\textit{logarithmic unbiased quantization}$ (LUQ) method to quantize both the forward and backward phases to 4-bit, achieving state-of-the-art results in 4-bit training without the overhead. For example, in ResNet50 on ImageNet, we achieved a degradation of 1.1%. We further improve this to a degradation of only 0.32% after three epochs of high precision fine-tuning, combined with a variance reduction method -- where both these methods add overhead comparable to previously suggested methods.",
            "corpus_id": 246634451,
            "sentences": [
                {
                    "corpus_id": "246634451",
                    "title": "Accurate Neural Training with 4-bit Matrix Multiplications at Standard Formats",
                    "text": "In Table 2 we show the Top-1 accuracy achieved in 4-bit training using LUQ to quantize the neural gradients to FP4 and combined with a previously suggested method, SAWB (Choi et al., 2018a), to quantize the weights and activations to INT4. We compare our method with Ultra-low (Sun et al., 2020) showing better results in all the models, achieving SOTA in 4-bit training. Moreover, we improve the results by using the proposed SMP (Section 5.1). In Table 3 we show the effect of the proposed fine-tuning, reducing or closing completely the gap from full-precision model. We verified that stochasticity has only a negligible effect on the variance of final performance by running a few different seeds. Additional experiments appear in Appendix A.5. Overhead of SMP and FNT We limit our experiments with the proposed SMP method to only two samples. This is to achieve a similar computational overhead as Ultra-low (Sun et al., 2020), with their suggested two-phase-rounding (TPR) which also generates a duplication for the neural gradient quantization. Additional ablation study of the SMP overhead appears in Appendix A.5. The throughput of a 4-bit training network is approximately 8x in comparison to full precision training (Sun et al., 2020). This means that doing one additional epoch in high precision reduces the throughput by \u223c 8%. In comparison, Ultra-low (Sun et al., 2020) does full-training with all the 1x1 convolutions in 8bit, which reduces the throughput by \u223c 50% in comparison to all 4bit training.",
                    "score": 0.6651318656816585,
                    "section_title": "Main results",
                    "char_start_offset": 23539,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 371
                        },
                        {
                            "start": 372,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 570
                        },
                        {
                            "start": 571,
                            "end": 701
                        },
                        {
                            "start": 702,
                            "end": 847
                        },
                        {
                            "start": 848,
                            "end": 1051
                        },
                        {
                            "start": 1052,
                            "end": 1122
                        },
                        {
                            "start": 1123,
                            "end": 1246
                        },
                        {
                            "start": 1247,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1515
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 277,
                            "end": 295,
                            "matchedPaperCorpusId": "227275152"
                        },
                        {
                            "start": 913,
                            "end": 931,
                            "matchedPaperCorpusId": "227275152"
                        },
                        {
                            "start": 1227,
                            "end": 1245,
                            "matchedPaperCorpusId": "227275152"
                        },
                        {
                            "start": 1365,
                            "end": 1383,
                            "matchedPaperCorpusId": "227275152"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96337890625
                }
            ],
            "relevance_judgement": 0.96337890625,
            "relevance_judgment_input_expanded": "# Title: Accurate Neural Training with 4-bit Matrix Multiplications at Standard Formats\n# Venue: International Conference on Learning Representations\n# Authors: Brian Chmiel, Ron Banner, Elad Hoffer, Hilla Ben Yaacov, Daniel Soudry\n## Abstract\nQuantization of the weights and activations is one of the main methods to reduce the computational footprint of Deep Neural Networks (DNNs) training. Current methods enable 4-bit quantization of the forward phase. However, this constitutes only a third of the training process. Reducing the computational footprint of the entire training process requires the quantization of the neural gradients, i.e., the loss gradients with respect to the outputs of intermediate neural layers. Previous works separately showed that accurate 4-bit quantization of the neural gradients needs to (1) be unbiased and (2) have a log scale. However, no previous work aimed to combine both ideas, as we do in this work. Specifically, we examine the importance of having unbiased quantization in quantized neural network training, where to maintain it, and how to combine it with logarithmic quantization. Based on this, we suggest a $\\textit{logarithmic unbiased quantization}$ (LUQ) method to quantize both the forward and backward phases to 4-bit, achieving state-of-the-art results in 4-bit training without the overhead. For example, in ResNet50 on ImageNet, we achieved a degradation of 1.1%. We further improve this to a degradation of only 0.32% after three epochs of high precision fine-tuning, combined with a variance reduction method -- where both these methods add overhead comparable to previously suggested methods.\n## Main results\nIn Table 2 we show the Top-1 accuracy achieved in 4-bit training using LUQ to quantize the neural gradients to FP4 and combined with a previously suggested method, SAWB (Choi et al., 2018a), to quantize the weights and activations to INT4. We compare our method with Ultra-low (Sun et al., 2020) showing better results in all the models, achieving SOTA in 4-bit training. Moreover, we improve the results by using the proposed SMP (Section 5.1). In Table 3 we show the effect of the proposed fine-tuning, reducing or closing completely the gap from full-precision model. We verified that stochasticity has only a negligible effect on the variance of final performance by running a few different seeds. Additional experiments appear in Appendix A.5. Overhead of SMP and FNT We limit our experiments with the proposed SMP method to only two samples. This is to achieve a similar computational overhead as Ultra-low (Sun et al., 2020), with their suggested two-phase-rounding (TPR) which also generates a duplication for the neural gradient quantization. Additional ablation study of the SMP overhead appears in Appendix A.5. The throughput of a 4-bit training network is approximately 8x in comparison to full precision training (Sun et al., 2020). This means that doing one additional epoch in high precision reduces the throughput by \u223c 8%. In comparison, Ultra-low (Sun et al., 2020) does full-training with all the 1x1 convolutions in 8bit, which reduces the throughput by \u223c 50% in comparison to all 4bit training.",
            "reference_string": "[246634451 | Chmiel et al. | 2021 | Citations: 23]"
        },
        {
            "title": "McDRAM v2: In-Dynamic Random Access Memory Systolic Array Accelerator to Address the Large Model Problem in Deep Neural Networks on the Edge",
            "venue": "IEEE Access",
            "year": 2020,
            "reference_count": 61,
            "citation_count": 41,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09146167.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.3011265?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.3011265, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2111878754",
                    "name": "Seunghwan Cho"
                },
                {
                    "authorId": "1716222897",
                    "name": "Haerang Choi"
                },
                {
                    "authorId": "2292315",
                    "name": "Eunhyeok Park"
                },
                {
                    "authorId": "10307183",
                    "name": "Hyunsung Shin"
                },
                {
                    "authorId": "1808405",
                    "name": "Sungjoo Yoo"
                }
            ],
            "abstract": "The energy efficiency of accelerating hundreds of MB-large deep neural networks (DNNs) in a mobile environment is less than that of a server-class big chip accelerator because of the limited power budget, silicon area, and smaller buffer size of static random access memory associated with mobile systems. To address this challenge and provide powerful computing capability for processing large DNN models in power/resource-limited mobile systems, we propose McDRAM v2, which is a novel in-dynamic random access memory (DRAM) systolic array accelerator architecture. McDRAM v2 makes the best use of large in-DRAM bandwidths for accelerating various DNN applications. It can handle large DNN models without off-chip memory accesses, in a fast and efficient manner, by exposing the large DRAM capacity and large in-DRAM bandwidth directly to an input systolic array of a processing element matrix. Additionally, it maximizes data reuse using a systolic multiply\u2013accumulate (MAC) structure. The proposed architecture maximizes the utilization of large-scale MAC units by judiciously exploiting the DRAM\u2019s internal bus and buffer structure. An evaluation of large DNN models in the fields of image classification, natural language processing, and recommendation systems shows that it achieves 1.7 times tera operations per second (TOPS), 3.7 times TOPS/watt, and 8.6 times TOPS/mm2 improvements over a state-of-the\u2013art mobile graphics processing unit accelerator, and 4.1 times better energy efficiency over a state-of-the-art server-class accelerator. Moreover, it incurs a minimal overhead, i.e., a 9.7% increase in area, and uses less than 4.4 W of peak operating power.",
            "corpus_id": 220891847,
            "sentences": [
                {
                    "corpus_id": "220891847",
                    "title": "McDRAM v2: In-Dynamic Random Access Memory Systolic Array Accelerator to Address the Large Model Problem in Deep Neural Networks on the Edge",
                    "text": "Quantization is one of the most effective compression techniques used for reducing the weight and activation bit-width of a neural network model, thereby resulting in reduced model size and execution cost in terms of energy, time, and silicon area. The general matrix multiply (GEMM) operation is most commonly used to perform inferences for DNNs. Although CPUs and GPUs generally process this GEMM operation using floating-point 32-bit precision without applying a quantization technique, previous studies have proposed numerous methods for quantizing DNN models for a variety of tasks in a range of domains, and have successfully attained a nearly lossless inference model, using less than 8-bit integer (int8) precision. \n\nFor CNNs, inferences with int8 precision has been widely adopted in an increasing number of real-world GPUs [17], [34]/NPUs [35], [37], as int8 precision has been shown to be sufficient for GEMM operation in the inference of various CNNs in recent studies. It has also been found that sub-8-bit quantization of the inference of a CNN does not lead to significant accuracy loss. For example, with the ImageNet classification dataset [27], [38] reports an activation 4-bit and weight 4-bit quantization of an ResNet-50 [10] with a 0.2% top-1 accuracy loss compared to the case of FP32, in which an activation 2-bit and weight 2-bit quantization was done and a 3.2% top-1 accuracy loss was obtained. \n\nAmong the recent studies concerned with the quantization on BERT [1], a model widely used in the field of NLP, [39] presented the state-of-the-art results. For example, in tasks of sentiment classification, natural language inference, named entity recognition, and machine reading comprehension with benchmarks of the general language understanding evaluation (GLUE) [40], Stanford Question Answering Dataset (SQuAD) [41], and CoNLL-03 [42], the activation 8-bit and weight 4-bit quantization precision yielded a comparable performance, without any significant quality degradation in the BERT model [1].",
                    "score": 0.6909397440950684,
                    "section_title": "B. LATEST QUANTIZATION RESULTS",
                    "char_start_offset": 10232,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 248
                        },
                        {
                            "start": 249,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 723
                        },
                        {
                            "start": 726,
                            "end": 982
                        },
                        {
                            "start": 983,
                            "end": 1103
                        },
                        {
                            "start": 1104,
                            "end": 1422
                        },
                        {
                            "start": 1425,
                            "end": 1580
                        },
                        {
                            "start": 1581,
                            "end": 2028
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 850,
                            "end": 854,
                            "matchedPaperCorpusId": "213621056"
                        },
                        {
                            "start": 856,
                            "end": 860,
                            "matchedPaperCorpusId": "4202768"
                        },
                        {
                            "start": 1158,
                            "end": 1162,
                            "matchedPaperCorpusId": "57246310"
                        },
                        {
                            "start": 1243,
                            "end": 1247,
                            "matchedPaperCorpusId": "206594692"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.962890625
                }
            ],
            "relevance_judgement": 0.962890625,
            "relevance_judgment_input_expanded": "# Title: McDRAM v2: In-Dynamic Random Access Memory Systolic Array Accelerator to Address the Large Model Problem in Deep Neural Networks on the Edge\n# Venue: IEEE Access\n# Authors: Seunghwan Cho, Haerang Choi, Eunhyeok Park, Hyunsung Shin, Sungjoo Yoo\n## Abstract\nThe energy efficiency of accelerating hundreds of MB-large deep neural networks (DNNs) in a mobile environment is less than that of a server-class big chip accelerator because of the limited power budget, silicon area, and smaller buffer size of static random access memory associated with mobile systems. To address this challenge and provide powerful computing capability for processing large DNN models in power/resource-limited mobile systems, we propose McDRAM v2, which is a novel in-dynamic random access memory (DRAM) systolic array accelerator architecture. McDRAM v2 makes the best use of large in-DRAM bandwidths for accelerating various DNN applications. It can handle large DNN models without off-chip memory accesses, in a fast and efficient manner, by exposing the large DRAM capacity and large in-DRAM bandwidth directly to an input systolic array of a processing element matrix. Additionally, it maximizes data reuse using a systolic multiply\u2013accumulate (MAC) structure. The proposed architecture maximizes the utilization of large-scale MAC units by judiciously exploiting the DRAM\u2019s internal bus and buffer structure. An evaluation of large DNN models in the fields of image classification, natural language processing, and recommendation systems shows that it achieves 1.7 times tera operations per second (TOPS), 3.7 times TOPS/watt, and 8.6 times TOPS/mm2 improvements over a state-of-the\u2013art mobile graphics processing unit accelerator, and 4.1 times better energy efficiency over a state-of-the-art server-class accelerator. Moreover, it incurs a minimal overhead, i.e., a 9.7% increase in area, and uses less than 4.4 W of peak operating power.\n## B. LATEST QUANTIZATION RESULTS\nQuantization is one of the most effective compression techniques used for reducing the weight and activation bit-width of a neural network model, thereby resulting in reduced model size and execution cost in terms of energy, time, and silicon area. The general matrix multiply (GEMM) operation is most commonly used to perform inferences for DNNs. Although CPUs and GPUs generally process this GEMM operation using floating-point 32-bit precision without applying a quantization technique, previous studies have proposed numerous methods for quantizing DNN models for a variety of tasks in a range of domains, and have successfully attained a nearly lossless inference model, using less than 8-bit integer (int8) precision. \n\nFor CNNs, inferences with int8 precision has been widely adopted in an increasing number of real-world GPUs [17], [34]/NPUs [35], [37], as int8 precision has been shown to be sufficient for GEMM operation in the inference of various CNNs in recent studies. It has also been found that sub-8-bit quantization of the inference of a CNN does not lead to significant accuracy loss. For example, with the ImageNet classification dataset [27], [38] reports an activation 4-bit and weight 4-bit quantization of an ResNet-50 [10] with a 0.2% top-1 accuracy loss compared to the case of FP32, in which an activation 2-bit and weight 2-bit quantization was done and a 3.2% top-1 accuracy loss was obtained. \n\nAmong the recent studies concerned with the quantization on BERT [1], a model widely used in the field of NLP, [39] presented the state-of-the-art results. For example, in tasks of sentiment classification, natural language inference, named entity recognition, and machine reading comprehension with benchmarks of the general language understanding evaluation (GLUE) [40], Stanford Question Answering Dataset (SQuAD) [41], and CoNLL-03 [42], the activation 8-bit and weight 4-bit quantization precision yielded a comparable performance, without any significant quality degradation in the BERT model [1].",
            "reference_string": "[220891847 | Cho et al. | 2020 | Citations: 41]"
        },
        {
            "title": "QReg: On Regularization Effects of Quantization",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 24,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2206.12372",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.12372, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1610857529",
                    "name": "Mohammadhossein Askarihemmat"
                },
                {
                    "authorId": "7872299",
                    "name": "Reyhane Askari Hemmat"
                },
                {
                    "authorId": "2062267546",
                    "name": "Alexander Hoffman"
                },
                {
                    "authorId": "4481888",
                    "name": "I. Lazarevich"
                },
                {
                    "authorId": "1682819",
                    "name": "Ehsan Saboori"
                },
                {
                    "authorId": "3422889",
                    "name": "Olivier Mastropietro"
                },
                {
                    "authorId": "1756949",
                    "name": "Y. Savaria"
                },
                {
                    "authorId": "145719986",
                    "name": "J. David"
                }
            ],
            "abstract": "In this paper we study the effects of quantization in DNN training. We hypothesize that weight quantization is a form of regularization and the amount of regularization is correlated with the quantization level (precision). We con\ufb01rm our hypothesis by providing analytical study and empirical results. By modeling weight quantization as a form of additive noise to weights, we explore how this noise propagates through the network at training time. We then show that the magnitude of this noise is correlated with the level of quantization. To con\ufb01rm our analytical study, we performed an extensive list of experiments sum-marized in this paper in which we show that the regularization effects of quantization can be seen in various vision tasks and models, over various datasets. Based on our study, we propose that 8-bit quantization provides a reliable form of regularization in different vision tasks and models.",
            "corpus_id": 250048704,
            "sentences": [
                {
                    "corpus_id": "250048704",
                    "title": "QReg: On Regularization Effects of Quantization",
                    "text": "Deep neural network (DNN) quantization has been an active research topic in the past few years. Quantization is the task of approximating high precision (floating point) weights and activations of a model with lower bit resolution counterparts (usually 1 to 8 bit integers). Quantization can be done after (post training quantization) or during training (quantization aware training). Although both methods can compress the model and reduce overall execution time, at the cost of model accuracy, post training quantization usually takes less time to quantize a model. On the other hand, quantization aware training usually requires a full training session and with a moderate quantization level (int8) can produce quantized models with little to no accuracy loss compared to their full precision counterparts. In fact, as we will discuss in Section 2, there are many existing works Objectiveness Loss Value Epoch Number Validation objective loss of 4-bit quantized , 8-bit quantized and full precision model for the YOLOv5n model on the VOC dataset. This figure illustrates that unlike the quantized model, the full precision model exhibits heavy overfitting. We believe that the quantized model has better generalization performance due to its regularization effect. \n\nclaiming that quantization aware training produced accuracy improvements compared to some of the best full precision models. In these studies, the authors usually attribute such accuracy gains to the regularization effect of quantization. This has motivated us to explore how quantization affects training. In this work, we first hypothesize the regularization effect of quantization. Moreover, we hypothesize that this regularization effect is correlated with the quantization level. To confirm our hypothesis, we analytically explore how quantization noise propagates in a network at training time, and how the quantization level is correlated with the amount of regularization. We then empirically explore the regularization effect of quantization by applying different quantization aware training methods on different models and datasets with different quantization levels. In both analytical and empirical studies, we confirm our initial hypothesis.",
                    "score": 0.6328636991027249,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 95
                        },
                        {
                            "start": 96,
                            "end": 274
                        },
                        {
                            "start": 275,
                            "end": 384
                        },
                        {
                            "start": 385,
                            "end": 567
                        },
                        {
                            "start": 568,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 1049
                        },
                        {
                            "start": 1050,
                            "end": 1159
                        },
                        {
                            "start": 1160,
                            "end": 1267
                        },
                        {
                            "start": 1270,
                            "end": 1394
                        },
                        {
                            "start": 1395,
                            "end": 1508
                        },
                        {
                            "start": 1509,
                            "end": 1576
                        },
                        {
                            "start": 1577,
                            "end": 1654
                        },
                        {
                            "start": 1655,
                            "end": 1754
                        },
                        {
                            "start": 1755,
                            "end": 1950
                        },
                        {
                            "start": 1951,
                            "end": 2147
                        },
                        {
                            "start": 2148,
                            "end": 2224
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9609375
                }
            ],
            "relevance_judgement": 0.9609375,
            "relevance_judgment_input_expanded": "# Title: QReg: On Regularization Effects of Quantization\n# Venue: arXiv.org\n# Authors: Mohammadhossein Askarihemmat, Reyhane Askari Hemmat, Alexander Hoffman, I. Lazarevich, Ehsan Saboori, Olivier Mastropietro, Y. Savaria, J. David\n## Abstract\nIn this paper we study the effects of quantization in DNN training. We hypothesize that weight quantization is a form of regularization and the amount of regularization is correlated with the quantization level (precision). We con\ufb01rm our hypothesis by providing analytical study and empirical results. By modeling weight quantization as a form of additive noise to weights, we explore how this noise propagates through the network at training time. We then show that the magnitude of this noise is correlated with the level of quantization. To con\ufb01rm our analytical study, we performed an extensive list of experiments sum-marized in this paper in which we show that the regularization effects of quantization can be seen in various vision tasks and models, over various datasets. Based on our study, we propose that 8-bit quantization provides a reliable form of regularization in different vision tasks and models.\n## Introduction\nDeep neural network (DNN) quantization has been an active research topic in the past few years. Quantization is the task of approximating high precision (floating point) weights and activations of a model with lower bit resolution counterparts (usually 1 to 8 bit integers). Quantization can be done after (post training quantization) or during training (quantization aware training). Although both methods can compress the model and reduce overall execution time, at the cost of model accuracy, post training quantization usually takes less time to quantize a model. On the other hand, quantization aware training usually requires a full training session and with a moderate quantization level (int8) can produce quantized models with little to no accuracy loss compared to their full precision counterparts. In fact, as we will discuss in Section 2, there are many existing works Objectiveness Loss Value Epoch Number Validation objective loss of 4-bit quantized , 8-bit quantized and full precision model for the YOLOv5n model on the VOC dataset. This figure illustrates that unlike the quantized model, the full precision model exhibits heavy overfitting. We believe that the quantized model has better generalization performance due to its regularization effect. \n\nclaiming that quantization aware training produced accuracy improvements compared to some of the best full precision models. In these studies, the authors usually attribute such accuracy gains to the regularization effect of quantization. This has motivated us to explore how quantization affects training. In this work, we first hypothesize the regularization effect of quantization. Moreover, we hypothesize that this regularization effect is correlated with the quantization level. To confirm our hypothesis, we analytically explore how quantization noise propagates in a network at training time, and how the quantization level is correlated with the amount of regularization. We then empirically explore the regularization effect of quantization by applying different quantization aware training methods on different models and datasets with different quantization levels. In both analytical and empirical studies, we confirm our initial hypothesis.",
            "reference_string": "[250048704 | Askarihemmat et al. | 2022 | Citations: 5]"
        },
        {
            "title": "Pruning and Quantization for Deep Neural Network Acceleration: A Survey",
            "venue": "Neurocomputing",
            "year": 2021,
            "reference_count": 296,
            "citation_count": 700,
            "influential_citation_count": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2101.09671",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2101.09671, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "123804153",
                    "name": "Tailin Liang"
                },
                {
                    "authorId": "145073018",
                    "name": "C. Glossner"
                },
                {
                    "authorId": "66148232",
                    "name": "Lei Wang"
                },
                {
                    "authorId": "2072685048",
                    "name": "Shaobo Shi"
                }
            ],
            "abstract": null,
            "corpus_id": 231699188,
            "sentences": [
                {
                    "corpus_id": "231699188",
                    "title": "Pruning and Quantization for Deep Neural Network Acceleration: A Survey",
                    "text": "In addition to accelerating neural networks, quantization has also been found in some cases to result in higher accuracy. As examples: 1) 3-bit weights VGG-16 outperforms its full precision counterpart by 1.1% top-1 [144], 2) AlexNet reduces 1.0% top-1 error of the reference with 2-bit weights and 8-bit activations [66], 3) ResNet-34 with 4-bit weights and activation obtained 74.52% top-1 accuracy while the 32-bit version is 73.59% [174], 4) Zhou showed a quantized model reduced the classification error by 0.15%, 2.28%, 0.13%, 0.71%, and 1.59% on AlexNet, VGG-16, GoogLeNet, ResNet-18 and ResNet-50, respectively [269], and 5) Xu showed reduced bit quantized networks help to reduce over-fitting on Fully Connected Networks (FCNs). By taking advantage of strict constraints in biomedical image segmentation they improved segmentation accuracy by 1% combined with a 6.4\u00d7 memory usage reduction [251].",
                    "score": 0.6263135165136311,
                    "section_title": "Quantization Reduces Over-fitting",
                    "char_start_offset": 114628,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 121
                        },
                        {
                            "start": 122,
                            "end": 737
                        },
                        {
                            "start": 738,
                            "end": 905
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 216,
                            "end": 221,
                            "matchedPaperCorpusId": "8840788"
                        },
                        {
                            "start": 317,
                            "end": 321,
                            "matchedPaperCorpusId": "49559664"
                        },
                        {
                            "start": 436,
                            "end": 441,
                            "matchedPaperCorpusId": "3455952"
                        },
                        {
                            "start": 619,
                            "end": 624,
                            "matchedPaperCorpusId": "12130431"
                        },
                        {
                            "start": 899,
                            "end": 904,
                            "matchedPaperCorpusId": "3860571"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9609375
                }
            ],
            "relevance_judgement": 0.9609375,
            "relevance_judgment_input_expanded": "# Title: Pruning and Quantization for Deep Neural Network Acceleration: A Survey\n# Venue: Neurocomputing\n# Authors: Tailin Liang, C. Glossner, Lei Wang, Shaobo Shi\n## Abstract\nNone\n## Quantization Reduces Over-fitting\nIn addition to accelerating neural networks, quantization has also been found in some cases to result in higher accuracy. As examples: 1) 3-bit weights VGG-16 outperforms its full precision counterpart by 1.1% top-1 [144], 2) AlexNet reduces 1.0% top-1 error of the reference with 2-bit weights and 8-bit activations [66], 3) ResNet-34 with 4-bit weights and activation obtained 74.52% top-1 accuracy while the 32-bit version is 73.59% [174], 4) Zhou showed a quantized model reduced the classification error by 0.15%, 2.28%, 0.13%, 0.71%, and 1.59% on AlexNet, VGG-16, GoogLeNet, ResNet-18 and ResNet-50, respectively [269], and 5) Xu showed reduced bit quantized networks help to reduce over-fitting on Fully Connected Networks (FCNs). By taking advantage of strict constraints in biomedical image segmentation they improved segmentation accuracy by 1% combined with a 6.4\u00d7 memory usage reduction [251].",
            "reference_string": "[231699188 | Liang et al. | 2021 | Citations: 700]"
        },
        {
            "title": "Fully Neural Network Low-Bit Quantization and Adjustment of Activation Distributions",
            "venue": "International Conference on the Software Process",
            "year": 2024,
            "reference_count": 34,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICSP62129.2024.10846012?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICSP62129.2024.10846012, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294335787",
                    "name": "Bing-jie Zhang"
                },
                {
                    "authorId": "2267164095",
                    "name": "Zhihong Yan"
                },
                {
                    "authorId": "2342016079",
                    "name": "Dong Wang"
                }
            ],
            "abstract": "Quantization Aware Training (QAT) can usually achieve better performance than Post-Training Quantization (PTQ) in low-bit quantization due to its parameter learnability. However, deploying the network after low-bit quantization to dedicated hardware is still challenging. In particular, convolutional neural networks maintain 8-bit quantization for some of their sensitive layers when quantized at low bit. To enable the underlying hardware circuitry to support two different computational bitwidth at the same time during hardware deployment leads to an increase in cost; In addition, some target detection networks using LeakyRelu, etc. as the activation function can have poor accuracy after quantization due to the asymmetric distribution of activation values. In this paper, we propose a static 4-bit activation value quantization method for the sensitive layer to unify the quantization bits and an Parameterizable LeakyReLU(PLReLU) activation function to adjust the distribution of activation values to make them symmetric, which reduce the hardware cost and improve the accuracy of the quantization network. Our method outperforms current low-bit quantization methods by 1.11% to 1.72% on ImageNet dataset and by 0.9% on PASCAL VOC dataset, demonstrating the contribution of the design in this paper.",
            "corpus_id": 275852824,
            "sentences": [],
            "relevance_judgement": 0.9609375,
            "relevance_judgment_input_expanded": "# Title: Fully Neural Network Low-Bit Quantization and Adjustment of Activation Distributions\n# Venue: International Conference on the Software Process\n# Authors: Bing-jie Zhang, Zhihong Yan, Dong Wang\n## Abstract\nQuantization Aware Training (QAT) can usually achieve better performance than Post-Training Quantization (PTQ) in low-bit quantization due to its parameter learnability. However, deploying the network after low-bit quantization to dedicated hardware is still challenging. In particular, convolutional neural networks maintain 8-bit quantization for some of their sensitive layers when quantized at low bit. To enable the underlying hardware circuitry to support two different computational bitwidth at the same time during hardware deployment leads to an increase in cost; In addition, some target detection networks using LeakyRelu, etc. as the activation function can have poor accuracy after quantization due to the asymmetric distribution of activation values. In this paper, we propose a static 4-bit activation value quantization method for the sensitive layer to unify the quantization bits and an Parameterizable LeakyReLU(PLReLU) activation function to adjust the distribution of activation values to make them symmetric, which reduce the hardware cost and improve the accuracy of the quantization network. Our method outperforms current low-bit quantization methods by 1.11% to 1.72% on ImageNet dataset and by 0.9% on PASCAL VOC dataset, demonstrating the contribution of the design in this paper.\n",
            "reference_string": "[275852824 | Zhang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "LiteEdge: Lightweight Semantic Edge Detection Network",
            "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
            "year": 2021,
            "reference_count": 35,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.zora.uzh.ch/id/eprint/217535/1/Wang_LiteEdge_Lightweight_Semantic_Edge_Detection_Network_ICCVW_2021_paper.pdf",
                "status": "GREEN",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCVW54120.2021.00300?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCVW54120.2021.00300, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Hao Wang"
                },
                {
                    "authorId": "118657651",
                    "name": "Hasan Al-Banna Mohamed"
                },
                {
                    "authorId": "15678675",
                    "name": "Zuowen Wang"
                },
                {
                    "authorId": "4765464",
                    "name": "Bodo Rueckauer"
                },
                {
                    "authorId": "1704961",
                    "name": "Shih-Chii Liu"
                }
            ],
            "abstract": "Scene parsing is a critical component for understanding complex scenes in applications such as autonomous driving. Semantic segmentation networks are typically reported for scene parsing but semantic edge networks have also become of interest because of the sparseness of the segmented maps. This work presents an end-to-end trained lightweight deep semantic edge detection architecture called LiteEdge suitable for edge deployment. By utilizing hierarchical supervision and a new weighted multi-label loss function to balance different edge classes during training, LiteEdge predicts with high accuracy category-wise binary edges. Our LiteEdge network with only \u2248 3M parameters, has a semantic edge prediction accuracy of 52.9% mean maximum F (MF) score on the Cityscapes dataset. This accuracy was evaluated on the network trained to produce a low resolution edge map. The network can be quantized to 6-bit weights and 8-bit activations and shows only a 2% drop in the mean MF score. This quantization leads to a memory footprint savings of 6X for an edge device.",
            "corpus_id": 244531228,
            "sentences": [
                {
                    "corpus_id": "244531228",
                    "title": "LiteEdge: Lightweight Semantic Edge Detection Network",
                    "text": "Quantization has become a significant method for optimizing deep-learning models so that they can accelerate inference when deployed on embedded systems with restricted memory footprint and computing resource. In this work, we use the Neural Network Intelligence toolkit 4 . To increase the network accuracy, we use the quantization aware training method (QAT) [19], where we started with the trained model and further refined the model with quantized parameters for 100 epochs.\n\nThe results in Table 3 show that the quantized model with 8-bit weights and activations only has a small drop of  2.2% in the mean MF score. The model size of the quantized network is around 4X smaller than the full precision network. In addition, QAT brings weight sparsity, where 2.65% of the parameters are zero after the quantization. The results for the 6-bit and 4-bit weight quantized models are also presented in Table 3. Compared with the 8-bit weight quantized model, the 6-bit weight quantized model shows a drop of 0.2% in the mean MF score but achieves around 4 times weight sparsity. The 4-bit weight quantized network shows a larger drop in the mean MF score but has more than 38% zero parameters. In addition, its model size is 13X smaller than the full precision model.",
                    "score": 0.7380215592179719,
                    "section_title": "Model compression study",
                    "char_start_offset": 17959,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 361,
                            "end": 365,
                            "matchedPaperCorpusId": "39867659"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9599609375
                }
            ],
            "relevance_judgement": 0.9599609375,
            "relevance_judgment_input_expanded": "# Title: LiteEdge: Lightweight Semantic Edge Detection Network\n# Venue: 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)\n# Authors: Hao Wang, Hasan Al-Banna Mohamed, Zuowen Wang, Bodo Rueckauer, Shih-Chii Liu\n## Abstract\nScene parsing is a critical component for understanding complex scenes in applications such as autonomous driving. Semantic segmentation networks are typically reported for scene parsing but semantic edge networks have also become of interest because of the sparseness of the segmented maps. This work presents an end-to-end trained lightweight deep semantic edge detection architecture called LiteEdge suitable for edge deployment. By utilizing hierarchical supervision and a new weighted multi-label loss function to balance different edge classes during training, LiteEdge predicts with high accuracy category-wise binary edges. Our LiteEdge network with only \u2248 3M parameters, has a semantic edge prediction accuracy of 52.9% mean maximum F (MF) score on the Cityscapes dataset. This accuracy was evaluated on the network trained to produce a low resolution edge map. The network can be quantized to 6-bit weights and 8-bit activations and shows only a 2% drop in the mean MF score. This quantization leads to a memory footprint savings of 6X for an edge device.\n## Model compression study\nQuantization has become a significant method for optimizing deep-learning models so that they can accelerate inference when deployed on embedded systems with restricted memory footprint and computing resource. In this work, we use the Neural Network Intelligence toolkit 4 . To increase the network accuracy, we use the quantization aware training method (QAT) [19], where we started with the trained model and further refined the model with quantized parameters for 100 epochs.\n\nThe results in Table 3 show that the quantized model with 8-bit weights and activations only has a small drop of  2.2% in the mean MF score. The model size of the quantized network is around 4X smaller than the full precision network. In addition, QAT brings weight sparsity, where 2.65% of the parameters are zero after the quantization. The results for the 6-bit and 4-bit weight quantized models are also presented in Table 3. Compared with the 8-bit weight quantized model, the 6-bit weight quantized model shows a drop of 0.2% in the mean MF score but achieves around 4 times weight sparsity. The 4-bit weight quantized network shows a larger drop in the mean MF score but has more than 38% zero parameters. In addition, its model size is 13X smaller than the full precision model.",
            "reference_string": "[244531228 | Wang et al. | 2021 | Citations: 3]"
        },
        {
            "title": "Integer-Only CNNs with 4 Bit Weights and Bit-Shift Quantization Scales at Full-Precision Accuracy",
            "venue": "Electronics",
            "year": 2021,
            "reference_count": 44,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2079-9292/10/22/2823/pdf?version=1637146181",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics10222823?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics10222823, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "10792639",
                    "name": "Maarten Vandersteegen"
                },
                {
                    "authorId": "2141305221",
                    "name": "Kristof Van Beeck"
                },
                {
                    "authorId": "1752649",
                    "name": "T. Goedem\u00e9"
                }
            ],
            "abstract": "Quantization of neural networks has been one of the most popular techniques to compress models for embedded (IoT) hardware platforms with highly constrained latency, storage, memory-bandwidth, and energy specifications. Limiting the number of bits per weight and activation has been the main focus in the literature. To avoid major degradation of accuracy, common quantization methods introduce additional scale factors to adapt the quantized values to the diverse data ranges, present in full-precision (floating-point) neural networks. These scales are usually kept in high precision, requiring the target compute engine to support a few high-precision multiplications, which is not desirable due to the larger hardware cost. Little effort has yet been invested in trying to avoid high-precision multipliers altogether, especially in combination with 4 bit weights. This work proposes a new quantization scheme, based on power-of-two quantization scales, that works on-par compared to uniform per-channel quantization with full-precision 32 bit quantization scales when using only 4 bit weights. This is done through the addition of a low-precision lookup-table that translates stored 4 bit weights into nonuniformly distributed 8 bit weights for internal computation. All our quantized ImageNet CNNs achieved or even exceeded the Top-1 accuracy of their full-precision counterparts, with ResNet18 exceeding its full-precision model by 0.35%. Our MobileNetV2 model achieved state-of-the-art performance with only a slight drop in accuracy of 0.51%.",
            "corpus_id": 244391498,
            "sentences": [
                {
                    "corpus_id": "244391498",
                    "title": "Integer-Only CNNs with 4 Bit Weights and Bit-Shift Quantization Scales at Full-Precision Accuracy",
                    "text": "\u2022 \n\nWe present an extensive literature overview of uniform and nonuniform quantization for fixed-point inference; \u2022 A novel modification to a neural network compute engine is introduced to improve the accuracy of models with 4 bit weights and 8 bit activations, in conjunction with bit-shift-based scaling, through the aid of a lookup-table; \u2022 A quantization-aware training method is proposed to optimize the models that need to run on our proposed compute engine; \u2022 We are the first to make a fair empirical comparison between the performance of (uniform) quantized models with full-precision and power-of-two scales with either per-layer or per-channel quantization using 4 bit weights; \u2022 Our source code has been made publicly available https://gitlab.com/EAVISE/lutmodel-quantization (accessed on 16 November 2021). \n\nThe remainder of this paper is organized as follows: Section 2 presents an extensive literature overview of quantization in greater detail, organized into different topics for convenience. For each topic, we also highlight the choices we made for our own approach. Our proposed method is explained in Section 3; our results are presented in Section 4; conclusions are made in final Section 5.",
                    "score": 0.7173546371365239,
                    "section_title": "Introduction",
                    "char_start_offset": 4330,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 1
                        },
                        {
                            "start": 4,
                            "end": 787
                        },
                        {
                            "start": 788,
                            "end": 819
                        },
                        {
                            "start": 822,
                            "end": 1010
                        },
                        {
                            "start": 1011,
                            "end": 1086
                        },
                        {
                            "start": 1087,
                            "end": 1214
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9599609375
                }
            ],
            "relevance_judgement": 0.9599609375,
            "relevance_judgment_input_expanded": "# Title: Integer-Only CNNs with 4 Bit Weights and Bit-Shift Quantization Scales at Full-Precision Accuracy\n# Venue: Electronics\n# Authors: Maarten Vandersteegen, Kristof Van Beeck, T. Goedem\u00e9\n## Abstract\nQuantization of neural networks has been one of the most popular techniques to compress models for embedded (IoT) hardware platforms with highly constrained latency, storage, memory-bandwidth, and energy specifications. Limiting the number of bits per weight and activation has been the main focus in the literature. To avoid major degradation of accuracy, common quantization methods introduce additional scale factors to adapt the quantized values to the diverse data ranges, present in full-precision (floating-point) neural networks. These scales are usually kept in high precision, requiring the target compute engine to support a few high-precision multiplications, which is not desirable due to the larger hardware cost. Little effort has yet been invested in trying to avoid high-precision multipliers altogether, especially in combination with 4 bit weights. This work proposes a new quantization scheme, based on power-of-two quantization scales, that works on-par compared to uniform per-channel quantization with full-precision 32 bit quantization scales when using only 4 bit weights. This is done through the addition of a low-precision lookup-table that translates stored 4 bit weights into nonuniformly distributed 8 bit weights for internal computation. All our quantized ImageNet CNNs achieved or even exceeded the Top-1 accuracy of their full-precision counterparts, with ResNet18 exceeding its full-precision model by 0.35%. Our MobileNetV2 model achieved state-of-the-art performance with only a slight drop in accuracy of 0.51%.\n## Introduction\n\u2022 \n\nWe present an extensive literature overview of uniform and nonuniform quantization for fixed-point inference; \u2022 A novel modification to a neural network compute engine is introduced to improve the accuracy of models with 4 bit weights and 8 bit activations, in conjunction with bit-shift-based scaling, through the aid of a lookup-table; \u2022 A quantization-aware training method is proposed to optimize the models that need to run on our proposed compute engine; \u2022 We are the first to make a fair empirical comparison between the performance of (uniform) quantized models with full-precision and power-of-two scales with either per-layer or per-channel quantization using 4 bit weights; \u2022 Our source code has been made publicly available https://gitlab.com/EAVISE/lutmodel-quantization (accessed on 16 November 2021). \n\nThe remainder of this paper is organized as follows: Section 2 presents an extensive literature overview of quantization in greater detail, organized into different topics for convenience. For each topic, we also highlight the choices we made for our own approach. Our proposed method is explained in Section 3; our results are presented in Section 4; conclusions are made in final Section 5.",
            "reference_string": "[244391498 | Vandersteegen et al. | 2021 | Citations: 8]"
        },
        {
            "title": "Robust Quantization: One Model to Rule Them All",
            "venue": "Neural Information Processing Systems",
            "year": 2020,
            "reference_count": 32,
            "citation_count": 76,
            "influential_citation_count": 11,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.07686, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2065832429",
                    "name": "Moran Shkolnik"
                },
                {
                    "authorId": "104387774",
                    "name": "Brian Chmiel"
                },
                {
                    "authorId": "2607278",
                    "name": "Ron Banner"
                },
                {
                    "authorId": "19190632",
                    "name": "Gil Shomron"
                },
                {
                    "authorId": "51498210",
                    "name": "Yury Nahshan"
                },
                {
                    "authorId": "49791556",
                    "name": "A. Bronstein"
                },
                {
                    "authorId": "1948330",
                    "name": "U. Weiser"
                }
            ],
            "abstract": "Neural network quantization methods often involve simulating the quantization process during training, making the trained model highly dependent on the target bit-width and precise way quantization is performed. Robust quantization offers an alternative approach with improved tolerance to different classes of data-types and quantization policies. It opens up new exciting applications where the quantization process is not static and can vary to meet different circumstances and implementations. To address this issue, we propose a method that provides intrinsic robustness to the model against a broad range of quantization processes. Our method is motivated by theoretical arguments and enables us to store a single generic model capable of operating at various bit-widths and quantization policies. We validate our method's effectiveness on different ImageNet models.",
            "corpus_id": 211146310,
            "sentences": [
                {
                    "corpus_id": "211146310",
                    "title": "Robust Quantization: One Model to Rule Them All",
                    "text": "Deep neural networks (DNNs) are a prominent choice for many machine learning applications. However, a significant drawback of these models is their computational costs. Low-precision arithmetic is one of the key techniques being actively studied to overcome this difficult. With appropriate hardware support, low-precision training and inference can perform more operations per second, reduce memory bandwidth and power consumption, and allow larger networks to fit into a device. \n\nNaively quantizing a single-precision floating point (FP32) model to 4 bits (INT4) or lower usually incurs a significant accuracy degradation. Many studies have tried to mitigate this accuracy decrease by offering different quantization methods. These methods differ in whether they require training or not. Methods that require training (known as quantization aware training or QAT (Choi et al., 2018;Baskin et al., 2018;Esser et al., 2019;Zhang et al., 2018;Zhou et al., 2016)) simulate the quantization arithmetic on the fly, while methods that avoid training (known as post-training quantization or PQT (Banner et al., 2019;Choukroun et al., 2019;Migacz, 2017;Gong et al., 2018;Finkelstein et al., 2019;Zhao et al., 2019)) minimize the quantization noise added to the model. \n\nUnfortunately, both approaches are not robust to common variations in the assumed quantization noise model. For example, (Krishnamoorthi, 2018) has observed that in order to avoid accuracy degradation at inference time, it is essential to ensure that all quantization-related artifacts are faithfully modeled at training time. Our experiments in this paper further asses this observation. For example, when quantizing ResNet-18 (He et al., 2015) with DoReFa (Zhou et al., 2016) to 4-bit precision, an error of less than 2% in the quantizer step size results in an accuracy drop of 58%. \n\nIn practice, there is almost always some degree of uncertainty related to how quantization is executed.",
                    "score": 0.599844278342718,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 90
                        },
                        {
                            "start": 91,
                            "end": 168
                        },
                        {
                            "start": 169,
                            "end": 273
                        },
                        {
                            "start": 274,
                            "end": 480
                        },
                        {
                            "start": 483,
                            "end": 625
                        },
                        {
                            "start": 626,
                            "end": 728
                        },
                        {
                            "start": 729,
                            "end": 790
                        },
                        {
                            "start": 791,
                            "end": 1261
                        },
                        {
                            "start": 1264,
                            "end": 1371
                        },
                        {
                            "start": 1372,
                            "end": 1590
                        },
                        {
                            "start": 1591,
                            "end": 1652
                        },
                        {
                            "start": 1653,
                            "end": 1849
                        },
                        {
                            "start": 1852,
                            "end": 1955
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 924,
                            "end": 943,
                            "matchedPaperCorpusId": "50784025"
                        },
                        {
                            "start": 1090,
                            "end": 1111,
                            "matchedPaperCorpusId": "59292009"
                        },
                        {
                            "start": 1147,
                            "end": 1165,
                            "matchedPaperCorpusId": "46899409"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9599609375
                }
            ],
            "relevance_judgement": 0.9599609375,
            "relevance_judgment_input_expanded": "# Title: Robust Quantization: One Model to Rule Them All\n# Venue: Neural Information Processing Systems\n# Authors: Moran Shkolnik, Brian Chmiel, Ron Banner, Gil Shomron, Yury Nahshan, A. Bronstein, U. Weiser\n## Abstract\nNeural network quantization methods often involve simulating the quantization process during training, making the trained model highly dependent on the target bit-width and precise way quantization is performed. Robust quantization offers an alternative approach with improved tolerance to different classes of data-types and quantization policies. It opens up new exciting applications where the quantization process is not static and can vary to meet different circumstances and implementations. To address this issue, we propose a method that provides intrinsic robustness to the model against a broad range of quantization processes. Our method is motivated by theoretical arguments and enables us to store a single generic model capable of operating at various bit-widths and quantization policies. We validate our method's effectiveness on different ImageNet models.\n## Introduction\nDeep neural networks (DNNs) are a prominent choice for many machine learning applications. However, a significant drawback of these models is their computational costs. Low-precision arithmetic is one of the key techniques being actively studied to overcome this difficult. With appropriate hardware support, low-precision training and inference can perform more operations per second, reduce memory bandwidth and power consumption, and allow larger networks to fit into a device. \n\nNaively quantizing a single-precision floating point (FP32) model to 4 bits (INT4) or lower usually incurs a significant accuracy degradation. Many studies have tried to mitigate this accuracy decrease by offering different quantization methods. These methods differ in whether they require training or not. Methods that require training (known as quantization aware training or QAT (Choi et al., 2018;Baskin et al., 2018;Esser et al., 2019;Zhang et al., 2018;Zhou et al., 2016)) simulate the quantization arithmetic on the fly, while methods that avoid training (known as post-training quantization or PQT (Banner et al., 2019;Choukroun et al., 2019;Migacz, 2017;Gong et al., 2018;Finkelstein et al., 2019;Zhao et al., 2019)) minimize the quantization noise added to the model. \n\nUnfortunately, both approaches are not robust to common variations in the assumed quantization noise model. For example, (Krishnamoorthi, 2018) has observed that in order to avoid accuracy degradation at inference time, it is essential to ensure that all quantization-related artifacts are faithfully modeled at training time. Our experiments in this paper further asses this observation. For example, when quantizing ResNet-18 (He et al., 2015) with DoReFa (Zhou et al., 2016) to 4-bit precision, an error of less than 2% in the quantizer step size results in an accuracy drop of 58%. \n\nIn practice, there is almost always some degree of uncertainty related to how quantization is executed.",
            "reference_string": "[211146310 | Shkolnik et al. | 2020 | Citations: 76]"
        },
        {
            "title": "Overcoming Oscillations in Quantization-Aware Training",
            "venue": "International Conference on Machine Learning",
            "year": 2022,
            "reference_count": 39,
            "citation_count": 108,
            "influential_citation_count": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2203.11086",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.11086, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "41229153",
                    "name": "Markus Nagel"
                },
                {
                    "authorId": "2090358941",
                    "name": "Marios Fournarakis"
                },
                {
                    "authorId": "2112207572",
                    "name": "Yelysei Bondarenko"
                },
                {
                    "authorId": "83133279",
                    "name": "Tijmen Blankevoort"
                }
            ],
            "abstract": "When training neural networks with simulated quantization, we observe that quantized weights can, rather unexpectedly, oscillate between two grid-points. The importance of this effect and its impact on quantization-aware training (QAT) are not well-understood or investigated in literature. In this paper, we delve deeper into the phenomenon of weight oscillations and show that it can lead to a significant accuracy degradation due to wrongly estimated batch-normalization statistics during inference and increased noise during training. These effects are particularly pronounced in low-bit ($\\leq$ 4-bits) quantization of efficient networks with depth-wise separable layers, such as MobileNets and EfficientNets. In our analysis we investigate several previously proposed QAT algorithms and show that most of these are unable to overcome oscillations. Finally, we propose two novel QAT algorithms to overcome oscillations during training: oscillation dampening and iterative weight freezing. We demonstrate that our algorithms achieve state-of-the-art accuracy for low-bit (3&4 bits) weight and activation quantization of efficient architectures, such as MobileNetV2, MobileNetV3, and EfficentNet-lite on ImageNet. Our source code is available at {https://github.com/qualcomm-ai-research/oscillations-qat}.",
            "corpus_id": 247595112,
            "sentences": [
                {
                    "corpus_id": "247595112",
                    "title": "Overcoming Oscillations in Quantization-Aware Training",
                    "text": "Quantization is one of the most successful methods for optimizing neural networks for efficient inference and ondevice execution while maintaining a high accuracy. By compressing the weights and activations from the regular 32-bit floating-point format to more efficient low bit fixedpoint representations, such as INT8, we can reduce power consumption and accelerate inference when deploying neural networks on edge devices (Horowitz, 2014). \n\nDespite its clear power and latency benefits, quantization comes at the cost of added noise due to the reduced precision. However, researchers in recent years have shown that neural networks are robust to this noise and can be quantized to 8-bits with a minimal drop in accuracy using post-training quantization techniques (PTQ) (Nagel et al., 2019;Banner et al., 2019;Nahshan et al., 2020). PTQ can be very efficient and, generally, only requires access to a small calibration dataset, but suffers when applied to low-bit quantization (\u2264 4-bits) of neural networks. Meanwhile, quantization-aware training (QAT) has become the de-facto standard method for achieving low-bit quantization while maintaining near full-precision accuracy (Krishnamoorthi, 2018;Esser et al., 2020;Nagel et al., 2021). By simulating the quantization operation during training or fine-tuning, the network can adapt to the quantization noise and reach better solutions than with PTQ. \n\nIn this paper, we focus on the oscillations of quantized weights that occur during quantization-aware training. This is a little-known and under-investigated phenomenon in the optimization of quantized neural networks, with significant consequences for the network during and after training. When using the popular straight-through estimator (STE) (Bengio et al., 2013) for QAT, weights seemingly randomly oscillate between adjacent quantization levels leading to detrimental noise during the optimization process. Equipped with this insight, we investigate recent advances in QAT that claim improved performance and assess their effectiveness in addressing this oscillatory behavior.",
                    "score": 0.6162486150048258,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 442
                        },
                        {
                            "start": 445,
                            "end": 566
                        },
                        {
                            "start": 567,
                            "end": 836
                        },
                        {
                            "start": 837,
                            "end": 1011
                        },
                        {
                            "start": 1012,
                            "end": 1240
                        },
                        {
                            "start": 1241,
                            "end": 1403
                        },
                        {
                            "start": 1406,
                            "end": 1517
                        },
                        {
                            "start": 1518,
                            "end": 1697
                        },
                        {
                            "start": 1698,
                            "end": 1920
                        },
                        {
                            "start": 1921,
                            "end": 2090
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 425,
                            "end": 441,
                            "matchedPaperCorpusId": "22028726"
                        },
                        {
                            "start": 774,
                            "end": 794,
                            "matchedPaperCorpusId": "184487878"
                        },
                        {
                            "start": 794,
                            "end": 814,
                            "matchedPaperCorpusId": "59292009"
                        },
                        {
                            "start": 1201,
                            "end": 1220,
                            "matchedPaperCorpusId": "67788003"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9580078125
                }
            ],
            "relevance_judgement": 0.9580078125,
            "relevance_judgment_input_expanded": "# Title: Overcoming Oscillations in Quantization-Aware Training\n# Venue: International Conference on Machine Learning\n# Authors: Markus Nagel, Marios Fournarakis, Yelysei Bondarenko, Tijmen Blankevoort\n## Abstract\nWhen training neural networks with simulated quantization, we observe that quantized weights can, rather unexpectedly, oscillate between two grid-points. The importance of this effect and its impact on quantization-aware training (QAT) are not well-understood or investigated in literature. In this paper, we delve deeper into the phenomenon of weight oscillations and show that it can lead to a significant accuracy degradation due to wrongly estimated batch-normalization statistics during inference and increased noise during training. These effects are particularly pronounced in low-bit ($\\leq$ 4-bits) quantization of efficient networks with depth-wise separable layers, such as MobileNets and EfficientNets. In our analysis we investigate several previously proposed QAT algorithms and show that most of these are unable to overcome oscillations. Finally, we propose two novel QAT algorithms to overcome oscillations during training: oscillation dampening and iterative weight freezing. We demonstrate that our algorithms achieve state-of-the-art accuracy for low-bit (3&4 bits) weight and activation quantization of efficient architectures, such as MobileNetV2, MobileNetV3, and EfficentNet-lite on ImageNet. Our source code is available at {https://github.com/qualcomm-ai-research/oscillations-qat}.\n## Introduction\nQuantization is one of the most successful methods for optimizing neural networks for efficient inference and ondevice execution while maintaining a high accuracy. By compressing the weights and activations from the regular 32-bit floating-point format to more efficient low bit fixedpoint representations, such as INT8, we can reduce power consumption and accelerate inference when deploying neural networks on edge devices (Horowitz, 2014). \n\nDespite its clear power and latency benefits, quantization comes at the cost of added noise due to the reduced precision. However, researchers in recent years have shown that neural networks are robust to this noise and can be quantized to 8-bits with a minimal drop in accuracy using post-training quantization techniques (PTQ) (Nagel et al., 2019;Banner et al., 2019;Nahshan et al., 2020). PTQ can be very efficient and, generally, only requires access to a small calibration dataset, but suffers when applied to low-bit quantization (\u2264 4-bits) of neural networks. Meanwhile, quantization-aware training (QAT) has become the de-facto standard method for achieving low-bit quantization while maintaining near full-precision accuracy (Krishnamoorthi, 2018;Esser et al., 2020;Nagel et al., 2021). By simulating the quantization operation during training or fine-tuning, the network can adapt to the quantization noise and reach better solutions than with PTQ. \n\nIn this paper, we focus on the oscillations of quantized weights that occur during quantization-aware training. This is a little-known and under-investigated phenomenon in the optimization of quantized neural networks, with significant consequences for the network during and after training. When using the popular straight-through estimator (STE) (Bengio et al., 2013) for QAT, weights seemingly randomly oscillate between adjacent quantization levels leading to detrimental noise during the optimization process. Equipped with this insight, we investigate recent advances in QAT that claim improved performance and assess their effectiveness in addressing this oscillatory behavior.",
            "reference_string": "[247595112 | Nagel et al. | 2022 | Citations: 108]"
        },
        {
            "title": "MOHAQ: Multi-Objective Hardware-Aware Quantization of recurrent neural networks",
            "venue": "Journal of systems architecture",
            "year": 2021,
            "reference_count": 55,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://umu.diva-portal.org/smash/get/diva2:1716556/FULLTEXT01",
                "status": "GREEN",
                "license": "other-oa",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.01192, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1911208",
                    "name": "Nesma M. Rezk"
                },
                {
                    "authorId": "2541510",
                    "name": "T. Nordstr\u00f6m"
                },
                {
                    "authorId": "145964559",
                    "name": "D. Stathis"
                },
                {
                    "authorId": "1403630586",
                    "name": "Z. Ul-Abdin"
                },
                {
                    "authorId": "34876449",
                    "name": "E. Aksoy"
                },
                {
                    "authorId": "9113752",
                    "name": "A. Hemani"
                }
            ],
            "abstract": "The compression of deep learning models is of fundamental importance in deploying such models to edge devices. The selection of compression parameters can be automated to meet changes in the hardware platform and application using optimization algorithms. This article introduces a Multi-Objective Hardware-Aware Quantization (MOHAQ) method, which considers hardware efficiency and inference error as objectives for mixed-precision quantization. The proposed method feasibly evaluates candidate solutions in a large search space by relying on two steps. First, post-training quantization is applied for fast solution evaluation (inference-only search). Second, we propose the\"beacon-based search\"to retrain selected solutions only and use them as beacons to know the effect of retraining on other solutions. We use a speech recognition model based on Simple Recurrent Unit (SRU) using the TIMIT dataset and apply our method to run on SiLago and Bitfusion platforms. We provide experimental evaluations showing that SRU can be compressed up to 8x by post-training quantization without any significant error increase. On SiLago, we found solutions that achieve 97\\% and 86\\% of the maximum possible speedup and energy saving, with a minor increase in error. On Bitfusion, beacon-based search reduced the error gain of inference-only search by up to 4.9 percentage points.",
            "corpus_id": 246077316,
            "sentences": [
                {
                    "corpus_id": "246077316",
                    "title": "MOHAQ: Multi-Objective Hardware-Aware Quantization of recurrent neural networks",
                    "text": "model as in S18.\n\nIn most of the solutions, 4 bits and 2 bits have been used extensively for the weights. The activation precision has been kept between 8-bit and 16-bit in some layers for most of the solutions. However, 4-bit and 2-bit activations have been used in few layers. It is also observed that some solutions have an error rate better than the baseline model. It has been shown that quantization has a regularization effect during training [4]. Therefore, we think the improved error is a result of the quantization error introducing a noise that reduces some of the over-fitting effect during inference.\n\nIn Table 3, we expected the W ER T to be higher than W ER V but also we hoped to see that the relative order is kept between the solutions for both W ER T and W ER V . However, if we look at the solutions sorted by the W ER V value, we find that the corresponding W ER T values are not perfectly sorted. S6 and S12 look as outliers. The reason is that S6 and S12 had better W ER T than expected. Still, for both cases, the variation was in the range of 0.2 p.p. and we believe that such small variations are expected as there is no guarantee for two different datasets' errors to be the same. To get a better understanding of how successful our posttraining quantization applied to the SRU-model is, we look at the previous work done on post-training quantization. Since researchers have found that 16-bit and 8-bit quantization do not significantly affect accuracy [10], we will focus on 4-bit quantization (8x compression). CNN ImageNet models have been used for quantization experiments in most of the work we have seen. The accuracy drop due to 4-bit post-training quantization on CNN models has varied in these papers as follows: LAPQ [6] (6.1 to 9.4 p.p.), ACIQ [11] (0.4 to 10.8 p.p.), OCS [10] (more than 5 p.p.), and ZeroQ [38] (1.6 p.p.). Where ZeroQ applied",
                    "score": 0.7653565510016771,
                    "section_title": "Multi-objective search to minimize two objectives: WER and memory size",
                    "char_start_offset": 44283,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 450,
                            "end": 453,
                            "matchedPaperCorpusId": "49669175"
                        },
                        {
                            "start": 1482,
                            "end": 1486,
                            "matchedPaperCorpusId": "59413897"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95751953125
                }
            ],
            "relevance_judgement": 0.95751953125,
            "relevance_judgment_input_expanded": "# Title: MOHAQ: Multi-Objective Hardware-Aware Quantization of recurrent neural networks\n# Venue: Journal of systems architecture\n# Authors: Nesma M. Rezk, T. Nordstr\u00f6m, D. Stathis, Z. Ul-Abdin, E. Aksoy, A. Hemani\n## Abstract\nThe compression of deep learning models is of fundamental importance in deploying such models to edge devices. The selection of compression parameters can be automated to meet changes in the hardware platform and application using optimization algorithms. This article introduces a Multi-Objective Hardware-Aware Quantization (MOHAQ) method, which considers hardware efficiency and inference error as objectives for mixed-precision quantization. The proposed method feasibly evaluates candidate solutions in a large search space by relying on two steps. First, post-training quantization is applied for fast solution evaluation (inference-only search). Second, we propose the\"beacon-based search\"to retrain selected solutions only and use them as beacons to know the effect of retraining on other solutions. We use a speech recognition model based on Simple Recurrent Unit (SRU) using the TIMIT dataset and apply our method to run on SiLago and Bitfusion platforms. We provide experimental evaluations showing that SRU can be compressed up to 8x by post-training quantization without any significant error increase. On SiLago, we found solutions that achieve 97\\% and 86\\% of the maximum possible speedup and energy saving, with a minor increase in error. On Bitfusion, beacon-based search reduced the error gain of inference-only search by up to 4.9 percentage points.\n## Multi-objective search to minimize two objectives: WER and memory size\nmodel as in S18.\n\nIn most of the solutions, 4 bits and 2 bits have been used extensively for the weights. The activation precision has been kept between 8-bit and 16-bit in some layers for most of the solutions. However, 4-bit and 2-bit activations have been used in few layers. It is also observed that some solutions have an error rate better than the baseline model. It has been shown that quantization has a regularization effect during training [4]. Therefore, we think the improved error is a result of the quantization error introducing a noise that reduces some of the over-fitting effect during inference.\n\nIn Table 3, we expected the W ER T to be higher than W ER V but also we hoped to see that the relative order is kept between the solutions for both W ER T and W ER V . However, if we look at the solutions sorted by the W ER V value, we find that the corresponding W ER T values are not perfectly sorted. S6 and S12 look as outliers. The reason is that S6 and S12 had better W ER T than expected. Still, for both cases, the variation was in the range of 0.2 p.p. and we believe that such small variations are expected as there is no guarantee for two different datasets' errors to be the same. To get a better understanding of how successful our posttraining quantization applied to the SRU-model is, we look at the previous work done on post-training quantization. Since researchers have found that 16-bit and 8-bit quantization do not significantly affect accuracy [10], we will focus on 4-bit quantization (8x compression). CNN ImageNet models have been used for quantization experiments in most of the work we have seen. The accuracy drop due to 4-bit post-training quantization on CNN models has varied in these papers as follows: LAPQ [6] (6.1 to 9.4 p.p.), ACIQ [11] (0.4 to 10.8 p.p.), OCS [10] (more than 5 p.p.), and ZeroQ [38] (1.6 p.p.). Where ZeroQ applied",
            "reference_string": "[246077316 | Rezk et al. | 2021 | Citations: 1]"
        },
        {
            "title": "Real-Time Spacecraft Pose Estimation Using Mixed-Precision Quantized Neural Network on COTS Reconfigurable MPSoC",
            "venue": "IEEE International New Circuits and Systems Conference",
            "year": 2024,
            "reference_count": 29,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.06170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2164124197",
                    "name": "Julien Posso"
                },
                {
                    "authorId": "2259651671",
                    "name": "Guy Bois"
                },
                {
                    "authorId": "2246316919",
                    "name": "Yvon Savaria"
                }
            ],
            "abstract": "This article presents a pioneering approach to real-time spacecraft pose estimation, utilizing a mixed-precision quantized neural network implemented on the FPGA components of a commercially available Xilinx MPSoC, renowned for its suitability in space applications. Our co-design methodology includes a novel evaluation technique for assessing the layer-wise neural network sensitivity to quantization, facilitating an optimal balance between accuracy, latency, and FPGA resource utilization. Utilizing the FINN library, we developed a bespoke FPGA dataflow accelerator that integrates on-chip weights and activation functions to minimize latency and energy consumption. Our implementation is 7.7 times faster and 19.5 times more energy-efficient than the best-reported values in the existing spacecraft pose estimation literature. Furthermore, our contribution includes the first real-time, open-source implementation of such algorithms, marking a significant advancement in making efficient spacecraft pose estimation algorithms widely accessible. The source code is available at https://github.com/possoj/FPGA-SpacePose.",
            "corpus_id": 271050128,
            "sentences": [
                {
                    "corpus_id": "271050128",
                    "title": "Real-Time Spacecraft Pose Estimation Using Mixed-Precision Quantized Neural Network on COTS Reconfigurable MPSoC",
                    "text": "This results in the initial depthwise convolution having merely 288 parameters, as highlighted in Figure 3.This finding suggests that architectural modifications in recent designs [30], influenced by vision transformers that position depthwise convolutions before expansion layers, might be sensitive to low-bit quantization.Further experiments have revealed that activations are notably more sensitive to low-bit quantization, with quantization errors below 4 bits frequently causing orientation decoding errors and disrupting the neural network's training process.This significant constraint influences the layer-wise bit-width activation choices in the next paragraph.Our findings also indicate that the binarization of early layers diminishes the generalization capabilities of the neural network.However, it improves in later layers, suggesting that low-bit quantization compromises information in the initial layers while acting as regularization in the last layers.Additionally, the sensitivity of early layers to quantization predominantly affects orientation estimation, as opposed to position estimation, which is comparatively simpler [10].Therefore, quantization below 8 bits has a more pronounced impact on orientation accuracy than on position accuracy.\n\nThis analysis leads us to identify an optimal balance among layer-wise bit-width, FPGA resource utilization, and throughput.Given the limitations of FINN, we quantize only the neural network's backbone for FPGA deployment, leaving the heads in Float32.We observed that activations are highly sensitive to low-bit quantization, yet the hardware resources required increase exponentially with the activation bit-width due to the current FINN C++ HLS backend [25].Consequently, we quantize all activations to four bits to conserve FPGA resources, as higher bit-widths would disproportionately consume resources [27].Weight bit-width selection varies by layer index due to differing quantization sensitivities; the weights of the first layer are quantized at four bits, reflecting their higher sensitivity.The most sensitive layer, the first depthwise convolution, uses six-bit weights.Subsequently, the first projection convolution weights are set to four bits, and the weights of all remaining 49 convolutional layers are set to three bits.",
                    "score": 0.6386364395345332,
                    "section_title": "III. CO-DESIGN METHODOLOGY A. Methodology Overview",
                    "char_start_offset": 8839,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 107
                        },
                        {
                            "start": 107,
                            "end": 325
                        },
                        {
                            "start": 325,
                            "end": 566
                        },
                        {
                            "start": 566,
                            "end": 671
                        },
                        {
                            "start": 671,
                            "end": 801
                        },
                        {
                            "start": 801,
                            "end": 972
                        },
                        {
                            "start": 972,
                            "end": 1151
                        },
                        {
                            "start": 1151,
                            "end": 1267
                        },
                        {
                            "start": 1269,
                            "end": 1393
                        },
                        {
                            "start": 1393,
                            "end": 1521
                        },
                        {
                            "start": 1521,
                            "end": 1730
                        },
                        {
                            "start": 1730,
                            "end": 1882
                        },
                        {
                            "start": 1882,
                            "end": 2071
                        },
                        {
                            "start": 2071,
                            "end": 2151
                        },
                        {
                            "start": 2151,
                            "end": 2307
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1146,
                            "end": 1150,
                            "matchedPaperCorpusId": "248512548"
                        },
                        {
                            "start": 1877,
                            "end": 1881,
                            "matchedPaperCorpusId": "219886219"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95703125
                }
            ],
            "relevance_judgement": 0.95703125,
            "relevance_judgment_input_expanded": "# Title: Real-Time Spacecraft Pose Estimation Using Mixed-Precision Quantized Neural Network on COTS Reconfigurable MPSoC\n# Venue: IEEE International New Circuits and Systems Conference\n# Authors: Julien Posso, Guy Bois, Yvon Savaria\n## Abstract\nThis article presents a pioneering approach to real-time spacecraft pose estimation, utilizing a mixed-precision quantized neural network implemented on the FPGA components of a commercially available Xilinx MPSoC, renowned for its suitability in space applications. Our co-design methodology includes a novel evaluation technique for assessing the layer-wise neural network sensitivity to quantization, facilitating an optimal balance between accuracy, latency, and FPGA resource utilization. Utilizing the FINN library, we developed a bespoke FPGA dataflow accelerator that integrates on-chip weights and activation functions to minimize latency and energy consumption. Our implementation is 7.7 times faster and 19.5 times more energy-efficient than the best-reported values in the existing spacecraft pose estimation literature. Furthermore, our contribution includes the first real-time, open-source implementation of such algorithms, marking a significant advancement in making efficient spacecraft pose estimation algorithms widely accessible. The source code is available at https://github.com/possoj/FPGA-SpacePose.\n## III. CO-DESIGN METHODOLOGY A. Methodology Overview\nThis results in the initial depthwise convolution having merely 288 parameters, as highlighted in Figure 3.This finding suggests that architectural modifications in recent designs [30], influenced by vision transformers that position depthwise convolutions before expansion layers, might be sensitive to low-bit quantization.Further experiments have revealed that activations are notably more sensitive to low-bit quantization, with quantization errors below 4 bits frequently causing orientation decoding errors and disrupting the neural network's training process.This significant constraint influences the layer-wise bit-width activation choices in the next paragraph.Our findings also indicate that the binarization of early layers diminishes the generalization capabilities of the neural network.However, it improves in later layers, suggesting that low-bit quantization compromises information in the initial layers while acting as regularization in the last layers.Additionally, the sensitivity of early layers to quantization predominantly affects orientation estimation, as opposed to position estimation, which is comparatively simpler [10].Therefore, quantization below 8 bits has a more pronounced impact on orientation accuracy than on position accuracy.\n\nThis analysis leads us to identify an optimal balance among layer-wise bit-width, FPGA resource utilization, and throughput.Given the limitations of FINN, we quantize only the neural network's backbone for FPGA deployment, leaving the heads in Float32.We observed that activations are highly sensitive to low-bit quantization, yet the hardware resources required increase exponentially with the activation bit-width due to the current FINN C++ HLS backend [25].Consequently, we quantize all activations to four bits to conserve FPGA resources, as higher bit-widths would disproportionately consume resources [27].Weight bit-width selection varies by layer index due to differing quantization sensitivities; the weights of the first layer are quantized at four bits, reflecting their higher sensitivity.The most sensitive layer, the first depthwise convolution, uses six-bit weights.Subsequently, the first projection convolution weights are set to four bits, and the weights of all remaining 49 convolutional layers are set to three bits.",
            "reference_string": "[271050128 | Posso et al. | 2024 | Citations: 0]"
        },
        {
            "title": "A compute-in-memory chip based on resistive random-access memory",
            "venue": "Nature",
            "year": 2022,
            "reference_count": 62,
            "citation_count": 503,
            "influential_citation_count": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41586-022-04992-8.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9385482, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3408296",
                    "name": "W. Wan"
                },
                {
                    "authorId": "3459360",
                    "name": "R. Kubendran"
                },
                {
                    "authorId": "1592769267",
                    "name": "Clemens J. S. Schaefer"
                },
                {
                    "authorId": "32842496",
                    "name": "S. Eryilmaz"
                },
                {
                    "authorId": "1780073248",
                    "name": "Wenqiang Zhang"
                },
                {
                    "authorId": "1637988624",
                    "name": "Dabin Wu"
                },
                {
                    "authorId": "47429806",
                    "name": "S. Deiss"
                },
                {
                    "authorId": "50501653",
                    "name": "Priyanka Raina"
                },
                {
                    "authorId": "72601203",
                    "name": "H. Qian"
                },
                {
                    "authorId": "152632389",
                    "name": "B. Gao"
                },
                {
                    "authorId": "145940386",
                    "name": "Siddharth Joshi"
                },
                {
                    "authorId": "13135960",
                    "name": "Huaqiang Wu"
                },
                {
                    "authorId": "144175897",
                    "name": "H. P. Wong"
                },
                {
                    "authorId": "2702388",
                    "name": "G. Cauwenberghs"
                }
            ],
            "abstract": "Realizing increasingly complex artificial intelligence (AI) functionalities directly on edge devices calls for unprecedented energy efficiency of edge hardware. Compute-in-memory (CIM) based on resistive random-access memory (RRAM)1 promises to meet such demand by storing AI model weights in dense, analogue and non-volatile RRAM devices, and by performing AI computation directly within RRAM, thus eliminating power-hungry data movement between separate compute and memory2\u20135. Although recent studies have demonstrated in-memory matrix-vector multiplication on fully integrated RRAM-CIM hardware6\u201317, it remains a goal for a RRAM-CIM chip to simultaneously deliver high energy efficiency, versatility to support diverse models and software-comparable accuracy. Although efficiency, versatility and accuracy are all indispensable for broad adoption of the technology, the inter-related trade-offs among them cannot be addressed by isolated improvements on any single abstraction level of the design. Here, by co-optimizing across all hierarchies of the design from algorithms and architecture to circuits and devices, we present NeuRRAM\u2014a RRAM-based CIM chip that simultaneously delivers versatility in reconfiguring CIM cores for diverse model architectures, energy efficiency that is two-times better than previous state-of-the-art RRAM-CIM chips across various computational bit-precisions, and inference accuracy comparable to software models quantized to four-bit weights across various AI tasks, including accuracy of 99.0\u2009percent on MNIST18 and 85.7\u2009percent on CIFAR-1019 image classification, 84.7-percent accuracy on Google speech command recognition20, and a 70-percent reduction in image-reconstruction error on a Bayesian image-recovery task. A compute-in-memory neural-network inference accelerator based on resistive random-access memory simultaneously improves energy efficiency, flexibility and accuracy compared with existing hardware by co-optimizing across all hierarchies of the design.",
            "corpus_id": 251644990,
            "sentences": [
                {
                    "corpus_id": "251644990",
                    "title": "A compute-in-memory chip based on resistive random-access memory",
                    "text": "In general, models trained with higher noises converge faster during inference. The model trained with 20% noise reaches the lowest error at the end of 100 Gibbs sampling steps. \n\nExtended Data Fig. 7e shows the effect of noise injection on weight distribution. Without noise injection, the weights have a Gaussian distribution. The neural-network outputs heavily depend on a small fraction of large weights, and thus become vulnerable to noise injection. With noise injection, the weights distribute more uniformly, making the model more noise resilient. \n\nTo efficiently implement the models on NeuRRAM, inputs to all convolutional and fully connected layers are quantized to 4-bit or below. The input bit-precisions of all the models are summarized in Table 1. We perform the quantized training using the parameterized clipping activation technique 46 . The accuracies of some of our quantized models are lower than that of the state-of-the-art quantized model because we apply <4-bit quantization to the most sensitive input and output layers of the neural networks, which have been reported to cause large accuracy degradation and are thus often excluded from low-precision quantization 46,54 . To obtain better accuracy for quantized models, one can use higher precision for sensitive input and output layers, apply more advanced quantization techniques, and use more optimized data preprocessing, data augmentation and regularization techniques during training. However, the focus of this work is to achieve comparable inference accuracy on hardware and on software while keeping all these variables the same, rather than to obtain state-of-the-art inference accuracy on all the tasks. The aforementioned quantization and training techniques will be equally beneficial for both our software baselines and hardware measurements.",
                    "score": 0.6344116592785207,
                    "section_title": "Noise-resilient neural-network training",
                    "char_start_offset": 53016,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 79
                        },
                        {
                            "start": 80,
                            "end": 177
                        },
                        {
                            "start": 180,
                            "end": 261
                        },
                        {
                            "start": 262,
                            "end": 328
                        },
                        {
                            "start": 329,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 555
                        },
                        {
                            "start": 558,
                            "end": 693
                        },
                        {
                            "start": 694,
                            "end": 763
                        },
                        {
                            "start": 764,
                            "end": 856
                        },
                        {
                            "start": 857,
                            "end": 1199
                        },
                        {
                            "start": 1200,
                            "end": 1468
                        },
                        {
                            "start": 1469,
                            "end": 1692
                        },
                        {
                            "start": 1693,
                            "end": 1834
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1195,
                            "end": 1197,
                            "matchedPaperCorpusId": "67788003"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9560546875
                }
            ],
            "relevance_judgement": 0.9560546875,
            "relevance_judgment_input_expanded": "# Title: A compute-in-memory chip based on resistive random-access memory\n# Venue: Nature\n# Authors: W. Wan, R. Kubendran, Clemens J. S. Schaefer, S. Eryilmaz, Wenqiang Zhang, Dabin Wu, S. Deiss, Priyanka Raina, H. Qian, B. Gao, Siddharth Joshi, Huaqiang Wu, H. P. Wong, G. Cauwenberghs\n## Abstract\nRealizing increasingly complex artificial intelligence (AI) functionalities directly on edge devices calls for unprecedented energy efficiency of edge hardware. Compute-in-memory (CIM) based on resistive random-access memory (RRAM)1 promises to meet such demand by storing AI model weights in dense, analogue and non-volatile RRAM devices, and by performing AI computation directly within RRAM, thus eliminating power-hungry data movement between separate compute and memory2\u20135. Although recent studies have demonstrated in-memory matrix-vector multiplication on fully integrated RRAM-CIM hardware6\u201317, it remains a goal for a RRAM-CIM chip to simultaneously deliver high energy efficiency, versatility to support diverse models and software-comparable accuracy. Although efficiency, versatility and accuracy are all indispensable for broad adoption of the technology, the inter-related trade-offs among them cannot be addressed by isolated improvements on any single abstraction level of the design. Here, by co-optimizing across all hierarchies of the design from algorithms and architecture to circuits and devices, we present NeuRRAM\u2014a RRAM-based CIM chip that simultaneously delivers versatility in reconfiguring CIM cores for diverse model architectures, energy efficiency that is two-times better than previous state-of-the-art RRAM-CIM chips across various computational bit-precisions, and inference accuracy comparable to software models quantized to four-bit weights across various AI tasks, including accuracy of 99.0\u2009percent on MNIST18 and 85.7\u2009percent on CIFAR-1019 image classification, 84.7-percent accuracy on Google speech command recognition20, and a 70-percent reduction in image-reconstruction error on a Bayesian image-recovery task. A compute-in-memory neural-network inference accelerator based on resistive random-access memory simultaneously improves energy efficiency, flexibility and accuracy compared with existing hardware by co-optimizing across all hierarchies of the design.\n## Noise-resilient neural-network training\nIn general, models trained with higher noises converge faster during inference. The model trained with 20% noise reaches the lowest error at the end of 100 Gibbs sampling steps. \n\nExtended Data Fig. 7e shows the effect of noise injection on weight distribution. Without noise injection, the weights have a Gaussian distribution. The neural-network outputs heavily depend on a small fraction of large weights, and thus become vulnerable to noise injection. With noise injection, the weights distribute more uniformly, making the model more noise resilient. \n\nTo efficiently implement the models on NeuRRAM, inputs to all convolutional and fully connected layers are quantized to 4-bit or below. The input bit-precisions of all the models are summarized in Table 1. We perform the quantized training using the parameterized clipping activation technique 46 . The accuracies of some of our quantized models are lower than that of the state-of-the-art quantized model because we apply <4-bit quantization to the most sensitive input and output layers of the neural networks, which have been reported to cause large accuracy degradation and are thus often excluded from low-precision quantization 46,54 . To obtain better accuracy for quantized models, one can use higher precision for sensitive input and output layers, apply more advanced quantization techniques, and use more optimized data preprocessing, data augmentation and regularization techniques during training. However, the focus of this work is to achieve comparable inference accuracy on hardware and on software while keeping all these variables the same, rather than to obtain state-of-the-art inference accuracy on all the tasks. The aforementioned quantization and training techniques will be equally beneficial for both our software baselines and hardware measurements.",
            "reference_string": "[251644990 | Wan et al. | 2022 | Citations: 503]"
        },
        {
            "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 73,
            "citation_count": 2606,
            "influential_citation_count": 322,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.14314",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3239480",
                    "name": "Tim Dettmers"
                },
                {
                    "authorId": "51152502",
                    "name": "Artidoro Pagnoni"
                },
                {
                    "authorId": "14487640",
                    "name": "Ari Holtzman"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                }
            ],
            "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.",
            "corpus_id": 258841328,
            "sentences": [
                {
                    "corpus_id": "258841328",
                    "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
                    "text": "While the 4-bit NormalFloat (NF4) data type is informationtheoretically optimal, it still needs to be determined if this property translates to empirical advantages. We follow the setup from Dettmers and Zettlemoyer [13] where quantized LLMs (OPT [72], BLOOM [52], Pythia [7], LLaMA) of different sizes (125M to 65B) with different data types are evaluated on language modeling and a set of zero-shot tasks. In Figure 3 and Table 2 we see that NF4 improves performance significantly over FP4 and Int4 and that double quantization reduces the memory footprint without degrading performance. \n\nk-bit QLORA matches 16-bit full finetuning and 16-bit LoRA performance Recent findings have established that 4-bit quantization for inference is possible, but leads to performance degradation relative to 16-bit [13,18]. This raises the crucial question of whether the lost performance can be recovered by conducting 4-bit adapter finetuning. We test this for two setups. The first focuses on a comparison with full 16-bit finetuning of RoBERTA and T5 models sized 125M to 3B parameters on GLUE and the Super-NaturalInstructions dataset. Results are shown in Table 3. In both datasets, we observe that 16-bit, 8-bit, and 4-bit adapter methods replicate the performance of the fully finetuned 16-bit baseline. This suggests that the performance lost due to the imprecise quantization can be fully recovered through adapter finetuning after quantization. \n\nFor our second setup, since full finetuning models at and beyond 11B parameters requires more than one server of high memory GPUs, we continue to test whether 4-bit QLORA can match 16-bit LoRA at the 7B to 65B parameter scales. To this end, we finetune LLaMA 7B through 65B on two instruction following datasets, Alpaca and FLAN v2, and evaluate on the MMLU benchmark via 5-shot accuracy. Results are shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit LoRA MMLU performance.",
                    "score": 0.6496070968287732,
                    "section_title": "4-bit NormalFloat yields better performance than 4-bit Floating Point",
                    "char_start_offset": 18538,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 165
                        },
                        {
                            "start": 166,
                            "end": 407
                        },
                        {
                            "start": 408,
                            "end": 589
                        },
                        {
                            "start": 592,
                            "end": 811
                        },
                        {
                            "start": 812,
                            "end": 933
                        },
                        {
                            "start": 934,
                            "end": 962
                        },
                        {
                            "start": 963,
                            "end": 1128
                        },
                        {
                            "start": 1129,
                            "end": 1158
                        },
                        {
                            "start": 1159,
                            "end": 1299
                        },
                        {
                            "start": 1300,
                            "end": 1443
                        },
                        {
                            "start": 1446,
                            "end": 1673
                        },
                        {
                            "start": 1674,
                            "end": 1834
                        },
                        {
                            "start": 1835,
                            "end": 1959
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.955078125
                }
            ],
            "relevance_judgement": 0.955078125,
            "relevance_judgment_input_expanded": "# Title: QLoRA: Efficient Finetuning of Quantized LLMs\n# Venue: Neural Information Processing Systems\n# Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n## Abstract\nWe present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.\n## 4-bit NormalFloat yields better performance than 4-bit Floating Point\nWhile the 4-bit NormalFloat (NF4) data type is informationtheoretically optimal, it still needs to be determined if this property translates to empirical advantages. We follow the setup from Dettmers and Zettlemoyer [13] where quantized LLMs (OPT [72], BLOOM [52], Pythia [7], LLaMA) of different sizes (125M to 65B) with different data types are evaluated on language modeling and a set of zero-shot tasks. In Figure 3 and Table 2 we see that NF4 improves performance significantly over FP4 and Int4 and that double quantization reduces the memory footprint without degrading performance. \n\nk-bit QLORA matches 16-bit full finetuning and 16-bit LoRA performance Recent findings have established that 4-bit quantization for inference is possible, but leads to performance degradation relative to 16-bit [13,18]. This raises the crucial question of whether the lost performance can be recovered by conducting 4-bit adapter finetuning. We test this for two setups. The first focuses on a comparison with full 16-bit finetuning of RoBERTA and T5 models sized 125M to 3B parameters on GLUE and the Super-NaturalInstructions dataset. Results are shown in Table 3. In both datasets, we observe that 16-bit, 8-bit, and 4-bit adapter methods replicate the performance of the fully finetuned 16-bit baseline. This suggests that the performance lost due to the imprecise quantization can be fully recovered through adapter finetuning after quantization. \n\nFor our second setup, since full finetuning models at and beyond 11B parameters requires more than one server of high memory GPUs, we continue to test whether 4-bit QLORA can match 16-bit LoRA at the 7B to 65B parameter scales. To this end, we finetune LLaMA 7B through 65B on two instruction following datasets, Alpaca and FLAN v2, and evaluate on the MMLU benchmark via 5-shot accuracy. Results are shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit LoRA MMLU performance.",
            "reference_string": "[258841328 | Dettmers et al. | 2023 | Citations: 2606]"
        },
        {
            "title": "Information Bottleneck: Exact Analysis of (Quantized) Neural Networks",
            "venue": "International Conference on Learning Representations",
            "year": 2021,
            "reference_count": 37,
            "citation_count": 18,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.12912, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2048034204",
                    "name": "S. Lorenzen"
                },
                {
                    "authorId": "1748824",
                    "name": "C. Igel"
                },
                {
                    "authorId": "2069766353",
                    "name": "M. Nielsen"
                }
            ],
            "abstract": "The information bottleneck (IB) principle has been suggested as a way to analyze deep neural networks. The learning dynamics are studied by inspecting the mutual information (MI) between the hidden layers and the input and output. Notably, separate fitting and compression phases during training have been reported. This led to some controversy including claims that the observations are not reproducible and strongly dependent on the type of activation function used as well as on the way the MI is estimated. Our study confirms that different ways of binning when computing the MI lead to qualitatively different results, either supporting or refusing IB conjectures. To resolve the controversy, we study the IB principle in settings where MI is non-trivial and can be computed exactly. We monitor the dynamics of quantized neural networks, that is, we discretize the whole deep learning system so that no approximation is required when computing the MI. This allows us to quantify the information flow without measurement errors. In this setting, we observed a fitting phase for all layers and a compression phase for the output layer in all experiments; the compression in the hidden layers was dependent on the type of activation function. Our study shows that the initial IB results were not artifacts of binning when computing the MI. However, the critical claim that the compression phase may not be observed for some networks also holds true.",
            "corpus_id": 235623747,
            "sentences": [
                {
                    "corpus_id": "235623747",
                    "title": "Information Bottleneck: Exact Analysis of (Quantized) Neural Networks",
                    "text": "This section presents information planes obtained using 4-and 32-bit quantized neural networks trained in the standard setting, in order to investigate the effect of the precision used in the quantization. Figure C.10 depicts the resulting information planes for the 4-bit networks. As each neuron can take only 2 4 = 16 different values, the total number of possible states per layer decreases significantly in this setting, and as expected we see lower overall MI measurements. For the TANH network, several more layers become distinguishable. The observed information planes looks similar to those observed in the original experiments by Shwartz-Ziv & Tishby (2017) and Saxe et al. (2018). However, the network accuracy has now degraded compared to the non-quantized networks (see Supplementary Material F), which indicates that the binning used in the estimation of the MI in previous experiments has discarded information vital to the network. Figure C.11 shows the resulting information planes for the 32-bit networks. As expected, we see an overall increase in MI; the information drops only very slowly through the network. Each layer has many possible states and -given the small data set -we we get closer to the behavior of a continuous system.",
                    "score": 0.6651882947805579,
                    "section_title": "C EFFECT OF THE QUANTIZATION PRECISION",
                    "char_start_offset": 31990,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 673,
                            "end": 691,
                            "matchedPaperCorpusId": "49584497"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95458984375
                }
            ],
            "relevance_judgement": 0.95458984375,
            "relevance_judgment_input_expanded": "# Title: Information Bottleneck: Exact Analysis of (Quantized) Neural Networks\n# Venue: International Conference on Learning Representations\n# Authors: S. Lorenzen, C. Igel, M. Nielsen\n## Abstract\nThe information bottleneck (IB) principle has been suggested as a way to analyze deep neural networks. The learning dynamics are studied by inspecting the mutual information (MI) between the hidden layers and the input and output. Notably, separate fitting and compression phases during training have been reported. This led to some controversy including claims that the observations are not reproducible and strongly dependent on the type of activation function used as well as on the way the MI is estimated. Our study confirms that different ways of binning when computing the MI lead to qualitatively different results, either supporting or refusing IB conjectures. To resolve the controversy, we study the IB principle in settings where MI is non-trivial and can be computed exactly. We monitor the dynamics of quantized neural networks, that is, we discretize the whole deep learning system so that no approximation is required when computing the MI. This allows us to quantify the information flow without measurement errors. In this setting, we observed a fitting phase for all layers and a compression phase for the output layer in all experiments; the compression in the hidden layers was dependent on the type of activation function. Our study shows that the initial IB results were not artifacts of binning when computing the MI. However, the critical claim that the compression phase may not be observed for some networks also holds true.\n## C EFFECT OF THE QUANTIZATION PRECISION\nThis section presents information planes obtained using 4-and 32-bit quantized neural networks trained in the standard setting, in order to investigate the effect of the precision used in the quantization. Figure C.10 depicts the resulting information planes for the 4-bit networks. As each neuron can take only 2 4 = 16 different values, the total number of possible states per layer decreases significantly in this setting, and as expected we see lower overall MI measurements. For the TANH network, several more layers become distinguishable. The observed information planes looks similar to those observed in the original experiments by Shwartz-Ziv & Tishby (2017) and Saxe et al. (2018). However, the network accuracy has now degraded compared to the non-quantized networks (see Supplementary Material F), which indicates that the binning used in the estimation of the MI in previous experiments has discarded information vital to the network. Figure C.11 shows the resulting information planes for the 32-bit networks. As expected, we see an overall increase in MI; the information drops only very slowly through the network. Each layer has many possible states and -given the small data set -we we get closer to the behavior of a continuous system.",
            "reference_string": "[235623747 | Lorenzen et al. | 2021 | Citations: 18]"
        },
        {
            "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 47,
            "citation_count": 209,
            "influential_citation_count": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.17888",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.17888, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109370860",
                    "name": "Zechun Liu"
                },
                {
                    "authorId": "9185192",
                    "name": "Barlas O\u011fuz"
                },
                {
                    "authorId": "2112729504",
                    "name": "Changsheng Zhao"
                },
                {
                    "authorId": "48025720",
                    "name": "Ernie Chang"
                },
                {
                    "authorId": "37502184",
                    "name": "Pierre Stock"
                },
                {
                    "authorId": "2121361882",
                    "name": "Yashar Mehdad"
                },
                {
                    "authorId": "152345059",
                    "name": "Yangyang Shi"
                },
                {
                    "authorId": "2065915235",
                    "name": "Raghuraman Krishnamoorthi"
                },
                {
                    "authorId": "144137037",
                    "name": "Vikas Chandra"
                }
            ],
            "abstract": "Several post-training quantization methods have been applied to large language models (LLMs), and have been shown to perform well down to 8-bits. We find that these methods break down at lower bit precision, and investigate quantization aware training for LLMs (LLM-QAT) to push quantization levels even further. We propose a data-free distillation method that leverages generations produced by the pre-trained model, which better preserves the original output distribution and allows quantizing any generative model independent of its training data, similar to post-training quantization methods. In addition to quantizing weights and activations, we also quantize the KV cache, which is critical for increasing throughput and support long sequence dependencies at current model sizes. We experiment with LLaMA models of sizes 7B, 13B, and 30B, at quantization levels down to 4-bits. We observe large improvements over training-free methods, especially in the low-bit settings.",
            "corpus_id": 258959117,
            "sentences": [
                {
                    "corpus_id": "258959117",
                    "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models",
                    "text": "One important question for practitioners is whether to use a small model at full precision, or a larger quantized model of similar inference cost. While the exact trade-offs can vary based on several factors, we can make several recommendations based on our results. First, 8-bit quantization should be preferred over smaller full precision models, and PTQ methods are sufficient for this case. An 8-8-8 30B quantized model outperforms a 13B model of similar size, and should have lower latency and higher throughput in practice. This also holds for an 8-bit 13B model compared with a 16-bit 7B model. Furthermore, 4-bit models quantized using LLM-QAT should be preferred over 8-bit models of similar size. For instance a 4-8-4 LLM-QAT 30B outperforms an 8-bit LLaMA-13B, and a 4-8-8 LLM-QAT 13B is better than an 8-bit LLaMA-7B. As a result, we recommend 4-bit LLM-QAT models for the best efficiency-accuracy tradeoff.",
                    "score": 0.6515126012161162,
                    "section_title": "Main Results",
                    "char_start_offset": 14824,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 146
                        },
                        {
                            "start": 147,
                            "end": 266
                        },
                        {
                            "start": 267,
                            "end": 394
                        },
                        {
                            "start": 395,
                            "end": 529
                        },
                        {
                            "start": 530,
                            "end": 601
                        },
                        {
                            "start": 602,
                            "end": 706
                        },
                        {
                            "start": 707,
                            "end": 829
                        },
                        {
                            "start": 830,
                            "end": 919
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9541015625
                }
            ],
            "relevance_judgement": 0.9541015625,
            "relevance_judgment_input_expanded": "# Title: LLM-QAT: Data-Free Quantization Aware Training for Large Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Zechun Liu, Barlas O\u011fuz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, Vikas Chandra\n## Abstract\nSeveral post-training quantization methods have been applied to large language models (LLMs), and have been shown to perform well down to 8-bits. We find that these methods break down at lower bit precision, and investigate quantization aware training for LLMs (LLM-QAT) to push quantization levels even further. We propose a data-free distillation method that leverages generations produced by the pre-trained model, which better preserves the original output distribution and allows quantizing any generative model independent of its training data, similar to post-training quantization methods. In addition to quantizing weights and activations, we also quantize the KV cache, which is critical for increasing throughput and support long sequence dependencies at current model sizes. We experiment with LLaMA models of sizes 7B, 13B, and 30B, at quantization levels down to 4-bits. We observe large improvements over training-free methods, especially in the low-bit settings.\n## Main Results\nOne important question for practitioners is whether to use a small model at full precision, or a larger quantized model of similar inference cost. While the exact trade-offs can vary based on several factors, we can make several recommendations based on our results. First, 8-bit quantization should be preferred over smaller full precision models, and PTQ methods are sufficient for this case. An 8-8-8 30B quantized model outperforms a 13B model of similar size, and should have lower latency and higher throughput in practice. This also holds for an 8-bit 13B model compared with a 16-bit 7B model. Furthermore, 4-bit models quantized using LLM-QAT should be preferred over 8-bit models of similar size. For instance a 4-8-4 LLM-QAT 30B outperforms an 8-bit LLaMA-13B, and a 4-8-8 LLM-QAT 13B is better than an 8-bit LLaMA-7B. As a result, we recommend 4-bit LLM-QAT models for the best efficiency-accuracy tradeoff.",
            "reference_string": "[258959117 | Liu et al. | 2023 | Citations: 209]"
        },
        {
            "title": "A Comprehensive Study on Quantization Techniques for Large Language Models",
            "venue": "2024 4th International Conference on Artificial Intelligence, Robotics, and Communication (ICAIRC)",
            "year": 2024,
            "reference_count": 35,
            "citation_count": 12,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2411.02530",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.02530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2329225877",
                    "name": "Jiedong Lang"
                },
                {
                    "authorId": "2329318690",
                    "name": "Zhehao Guo"
                },
                {
                    "authorId": "2329202190",
                    "name": "Shuyu Huang"
                }
            ],
            "abstract": "Large Language Models (LLMs) have been extensively researched and used in both academia and industry since the rise in popularity of the Transformer model, which demonstrates excellent performance in AI. However, the computational demands of LLMs are immense, and the energy resources required to run them are often limited. For instance, popular models like GPT-3, with 175 billion parameters and a storage requirement of 350 GB, present significant challenges for deployment on resource-constrained IoT devices and embedded systems. These systems often lack the computational capacity to handle such large models. Quantization, a technique that reduces the precision of model values to a smaller set of discrete values, offers a promising solution by reducing the size of LLMs and accelerating inference. In this research, we provide a comprehensive analysis of quantization techniques within the machine learning field, with a particular focus on their application to LLMs. We begin by exploring the mathematical theory of quantization, followed by a review of common quantization methods and how they are implemented. Furthermore, we examine several prominent quantization methods applied to LLMs, detailing their algorithms and performance outcomes.",
            "corpus_id": 273821224,
            "sentences": [
                {
                    "corpus_id": "273821224",
                    "title": "A Comprehensive Study on Quantization Techniques for Large Language Models",
                    "text": "After thoroughly examining various popular quantization techniques, it is evident that their performance varies depending on the precision of the quantization. For example, GPTQ achieves optimal performance at 4-bit precision but experiences a decline at 3-bit precision. In addition, LLM-QAT demonstrates better accuracy with a configuration of 4-bit weights, 4-bit KV caches, and 8-bit activations, compared to a uniform 4-bit setting across all precision [35]. To maximize the performance of a given quantization technique, developers must carefully select appropriate precision settings. Future research on quantization techniques could further explore the impact of precision configuration, potentially leading to more refined and efficient quantization strategies.",
                    "score": 0.6098403479191405,
                    "section_title": "VII. CONCLUSION",
                    "char_start_offset": 30229,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 159
                        },
                        {
                            "start": 160,
                            "end": 271
                        },
                        {
                            "start": 272,
                            "end": 463
                        },
                        {
                            "start": 464,
                            "end": 591
                        },
                        {
                            "start": 592,
                            "end": 770
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9541015625
                }
            ],
            "relevance_judgement": 0.9541015625,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Study on Quantization Techniques for Large Language Models\n# Venue: 2024 4th International Conference on Artificial Intelligence, Robotics, and Communication (ICAIRC)\n# Authors: Jiedong Lang, Zhehao Guo, Shuyu Huang\n## Abstract\nLarge Language Models (LLMs) have been extensively researched and used in both academia and industry since the rise in popularity of the Transformer model, which demonstrates excellent performance in AI. However, the computational demands of LLMs are immense, and the energy resources required to run them are often limited. For instance, popular models like GPT-3, with 175 billion parameters and a storage requirement of 350 GB, present significant challenges for deployment on resource-constrained IoT devices and embedded systems. These systems often lack the computational capacity to handle such large models. Quantization, a technique that reduces the precision of model values to a smaller set of discrete values, offers a promising solution by reducing the size of LLMs and accelerating inference. In this research, we provide a comprehensive analysis of quantization techniques within the machine learning field, with a particular focus on their application to LLMs. We begin by exploring the mathematical theory of quantization, followed by a review of common quantization methods and how they are implemented. Furthermore, we examine several prominent quantization methods applied to LLMs, detailing their algorithms and performance outcomes.\n## VII. CONCLUSION\nAfter thoroughly examining various popular quantization techniques, it is evident that their performance varies depending on the precision of the quantization. For example, GPTQ achieves optimal performance at 4-bit precision but experiences a decline at 3-bit precision. In addition, LLM-QAT demonstrates better accuracy with a configuration of 4-bit weights, 4-bit KV caches, and 8-bit activations, compared to a uniform 4-bit setting across all precision [35]. To maximize the performance of a given quantization technique, developers must carefully select appropriate precision settings. Future research on quantization techniques could further explore the impact of precision configuration, potentially leading to more refined and efficient quantization strategies.",
            "reference_string": "[273821224 | Lang et al. | 2024 | Citations: 12]"
        },
        {
            "title": "Optical Computing for Deep Neural Network Acceleration: Foundations, Recent Developments, and Emerging Directions",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 139,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.21184, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1681003",
                    "name": "S. Pasricha"
                }
            ],
            "abstract": "Emerging artificial intelligence applications across the domains of computer vision, natural language processing, graph processing, and sequence prediction increasingly rely on deep neural networks (DNNs). These DNNs require significant compute and memory resources for training and inference. Traditional computing platforms such as CPUs, GPUs, and TPUs are struggling to keep up with the demands of the increasingly complex and diverse DNNs. Optical computing represents an exciting new paradigm for light-speed acceleration of DNN workloads. In this article, we discuss the fundamentals and state-of-the-art developments in optical computing, with an emphasis on DNN acceleration. Various promising approaches are described for engineering optical devices, enhancing optical circuits, and designing architectures that can adapt optical computing to a variety of DNN workloads. Novel techniques for hardware/software co-design that can intelligently tune and map DNN models to improve performance and energy-efficiency on optical computing platforms across high performance and resource constrained embedded, edge, and IoT platforms are also discussed. Lastly, several open problems and future directions for research in this domain are highlighted.",
            "corpus_id": 271571530,
            "sentences": [
                {
                    "corpus_id": "271571530",
                    "title": "Optical Computing for Deep Neural Network Acceleration: Foundations, Recent Developments, and Emerging Directions",
                    "text": "Another approach to realize optical accelerators for power and performance constrained edge and IoT platforms is to utilize quantization in DNNs and perform hardware/software co-design to devise an optical accelerator tuned for the chosen quantization level. In quantization, the bitwidths of DNN parameters (weights) and activations are reduced from the default (typically 32-bit floating point) to a lower value (e.g., 8 bit fixed-point). In [111], an optical accelerator for binary quantized CNNs was proposed. Here binary refers to utilizing a single bit to represent parameters and/or weights. Unfortunately, using such a small bitwidth significantly reduces CNN inference accuracy. Studies have shown that activations are more susceptible to bitwidth reduction than weight parameters. Therefore, an alternative for increasing inference accuracy is to not binarize activations, while keeping the weights binarized. To determine the appropriate activation bitwidth (precision), an analysis was performed in [111], where weights were restricted to binary (1-bit) values, but the bit precision level of the activations was altered from 1-bit to 16-bits, across various CNN models and learning tasks. During CNN model training, weights were binarized during the forward and backward propagations but not during the parameter update step, because keeping high precision weights during the updates is necessary for stochastic gradient descent (SGD) to work efficiently. After training, all weights were in binary format, while the precision of input activations was varied. Figure 9(a) shows the results of varying activation bitwidth across four different CNN models and their datasets. It was observed that the accuracy had notable change initially as activations bits were increased, but this gain in accuracy soon saturated. Based on the results, 1-bit weights were considered along with 4-bit activations, which allowed reducing bitwidths without significantly reducing model accuracy. \n\nThe reduction in bit-width for weights and activations, from a baseline of 32-bit to 1 and 4 bits, respectively, allowed significantly reducing the number of ADCs and DACs in the proposed binary optical accelerator called ROBIN [111].",
                    "score": 0.6015380683619859,
                    "section_title": "Quantization Approaches",
                    "char_start_offset": 64602,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 258
                        },
                        {
                            "start": 259,
                            "end": 440
                        },
                        {
                            "start": 441,
                            "end": 513
                        },
                        {
                            "start": 514,
                            "end": 598
                        },
                        {
                            "start": 599,
                            "end": 687
                        },
                        {
                            "start": 688,
                            "end": 790
                        },
                        {
                            "start": 791,
                            "end": 919
                        },
                        {
                            "start": 920,
                            "end": 1201
                        },
                        {
                            "start": 1202,
                            "end": 1468
                        },
                        {
                            "start": 1469,
                            "end": 1572
                        },
                        {
                            "start": 1573,
                            "end": 1686
                        },
                        {
                            "start": 1687,
                            "end": 1827
                        },
                        {
                            "start": 1828,
                            "end": 1989
                        },
                        {
                            "start": 1992,
                            "end": 2226
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 444,
                            "end": 449,
                            "matchedPaperCorpusId": "235795063"
                        },
                        {
                            "start": 1011,
                            "end": 1016,
                            "matchedPaperCorpusId": "235795063"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.953125
                }
            ],
            "relevance_judgement": 0.953125,
            "relevance_judgment_input_expanded": "# Title: Optical Computing for Deep Neural Network Acceleration: Foundations, Recent Developments, and Emerging Directions\n# Venue: arXiv.org\n# Authors: S. Pasricha\n## Abstract\nEmerging artificial intelligence applications across the domains of computer vision, natural language processing, graph processing, and sequence prediction increasingly rely on deep neural networks (DNNs). These DNNs require significant compute and memory resources for training and inference. Traditional computing platforms such as CPUs, GPUs, and TPUs are struggling to keep up with the demands of the increasingly complex and diverse DNNs. Optical computing represents an exciting new paradigm for light-speed acceleration of DNN workloads. In this article, we discuss the fundamentals and state-of-the-art developments in optical computing, with an emphasis on DNN acceleration. Various promising approaches are described for engineering optical devices, enhancing optical circuits, and designing architectures that can adapt optical computing to a variety of DNN workloads. Novel techniques for hardware/software co-design that can intelligently tune and map DNN models to improve performance and energy-efficiency on optical computing platforms across high performance and resource constrained embedded, edge, and IoT platforms are also discussed. Lastly, several open problems and future directions for research in this domain are highlighted.\n## Quantization Approaches\nAnother approach to realize optical accelerators for power and performance constrained edge and IoT platforms is to utilize quantization in DNNs and perform hardware/software co-design to devise an optical accelerator tuned for the chosen quantization level. In quantization, the bitwidths of DNN parameters (weights) and activations are reduced from the default (typically 32-bit floating point) to a lower value (e.g., 8 bit fixed-point). In [111], an optical accelerator for binary quantized CNNs was proposed. Here binary refers to utilizing a single bit to represent parameters and/or weights. Unfortunately, using such a small bitwidth significantly reduces CNN inference accuracy. Studies have shown that activations are more susceptible to bitwidth reduction than weight parameters. Therefore, an alternative for increasing inference accuracy is to not binarize activations, while keeping the weights binarized. To determine the appropriate activation bitwidth (precision), an analysis was performed in [111], where weights were restricted to binary (1-bit) values, but the bit precision level of the activations was altered from 1-bit to 16-bits, across various CNN models and learning tasks. During CNN model training, weights were binarized during the forward and backward propagations but not during the parameter update step, because keeping high precision weights during the updates is necessary for stochastic gradient descent (SGD) to work efficiently. After training, all weights were in binary format, while the precision of input activations was varied. Figure 9(a) shows the results of varying activation bitwidth across four different CNN models and their datasets. It was observed that the accuracy had notable change initially as activations bits were increased, but this gain in accuracy soon saturated. Based on the results, 1-bit weights were considered along with 4-bit activations, which allowed reducing bitwidths without significantly reducing model accuracy. \n\nThe reduction in bit-width for weights and activations, from a baseline of 32-bit to 1 and 4 bits, respectively, allowed significantly reducing the number of ADCs and DACs in the proposed binary optical accelerator called ROBIN [111].",
            "reference_string": "[271571530 | Pasricha | 2024 | Citations: 0]"
        },
        {
            "title": "Joint Pruning and Channel-Wise Mixed-Precision Quantization for Efficient Deep Neural Networks",
            "venue": "IEEE transactions on computers",
            "year": 2024,
            "reference_count": 45,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2407.01054",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01054, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2223967064",
                    "name": "Beatrice Alessandra Motetti"
                },
                {
                    "authorId": "2114394120",
                    "name": "Matteo Risso"
                },
                {
                    "authorId": "51091982",
                    "name": "Alessio Burrello"
                },
                {
                    "authorId": "2231805248",
                    "name": "Enrico Macii"
                },
                {
                    "authorId": "1695243",
                    "name": "M. Poncino"
                },
                {
                    "authorId": "36204425",
                    "name": "D. J. Pagliari"
                }
            ],
            "abstract": "The resource requirements of deep neural networks (DNNs) pose significant challenges to their deployment on edge devices. Common approaches to address this issue are pruning and mixed-precision quantization, which lead to latency and memory occupation improvements. These optimization techniques are usually applied independently. We propose a novel methodology to apply them jointly via a lightweight gradient-based search, and in a hardware-aware manner, greatly reducing the time required to generate Pareto-optimal DNNs in terms of accuracy versus cost (i.e., latency or memory). We test our approach on three edge-relevant benchmarks, namely CIFAR-10, Google Speech Commands, and Tiny ImageNet. When targeting the optimization of the memory footprint, we are able to achieve a size reduction of 47.50% and 69.54% at iso-accuracy with the baseline networks with all weights quantized at 8 and 2-bit, respectively. Our method surpasses a previous state-of-the-art approach with up to 56.17% size reduction at iso-accuracy. With respect to the sequential application of state-of-the-art pruning and mixed-precision optimizations, we obtain comparable or superior results, but with a significantly lowered training time. In addition, we show how well-tailored cost models can improve the cost versus accuracy trade-offs when targeting specific hardware for deployment.",
            "corpus_id": 270870579,
            "sentences": [
                {
                    "corpus_id": "270870579",
                    "title": "Joint Pruning and Channel-Wise Mixed-Precision Quantization for Efficient Deep Neural Networks",
                    "text": "For all the three \"High\" DNNs (leftmost plot), most of the weight parameters are quantized at 8 bits, i.e. the highest possibile precision, as expected. In \"Medium\" models (central plot), very few parameters, if any, are quantized at 2 bits, since it is the least interesting precision due to the low representational capacity, but not negligible cost. In contrast, 4-bit channels represent a significant component, especially for the size and NE16 regularizers, costing less then twice the 2-bit ones while providing significant accuracy advantages. For what concerns \"Low\" models, except when trained with the MPIC regularizer, the 4-bit precision is assigned more often than 8-bit. In particular, with a more deepened analysis, it is possible to see that the 8-bit precision is favored only in the final layer, which is a common finding [7], [28]. Size is the only cost regularizer leading to some weights' channels being quantized at 2 bits. The MPIC cost model mainly favors pruning and keeps most of the other weights at 8 bits, since there is not a sufficient cost difference between this and smaller bit-widths. The NE16 cost model, instead, encourages a more spread-out distribution between 4-and 8-bit but entirely avoids 2-bit precision. The reason is that the NE16 cost model does not scale linearly with the output channels, as each processing element (PE) handles groups of 32 output channels (Sec. 4.3.3). Consequently, running a single channel at one precision incurs the same cost as running 32 channels, implying that to enhance latency, the NE16 accelerator should execute at least 32 2-bit filters. However, this would lead to suboptimal solutions from an accuracy standpoint, and is thus avoided by the optimization.",
                    "score": 0.6394651388785003,
                    "section_title": "Models analysis",
                    "char_start_offset": 59907,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 152
                        },
                        {
                            "start": 153,
                            "end": 352
                        },
                        {
                            "start": 353,
                            "end": 550
                        },
                        {
                            "start": 551,
                            "end": 684
                        },
                        {
                            "start": 685,
                            "end": 850
                        },
                        {
                            "start": 851,
                            "end": 945
                        },
                        {
                            "start": 946,
                            "end": 1119
                        },
                        {
                            "start": 1120,
                            "end": 1248
                        },
                        {
                            "start": 1249,
                            "end": 1412
                        },
                        {
                            "start": 1413,
                            "end": 1420
                        },
                        {
                            "start": 1421,
                            "end": 1618
                        },
                        {
                            "start": 1619,
                            "end": 1737
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 840,
                            "end": 843,
                            "matchedPaperCorpusId": "215745195"
                        },
                        {
                            "start": 845,
                            "end": 849,
                            "matchedPaperCorpusId": "220363587"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.951171875
                }
            ],
            "relevance_judgement": 0.951171875,
            "relevance_judgment_input_expanded": "# Title: Joint Pruning and Channel-Wise Mixed-Precision Quantization for Efficient Deep Neural Networks\n# Venue: IEEE transactions on computers\n# Authors: Beatrice Alessandra Motetti, Matteo Risso, Alessio Burrello, Enrico Macii, M. Poncino, D. J. Pagliari\n## Abstract\nThe resource requirements of deep neural networks (DNNs) pose significant challenges to their deployment on edge devices. Common approaches to address this issue are pruning and mixed-precision quantization, which lead to latency and memory occupation improvements. These optimization techniques are usually applied independently. We propose a novel methodology to apply them jointly via a lightweight gradient-based search, and in a hardware-aware manner, greatly reducing the time required to generate Pareto-optimal DNNs in terms of accuracy versus cost (i.e., latency or memory). We test our approach on three edge-relevant benchmarks, namely CIFAR-10, Google Speech Commands, and Tiny ImageNet. When targeting the optimization of the memory footprint, we are able to achieve a size reduction of 47.50% and 69.54% at iso-accuracy with the baseline networks with all weights quantized at 8 and 2-bit, respectively. Our method surpasses a previous state-of-the-art approach with up to 56.17% size reduction at iso-accuracy. With respect to the sequential application of state-of-the-art pruning and mixed-precision optimizations, we obtain comparable or superior results, but with a significantly lowered training time. In addition, we show how well-tailored cost models can improve the cost versus accuracy trade-offs when targeting specific hardware for deployment.\n## Models analysis\nFor all the three \"High\" DNNs (leftmost plot), most of the weight parameters are quantized at 8 bits, i.e. the highest possibile precision, as expected. In \"Medium\" models (central plot), very few parameters, if any, are quantized at 2 bits, since it is the least interesting precision due to the low representational capacity, but not negligible cost. In contrast, 4-bit channels represent a significant component, especially for the size and NE16 regularizers, costing less then twice the 2-bit ones while providing significant accuracy advantages. For what concerns \"Low\" models, except when trained with the MPIC regularizer, the 4-bit precision is assigned more often than 8-bit. In particular, with a more deepened analysis, it is possible to see that the 8-bit precision is favored only in the final layer, which is a common finding [7], [28]. Size is the only cost regularizer leading to some weights' channels being quantized at 2 bits. The MPIC cost model mainly favors pruning and keeps most of the other weights at 8 bits, since there is not a sufficient cost difference between this and smaller bit-widths. The NE16 cost model, instead, encourages a more spread-out distribution between 4-and 8-bit but entirely avoids 2-bit precision. The reason is that the NE16 cost model does not scale linearly with the output channels, as each processing element (PE) handles groups of 32 output channels (Sec. 4.3.3). Consequently, running a single channel at one precision incurs the same cost as running 32 channels, implying that to enhance latency, the NE16 accelerator should execute at least 32 2-bit filters. However, this would lead to suboptimal solutions from an accuracy standpoint, and is thus avoided by the optimization.",
            "reference_string": "[270870579 | Motetti et al. | 2024 | Citations: 4]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "259226792",
            "title": "Neural Network Exploration for Keyword Spotting on Edge Devices",
            "text": "To evaluate the impact of quantization schemes on accuracy, the inference accuracy versus model size for each neural network architecture after training for 100 epochs were plotted under the four different quantization schemes in Figure 7. \n\nOverall, the models with the 16-bit floating point, the dynamic range, and the integerfloat quantization schemes closely mimicked their nonquantized counterparts in terms of accuracy, with reduced model sizes. For 16-bit floating-point quantization, this is likely due to the simplicity of converting 32-bit floating-point values to 16-bit floating-point values with minimal error. The ability of the dynamic range and the integer-float quantization models to provide the benefits of 8-bit integer quantization with best effort post training or at inference while avoiding the associated drop in accuracy was a pleasant surprise. For model sizes above 30 kB, most architectures, except FFCN, still exceeded 90% accuracy. For model sizes smaller than 30 kB, however, a drop in accuracy was observed. Compared with the nonquantized models, the accuracy achieved with 16-bit floating-point quantization and dynamic range quantization is about the same, whereas the drop in accuracy is more pronounced for models with integer-float quantization. For example, the smallest ResNet model achieves 76.67% accuracy under the integer-float quantization, which is more a than 1.5% reduction from 78.34% with no quantization. For the smallest DenseNet model, the accuracy dropped from 61.83% without quantization to 54.24% with integer-float quantization. \n\nThe full-integer quantization scheme greatly impacted the performance of the models, with an approximate 20% drop in accuracy. The drop in accuracy was likely caused by the limitations of PTQ. While PTQ aims to maintain network performance using a representative dataset for calibration, some inaccuracies are inevitably introduced by converting all 32-bit floating-point values to 8-bit integers. QAT could achieve better results, but the model would have to be completely retrained.",
            "score": 0.8117104228374248,
            "section_title": "Effect of Quantization on Accuracy",
            "char_start_offset": 35518,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 242,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1585
                },
                {
                    "start": 1588,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2072
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.921875
        },
        {
            "corpus_id": "218788236",
            "title": "Accurate Inference With Inaccurate RRAM Devices: A Joint Algorithm-Design Solution",
            "text": "In deep learning, a 32-bit floating-point (FP32) format is used for training and inference purposes. Recent works [26] have achieved an 8-bit fixed-point precision of weights and activations without incurring any inference accuracy loss. However, unlike GPUs and CPUs, RRAM devices are limited by finite quantization levels [13]. Hence, an 8-bit quantization of parameters is not a viable solution for RRAM-based crossbars. In this article, we perform a more conservative 4-bit quantization for the model parameters, i.e., weights and biases. We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization.",
            "score": 0.7807466345707892,
            "section_title": "B. DEVICE QUANTIZATION",
            "char_start_offset": 7268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 894
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 118,
                    "matchedPaperCorpusId": "44071489"
                },
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "16877209"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9853515625
        },
        {
            "corpus_id": "245351885",
            "title": "Quantization noise in low bit quantization and iterative adaptation to quantization noise in quantizable neural networks",
            "text": "Quantization is one of the most popular and widely used methods of speeding up a neural network. At the moment, the standard is 8-bit uniform quantization. Nevertheless, the use of uniform low-bit quantization (4- and 6-bit quantization) has significant advantages in speed and resource requirements for inference. We present our quantization algorithm that offers advantages when using uniform low-bit quantization. It is faster than quantization-aware training from scratch and more accurate than methods aimed only at selecting thresholds and reducing noise from quantization. We also investigated quantization noise in neural networks for low-bit quantization and concluded that quantization noise is not always a good metric for quantization quality.",
            "score": 0.7705364021492916,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9658203125
        },
        {
            "corpus_id": "235266137",
            "title": "Integer-Only Neural Network Quantization Scheme Based on Shift-Batch-Normalization",
            "text": "We proposed a quantization scheme that can avoid big integer multiplication, which can be used in low precision neural network training and inference. In the whole integer-only inference process, only these integer operations are used: \n\n\u2022 Multiplication of 4 bit integer, producing 8 bit integer. \u2022 Addition of 8 bit and 32 bit integer, producing 32 bit integer. \u2022 Right shifting of 32 bit integer, producing 4 bit integer. We tried 4-bit quantization on some simple datasets(MNIST, CIFAR10, CIFAR100), and there was no quantization loss. We also tried 4-bit quantization on big dataset(ImageNet). The experiment result shows that there is about 12% performance loss, but due to the inference efficiency of 4-bit quantization, this loss may be tolerable in some application scenarios.",
            "score": 0.7683589669550788,
            "section_title": "IV. DISCUSSION",
            "char_start_offset": 14046,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 235
                },
                {
                    "start": 238,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 785
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9814453125
        },
        {
            "corpus_id": "246077316",
            "title": "MOHAQ: Multi-Objective Hardware-Aware Quantization of recurrent neural networks",
            "text": "model as in S18.\n\nIn most of the solutions, 4 bits and 2 bits have been used extensively for the weights. The activation precision has been kept between 8-bit and 16-bit in some layers for most of the solutions. However, 4-bit and 2-bit activations have been used in few layers. It is also observed that some solutions have an error rate better than the baseline model. It has been shown that quantization has a regularization effect during training [4]. Therefore, we think the improved error is a result of the quantization error introducing a noise that reduces some of the over-fitting effect during inference.\n\nIn Table 3, we expected the W ER T to be higher than W ER V but also we hoped to see that the relative order is kept between the solutions for both W ER T and W ER V . However, if we look at the solutions sorted by the W ER V value, we find that the corresponding W ER T values are not perfectly sorted. S6 and S12 look as outliers. The reason is that S6 and S12 had better W ER T than expected. Still, for both cases, the variation was in the range of 0.2 p.p. and we believe that such small variations are expected as there is no guarantee for two different datasets' errors to be the same. To get a better understanding of how successful our posttraining quantization applied to the SRU-model is, we look at the previous work done on post-training quantization. Since researchers have found that 16-bit and 8-bit quantization do not significantly affect accuracy [10], we will focus on 4-bit quantization (8x compression). CNN ImageNet models have been used for quantization experiments in most of the work we have seen. The accuracy drop due to 4-bit post-training quantization on CNN models has varied in these papers as follows: LAPQ [6] (6.1 to 9.4 p.p.), ACIQ [11] (0.4 to 10.8 p.p.), OCS [10] (more than 5 p.p.), and ZeroQ [38] (1.6 p.p.). Where ZeroQ applied",
            "score": 0.7653565510016771,
            "section_title": "Multi-objective search to minimize two objectives: WER and memory size",
            "char_start_offset": 44283,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 450,
                    "end": 453,
                    "matchedPaperCorpusId": "49669175"
                },
                {
                    "start": 1482,
                    "end": 1486,
                    "matchedPaperCorpusId": "59413897"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95751953125
        },
        {
            "corpus_id": "259937594",
            "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study",
            "text": "In this part, we present the experimental results and the corresponding analysis. \n\nOverall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K. It indicates that 4-bit quantization can effectively retain emergent abilities on these test datasets. \n\n4-bit precision exhibits a favorable trade-off in terms of both total bits and performance. As shown in Table 1, it can be observed that 4-bit quantization offers a notable reduction in memory cost. \n\nTo further examine the relation between model performance and resource usage, we follow Dettmers and Zettlemoyer (2022) to introduce the measure of total bits by multiplying the number of the parameters and the bits, and report the test results in Figure 1 by varying the number of total bits. s From the four accuracy curves corresponding to different bit precision, we can see that 4-bit preci-sion consistently exhibits higher model accuracy under the same amount of total model bits. Thus, 4-bit quantization is recommended to be used for a favorable balance between memory cost and model performance in practice. \n\nThe scaling effect depends on specific tasks, and increasing the model scale benefits the CoT task the most. We conducted an investigation, as depicted in Figure 1, to examine the impact of scaling the total number of bits on the performance of a low-bit model across multiple tasks.",
            "score": 0.7644134163808378,
            "section_title": "Results and Analysis",
            "char_start_offset": 10205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 84,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1704
                },
                {
                    "start": 1707,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1990
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98291015625
        },
        {
            "corpus_id": "220836181",
            "title": "EDSSA: An Encoder-Decoder Semantic Segmentation Networks Accelerator on OpenCL-Based FPGA Platform",
            "text": "In order to determine the optimal quantization strategy, we explored the effect of different quantization bit widths M on algorithm accuracy. In caffe, 32-bit floating-point data is used for network training and inference, and the final global accuracy, class accuracy, and mIoU(Mean Intersection over Union) are 82.80%, 62.30%, and 46.30%, respectively. Based on this model, we used the proposed quantization strategy in the inference process of SegNet-Basic [16] with the data set of CamVid at 480 \u00d7 360 resolution, and the results obtained are given in Figure 7. We can see that when the data is quantified with a bit width less than 16 bits, the quantization error starts to appear and increases as the bit width decreases. Without the dynamic quantization strategy, class accuracy and mIoU decrease significantly when the bit width is less than 12 bits. However, the trend of accuracy declines after using dynamic quantization has obviously eased. This shows the necessity of the dynamic fixed-point quantization strategy in the low-bit width quantization. In addition, even if dynamic quantization is performed, when the bit width is lower than 8 bits, the three accuracies are greatly reduced in value. The quantization accuracy losses of global accuracy, class accuracy, and mIoU are 3.82%, 6.30%, and 4.78%, respectively. \n\nIntersection over Union) are 82.80%, 62.30%, and 46.30%, respectively. Based on this model, we used the proposed quantization strategy in the inference process of SegNet-Basic [16] with the data set of CamVid at 480 \u00d7 360 resolution, and the results obtained are given in Figure 7. We can see that when the data is quantified with a bit width less than 16 bits, the quantization error starts to appear and increases as the bit width decreases. Without the dynamic quantization strategy, class accuracy and mIoU decrease significantly when the bit width is less than 12 bits. However, the trend of accuracy declines after using dynamic quantization has obviously eased. This shows the necessity of the dynamic fixed-point quantization strategy in the low-bit width quantization.",
            "score": 0.7506840949343949,
            "section_title": "Quantization",
            "char_start_offset": 39978,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1330
                },
                {
                    "start": 1333,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2110
                }
            ],
            "ref_mentions": [
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "60814714"
                },
                {
                    "start": 1509,
                    "end": 1513,
                    "matchedPaperCorpusId": "60814714"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92919921875
        },
        {
            "corpus_id": "247470589",
            "title": "Customizable FPGA-Based Hardware Accelerator for Standard Convolution Processes Empowered with Quantization Applied to LiDAR Data",
            "text": "Deep learning algorithms are usually implemented in software using 32 bit floatingpoint values (FP32). Migrating a deep learning algorithm to an ASIC or an FPGA requires for a bit width reduction which is possible using the quantization technique [24][25][26]. A quantized model and a non-quantized model execute the same operations, however, a quantized model with bit-width reduction promotes a memory reduction and allows the execution of more operations per cycle. This memory reduction allows a more compact model representation, which leads to a better deployment in a hardware platform. For hardware, implementation is intended to convert a 32 bit floating-point value to a 16/8/4 bit fixed-point value INT (fixed-point expression), respectively [27]. The bit reduction may lead to a considerable accuracy gap on full precision models as suggested by [28]. \n\nTherefore, it is necessary to achieve a trade-off regarding model accuracy, model parameters, and hardware (HW) performance. The work in [29,30] presents a method that takes full advantage of a DSP block for 8 bit quantization. However, the trade-off between accuracy and inference time might be required and applied whenever possible, therefore, this study provides insights about the model degradation for various model depths, i.e., number of layers.",
            "score": 0.7445329360243053,
            "section_title": "Optimization Methods",
            "char_start_offset": 9443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1319
                }
            ],
            "ref_mentions": [
                {
                    "start": 247,
                    "end": 251,
                    "matchedPaperCorpusId": "221474323"
                },
                {
                    "start": 251,
                    "end": 255,
                    "matchedPaperCorpusId": "207759458"
                },
                {
                    "start": 753,
                    "end": 757,
                    "matchedPaperCorpusId": "216184556"
                },
                {
                    "start": 1007,
                    "end": 1010,
                    "matchedPaperCorpusId": "207959781"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7822265625
        },
        {
            "corpus_id": "219177412",
            "title": "Quantized Neural Networks: Characterization and Holistic Optimization",
            "text": "Most DNN models are trained using 32-bit floating-point numbers. Apparently, DNN models do not demand 32-bit precision. Many quantization methods have been developed, some of which use an extremely small bit-width for a weight representation, such as 1-bit binary [3], [4] or 2-bit ternary [6]. The signal-to-quantization-noise ratio (SQNR) of several weight quantizers was also compared [7]. Quantization noise has been measured to find a better training scheme [8] or optimal quantization precision [9]. Activation quantization has also been developed to lower the computational costs [10]. The weight quantization effects usually depend on the model size; small DNN models tend to show considerable performance degradation after quantization [11]. In particular, increasing the number of parameters in CNNs reduces the quantization sensitivity [12]. However, considering the purpose of model compression, the number of parameters needs to be constrained. A recent study showed that weight quantization up to certain bits does not reduce the memorization capacity [13]. During the last several years, residual connections have been developed mainly for improved training of neural networks [14]. Architectural modifications of increasing the width or moving the location of activation and batch normalization have been studied [15], [16]. These architectural changes also affect the quantization sensitivity.\n\nThe activation quantization has not been discussed as much as weight quantization, and most studies have not distinguished the effects of activation and weight quantization [4], [17]. It has been observed that activation usually demands more bits than weights [5].",
            "score": 0.7415793803271433,
            "section_title": "A. Related Works on Network Quantization",
            "char_start_offset": 2034,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 267,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 269,
                    "end": 272,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 290,
                    "end": 293,
                    "matchedPaperCorpusId": "224893"
                },
                {
                    "start": 388,
                    "end": 391,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "3520830"
                },
                {
                    "start": 501,
                    "end": 504,
                    "matchedPaperCorpusId": "56717582"
                },
                {
                    "start": 1066,
                    "end": 1070,
                    "matchedPaperCorpusId": "86786176"
                },
                {
                    "start": 1192,
                    "end": 1196,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 1585,
                    "end": 1588,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 1590,
                    "end": 1594,
                    "matchedPaperCorpusId": "53719799"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62451171875
        },
        {
            "corpus_id": "276250319",
            "title": "Towards Sustainable NLP: Insights from Benchmarking Inference Energy in Large Language Models",
            "text": "We noticed very similar results with 8-bit quantization and thus, its energy plot is given in Appendix F. \n\nQuantization Accuracy metrics: the change in performance of the quantized models compared to the original is given in the middle set of rows of Table 4. Quantization seems to reduce the performance by less than 5% for some models (mostly decoder-only models in 8-bit quantization and most of the models for 4-bit quantization) and even increases performance slightly for some smaller models, which may have been overfitting earlier. Thus, quantization does not seem to degrade performance too much and should be used to speed up inference time by increasing batch size.",
            "score": 0.7388951280472243,
            "section_title": "Batch size and Quantization",
            "char_start_offset": 24092,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 108,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 677
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89892578125
        },
        {
            "corpus_id": "244531228",
            "title": "LiteEdge: Lightweight Semantic Edge Detection Network",
            "text": "Quantization has become a significant method for optimizing deep-learning models so that they can accelerate inference when deployed on embedded systems with restricted memory footprint and computing resource. In this work, we use the Neural Network Intelligence toolkit 4 . To increase the network accuracy, we use the quantization aware training method (QAT) [19], where we started with the trained model and further refined the model with quantized parameters for 100 epochs.\n\nThe results in Table 3 show that the quantized model with 8-bit weights and activations only has a small drop of  2.2% in the mean MF score. The model size of the quantized network is around 4X smaller than the full precision network. In addition, QAT brings weight sparsity, where 2.65% of the parameters are zero after the quantization. The results for the 6-bit and 4-bit weight quantized models are also presented in Table 3. Compared with the 8-bit weight quantized model, the 6-bit weight quantized model shows a drop of 0.2% in the mean MF score but achieves around 4 times weight sparsity. The 4-bit weight quantized network shows a larger drop in the mean MF score but has more than 38% zero parameters. In addition, its model size is 13X smaller than the full precision model.",
            "score": 0.7380215592179719,
            "section_title": "Model compression study",
            "char_start_offset": 17959,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 361,
                    "end": 365,
                    "matchedPaperCorpusId": "39867659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9599609375
        },
        {
            "corpus_id": "271946892",
            "title": "Robust iterative value conversion: Deep reinforcement learning for neurochip-driven edge robots",
            "text": "This experiment evaluated the effect of the number of quantization bits of the QNN on learning performances. We evaluated whether the learning performance is high when the quantization bits are close to those of SNNs. This experiment was inspired by previous work [50,51,52] that utilized conversion from binarized neural networks (BNNs) with 1-bit weights and 1-bit activation. This experiment evaluated the number of bits with the best learning performance, such that the quantization error was small and the approximation accuracy was not considerably reduced by quantization. Thus, we compared the learning performance by various quantization bits. The results (Fig. 6) \n\nshow that the highest performance (in total reward and convergence speed) was obtained in 4-or 8-bits, followed by 2-, 16-, and 32-bits. These results confirm that the higher the NN quantization weights, the lower is the performance of the converted SNN. \n\nOn the other hand, lower weights are not necessarily better; around 4-or 8-bits are appropriate. The most significant factor causing this result is that 8 is the upper limit of the number of quantization bits of the neurochip. Since the trained policies need to convert to the same bit as the SNN policies, the quantization of the policy conversion results in large changes in the parameters of the policies trained with weights over 8 bits. Therefore, the performances of the 16-and 32-bit policies were significantly degraded. The latter's performance was even worse than the former's due to the large difference in the number of quantization bits between the 32-bit policies and the SNN policies. \n\nThe other experiments used 4-bit quantization for training QNNs based on two aspects: 1) these experiments show that 4-or 8-bits achieved the best performance, and 2) a QNN with lower bits can be calculated quickly.",
            "score": 0.7335389717600764,
            "section_title": "Evaluation of Best Quantization-bit number from Learning Performance",
            "char_start_offset": 32159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 930
                },
                {
                    "start": 933,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1632
                },
                {
                    "start": 1635,
                    "end": 1850
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 268,
                    "matchedPaperCorpusId": "220403513"
                },
                {
                    "start": 268,
                    "end": 271,
                    "matchedPaperCorpusId": "211259291"
                },
                {
                    "start": 271,
                    "end": 274,
                    "matchedPaperCorpusId": "12356965"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.931640625
        },
        {
            "corpus_id": "202540941",
            "title": "Training High-Performance and Large-Scale Deep Neural Networks with Full 8-bit Integers",
            "text": "With the wide applications of DNNs, the related compression technologies have been proposed rapidly, among which the quantization plays an important role.The development of DNN quantization can be divided into two stages, inference quantization and training quantization, according to the different quantization objects.\n\nInference quantization: Inference quantization starts from constraining W into {\u22121, 1} (BWN [7]), replacing complex FP MACs with simple accumulations.BNN [25] and XNOR-Net [8] further quantize both W and A, making the inference computation dominated by bit-wise operations.However, extremely low bit-width quantization usually leads to significant accuracy loss.For example, when the bit width comes to <4 bits, the accuracy degradation becomes obvious, especially for large-scale DNNs.Instead, the bit width of W and A for inference quantization can be reduced to 8 bits with little accuracy degradation.The study of inference quantization is sufficient for the deep learning inference accelerators.Whereas, this is not enough for efficient online learning accelerators because only the data in the forward pass are considered.\n\nTraining quantization: To further extend the quantization towards the training stage, DoReFa [10] trains DNNs with low bit-width W, A, and G, while leaving E and BN unprocessed.MP [14] and MP-INT [16] use FP16 and INT16 values, respectively, to constrain W, A, and G. Recently, FP8 [12] further pushes W, A, G, E, and U to 8, 8, 8, 8, and 16-bit FP values, respectively, still leaving BN untouched.QBP2 [13] replaces the conventional BN with range BN and constrains W, A, and E to INT8 values while calculating G with FP MACS.Recently, WAGE [18] adopts a layer-wise scaling factor instead of using the BN layer and quantizes W, A, G, E, and U to 2, 8, 8, 8, and 8 bits, respectively.",
            "score": 0.7287136830881511,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 5504,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 154,
                    "end": 320
                },
                {
                    "start": 322,
                    "end": 472
                },
                {
                    "start": 472,
                    "end": 595
                },
                {
                    "start": 595,
                    "end": 684
                },
                {
                    "start": 684,
                    "end": 808
                },
                {
                    "start": 808,
                    "end": 927
                },
                {
                    "start": 927,
                    "end": 1022
                },
                {
                    "start": 1022,
                    "end": 1150
                },
                {
                    "start": 1152,
                    "end": 1329
                },
                {
                    "start": 1329,
                    "end": 1550
                },
                {
                    "start": 1550,
                    "end": 1678
                },
                {
                    "start": 1678,
                    "end": 1835
                }
            ],
            "ref_mentions": [
                {
                    "start": 414,
                    "end": 417,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 494,
                    "end": 497,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 1434,
                    "end": 1438,
                    "matchedPaperCorpusId": "53977760"
                },
                {
                    "start": 1555,
                    "end": 1559,
                    "matchedPaperCorpusId": "44071489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "245704572",
            "title": "End-to-End Autoencoder Communications with Optimized Interference Suppression",
            "text": "In embedded implementation, the quantization operation reduces the resolution of bits and can create error propagation for the DNN due to rounding errors. Therefore, we investigate the effects of quantization on the BER performance. The DNNs are trained on a single NVIDIA GeForce RTX 2080 Ti GPU and quantized after training (post-training quantization). We use dynamic quantization that updates the bit resolution per neural network layer. While we may observe some performance degradation, quantization significantly reduces the model size and model weights need to be expressed in fixed point representation for FGPA and embedded GPU implementations. \n\nWe study three performance measures: (i) model size, (ii) inference time, and (iii) error performance. Table II compares the model size of floating point model and 8-bit quantized models. We observe that the quantization reduces the model size by a factor of 4. Table III presents the inferences time of floating-point model and 8-bit quantized model. The results are shown for per sample. A batch of 50 samples are evaluated and 10000 runs are averaged. We observe that the inference time of both models are very similar. The error performance of the floating point model and 8-bit quantized model are shown in Fig. 10 for n ch = 1, 2, 4. Tables IV-VI present the BER versus the SNR performance for different values of n ch . We observe that quantization error is higher in low SNR cases (up to 0.0004) compared to the high SNR, but in all cases the effect of quantization is minor (less than 2.2%).",
            "score": 0.7249185823903315,
            "section_title": "C. Quantization",
            "char_start_offset": 18786,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1557
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91650390625
        },
        {
            "corpus_id": "53568871",
            "title": "Protection of Superconducting Industrial Machinery Using RNN-Based Anomaly Detection for Implementation in Smart Sensor",
            "text": "Table 4 shows results of coefficients quantization for the neural models from Tables 2 and 3 using several different methods. For all methods, the quantization above the ten bits yields results nearly identical to original. A significant drop in accuracy is observed below 8 bits of representation. It may be noted (Table 3) that for lower number of bits the performance oscillates between \u22480.7 and \u22480.3 which means classifying all the features as one category.",
            "score": 0.724607968504577,
            "section_title": "Coefficients Quantization",
            "char_start_offset": 27830,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 461
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76611328125
        },
        {
            "corpus_id": "269605557",
            "title": "Integrating Embedded Neural Networks and Self-Mixing Interferometry for Smart Sensors Design",
            "text": "The effect of quantization of the neural network is evaluated by generating the C code with Qualia-CodeGen.Three configurations are evaluated: floating-point numbers with no quantization (float32), fixed-point 16-bit quantization with format Q7.9 (int16), and fixed-point 8-bit quantization with format Q2.6.The inference is then performed on a workstation with the test dataset.Fig. 5 shows the scatter plot for predictions of the neural network for int8 quantization (green), int16 quantization Fig. 5: Predictions for float32, int16 and int8 models vs. ground truth.The solid lines are optimal linear fits with coefficients 0.74, 0.76 and 0.56 for float32, int16 and int8 respectively.\n\n(orange), and non-quantized float32 (blue) versus the ground truth.Table I provides the Pearson correlation coefficient with respect to the ground truth for each configuration.\n\nThe difference in correlation coefficient between float32 and int16 is only +0.0005, we consider this not to be significant.The scatter plot shows a slight shift in the distribution towards lower values from float32 to int16.This can be explained by the floor rounding mode used for quantization.A less biased result may be obtained by using round to nearest instead.\n\nOn the other hand, int8 quantization causes a significant loss in prediction performance as the correlation coefficient drops by 0.1474 compared to float32.This highlights the limits of such a simple quantization scheme.In order to improve the results of 8-bit quantization, the scale factor must be chosen separately for each layer, and it must also be decoupled for the inputs, the weights and the activations.Quantization-aware training could also be considered.Table II shows latency and memory for different configurations of the neural network when deployed on the microcontroller.The latency is measured as the time taken to perform the inference, after collecting a window of data in the input buffer.For the memory footprint, both ROM and RAM footprints are provided.They are extracted from the memory allocation in the firmware after compilation.The ROM contains the program instructions and the neural network weights.",
            "score": 0.7242050175846639,
            "section_title": "B. Evaluation of quantization",
            "char_start_offset": 13989,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 308
                },
                {
                    "start": 308,
                    "end": 379
                },
                {
                    "start": 379,
                    "end": 569
                },
                {
                    "start": 569,
                    "end": 688
                },
                {
                    "start": 690,
                    "end": 757
                },
                {
                    "start": 757,
                    "end": 866
                },
                {
                    "start": 868,
                    "end": 992
                },
                {
                    "start": 992,
                    "end": 1093
                },
                {
                    "start": 1093,
                    "end": 1164
                },
                {
                    "start": 1164,
                    "end": 1235
                },
                {
                    "start": 1237,
                    "end": 1393
                },
                {
                    "start": 1393,
                    "end": 1457
                },
                {
                    "start": 1457,
                    "end": 1649
                },
                {
                    "start": 1649,
                    "end": 1702
                },
                {
                    "start": 1702,
                    "end": 1824
                },
                {
                    "start": 1824,
                    "end": 1946
                },
                {
                    "start": 1946,
                    "end": 2013
                },
                {
                    "start": 2013,
                    "end": 2093
                },
                {
                    "start": 2093,
                    "end": 2166
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83203125
        },
        {
            "corpus_id": "225076422",
            "title": "A Statistical Framework for Low-bitwidth Training of Deep Neural Networks",
            "text": "Deep neural networks (DNNs) have a high computational cost and memory footprint that slow down their training and inference. By taking advantage of low-bitwidth computational units in hardware, neural network quantization methods provide promising approaches for reducing the cost of timing, memory, and energy consumption, for both training and inference. \n\nNotable quantization methods can be mainly categorized into two groups, inference quantization and training quantization. In inference quantization, the weights and the activations are quantized to speed up the inference phase. Among inference quantization approaches, post training quantization usually does not require access to the partial/full training dataset, and it does not need to re-train/fine-tune the quantized model [1,2,3,4,5]. To reduce the performance gap between the quantized model and its full precision counterpart, quantization-aware training (QAT) fine-tunes the quantized model on the training dataset [6,7,8,9,10,11,12,13,14]. However, QAT computes the gradients in full precision, so the training phase is not accelerated. \n\nTraining quantization methods, also known as fully quantized training (FQT), further quantize the gradients, compared with QAT. In FQT, all the activations, weights, and gradients are quantized in both the forward and backward propagation. Hence, training can be implemented efficiently on low-bitwidth computational units, such as tensor cores [15]. Low-bitwidth hardware is faster and more power-efficient, as compared to FP32 counterparts. As the need for training huge models continues to grow [16,17,18], there has been increasing attention on FQT. Earlier work on FQT includes mixed-precision FP16/FP32 training [19] and lossy 2-bit training [6]. Recently, 8-bit FQT has emerged as a sweet spot on the accuracy versus efficiency tradeoff. Various 8-bit numerical formats have been proposed, including INT8 [20,21,22,23], FP8 [24,25],",
            "score": 0.7236019415349751,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 356
                },
                {
                    "start": 359,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1948
                }
            ],
            "ref_mentions": [
                {
                    "start": 788,
                    "end": 791,
                    "matchedPaperCorpusId": "67750088"
                },
                {
                    "start": 793,
                    "end": 795,
                    "matchedPaperCorpusId": "59413897"
                },
                {
                    "start": 987,
                    "end": 989,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 991,
                    "end": 993,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 993,
                    "end": 996,
                    "matchedPaperCorpusId": "12130431"
                },
                {
                    "start": 996,
                    "end": 999,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1607,
                    "end": 1611,
                    "matchedPaperCorpusId": "167217261"
                },
                {
                    "start": 1611,
                    "end": 1614,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1727,
                    "end": 1731,
                    "matchedPaperCorpusId": "2547043"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82177734375
        },
        {
            "corpus_id": "272593227",
            "title": "Quantized neural network for complex hologram generation",
            "text": "Quantization in a neural network is a technique that reduces the bit precision of weights and activations. Specifically, quantization converts FP32, used in most machine learning frameworks for model parameters, to lower-precision formats such as 16-bit floating-point precision (FP16), 8-bit or 4-bit integers precision (INT8 and INT4), and ternary or binary values. Reducing the bit widths benefits the model size and memory footprint. Furthermore, computational speed increases if the processor supports high-throughput math pipelines in low-bit formats. However, reducing bit precision may reduce accuracy, particularly for small models. Nonetheless, most neural network models are over-parameterized, meaning that appropriate quantization can reduce the bit widths without significant accuracy degradation. We begin by introducing quantization and dequantization operations on model weights and activations. Here, we focus on uniform integer quantization, where a floating point number  is mapped to a -bit signed integer \n\n. The quantization function is defined as: \n\nwhere round() rounds to the nearest integer, and  and  are quantization parameters called the scaling factor and zero-point, respectively. The reconversion from an integer to a floating-point number uses a dequantization function defined as: \n\nwhere  is determined by the ratio of the input and output ranges as \n\nwhere [, ] is the clipping range of the input (i.e., the boundaries of the permissible inputs). The process of selecting a clipping range is often referred to as calibration.  is chosen to be zero for symmetric quantization and nonzero for asymmetric quantization. Asymmetric quantization uses \n\nQuantized weights and activations enable multiply-accumulate (MAC) operations with low precision, which is computationally intensive in most neural networks. \n\nThe clipping ranges of weights can be computed statically, but those of activations vary for each input and can be determined either dynamically or statically. Dynamic quantization calculates the range for each activation during inference and quantizes it on the fly. By contrast, static quantization calculates the range using a representative dataset (i.e., calibration dataset) before inference to pre-quantize the activations.",
            "score": 0.7229759544303527,
            "section_title": "Neural network quantization",
            "char_start_offset": 8411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1026
                },
                {
                    "start": 1029,
                    "end": 1071
                },
                {
                    "start": 1074,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1385
                },
                {
                    "start": 1388,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1841
                },
                {
                    "start": 1844,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2111
                },
                {
                    "start": 2112,
                    "end": 2274
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84423828125
        },
        {
            "corpus_id": "266421411",
            "title": "Deployment of Machine Learning Algorithms on Resource-Constrained Hardware Platforms for Prosthetics",
            "text": "With the success of deep neural networks and their everincreasing sizes, the quantization of neural networks has emerged as a fundamental technique to reduce model size and memory footprints. Remarkable progress in this area has led to quantized neural networks achieving similar levels of accuracy as their full-precision counterparts [14], [15], [17]. In neural network quantization, three key components can be targeted: weights, activations, and gradients [40]. In this study, our focus is on quantizing weights and activations. \n\nWhile various approaches have explored 4-bit [41], [42], binary [43], [44], [45], [46], [47], and adaptive [48], [49] quantization, our work centers on 8-bit quantization, which is widely supported by most microcontrollers (MCUs). The quantization of parameters offers the following advantages:  [50], [51], or done without re-training, a process that is often referred to as Post-Training Quantization [42], [52], [53]. In this work, we use the Quantization-Aware Training [40], [54] since it has proven to achieve higher accuracy value [41].",
            "score": 0.7226617771184808,
            "section_title": "B. NETWORK QUANTIZATION",
            "char_start_offset": 6669,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1078
                }
            ],
            "ref_mentions": [
                {
                    "start": 336,
                    "end": 340,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 342,
                    "end": 346,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 348,
                    "end": 352,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 586,
                    "end": 590,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 599,
                    "end": 603,
                    "matchedPaperCorpusId": "1875476"
                },
                {
                    "start": 605,
                    "end": 609,
                    "matchedPaperCorpusId": "195601375"
                },
                {
                    "start": 611,
                    "end": 615,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 617,
                    "end": 621,
                    "matchedPaperCorpusId": "102352789"
                },
                {
                    "start": 623,
                    "end": 627,
                    "matchedPaperCorpusId": "11244259"
                },
                {
                    "start": 648,
                    "end": 652,
                    "matchedPaperCorpusId": "67732874"
                },
                {
                    "start": 831,
                    "end": 835,
                    "matchedPaperCorpusId": "221136343"
                },
                {
                    "start": 837,
                    "end": 841,
                    "matchedPaperCorpusId": "226203265"
                },
                {
                    "start": 938,
                    "end": 942,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 944,
                    "end": 948,
                    "matchedPaperCorpusId": "209531713"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "213004198"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77099609375
        },
        {
            "corpus_id": "195767218",
            "title": "Compression of Acoustic Event Detection Models With Quantized Distillation",
            "text": "In quantization experiments, we will experiment 8-bit and 4-bit quantization with training. We did not quantize the model with less than 4-bit, as it results in significant performance degradation. Both parameters and input are quantized to the target number of bit (8 or 4) except that the cell state (Ct in equation 5) will be quantized into 16 bits. Empirically we find quantizing Ct to low-bits leads to divergence in training. This may be related to its potential unbounded value. Such behavior on quantizing recurrent neural networks is also reported in [13]. \n\nFor all experiments we use Adam optimizer with learning rate of 0.001 and batch size of 64. We tuned penalty on positive loss (wc in equation 1) on validation set and found setting it to be the ratio between positive and negative examples of each class in the training set gives overall best results. This practice also prevents us from tuning wc for every class. \n\nEvaluation Metric We evaluate the performance of models based on AUC (area under curve) on ROC curve (true positive rate vs. false positive rate) and EER (equal error rate) on DET curve (false negative rate vs. false positive rate). Higher AUC and lower EER indicate better performance. We report their values on individual events as well as average over all three events. To evaluate the compression effect, we measure number of parameters (in Millions), size of parameters (in MB) and number of floating point operations (FLOPs). FLOPs measure the amount of computation while the first two metrics measure storage size of the model.",
            "score": 0.718537588977001,
            "section_title": "Experimental Setting",
            "char_start_offset": 11731,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1568
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.896484375
        },
        {
            "corpus_id": "244391498",
            "title": "Integer-Only CNNs with 4 Bit Weights and Bit-Shift Quantization Scales at Full-Precision Accuracy",
            "text": "\u2022 \n\nWe present an extensive literature overview of uniform and nonuniform quantization for fixed-point inference; \u2022 A novel modification to a neural network compute engine is introduced to improve the accuracy of models with 4 bit weights and 8 bit activations, in conjunction with bit-shift-based scaling, through the aid of a lookup-table; \u2022 A quantization-aware training method is proposed to optimize the models that need to run on our proposed compute engine; \u2022 We are the first to make a fair empirical comparison between the performance of (uniform) quantized models with full-precision and power-of-two scales with either per-layer or per-channel quantization using 4 bit weights; \u2022 Our source code has been made publicly available https://gitlab.com/EAVISE/lutmodel-quantization (accessed on 16 November 2021). \n\nThe remainder of this paper is organized as follows: Section 2 presents an extensive literature overview of quantization in greater detail, organized into different topics for convenience. For each topic, we also highlight the choices we made for our own approach. Our proposed method is explained in Section 3; our results are presented in Section 4; conclusions are made in final Section 5.",
            "score": 0.7173546371365239,
            "section_title": "Introduction",
            "char_start_offset": 4330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 1
                },
                {
                    "start": 4,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 819
                },
                {
                    "start": 822,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1214
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9599609375
        },
        {
            "corpus_id": "245351885",
            "title": "Quantization noise in low bit quantization and iterative adaptation to quantization noise in quantizable neural networks",
            "text": "Thus it was possible to quantize a MobileNetV2 [9] and EfficientNet-B0 [6] network at 4 bits, avoiding a significant drop in accuracy ( < 20%). In this case, for neural networks with a large number of parameters and at 8-bit quantization the method of iterative adaptation can give results worse than the basic FAT method, as seen in the example of ResNet-50 [10].",
            "score": 0.7142840122650151,
            "section_title": "Experiments and Results",
            "char_start_offset": 9718,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 364
                }
            ],
            "ref_mentions": [
                {
                    "start": 47,
                    "end": 50,
                    "matchedPaperCorpusId": "4555207"
                },
                {
                    "start": 71,
                    "end": 74,
                    "matchedPaperCorpusId": "167217261"
                },
                {
                    "start": 359,
                    "end": 363,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93603515625
        },
        {
            "corpus_id": "267312437",
            "title": "HEQuant: Marrying Homomorphic Encryption and Quantization for Communication-Efficient Private Inference",
            "text": "Quantization is one of the most impactful ways to decrease the computational time and energy consumption of neural networks. In neural network quantization, the weights and activation tensors are stored in lower bit precision than the 16 or 32-bit precision they are usually trained in. When moving from 32 to 8 bits, the memory overhead of storing tensors decreases by a factor of 4 while the computational cost for matrix multiplication reduces quadratically by a factor of 16. Neural networks have been shown to be robust to quantization [13,40], meaning they can be quantized to lower bit-widths with a relatively small impact on the network's accuracy. Neural network quantization is an essential step in the model efficiency pipeline for any practical use-case of deep learning [2,28,36,51].",
            "score": 0.7136149217691159,
            "section_title": "A. Network quantization",
            "char_start_offset": 27787,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 797
                }
            ],
            "ref_mentions": [
                {
                    "start": 541,
                    "end": 545,
                    "matchedPaperCorpusId": "232352683"
                },
                {
                    "start": 784,
                    "end": 787,
                    "matchedPaperCorpusId": "216036085"
                },
                {
                    "start": 790,
                    "end": 793,
                    "matchedPaperCorpusId": "244715141"
                },
                {
                    "start": 793,
                    "end": 796,
                    "matchedPaperCorpusId": "232223166"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.796875
        },
        {
            "corpus_id": "246823686",
            "title": "Neural-Network Decoders for Quantum Error Correction Using Surface Codes: A Space Exploration of the Hardware Cost-Performance Tradeoffs",
            "text": "Using a fixed-point representation for the data in the NN instead of a floating-point representation can significantly save hardware cost, but at the price of decoding performance. Since there is no strategy to determine the optimal number of bits in the fixed-point representation [56], we explore how performance varies for different quantization levels. Ideally, the optimum number of bits for each node depends on the number of input nodes and the domain of the transfer function, and in principle can be different for node output, weights and biases. However, for simplicity of the study, we adopt the same number of bits to represent the outputs, the weights and the biases in all layers, except for the 1-bit global inputs and outputs of the NN. \n\nTo see the effect of quantization, we compare the per- formance of the neural network before and after quantization. This is done by dividing the quantized (fixed-point) p th by the floating-point p th . The floating point performance before quantization does include the regularization term that attracts the weights to certain quantization levels. These results are shown in Fig. 16, plotting the performance degradation due to the quantization for all different configurations of layer sizes and quantization levels used in weight regularization. For layer sizes this is between 4 and 256 in the first hidden layer and between 4 and 64 in the second hidden layer. The weight regularization levels are varied between 4 and 256 levels (2 and 8 bits). The x-axis shows the amount of bits used for representing the data during evaluation. A couple of trends can be observed. First, 9 bits are enough for most configurations to reach floating point performance (a value of 1). Next, larger distances need more bits for the same performance degradation. These trends are captured in the thick lines as a cumulative average performance degradation. These lines represent the average degradation in performance when going down to less bits. First all point at 9 bits are averaged for a certain distance. Then every following step down in bits, we subtract the average degradation of all lines in that distance. \n\nFrom the data [1] we also see that we need more bits to reach the MWPM performance for larger distances. For increasing distances we need 3, 4, 5 and 7 bits.",
            "score": 0.7094389645172287,
            "section_title": "D. Quantization",
            "char_start_offset": 41361,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2160
                },
                {
                    "start": 2163,
                    "end": 2267
                },
                {
                    "start": 2268,
                    "end": 2320
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79638671875
        },
        {
            "corpus_id": "233443785",
            "title": "Hessian Aware Quantization of Spiking Neural Networks",
            "text": "each layer did not lower the accuracy at all, while reducing the network size by 50%. This shows that full precision arithmetic is not needed. As expected, the accuracy dropped when lowering the bit-precision of all 3 layers from 16 to 8 and 4 bits, from 98.1% to 96.3% and 81.1% respectively. We also compared the accuracy when using a layer-wise bit-precision, with a (16,8) and (8, 4) bit combination. As indicated by the Hessian trace shown on table 1, we expected assigning L1 with a low bit-precision to have the least impact to accuracy, followed by L2 and L3. This was confirmed by the results, with the accuracy dropping from 98.0% to 97.9% and 97.6% as we moved an 8-bit quantizer from L1 to L3 in a 16-bit network. The same observation was found when moving a 4-bit quantizer from L1 to L3 in an 8-bit network, with the accuracy dropping from 95.0% to 94.0% and 93.6%. Finally we quantized the first two layers of the network with lower bit-precision, and found that the (8,8,16) combination provided a good balance between accuracy and network size, with the accuracy dropping by only 0.3% while the network size was reduced by 58%.",
            "score": 0.7077559717050941,
            "section_title": "EXPERIMENTS AND RESULTS",
            "char_start_offset": 13037,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 370,
                    "end": 374,
                    "matchedPaperCorpusId": "225041435"
                },
                {
                    "start": 987,
                    "end": 990,
                    "matchedPaperCorpusId": "225041435"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9814453125
        },
        {
            "corpus_id": "274137777",
            "title": "Advancing Question-Answering in Ophthalmology with Retrieval Augmented Generations (RAG): Benchmarking Open-source and Proprietary Large Language Models",
            "text": "Quantization is a process in machine learning aimed at reducing the precision of model parameters, such as weights and activations, from high-precision floating-point representations (commonly 32-bit, known as FP32) to lower precision formats, such as 16-bit, 8-bit, or even fewer bits Nagel et al. (2021). This technique enables e\"cient computation and storage, as many operations can be executed with lower precision without significantly compromising the model's performance. Empirical evidence indicates that 8-bit quantization 4. https://www.langchain.com maintains performance comparable to full-precision models Dettmers (2015). Further reduction to 4-bit quantization has been shown to o!er additional reduction in model size and inference speed, with minimal impact on performance Dettmers and Zettlemoyer (2023). \n\nIn this study, we applied quantization to the weights of the open-source models, converting them to a 4-bit integer (INT4) and 8-bit (INT8) representations (or Q4 and Q8, respectively) from full-precision (FP) float16 or float32. This transformation reduces the number of bits per parameter to 4, leading to a substantial reduction in both disk storage and GPU memory usage, approximately by 75% (e.g. Llama-3-70B's memory on disk can be reduced from 140GB (FP) to 40GB (Q4). This reduction is critical for deploying large-scale models in resource-constrained environments.",
            "score": 0.7062052228946959,
            "section_title": "Quantization",
            "char_start_offset": 16212,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1398
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82275390625
        },
        {
            "corpus_id": "255942284",
            "title": "RedBit: An End-to-End Flexible Framework for Evaluating the Accuracy of Quantized CNNs",
            "text": "Models can be quantized with memory savings of up to 32\u00d7 with minimal accuracy degradation. \n\nReducing the bit width of input activations down to 2 bits while also reducing weight bit widths results in minimal accuracy losses. The use of fewer bits helps reduce the computational cost of performing inference in quantized neural networks. input activations at a higher bit width is clearly noticeable here, especially on Figure 4b that shows the results for DoReFa-Net. Here, the blue bars at the left of the image correspond to the accuracy results obtained when the input activations were binarized. Figures 4c and 4d show the results obtained with QNN and DoReFa-Net, respectively, when quantizing ResNet-20 applied to CIFAR-10. Again, if the input activations are kept with bit widths greater than 1 bit, the accuracy results are consistent across all quantization plane. The accuracy loss due to quantization of weights is minimal all the way down to 3 bits. 2-and 1-bit quantization still offer acceptable results. This result continues to show the importance of input activations.",
            "score": 0.7037987271420444,
            "section_title": "KEY TAKEAWAY 1",
            "char_start_offset": 28512,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 94,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1087
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9296875
        },
        {
            "corpus_id": "235434086",
            "title": "MQBench: Towards Reproducible and Deployable Model Quantization Benchmark",
            "text": "Modern deep learning is increasingly consuming larger memory and computation to pursue higher performance. While large-scale models can be trained on the cloud, transition to edge devices during deployment is notoriously hard due to the limited resource budget, including latency, energy and memory consumption. For this reason various techniques have been developed to accelerate the deep learning inference, including model quantization [1,2,3,4,5], pruning [6,7,8,9,10], neural network distillation [11,12], lightweight network design [13], and weight matrix decomposition [14]. \n\nIn this work, we focus on model quantization for efficient inference. Quantization targets to map the (nearly) continuous 32-bit floating-point (FP) numbers into discrete low-bit integers. As a result, the neural networks could rely on the integer-arithmetic units to speed up the inference. In academic research, there is a trend towards steadily reducing the bit-width and maintaining the accuracy across a range of quantized network architectures on ImageNet. It is incredible that the even 3-bit quantization of both weights and activations can reach FP-level accuracy [15]. Exciting though the breakthrough is, there lacks a systematic study that whether these research works can really be applied to practice, and whether the major improvement is brought by the algorithm rather than the training techniques. \n\nWe point out two long-neglected key factors in quantization research, namely reproducibility and deployability. First, we observe that the training hyper-parameters can significantly affect the performance of a quantized network. As an example, Esser et al. [15]   rate [16] and better weight decay choice, improving the Top-1 accuracy of 2-bit ResNet-18 [17] by 0.7% and 0.4% on ImageNet. Full precision network pre-training can also boost quantization results [15,18]. The reproducibility issue has received considerable attention in other areas as well, e.g. NAS-Bench-101 [19]. So far, there lacks a benchmark that unifies training pipelines and compares the quantization algorithms in a thorough and impartial sense.",
            "score": 0.7026264414657523,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1398
                },
                {
                    "start": 1401,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 444,
                    "end": 446,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 446,
                    "end": 448,
                    "matchedPaperCorpusId": "15817277"
                },
                {
                    "start": 448,
                    "end": 450,
                    "matchedPaperCorpusId": "184487878"
                },
                {
                    "start": 460,
                    "end": 463,
                    "matchedPaperCorpusId": "5750817"
                },
                {
                    "start": 463,
                    "end": 465,
                    "matchedPaperCorpusId": "7057040"
                },
                {
                    "start": 467,
                    "end": 469,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 469,
                    "end": 472,
                    "matchedPaperCorpusId": "11169209"
                },
                {
                    "start": 506,
                    "end": 509,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 538,
                    "end": 542,
                    "matchedPaperCorpusId": "4555207"
                },
                {
                    "start": 1157,
                    "end": 1161,
                    "matchedPaperCorpusId": "67788003"
                },
                {
                    "start": 1659,
                    "end": 1663,
                    "matchedPaperCorpusId": "67788003"
                },
                {
                    "start": 1756,
                    "end": 1760,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 1863,
                    "end": 1867,
                    "matchedPaperCorpusId": "67788003"
                },
                {
                    "start": 1867,
                    "end": 1870,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 1977,
                    "end": 1981,
                    "matchedPaperCorpusId": "67856022"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8271484375
        },
        {
            "corpus_id": "254408710",
            "title": "Approximations in Deep Learning",
            "text": "For fixed-point arithmetic, [LTA16] explored the use of various bit width combinations (4, 8 and 16 bits) of weights and activations. Notable results with integer arithmetic are presented in [JKC + 18], which showcases how 8-bit integer quantization on ARM CPUs can achieve nearidentical accuracy compared to baseline float32 models based on MobileNet architectures for classification and detection tasks, but with improved on-device latency. Good quantization results with 4-bit weights and activations are presented in [BNS19] by combining three complementary methods for minimizing quantization error at the tensor level. Heterogeneous/mixed-precision quantization approaches also heavily focus on integer/fixed-point formats [WWZ + 18, WLL + 19, DYG + 19, DYC + 19, CYD + 20]. \n\nOne problem with low precision integer/fixed-point formats is that they have limited dynamic range, which might make them inappropriate, especially for networks used in Natural Language Processing (NLP) tasks, where weights tend to have values that are more than 10\u00d7 larger than the largest magnitude values found in popular CNNs [TYW + 20, Fig. 1]. While not that widespread, there has been some work looking into low precision floating-point quantization for CNN inference. For instance, [SBD + 18] explores the use of up to 8-bit (scaled) floating-point formats for weight and activation quantization in classification networks such as GoogLeNet, ResNet, and MobileNet, without any accuracy degradation. More recently, [WWC + 20, WWL + 20] show how an 8-bit floating-point quantization format (4-bit mantissa and 3-bit exponent) can be used in FPGA-based accelerators for deep CNN inference, without any retraining. Another approach [TYW + 20] consists of an adaptive floating-point quantization method, where the exponent range of quantized values is dynamically shifted at each network layer (through changing the bias term of the exponent), yielding competitive results on NLP networks and tasks.",
            "score": 0.7020653510315704,
            "section_title": "Quantization formats.",
            "char_start_offset": 29752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 780
                },
                {
                    "start": 783,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1985
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 35,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 521,
                    "end": 528,
                    "matchedPaperCorpusId": "59292009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91796875
        },
        {
            "corpus_id": "253410123",
            "title": "SCA: Search-Based Computing Hardware Architecture with Precision Scalable and Computation Reconfigurable Scheme",
            "text": "Typically, the inference and training of neural network algorithmsemploy floatingpoint precision to ensure a higher level of accuracy. However, high precision computation requires large computing logic resources and more cycles for hardware implementation. To reduce computation and memory resources, an increasing number of designs based on model quantization have been proposed which quantize the model to 8 bits or even lower to 4 bits [35][36][37]. For example, ref. [35] proposed a parametric clipping activation technology PACT, which used the activation clipping parameter \u03b1 optimized during training to find the right quantization scale, with negligible accuracy loss (only 0.3% degradation in ResNet50). \n\nAs shown in Figure 2, through a series of experiments, neural network models were quantized to 4 bits with almost no accuracy loss, where the quantized model greatly reduced the hardware overhead and relieved the bandwidth pressure.",
            "score": 0.6992965364260711,
            "section_title": "Neural Network under Quantization",
            "char_start_offset": 7979,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 947
                }
            ],
            "ref_mentions": [
                {
                    "start": 447,
                    "end": 451,
                    "matchedPaperCorpusId": "50784025"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "225583435",
            "title": "Face Recognition and Machine Learning at the Edge",
            "text": "Figure 1 compares the difference in model size after different bit-width quantization are applied on the weights in a single layer. Model size reduced most after apply 4-bit fixed point quantization in all of the layers compared with 8-bit and 16bit because the weights are stored in lower precision format which means it consumes less memory storage. For 4-bit fixed-point quantization, the accuracy drops to 72.5% and 85% from the original 90% after quantization on the weights in the layer 'fire8-expand3x3' and 'fire9expand3x3'. The accuracy drops the to the lowest which is 42.5% after 4-bit fixed point quantization is being applied on the weights in the layer 'fire6-expand3x3'. This might due to the large value changes in total weight after quantization. Although the model size is compressed the most after applying weight quantization in the layer 'fire8-expand3x3', but there is 17.5% accuracy drop. Hence, it is recommended to apply 4-bit quantization on weights in layer 'fire-9-expand3x3' when doing 4-bit fixed point quantization on the weights in a single layer of the network as it causes a slight drop in accuracy while reducing the model size. For 8-bit and 16-bit fixed-point quantization, there is no change even when weight quantization is applied on any of the layers. Hence, it is recommended to apply 8-bit or 16-bit fixed point quantization on the weights in layer 'fire-8-expand3x3' when implementing weight quantization in a single layer as they can help to reduce model size efficiency.",
            "score": 0.6981499634369914,
            "section_title": "RESULTS AND DISCUSSION",
            "char_start_offset": 15210,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1516
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98193359375
        },
        {
            "corpus_id": "67855262",
            "title": "Learning low-precision neural networks without Straight-Through Estimator(STE)",
            "text": "Finally, we quantized the well-known neural networks, Mo-bileNet 1.0 224 v1 and ResNet 50 v1/v2, using 4-bit weights and 4-bit activations. The 4 th column in table 3 is for perlayer quantization, whose accuracy is 5.5% lower than fp32's in average. The per-channel quantization in the 5 th column has 4.66% accuracy loss. AB's INT4-4 result, using perchannel quantization, achieves similar accuracy as the TF4-8 scheme [Krishnamoorthi, 2018], which has 4-bits weight and 8-bits activation as shown in the 3 rd column.",
            "score": 0.6977394803174284,
            "section_title": "Evaluation of 4-bits weight and 4-bits activation (INT4-4)",
            "char_start_offset": 17764,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.890625
        },
        {
            "corpus_id": "210116550",
            "title": "Least squares binary quantization of neural networks",
            "text": "In this section we apply post-training quantization to the weights of a pre-trained full precision network. We then use the quantized network for inference and report the classifi- cation accuracy. This procedure can result in an acceptable accuracy for a moderate number of bits (e.g., 16 or 8). However, the error significantly grows with a lower number of bits, which is the case in this experiment. Therefore, we only care about the relative differences between different quantization strategies. This experiment demonstrates the effect of quantization errors on the accuracy of the quantized DNNs. The results are shown in Table 1. When the least squares 2-bits quantization is used, significant accuracy improvement (more than one order of magnitude) is observed compared to the greedy 2-bits quantization, which illustrate the effectiveness of the optimal quantization.",
            "score": 0.6977389743617163,
            "section_title": "Post-training quantization",
            "char_start_offset": 19717,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9287109375
        },
        {
            "corpus_id": "273963038",
            "title": "When are 1.58 bits enough? A Bottom-up Exploration of BitNet Quantization",
            "text": "\u2022 A bottom-up exploration of 1.58-bit quantization-aware training, ranging from the X-OR problem (Section 3.1) and 1.58-bit multi-layer perceptrons for text classification (Section 3.2) to 1.58-bit graph neural networks for node classification (Section 3.3). \n\n\u2022 Experiments on 1.58-bit encoder-only language models showing that an increase in model capacity can compensate for the lower bit precision -yet sub-proportionally to the decrease in bit width (Section 4.1). \n\n\u2022 Experiments on 1.58-bit encoder-decoder language models demonstrated superior performance on the 1.58-bit variant compared to their 16-bit counterpart, without the the need for parameter-compensating (Section 4.2). \n\n2 Background: b1.58 Quantization \n\nWe recapitulate the basics of 1.58-bit quantization proposed by Nielsen and Schneider-Kamp (2024), which generalizes the one from (Wang et al., 2023). The core of the 1.58-bit quantization scheme is to introduce a drop-in replacement for the Linear layers in common machine learning frameworks such as PyTorch, which we denote as BitLinear. \n\nDuring training, the BitLinear layer retains 16-bit shadow weights. During the forward pass, the shadow weights are quantized to 1.58-bit precision which corresponds to ternary weights: {\u22121, 0, 1}. During the backward pass, the shadow weights are optimized via the straight-through estimator (Bengio et al., 2013). Because forward passes are always conducted with quantized weights, we can drop the shadow weights when training concludes, using solely the ternary weights during inference. \n\nThe computation flow of a BitLinear layer follows a five-step procedure: First, the activations are normalized via a parameter-free Layer-Norm (Ba et al., 2016). We denote the layer normalization of input I as \u00ce. \n\nSecond, the layer normalized activations are quantized to k-bit precision (usually k = 8) via AbsMax quantization. We first calculate a scaling factor x scale such that",
            "score": 0.6949306041890564,
            "section_title": "Introduction",
            "char_start_offset": 3531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 261,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 688
                },
                {
                    "start": 691,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1066
                },
                {
                    "start": 1069,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1558
                },
                {
                    "start": 1561,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1773
                },
                {
                    "start": 1776,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 1944
                }
            ],
            "ref_mentions": [
                {
                    "start": 790,
                    "end": 823,
                    "matchedPaperCorpusId": "271212141"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.349853515625
        },
        {
            "corpus_id": "166227926",
            "title": "Feature Map Transform Coding for Energy-Efficient CNN Inference",
            "text": "based on transform-domain quantization of the neural network activations followed by lossless variable length coding. The method does not require neither backpropogation nor training data except for a one calibration batch. We demonstrate that this approach reduces memory bandwidth by 40% when applied in the post-training regime (i.e., without fine-tuning) with small computational overhead and no accuracy degradation. Relaxing the accuracy requirements increases bandwidth savings to 60%. Moreover, we outperform previous methods in term of number bit per value with minor accuracy degradation. A detailed evaluation of various ingredients and parameters of the proposed method is presented. We also demonstrate a reference hardware implementation that confirms a reduction in memory energy consumption during inference. I I . R E L AT E D W O R K a) Quantization: Low-precision representation of the weights and activations is a common means of reducing com-putational complexity. On appropriate hardware, this typically results in the reduction of the energy footprint as well. It has been demonstrated that in standard architectures quantization down to 16 [15] or 8 bits [20,22,42] per parameter is practically harmless. However, further reduction of bitwidth requires non-trivial techniques [26,44], often with adverse impact on training complexity. Lately, the quantization of weights and activations of neural networks to 2 bits or even 1 [18,30] has attracted the attention of many researchers. While the performance of binary (i.e., 1-bit) neural networks still lags behind their full-precision counterparts [10,29], existing quantization methods allow 2-4 bit quantization with a negligible impact on accuracy [6,7,11].\n\nQuantizing the neural network typically requires introducing the quantizer model at the training stage. However, in many applications the network is already trained in full precision, and there is no access to the training set to configure the quantizer. In such a post-training regime, most quantization methods employ statistical clamping, i.e., the choice of quantization bounds based on the statistics acquired in a small test set. Migacz [25] proposed using a small calibration set to gather activation statistics and then randomly searching",
            "score": 0.693862653016537,
            "section_title": "I . I N T R O D U C T I O N",
            "char_start_offset": 3747,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1164,
                    "end": 1168,
                    "matchedPaperCorpusId": "2547043"
                },
                {
                    "start": 1179,
                    "end": 1183,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1300,
                    "end": 1304,
                    "matchedPaperCorpusId": "3455952"
                },
                {
                    "start": 1304,
                    "end": 1307,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 1450,
                    "end": 1454,
                    "matchedPaperCorpusId": "15817277"
                },
                {
                    "start": 1454,
                    "end": 1457,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 1625,
                    "end": 1628,
                    "matchedPaperCorpusId": "145846291"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "57573783",
            "title": "DSConv: Efficient Convolution Operator",
            "text": "Our method is designed to produce accurate results even when training data is not available, by quantizing from a pre-trained network. The second and fifth rows of Table 3 show that for both the compression and fast inference problems, no loss of accuracy can be achieved with 8-bit networks even with very high block sizes, as already demonstrated by previous papers and real-life applications [26,16]. The results also shows that compression down to 4-bits (which in convolutions with channel size input of 256 would yield a 5x compression rate) results in an accuracy drop of only 1% to 2% depending on the architecture. It can also be seen that very low-bit quantizations become noticeably unstable, varying greatly with architecture. At the extreme, using 2bits, losses vary by as much as -40% for GoogLeNet and only -11% for ResNet50. \n\nThe last four rows show the results for the fast inference problem. Also as known in previous research papers [26,16], models of 8/8 bits lose only around 0.1% accuracy. For models of 5/5 and 4/4, we get a drop of 1% to 3% in accuracy. To our knowledge, this is the smallest bit-width for fast inference that has been reported when models are neither retrained nor adapted. \n\nThe variance with respect to architecture suggests that quantization for 5 or less bits is unstable. However, even for fast-inference with 8-bit accuracy, it can achieve stable and satisfactory results within 1% of the full precision model. Accuracy with respect to Block Size Table 4 shows the accuracy with respect to block size. The table shows the results of quantizing the weights only, where the number in parenthesis represents the bit-width of the weights. Naturally, this represents a trade-off between memory and computational load against precision of the network. The largest discrepancy in accuracy can be seen in models that use 3 or 2 bit weights. For example, the GoogLeNet model with 3bits improves its Top1 accuracy from 5.7% to 56.8% when changing from a block-size of 256 to 8.",
            "score": 0.6935088675822614,
            "section_title": "Accuracy Before Retraining or Adaptation",
            "char_start_offset": 21747,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1216
                },
                {
                    "start": 1219,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2016
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 402,
                    "matchedPaperCorpusId": "51610353"
                },
                {
                    "start": 957,
                    "end": 960,
                    "matchedPaperCorpusId": "51610353"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9765625
        },
        {
            "corpus_id": "273501713",
            "title": "Evaluating Quantized Large Language Models for Code Generation on Low-Resource Language Benchmarks",
            "text": "The results suggest that 4-bit integer quantization provides the best balance between model performance and model size. \n\nIt is consistent with a conclusion made in the earlier study that evaluated quantized LLMs on general reasoning and knowledge tasks [16]. Furthermore, while still being smaller in size, quantized 4-bit models with 7 billion parameters performed better than non-quantized half-precision models with 3 billion or less parameters. \n\nOn the other hand, 2-bit integer quantization resulted in a significant performance degradation. In the extreme case of 2-bit CodeGemma, there was a complete breakdown in the model's ability to generate coherent responses. This is likely an effect of hallucination [38; 39]. The low precision rounding likely impacted the model's next token prediction ability (underlying probability distributions) resulting in a sequence of repetitive out-of-context tokens. \n\nAccording to [14], StarCoderBase 1B, StarCoderBase 15B, and CodeLlama 34B demonstrated MultiPL-E pass@1 percentages of 12.1, 26.6, and 43.9 for Lua. In another study [33], the InCoder 6.7B, CodeGen 16.1B, and Codex 12B models demonstrated MultiPL-HumanEval pass@1 rates of approximately 0.05, 0.08, and 0.41 for Lua. In the same study, the corresponding pass@1 rates in the MultiPL-MBPP Lua benchmark were 0.13, 0.09, and 0.49. These values can be compared with the pass@1 rates in Table 4. The 4-bit and 8-bit quantized models with 7B parameters generally do not perform much worse than the non-quantized models in [14; 33] 7: The results of the linear regression on inference time with benchmarks, model, q-bits, and correctness as nominal predictors. The regression model includes all two-way interactions but only the significant interactions are listed in the table. The overall results suggest that code LLMs quantized at 4-bit integer precision can be comfortably run on an average CPU-only consumer laptop while maintaining good performance relative to other quantized and non-quantized code LLMs.",
            "score": 0.6931744480807701,
            "section_title": "Discussion",
            "char_start_offset": 40279,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 122,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 449
                },
                {
                    "start": 452,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 2019
                }
            ],
            "ref_mentions": [
                {
                    "start": 1080,
                    "end": 1084,
                    "matchedPaperCorpusId": "258205341"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74560546875
        },
        {
            "corpus_id": "271962800",
            "title": "DCT-CryptoNets: Scaling Private Inference in the Frequency Domain",
            "text": "A.1 Quantization Ablation on ImageNet To examine the impact of quantization on performance, an ablation study was performed varying quantization levels on ImageNet, as detailed in Table 5. Adopting 5-bit quantization, consistent with the approach in SHE [21], resulted in a notable accuracy improvement compared to 4-bit quantization, with a manageable increase in latency of approximately 25%. Further increasing the quantization level to 8-bit yielded only marginal accuracy gains, while incurring a substantial increase in latency. These results suggest that 5-bit quantization strikes an effective balance between accuracy and efficiency for large-scale datasets like ImageNet. Encryption of the neural network requires careful selection of cryptographic hyperparameters to balance accuracy and latency. Starting with our pre-selected values for ImageNet (precision rounding of 7 and PBS error probability of 0.01), as detailed in Table 7, we systematically explore the effects of varying these parameters. Reducing either parameter leads to a significant drop in top-1 accuracy. Increasing these parameters generally yields a noticeable increase in latency, with only marginal accuracy gains. Notably, raising the bit-precision rounding to 8 and halving the PBS probability increases top-1 accuracy on ImageNet subsamples by up to 1.2% but incurs a threefold increase in latency.",
            "score": 0.6922577633427636,
            "section_title": "A Appendix",
            "char_start_offset": 30074,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1384
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 258,
                    "matchedPaperCorpusId": "202781343"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9326171875
        },
        {
            "corpus_id": "53047435",
            "title": "To Compress, or Not to Compress: Characterizing Deep Learning Model Compression for Embedded Inference",
            "text": "Figure 3b compares the inference time when using different bit widths to represent a 32-bit floating number for neural network weights. Intuitively, a smaller model should run faster. However, data quantization does not shorten the inference time but prolongs it. The reasons are described as follows. Data quantization can speedup the computation (i.e., matrix multiplications) performed on some of the input data by avoiding expensive floating point arithmetics and enabling SIMD vectorization by using a compact data representation. However, we found that the overhead of the de-quantization process during inference can outweigh its benefit. Besides the general inference operation, a data quantization and dequantization function has to be added into the compressed model. Inference performed on a quantized model accounts for 59.9% of its running time. The de-quantization functions converts input values back to a 32-bit representation on some of the layers (primarily the output layer) in order to recover the loss in precision. As can be seen from Figure 7, this process could be expensive, contributing to 30% to 50% of the inference time. \n\nUsing fewer bits for representation can reduce the overhead of de-quantization. For example, using a 6-bit representation is 1.05x and 1.03x faster than using a 16-bit and a 8-bit representations, respectively. However, as we will demonstrate later when discussing Figure 3c, using fewer bits has the drawback of causing larger degradation in the prediction accuracy. Hence, one must carefully find a balance between the storage size, inference time, and prediction accuracy when applying data quantification. \n\nWe also find that the percentage of increased inference time depends on the neural network structure. Applying data quantization to Inception, the most complex network in our CNN tested set, will double the inference time. By contrast, data quantization only leads to a 20% increase in inference time for Mobilenet, a compact model. This observation suggests that data quantization may be beneficial for simple neural networks on resource-constrained devices. \n\nIn contrast to quantization, Figure 4b shows that pruning leads to faster inference time across evaluated networks, because there is no extra overhead added to a pruned network.",
            "score": 0.6921838231082127,
            "section_title": "E. Impact on Inference Time",
            "char_start_offset": 13764,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1661
                },
                {
                    "start": 1664,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2123
                },
                {
                    "start": 2126,
                    "end": 2303
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91796875
        },
        {
            "corpus_id": "265295025",
            "title": "Low-Precision Floating-Point for Efficient On-Board Deep Neural Network Processing",
            "text": "Much work in recent years has focused on reducing the bit-width of data and arithmetic in DNN models without impacting task accuracy. Quantization to 8-bit [18] and sub 8-bit [34] integer formats has been shown to match singleprecision baselines on convolutional networks for several computer vision tasks. In certain cases, it is even possible to go down to extremely low precisions, such as binary [8] and ternary [23] DNNs. \n\nOn the FP arithmetic side, 16-bit formats such as half precision FP16 (5-bit exponent) and bfloat16 (8-bit exponent) have found success in both inference acceleration and mixedprecision DNN training [19]. More recently, 8-bit floatingpoint formats have also been explored in inference/training scenarios [21], [25], [32] and are starting to get hardware support (e.g. the NVIDIA Hopper architecture is an example). Sub 8-bit FP custom formats are also being studied in the literature in the context of model compression and inference acceleration [28]. \n\nTo apply such quantization schemes on a DNN model, it is necessary to adjust the parameters according to the target format. There are two main approaches to do this in practice. The first is Post-Training Quantization (PTQ), in which quantization is applied after training. While fast, it can lead to non-negligible loss in accuracy for extremely low bit width formats [2]. The other approach is to iteratively quantize the model during training. This process is known as Quantization-Aware Training (QAT). While much slower, it generally leads to better quantization results. For an in-depth overview of quantization in the context of DNNs the reader can consult [13].",
            "score": 0.6915426077662731,
            "section_title": "A. DNN Quantization",
            "char_start_offset": 2698,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 981
                },
                {
                    "start": 984,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1653
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 160,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 733,
                    "end": 737,
                    "matchedPaperCorpusId": "251710272"
                },
                {
                    "start": 976,
                    "end": 980,
                    "matchedPaperCorpusId": "221094303"
                },
                {
                    "start": 1353,
                    "end": 1356,
                    "matchedPaperCorpusId": "59292009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86865234375
        },
        {
            "corpus_id": "220891847",
            "title": "McDRAM v2: In-Dynamic Random Access Memory Systolic Array Accelerator to Address the Large Model Problem in Deep Neural Networks on the Edge",
            "text": "Quantization is one of the most effective compression techniques used for reducing the weight and activation bit-width of a neural network model, thereby resulting in reduced model size and execution cost in terms of energy, time, and silicon area. The general matrix multiply (GEMM) operation is most commonly used to perform inferences for DNNs. Although CPUs and GPUs generally process this GEMM operation using floating-point 32-bit precision without applying a quantization technique, previous studies have proposed numerous methods for quantizing DNN models for a variety of tasks in a range of domains, and have successfully attained a nearly lossless inference model, using less than 8-bit integer (int8) precision. \n\nFor CNNs, inferences with int8 precision has been widely adopted in an increasing number of real-world GPUs [17], [34]/NPUs [35], [37], as int8 precision has been shown to be sufficient for GEMM operation in the inference of various CNNs in recent studies. It has also been found that sub-8-bit quantization of the inference of a CNN does not lead to significant accuracy loss. For example, with the ImageNet classification dataset [27], [38] reports an activation 4-bit and weight 4-bit quantization of an ResNet-50 [10] with a 0.2% top-1 accuracy loss compared to the case of FP32, in which an activation 2-bit and weight 2-bit quantization was done and a 3.2% top-1 accuracy loss was obtained. \n\nAmong the recent studies concerned with the quantization on BERT [1], a model widely used in the field of NLP, [39] presented the state-of-the-art results. For example, in tasks of sentiment classification, natural language inference, named entity recognition, and machine reading comprehension with benchmarks of the general language understanding evaluation (GLUE) [40], Stanford Question Answering Dataset (SQuAD) [41], and CoNLL-03 [42], the activation 8-bit and weight 4-bit quantization precision yielded a comparable performance, without any significant quality degradation in the BERT model [1].",
            "score": 0.6909397440950684,
            "section_title": "B. LATEST QUANTIZATION RESULTS",
            "char_start_offset": 10232,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1422
                },
                {
                    "start": 1425,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 2028
                }
            ],
            "ref_mentions": [
                {
                    "start": 850,
                    "end": 854,
                    "matchedPaperCorpusId": "213621056"
                },
                {
                    "start": 856,
                    "end": 860,
                    "matchedPaperCorpusId": "4202768"
                },
                {
                    "start": 1158,
                    "end": 1162,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 1243,
                    "end": 1247,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "257365557",
            "title": "Scalable Object Detection on Embedded Devices Using Weight Pruning and Singular Value Decomposition",
            "text": "Model quantization is an effective compression technique that reduces the precision of weights and activations to reduce the model size. This technique involves converting high-precision data, such as 32-bit floating-point numbers, into low-precision data, such as 8-bit integers. While post-training quantization is the most common approach, it may lead to accuracy degradation in small models, particularly when there is a large deviation in the weight range of each channel or when an outlier weight exists. \n\nTo address these issues, quantization-aware training (QAT) has been developed, which simulates the quantization effect during the net propagation process of training [4]. During QAT, all weights and biases are stored as floating-point numbers, and backpropagation is performed as usual. At the time of inference, the weights and biases are converted into 8-bit integers and used in calculations. \n\nIn addition to 8-bit integer quantization, it is also possible to use even lower bit widths to represent weights and activations. For example, Ternary Weight Networks (TWN) use -1, 0, and 1 bits to represent weights [5], while Binary Neural Networks (BNN) use -1 and 1 bits [6]. XNOR-Net is a model that uses binary weight and input representations, which simplifies the convolution process to a scaling operation after the XNOR Gate operation [7]. This results in a 58x speedup compared to conventional convolution while saving 32x more memory. While the efficiency of these bit operators can be applied to specific custom hardware, it may not be applicable to all real hardware. In summary, bit quantization techniques offer significant benefits in terms of model size and computational efficiency. However, their applicability to real hardware may be limited. Thus, researchers must carefully consider the trade-offs between model size, computational efficiency, and hardware constraints when choosing a quantization technique for their specific use case.",
            "score": 0.688802052597115,
            "section_title": "Quantization",
            "char_start_offset": 3255,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 510
                },
                {
                    "start": 513,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 908
                },
                {
                    "start": 911,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1969
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85498046875
        },
        {
            "corpus_id": "233459296",
            "title": "Quantization and Deployment of Deep Neural Networks on Microcontrollers",
            "text": "First, a high variance is observable when we compare the accuracy results obtained on the three datasets versus the model size. This variability makes it difficult to draw any definitive conclusions. However, there is a trend in our results that provides some insights into performance for each experiment. \n\nAs has been shown, execution using fixed-point on 8-bit and 16-bit integers provides a significant decrease in the inference time, thus also reducing the average power consumption. As power consumption is a key parameter in embedded systems, shorter inference times are interesting as they make it possible either to reduce the microcontroller's operating frequency or to put the microcontroller in sleep mode for a longer period between two inferences. In addition, execution using 8-bit and 16-bit integers also provides a significant reduction in memory footprint. The memory required for the model parameters is divided by 4 and 2 for for 8-bit and 16-bit quantization, respectively. It is worth noting that the RAM usage, which is not illustrated here, is also reduced. \n\nOur results also show that performing inference using quantization with fixed-point on 16-bit integers does not lead to a drop in accuracy, whatever test case is considered. Moreover, inference using 16 bits does not require quantization-aware training to achieve such results. As both the power consumption and the memory footprint can be decreased, fixed-point quantization on 16-bit integers is therefore always preferable to 32-bit floating-point inference. \n\nConversely, 8-bit quantization does not provide a substantial improvement over 16-bit quantization. Moreover, 8-bit quantization requires performing quantization-aware training. It is worth noting that quantization-aware training for 8-bit quantization introduces more variance in the results over the baseline, and is also more sensitive to a change in the training parameters. As it is quite difficult to achieve a stable training, it is preferable to use an optimizer such as SGD with conservative parameters, instead of optimizers such as Adam or RAdam, to reduce the variance of the results, even though it means achieving a lower maximum accuracy.",
            "score": 0.6885642494173891,
            "section_title": "Discussion",
            "char_start_offset": 64102,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 306
                },
                {
                    "start": 309,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1547
                },
                {
                    "start": 1550,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2203
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89111328125
        },
        {
            "corpus_id": "233459296",
            "title": "Quantization and Deployment of Deep Neural Networks on Microcontrollers",
            "text": "Using this approach, it is possible to obtain a better resolution compared to a fixed-point representation for numbers near 0. This approach also allows large values to be represented, but at the cost of a lower resolution. The quantization step is determined by minimizing the quantization error at the output of the layer, thus balancing the precision and the dynamic range. As the implementation relies on bit shifts rather than on integer multiplications, this solution has some benefits in terms of resource usage and latency for an FPGA target. Additionally, results show that there is a slight degradation of accuracy when using the proposed non-uniform quantization versus a uniform quantization. \n\nUsing lower-precision computation for deep neural networks has been explored in [30] where the authors compare the test error rates on various image datasets for single-precision floating point, half-precision floating point, 20-bit fixed point and their own dynamic fixedpoint approach with 10 bits for activations and 12 bits for weights. In their work, it is worth noting that the training is also performed using lower-precision arithmetic. Training with fixed-point arithmetic was presented in [31] with 16-bit weights and 8-bit inputs, causing an accuracy loss of a few percent in the evaluation of text-to-speech, parity bit computation, protein structure prediction and sonar signal classification problems. In [32], the authors showed that on an Intel\u00ae E5640 microprocessor with an x86 architecture, using 8-bit integer instructions instead of floating-point instructions provided an execution speedup of more than 2, without a loss of accuracy, on a speech recognition problem. In this case the training was performed using single-precision floating-point arithmetic, and the evaluation was done after the quantization of the network parameters. \n\nThese prior works were however mostly not concerned with embedded computing on microcontrollers. Running deep neural networks on microcontrollers began to be popular in the last few years thanks to the rise of the Internet of Things and the improved efficiency of deep neural networks. \n\nIn [33], the authors emphasize that the instruction set architecture (ISA) of available microcontrollers can be a great limitation to running quantized neural networks.",
            "score": 0.6852640838044377,
            "section_title": "The State of the Art in Embedded Execution of Quantized Neural Networks",
            "char_start_offset": 10046,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 704
                },
                {
                    "start": 707,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1862
                },
                {
                    "start": 1865,
                    "end": 1961
                },
                {
                    "start": 1962,
                    "end": 2150
                },
                {
                    "start": 2153,
                    "end": 2321
                }
            ],
            "ref_mentions": [
                {
                    "start": 1206,
                    "end": 1210,
                    "matchedPaperCorpusId": "62248175"
                },
                {
                    "start": 1426,
                    "end": 1430,
                    "matchedPaperCorpusId": "15196840"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.751953125
        },
        {
            "corpus_id": "49356451",
            "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
            "text": "We note that at 8-bits of precision, post training quantization schemes provide close to floating point accuracy. In order to better understand the benefits of quantization aware training, we perform experiments to assess performance at 4 bit quantization for weights and activations. \n\nWe perform the following experiments: \n\n\u2022 Experiment 1: Per-channel quantization is significantly better than perlayer quantization at 4 bits. We show that per-channel quantization provides big gains over per-layer quantization for all networks. At 8-bits, the gains were not significant as there were sufficient levels to represent the weights with high fidelity. At four bits, the benefits of per-channel quantization are apparent, even for post training quantization (columns 2 and 3 of Table 5). \n\n\u2022 Experiment 2: Fine tuning can provide substantial accuracy improvements at lower bitwidths. It is interesting to see that for most networks, one can obtain accuracies within 5% of 8-bit quantization with fine tuning 4 bit weights (column 4 of Table 5). The improvements due to fine tuning are also more apparent at 4 bits. Note that activations are quantized to 8-bits in these experiments. \n\n\u2022 Experiment 3: Lower precision activations: We investigate the accuracies obtained with 4-bit activations for all layers with and without fine tuning. Note that activations are quantized on a per-layer basis. The weights are quantized at 8bits of precision with per-channel granularity. We note that fine tuning improves accuracy in this case also. The losses due to activation quantization are more severe than that of weight quantization (see Table 6). Note that the quantization granularity is different for activations and weights, so this is not a fair comparison of the impact of quantization. We hypothesize that quantizing activations We experiment with several configurations for training quantized models: Our first experiment compares stochastic quantization with deterministic quantization. Subse-quently, we study if training a quantized model from scratch provides higher accuracies than fine tuning from a floating point model.",
            "score": 0.6842441086209085,
            "section_title": "Lower Precision Networks",
            "char_start_offset": 19489,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 324
                },
                {
                    "start": 327,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2127
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98876953125
        },
        {
            "corpus_id": "278327593",
            "title": "Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques",
            "text": "Model Quantization reduces the numerical precision, i.e., number of bits required, for the model's parameters. A trained model consists of its graph structure and parameters (weights and activations). For a non-quantized model, the parameters are usually in the form of 32-bit (FP32) or 16-bit floats (FP16, BF16). Quantization reduces the numerical precision of these weights and, often, the activations, by storing them as 8-bit integers, 4-bit integers, or even single bits [36]. The lower bitprecision leads to a reduced memory footprint for the model (by 4x, 8x or even 32x compared to the FP32 representation) and speeds up inference as integer arithmetic can be more efficient than floating-point operations. This has the added benefit of reducing the energy required to serve the models. Quantization to INT8 or lower precision also enables deployment on hardware optimized for or restricted to integer operations. The primary trade-off of quantization is a potential drop in accuracy due to the reduced precision, but various techniques can minimize this loss. In the following section we discuss different quantization strategies and a few applications of quantization.",
            "score": 0.6834663156093116,
            "section_title": "B. Model Quantization",
            "char_start_offset": 8467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1179
                }
            ],
            "ref_mentions": [
                {
                    "start": 477,
                    "end": 481,
                    "matchedPaperCorpusId": "1518846"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "265457037",
            "title": "PIPE : Parallelized Inference Through Post-Training Quantization Ensembling of Residual Expansions",
            "text": "Deep neural networks (DNNs) achieve outstanding performance on several challenging computer vision tasks such as image classification [1], object detection [2] and semantic segmentation [3], as well as natural language processing benchmarks such as text classification [4]. However, their accuracy comes at a high computational inference cost which limits their deployment, more so on edge devices when real-time treatment as well as energy consumption are a concern. This problem can be tackled via DNN quantization, i.e. by reducing the bit-width representation of the computations from floating point operations (FP) to e.g. int8 (8-bits integer representation), int4, int3 or even lower-bit representation such as ternary (where weights values are either \u22121, 0 or +1) quantization. Because DNN inference principally relies on matrix multiplication, such quantization dramatically diminishes the number of bit-wise operations (as defined in [5]), thus limiting the DNN latency and energy consumption. However, DNN quantization usually comes at the expense of the network accuracy. As a consequence, DNN quantization is an active field of research [6,7,8,9,10,11,12,13] that aims at limiting this accuracy drop while reducing the number of bit-wise operations. \n\nAll the aforementioned methods are data-driven, as they either involve training a network from scratch or fine-tune an already trained and quantized one. However, while such approaches usually allow lower quantization errors using low bit-wise representations, due to the growing concerns on privacy rights and data privacy, there is an ever-increasing number of real-case scenarios (e.g. health and military services) where data may not be available for quantization purposes. Motivated by these observations, recently, several data-free quantization algorithms were published [14,15,16,17,18,19], focusing on the quantization operator, i.e. the transformation which maps the floating point weights to their low-bit, fixed point, values. However, these approaches still struggle to offer an interesting alternative to data-driven techniques in terms of accuracy preservation.",
            "score": 0.6833077405700039,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1262
                },
                {
                    "start": 1265,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2141
                }
            ],
            "ref_mentions": [
                {
                    "start": 1153,
                    "end": 1155,
                    "matchedPaperCorpusId": "3603886"
                },
                {
                    "start": 1159,
                    "end": 1162,
                    "matchedPaperCorpusId": "52920066"
                },
                {
                    "start": 1162,
                    "end": 1165,
                    "matchedPaperCorpusId": "4303893"
                },
                {
                    "start": 1859,
                    "end": 1862,
                    "matchedPaperCorpusId": "246863709"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83447265625
        },
        {
            "corpus_id": "233459296",
            "title": "Quantization and Deployment of Deep Neural Networks on Microcontrollers",
            "text": "In this work, we presented a framework to perform quantization and then deployment of deep neural networks on microcontrollers. This framework represents an alternative to the STM32Cube.AI proprietary solution and TensorFlow Lite for Microcontrollers, an opensource but complex environment. Inference time and energy efficiency measured on two different embedded platforms demonstrated that our framework is a viable alternative to the aforementioned solutions to perform deep neural network inference. Our framework also introduces a fixed-point on 16-bit integer post-training quantization which is not available with the two other frameworks. We have shown that this 16-bit fixed-point quantization provides an improvement over a 32-bit floating-point inference, while being competitive with fixed-point on 8-bit integer quantization-aware training. It provides a reduced inference time compared to floating-point inference. Moreover, the memory footprint is divided by two while keeping the same accuracy. The 8-bit quantization provides further improvements in inference time and memory footprint but at the cost of a slight decrease in accuracy and a more complex implementation. \n\nWork is still in progress to implement some optimization techniques for fixed-point on 8-bit integer inference. Three optimizations are especially targeted: per-filter quantization, asymmetric range and non-power-of-two scale factor. In addition, using SIMD instructions in the inference engine should help further decrease the inference time. These optimizations would therefore make our framework more competitive in terms of inference time and accuracy. Another possible improvement for fixed-point on integers inference consists of using 8-bit quantization for the weights and 16-bit quantization for the activations. TensorFlow Lite for Microcontrollers is currently in the process of implementing this technique. Mixed precision can indeed provide a way to reduce the memory footprint of layers that do not need a high-precision representation (using 8 bits for weights and activations), while keeping a higher precision (16-bit representation) for layers that need it. The CMix-NN [58] library already provides an implementation of convolution functions for various data type configurations (in 2, 4 and 8 bits). To further improve power consumption and memory footprint, binary neural networks can also be considered.",
            "score": 0.6799922500676608,
            "section_title": "Conclusions",
            "char_start_offset": 69603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2163
                },
                {
                    "start": 2164,
                    "end": 2307
                },
                {
                    "start": 2308,
                    "end": 2413
                }
            ],
            "ref_mentions": [
                {
                    "start": 2176,
                    "end": 2180,
                    "matchedPaperCorpusId": "216187122"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84912109375
        },
        {
            "corpus_id": "1125974",
            "title": "Hello Edge: Keyword Spotting on Microcontrollers",
            "text": "Neural networks are typically trained with floating point weights and activations. Previous research [33,34,35] have shown that fixed-point weights is sufficient to run neural networks with minimal loss in accuracy. Microcontroller systems have limited memory, which motivates the quantization of 32-bit floating point weights to 8-bit fixed point weights for deployment, thus reducing the model size by 4\u00d7. Moreover, fixed-point integer operations run much faster than floating point i=0 B i .2 i\u2212N , where N is the fractional length, which can also be negative. N is fixed for a given layer, but can be different in other layers. The weights are quantized to 8-bits progressively one layer at a time by finding the optimal N for each layer that minimizes the loss in accuracy because of quantization. After all the weights are quantized, the activations are also quantized in a similar way to find the appropriate fractional length N for each layer. Table 6 shows the accuracies of representative 8-bit networks quantized using this method and compared with those of the original full-precision networks. The table shows that the accuracy of the quantized network is either same or marginally better than the full-precision network, possibly due to better regularization because of quantization. We believe that the same conclusion will hold for the other neural network models explored in this work.",
            "score": 0.6748127706313662,
            "section_title": "Neural Network Quantization",
            "char_start_offset": 20804,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1402
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 105,
                    "matchedPaperCorpusId": "207233292"
                },
                {
                    "start": 105,
                    "end": 108,
                    "matchedPaperCorpusId": "207233273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72216796875
        },
        {
            "corpus_id": "247616592",
            "title": "Low-precision feature selection on microarray data: an information theoretic approach",
            "text": "In relation to inference accuracy, many studies have shown that it is possible to achieve the same results with reduced precision of weights and activations [14,24]. Regarding learning, Hubara et al. [18] introduced a method to train Quantized Neural Networks using extremely low precision and runtime activations, reaching an accuracy comparable to networks trained using 32 bits. The research of Yu et al. [39] presents a method of quantification with mixed data structure and proposes a hardware accelerator. This allows them to reduce the number of bits needed to represent neural networks from 32 to 5, also without affecting their accuracy. Banner et al. [3] introduced a 4-bit post training quantization approach with just a few percent accuracy degradation. Finally, the work of Sun et al. [33] shows that it is possible to train deep neural networks using only 4 bits with non-significant loss in accuracy while enabling significant hardware acceleration. \n\nWith regard to reducing energy consumption in feature selection, we can only find our own work in which we presented a limited bit depth mutual information that can be applicable to any feature selection method that uses internally the mutual information measure [25,27], which will be detailed in the following section.",
            "score": 0.6713734851458263,
            "section_title": "State of the art",
            "char_start_offset": 6455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1287
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 200,
                    "end": 204,
                    "matchedPaperCorpusId": "15817277"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "57189650"
                },
                {
                    "start": 798,
                    "end": 802,
                    "matchedPaperCorpusId": "227275152"
                },
                {
                    "start": 1230,
                    "end": 1234,
                    "matchedPaperCorpusId": "231929865"
                },
                {
                    "start": 1234,
                    "end": 1237,
                    "matchedPaperCorpusId": "216536413"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9365234375
        },
        {
            "corpus_id": "218628799",
            "title": "Bayesian Bits: Unifying Quantization and Pruning",
            "text": "To reduce the computational cost of neural network inference, quantization and compression techniques are often applied before deploying a model in real life. The former reduces the bit width of weight and activation tensors by quantizing floating-point values onto a regular grid, allowing the use of cheap integer arithmetic, while the latter aims to reduce the total number of multiply-accumulate (MAC) operations required by reducing the number of network parameters. We refer the reader to Krishnamoorthi (2018) and Kuzmin et al. (2019) for overviews of hardware-friendly quantization and compression techniques, respectively. \n\nIn quantization, the default assumption is that all layers should be quantized to the same bit width. While it has long been understood that low bit-width quantization can be achieved by keeping the first and last layers of a network in higher precision (Shayer et al., 2017;Choi et al., 2018), recent work (Dong et al., 2019b;Uhlich et al., 2020;Wang et al., 2019) has shown that carefully selecting the bit width of each tensor can yield a better trade-off between accuracy and complexity. Since the choice of quantization bit width for one tensor affects the quantization sensitivity of all other tensors, the choice of bit width cannot be made without regarding the rest of the network. \n\nThe number of possible bit width configurations for a neural network is exponential in the number of layers in the network. Therefore, we cannot exhaustively search all possible configurations and pick the best one. Several approaches to learning the quantization bit widths from data have been proposed, either during training (Uhlich et al., 2020;Louizos et al., 2017), or on pre-trained networks (Wang et al., 2019;Dong et al., 2019b;a). However, these works do not consider the fact that commercially available hardware typically only supports efficient computation in power-of-two bit widths (see e.g., Ignatov et al. (2019) for a mobile hardware overview and Moons et al. (2017) for a method to perform four 4-bit multiplications in a 16-bit hardware multiplication unit.) \n\nIn this paper, we introduce a novel decomposition of the quantization operation.",
            "score": 0.6703299225274832,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1324
                },
                {
                    "start": 1327,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 2105
                },
                {
                    "start": 2108,
                    "end": 2188
                }
            ],
            "ref_mentions": [
                {
                    "start": 888,
                    "end": 909,
                    "matchedPaperCorpusId": "44981793"
                },
                {
                    "start": 941,
                    "end": 961,
                    "matchedPaperCorpusId": "148571720"
                },
                {
                    "start": 961,
                    "end": 981,
                    "matchedPaperCorpusId": "204803866"
                },
                {
                    "start": 981,
                    "end": 999,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 1655,
                    "end": 1676,
                    "matchedPaperCorpusId": "204803866"
                },
                {
                    "start": 1676,
                    "end": 1697,
                    "matchedPaperCorpusId": "9328854"
                },
                {
                    "start": 1726,
                    "end": 1745,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 1745,
                    "end": 1764,
                    "matchedPaperCorpusId": "148571720"
                },
                {
                    "start": 1992,
                    "end": 2011,
                    "matchedPaperCorpusId": "22632506"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76611328125
        },
        {
            "corpus_id": "276784538",
            "title": "4bit-Quantization in Vector-Embedding for RAG",
            "text": "INT8 uses 8 bits integers instead of floating point numbers and integer computation instead of floating point computing, reducing both memory and computing requirements. INT8 quantization has been applied to many neural networks to decrease the computational time and energy consumption of neural networks [18]. In neural network quantization, the weights and activation tensors are stored in 8-bit precision than the 32-bit precision which were trained. When transforming from 32 to 8 bits, the memory overhead of storing tensors decreases by a factor of 4 while the computational cost for matrix multiplication reduces by a factor of 16 [19]. However, 8-bit quantization introduces quantization loss to the network that can lead to a drop in accuracy.",
            "score": 0.6703231962444127,
            "section_title": "B. INT8",
            "char_start_offset": 10600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 753
                }
            ],
            "ref_mentions": [
                {
                    "start": 306,
                    "end": 310,
                    "matchedPaperCorpusId": "251564521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78369140625
        },
        {
            "corpus_id": "240353756",
            "title": "RMSMP: A Novel Deep Neural Network Quantization Framework with Row-wise Mixed Schemes and Multiple Precisions",
            "text": "With tremendous success of the deep learning or deep neural networks (DNNs), there exist urgent needs for deployments of inference models onto edge-computing plat-* Equal contribution. forms/devices. As a major type of model compression technique, the DNN quantization becomes an essential method to reduce the computation, memory, and storage requirements in on-device inference, especially for platforms with capability of customized architecture design, such as FPGA devices and ASIC chips. Generally speaking, DNN quantization learns DNN models in low bit-width representation with accuracy performance close to that of the full-precision models, while accelerating inference speed. \n\nVarious quantization schemes have been investigated including binary [5,6,26,22], ternary [20,15,39], Fixed-point (Fixed) [38,4,12,16,3,11], Power-of-Two (PoT) [24,37,19,36], Additive Power-of-Two (APoT) [21], etc. Those schemes have diverse accuracy and hardware performance. Binary and ternary significantly reduce computation by eliminating multiplication operations, but experience relatively large accuracy loss (> 2% in general). Low bit-width Fixed quantization has better accuracy The proposed DNN quantization framework with row-wise mixed schemes and multiple precisions, which assigns quantization scheme and precision to filters of the weight tensor (or rows of the weight matrix). It features (i) layer-wise uniformality to fulfill the requirement of practical hardware implementation, (ii) row-wise flexibility for mixed schemes and multiple precisions, (iii) hardware-informative selection of candidate schemes and precisions (bit-widths) for significantly reducing the algorithm search space, and (iv) superior accuracy performance among the state-of-the-arts. \n\nperformance. For example, 4-bit Fixed can achieve negligible accuracy loss comparing with its 32-bit floating-point counterpart, although it still needs multiplication operations during inference computation. Different from binary, ternary, and Fixed, PoT is a nonlinear quantization scheme, where with quantization levels as power-of-two numbers, multiplications can be replaced with bit shifting operations, thereby reducing computation to speedup inference.",
            "score": 0.669803273193557,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 758,
                    "end": 761,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 763,
                    "end": 766,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 766,
                    "end": 769,
                    "matchedPaperCorpusId": "10533533"
                },
                {
                    "start": 783,
                    "end": 786,
                    "matchedPaperCorpusId": "52909828"
                },
                {
                    "start": 786,
                    "end": 789,
                    "matchedPaperCorpusId": "224893"
                },
                {
                    "start": 817,
                    "end": 820,
                    "matchedPaperCorpusId": "199577815"
                },
                {
                    "start": 820,
                    "end": 823,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 823,
                    "end": 825,
                    "matchedPaperCorpusId": "203605628"
                },
                {
                    "start": 853,
                    "end": 856,
                    "matchedPaperCorpusId": "12130431"
                },
                {
                    "start": 856,
                    "end": 859,
                    "matchedPaperCorpusId": "8840788"
                },
                {
                    "start": 859,
                    "end": 862,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "210848001"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89794921875
        },
        {
            "corpus_id": "272689153",
            "title": "Robust Training of Neural Networks at Arbitrary Precision and Sparsity",
            "text": "In addition to precision and sparsity, block size also presents a trade-off between accuracy and efficiency. We observe that its influence is more pronounced at extremely low precision levels, such as 1-bit. While using a smaller block size can improve performance, the effective bits per element can easily exceed the original design. We provide some comparisons here. Considering this trade-off, lower precision models may not always be more efficient (Table 7). Firstly, the accuracy achieved with smaller blocks in an A1W1 models may not surpass the accuracy achieved using A2W2. The perturbations introduced during lower precision training often remain high, hindering the achievement of high quality. Optimal selection needs to be made based on the underlying hardware support and the problem of interest. Our work facilitates a comprehensive study of this trade-off. \n\nThe parameter \u03bb suppresses the impact of perturbations and prevents training explosion, especially during the transition to 1-bit quantization. Standard quantization approaches often omit the use of \u03bb in an attempt to minimize quantization error. However, this can lead to training divergence in the early stages, as demonstrated in Fig. 1(a). We compared with the classic inverse scaling-based quantization implementation in the AQT library [1] to observe this effect. In our experiments, we observe that a wide range of \u03bb values yield satisfactory results (Table 8). However, our preference set on the safer side, and use \u03bb = 0.01 for all settings. For higher precision (\u2265 4-bits), smaller \u03bb values, such as 0.0001, can be safely used to allow more signals to pass through without causing training instability. Extremely small \u03bb can cause numerical instability, particularly exacerbated by the inherent numerical noise introduced by neural network operations.",
            "score": 0.6695953096510944,
            "section_title": "Block Size and \u03bb",
            "char_start_offset": 29190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1837
                }
            ],
            "ref_mentions": [
                {
                    "start": 1318,
                    "end": 1321,
                    "matchedPaperCorpusId": "234337594"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.876953125
        },
        {
            "corpus_id": "272689153",
            "title": "Robust Training of Neural Networks at Arbitrary Precision and Sparsity",
            "text": "Neural network quantization has become a widely adopted technique for reducing memory footprint and accelerating inference time, enabling efficient deployment on resourceconstrained devices [16]. While full-precision models typically store weights in floating-point format, quantized weights are represented as integers, typically using 8 bits [8,21,37], 3-4 bits [1,8,11,27], or even 1 bit [7,26,32,35,42,43]. In addition to quantizing weights, model activations can also be quantized to further enhance computational efficiency [8,12,21]. Although 8-bit quantization is commonly used as a standard practice in industry [21], achieving lower-bit quantization remains challenging and requires specialized techniques to ensure robust training. Several common techniques include: 1. Mixed precision quantization: This approach selectively assigns different bit levels to different weights, aiming to optimize the trade-off between model size and accuracy [10,18,25,36]. 2. Training recipes: These techniques compensate for the discontinuities introduced by quantization by employing strategies such as sharpness-aware minimization [14,27], state-aware optimization [26], knowledge distillation [22], and multi-phase training [28]. 3. Quantization-friendly network architectures: This approach involves replacing original network layers with alternatives that are more amenable to quantization [42]. \n\nIn contrast to prior work, our method explicitly models quantization discontinuities as perturbations. We decompose the perturbed signal into clean and noisy components, then apply denoising to suppress the noise. This approach leads to a closed-form solution that guarantees training convergence even at extremely low bitwidths. While previous methods have also modeled quantization noise using continuous distributions (e.g., Uniform or Gaussian) for gradient estimation [2,10], they do not optimize the reconstruction process itself to enhance training stability. \n\nTo further reduce model footprint, researchers have been combining sparsity/pruning and quantization in a unified formulation to further compress neural networks [30,40].",
            "score": 0.6693236829915489,
            "section_title": "Related Work",
            "char_start_offset": 7411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1965
                },
                {
                    "start": 1968,
                    "end": 2138
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 194,
                    "matchedPaperCorpusId": "232352683"
                },
                {
                    "start": 344,
                    "end": 347,
                    "matchedPaperCorpusId": "231855747"
                },
                {
                    "start": 347,
                    "end": 350,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 364,
                    "end": 367,
                    "matchedPaperCorpusId": "234337594"
                },
                {
                    "start": 367,
                    "end": 369,
                    "matchedPaperCorpusId": "231855747"
                },
                {
                    "start": 394,
                    "end": 397,
                    "matchedPaperCorpusId": "235306564"
                },
                {
                    "start": 397,
                    "end": 400,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 403,
                    "end": 406,
                    "matchedPaperCorpusId": "244773000"
                },
                {
                    "start": 530,
                    "end": 533,
                    "matchedPaperCorpusId": "231855747"
                },
                {
                    "start": 536,
                    "end": 539,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 621,
                    "end": 625,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 963,
                    "end": 966,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 1163,
                    "end": 1167,
                    "matchedPaperCorpusId": "235306564"
                },
                {
                    "start": 1223,
                    "end": 1227,
                    "matchedPaperCorpusId": "235398404"
                },
                {
                    "start": 1391,
                    "end": 1395,
                    "matchedPaperCorpusId": "244773000"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86865234375
        },
        {
            "corpus_id": "261049574",
            "title": "Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers",
            "text": "Moving from 8-bit to 4-bit weight quantization, we see an accuracy degradation of about 2 \u2212 5% across all models. In Tab. 3, Evol-Q performs similarly to what is shown for 8-bit quantization. In particular, we still see improvement for DeiT-Small, DeiT-Base, and ViT-Base, but now the top-1 accuracy improvement is 0.13%, 0.16%, and 0.77%, respectively.",
            "score": 0.6693134368504698,
            "section_title": "4-bit Quantization",
            "char_start_offset": 16001,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 353
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9658203125
        },
        {
            "corpus_id": "244156385",
            "title": "Evaluation of Deep Neural Network Compression Methods for Edge Devices Using Weighted Score-Based Ranking Scheme",
            "text": "Bit precision reduction is an important concept in mathematics that has been widely adopted in different applications, including deep neural network compression. Quantization limits the width of the bit that is used to represent a digit or number. The bit width of the operands controls the precision level of the output when mathematical operations are performed. \n\nThe most common types of operations that are performed in CNNs are convolution operation, bias addition, and the dot product of the weights matrix and float input tensor, as described in Figure 4. These operations are computed in a 32-bit full precision floating point. Quantization aims to replace the 32-bit floating-point operations with low-precision number formats such as 1 bit, 4 bit, 8 bit, or 16 bit. Binarization transforms full precision models into a single-bit model (the weights and activations are encoded using 1 bit). Binarization can be described as an extreme case of quantization where the weights and activations are encoded using 4 bit, 8 bit, or 16 bit. We quantized the weights and activations of the baseline model using a symmetric mode 8-bit signed full integer quantizer implemented in Keras [38], using TensorFlow [39] as the computing engine. The mapping of the 32-bit input float tensors (i.e., weight matrix, activations) to the 8-bit quantized range is described in Figure 5. The mapping function (i.e., 8-bit quantizer) maps the input float tensors of the baseline model to the 8-bit quantized output. This function is defined in Equation (1): \n\nwhere q 8bit is the 8-bit quantizer, m f is the multiplier, and i f is the input float tensor. The multiplier is the quantization constant that is multiplied with the float input tensor, as expressed in Equation (2):",
            "score": 0.6687058317794479,
            "section_title": "Quantization",
            "char_start_offset": 19790,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 364
                },
                {
                    "start": 367,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1763
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.732421875
        },
        {
            "corpus_id": "220936526",
            "title": "High Throughput Matrix-Matrix Multiplication between Asymmetric Bit-Width Operands",
            "text": "Use of deeper and wider convolutional neural networks (CNNs) has led to impressive predictive performance in many machine learning applications, such as image classification, object detection, semantic segmentation, etc. However, the large model size and associated computational inefficiency of these deep neural networks often make it impossible to run many realtime machine learning tasks on resource-constrained mobile and embedded devices, such as smartphones, AR/VR devices, etc. One particularly effective approach has been the use of model quantization to enable this size and computation compression of CNN models. Quantization of model parameters to sub-byte values (i.e. numerical precision of \u2264 8 bits), especially to 4-bits has shown minimal loss in predictive performance across a range of representative networks and datasets in recent works. As a result, some heavily quantized machine learning models may use kernel weights which have fewer bits than the corresponding activations which they are to be multiplied with. For example, there is an increasing interest in using 4-bit weights and 8-bit activations, which means that matrix multiplications between 4-bit weights and 8-bit activations are likely to become a fundamental kernel of many important workloads including neural networks and machine learning, although such multiplications may also be useful for other purposes. This is evident by the increasing interest and successful development of a large number of novel machine learning and linear algebra techniques [1], [2], [10], [11], [18] to preserve the predictive performance of deep neural networks with 4-bit weights and 8-bit activations in recent years. \n\nHowever, in 4-bit-weight networks, the weights are encoded by 4 bits, while the activation matrices are represented by more bits (e.g., 8 bits in this example, although other examples could have larger activations). This creates a read width imbalance between 4-bit weights, 8-bit activations and outputs (accumulators) compared to previous technology. Ideally, we would like to sustain matched vector width of read and write operands while exploiting 4-bit weights for the best performance. In other words, we would like to utilize the full bandwidth of read and write ports while exploiting 4-bit weights for the best performance.",
            "score": 0.6672716044348654,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1689
                },
                {
                    "start": 1692,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2324
                }
            ],
            "ref_mentions": [
                {
                    "start": 1542,
                    "end": 1545,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 1547,
                    "end": 1550,
                    "matchedPaperCorpusId": "199577815"
                },
                {
                    "start": 1552,
                    "end": 1556,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 1558,
                    "end": 1562,
                    "matchedPaperCorpusId": "52920066"
                },
                {
                    "start": 1564,
                    "end": 1568,
                    "matchedPaperCorpusId": "198903430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9501953125
        },
        {
            "corpus_id": "275932373",
            "title": "Optimizing Large Language Model Training Using FP4 Quantization",
            "text": "Meanwhile, the Blackwell series GPUs extend this capability by supporting FP6 and FP4 formats, with FP4 offering the potential to double computational throughput over FP8. Studies like FP8-LM (Peng et al., 2023) and NVIDIA's Transformer Engine (Nvidia, 2022) have demonstrated the feasibility of FP8 tensor cores for model training. But the application of FP4 tensor cores in model training remains an open research question. \n\nHowever, leveraging 4-bit data formats for neural network training presents significant challenges due to the extremely limited bit width. Directly quantizing LLMs to such a lowbit format often results in substantial accuracy degradation, as shown in Figure 1. This is primarily because low-bit formats are constrained by a limited dynamic range, which increases the risk of overflow and underflow. Even existing methods for 8-bit quantization experience some degree of accuracy loss, underscoring the difficulties of employing a 4-bit format, which provides only 16 distinct representable values. \n\nIn this study, we pioneeringly propose a framework for training language models using the FP4 format, providing a validation of the feasibility of this ultra-low precision representation. To tackle the significant quantization errors associated with weights and activations during model training, we present a series of optimization techniques: (1) For weights, we present a differentiable quantization estimator to improve gradient updates in FP4 computations. By analyzing the impact of quantization on neural network forward and backward passes, we derive a function with correction terms for accurate gradient estimation; (2) For activations, we develop an outlier clamping and compensation strategy to address the issue of outlier values commonly observed during LLM training. By analyzing activation distributions in LLMs, we introduce a clamping method and a sparse auxiliary matrix to preserve quantization accuracy and maintain model performance. \n\nWe conduct comprehensive experiments to demonstrate that our FP4 training framework achieves accuracy comparable to models trained in BF16 or FP8 formats with the same hyperparameters.",
            "score": 0.6671064952974478,
            "section_title": "Introduction",
            "char_start_offset": 1824,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1983
                },
                {
                    "start": 1986,
                    "end": 2170
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "221655691",
            "title": "Fast Implementation of 4-bit Convolutional Neural Networks for Mobile Devices",
            "text": "Quantized low-precision neural networks are very popular because they require less computational resources for inference and can provide high performance, which is vital for real-time and embedded recognition systems. However, their advantages are apparent for FPGA and ASIC devices, while general-purpose processor architectures are not always able to perform low-bit integer computations efficiently. The most frequently used low-precision neural network model for mobile central processors is an 8-bit quantized network. However, in a number of cases, it is possible to use fewer bits for weights and activations, and the only problem is the difficulty of efficient implementation. We introduce an efficient implementation of 4-bit matrix multiplication for quantized neural networks and perform time measurements on a mobile ARM processor. It shows 2.9 times speedup compared to standard floating-point multiplication and is 1.5 times faster than 8-bit quantized one. We also demonstrate a 4-bit quantized neural network for OCR recognition on the MIDV-500 dataset. 4-bit quantization gives 95.0% accuracy and 48% overall inference speedup, while an 8-bit quantized network gives 95.4% accuracy and 39% speedup. The results show that 4-bit quantization perfectly suits mobile devices, yielding good enough accuracy and low inference time.",
            "score": 0.6669654331213666,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98291015625
        },
        {
            "corpus_id": "235623747",
            "title": "Information Bottleneck: Exact Analysis of (Quantized) Neural Networks",
            "text": "This section presents information planes obtained using 4-and 32-bit quantized neural networks trained in the standard setting, in order to investigate the effect of the precision used in the quantization. Figure C.10 depicts the resulting information planes for the 4-bit networks. As each neuron can take only 2 4 = 16 different values, the total number of possible states per layer decreases significantly in this setting, and as expected we see lower overall MI measurements. For the TANH network, several more layers become distinguishable. The observed information planes looks similar to those observed in the original experiments by Shwartz-Ziv & Tishby (2017) and Saxe et al. (2018). However, the network accuracy has now degraded compared to the non-quantized networks (see Supplementary Material F), which indicates that the binning used in the estimation of the MI in previous experiments has discarded information vital to the network. Figure C.11 shows the resulting information planes for the 32-bit networks. As expected, we see an overall increase in MI; the information drops only very slowly through the network. Each layer has many possible states and -given the small data set -we we get closer to the behavior of a continuous system.",
            "score": 0.6651882947805579,
            "section_title": "C EFFECT OF THE QUANTIZATION PRECISION",
            "char_start_offset": 31990,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 673,
                    "end": 691,
                    "matchedPaperCorpusId": "49584497"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95458984375
        },
        {
            "corpus_id": "246634451",
            "title": "Accurate Neural Training with 4-bit Matrix Multiplications at Standard Formats",
            "text": "In Table 2 we show the Top-1 accuracy achieved in 4-bit training using LUQ to quantize the neural gradients to FP4 and combined with a previously suggested method, SAWB (Choi et al., 2018a), to quantize the weights and activations to INT4. We compare our method with Ultra-low (Sun et al., 2020) showing better results in all the models, achieving SOTA in 4-bit training. Moreover, we improve the results by using the proposed SMP (Section 5.1). In Table 3 we show the effect of the proposed fine-tuning, reducing or closing completely the gap from full-precision model. We verified that stochasticity has only a negligible effect on the variance of final performance by running a few different seeds. Additional experiments appear in Appendix A.5. Overhead of SMP and FNT We limit our experiments with the proposed SMP method to only two samples. This is to achieve a similar computational overhead as Ultra-low (Sun et al., 2020), with their suggested two-phase-rounding (TPR) which also generates a duplication for the neural gradient quantization. Additional ablation study of the SMP overhead appears in Appendix A.5. The throughput of a 4-bit training network is approximately 8x in comparison to full precision training (Sun et al., 2020). This means that doing one additional epoch in high precision reduces the throughput by \u223c 8%. In comparison, Ultra-low (Sun et al., 2020) does full-training with all the 1x1 convolutions in 8bit, which reduces the throughput by \u223c 50% in comparison to all 4bit training.",
            "score": 0.6651318656816585,
            "section_title": "Main results",
            "char_start_offset": 23539,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1515
                }
            ],
            "ref_mentions": [
                {
                    "start": 277,
                    "end": 295,
                    "matchedPaperCorpusId": "227275152"
                },
                {
                    "start": 913,
                    "end": 931,
                    "matchedPaperCorpusId": "227275152"
                },
                {
                    "start": 1227,
                    "end": 1245,
                    "matchedPaperCorpusId": "227275152"
                },
                {
                    "start": 1365,
                    "end": 1383,
                    "matchedPaperCorpusId": "227275152"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96337890625
        },
        {
            "corpus_id": "219568058",
            "title": "Hardware-Centric AutoML for Mixed-Precision Quantization",
            "text": "In many real-time machine learning applications (such as robotics, autonomous driving, and mobile VR/AR), deep neural networks is strictly constrained by the latency, energy, and model size. In order to improve the hardware efficiency, many researchers have proposed to directly design efficient models Howard et al. 2017;Cai et al. 2019) or to quantize the weights and activations to low precision (Han et al. 2016;Zhu et al. 2017) (Fig. 1).\n\nConventional quantization methods use the same number of bits for all layers (Choi and Wang 2019;Jacob et al. 2018), but as different layers have different redundancy and behave differently on the hardware (computation bounded or memory bounded), it is necessary to use flexible bitwidths for different layers (as shown in Fig. 2). This flexibility was originally not supported by chip vendors until recently the hardware manufacturers started to implement We need flexible number of bits for different layers. We quantize MobileNets (Howard et al. 2017) to different number of bits (both weights and activations), and it lies on a better pareto curve (yellow) than fixed-bit quantization (blue). This is because different layers have different redundancy and have different operation intensities (operations per byte) on the hardware, which advocates for using flexible bitwidths for different layers layers (Color figure online) this feature: Apple released the A12 Bionic chip that supports flexible bits for the neural network inference (Apple 2018); NVIDIA recently introduced the Turing GPU architecture that supports 1-bit, 4-bit, 8-bit and 16-bit arithmetic operations (Nvidia 2018); Imagination launched a flexible neural network IP that supports per-layer bitwidth adjustment for both weights and activations (Imagination 2018). Besides industry, recently academia also works on the bitlevel flexible hardware design: BISMO (Umuroglu et al. 2018) proposed the bit-serial multiplier to support multiplications of 1 to 8 bits; BitFusion (Sharma et al. 2018) supports multiplications of 2, 4, 8 and 16 bits in a spatial manner.\n\nA very important missing part is, however, how to determine the bitwidth of both weights and activations for",
            "score": 0.6648240846919053,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 303,
                    "end": 322,
                    "matchedPaperCorpusId": "12670695"
                },
                {
                    "start": 322,
                    "end": 338,
                    "matchedPaperCorpusId": "54438210"
                },
                {
                    "start": 399,
                    "end": 416,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 416,
                    "end": 432,
                    "matchedPaperCorpusId": "224893"
                },
                {
                    "start": 541,
                    "end": 558,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 978,
                    "end": 997,
                    "matchedPaperCorpusId": "12670695"
                },
                {
                    "start": 1878,
                    "end": 1900,
                    "matchedPaperCorpusId": "49421250"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6533203125
        },
        {
            "corpus_id": "237327688",
            "title": "Design of a 2-Bit Neural Network Quantizer for Laplacian Source",
            "text": "Although non-uniform quantization provides a better performance for a wide range of input signal variances [6,7] and advanced dual-mode asymptotic solutions are developed [8,9], simple uniform quantization [6,7,[10][11][12] is the first choice when the simplicity of the system is one of the major goals. Thus, uniform quantization has been widely applied for quantizing parameters of neural networks (i.e., for neural network compression) [13][14][15][16][17][18], and different solutions have been considered, e.g., using 8-bits [13], 4-bits [14], or 2-bits [15][16][17][18]; further, non-uniform quantization has also been used [19][20][21]. It has been found that quantizing network parameters using 8-bits [13] or 16-bits [19] enable slightly lower performance when compared to the full precision case, mainly due to the ability of quantizers to achieve high quality reconstructed data. Further, in the case of applying quantizers with smaller resolution, e.g., with 4-bits [14] or 2-bits [15][16][17][18]20,21], performance degradation has been observed; however, the achieved results are still comparable, accompanied with a significantly high level of compression. Eventually, significant attention was paid to the development of binary quantizer models to compress neural networks [22][23][24][25][26], whose attractiveness lies in the amount of compression that can be achieved, with a goal to preserve competitive performance achievements. \n\nIn general, 2-bit quantization models require less energy compared to the models with a higher number of representative levels, which makes them appropriate for resourceconstrained real-time systems. Consequently, we decided to focus on the designing of the simplest multilevel scalar quantizer model.",
            "score": 0.6637920657733173,
            "section_title": "Introduction",
            "char_start_offset": 1663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1754
                }
            ],
            "ref_mentions": [
                {
                    "start": 171,
                    "end": 174,
                    "matchedPaperCorpusId": "219061952"
                },
                {
                    "start": 174,
                    "end": 176,
                    "matchedPaperCorpusId": "7863405"
                },
                {
                    "start": 211,
                    "end": 215,
                    "matchedPaperCorpusId": "1346687"
                },
                {
                    "start": 215,
                    "end": 219,
                    "matchedPaperCorpusId": "3957767"
                },
                {
                    "start": 219,
                    "end": 223,
                    "matchedPaperCorpusId": "67869882"
                },
                {
                    "start": 440,
                    "end": 444,
                    "matchedPaperCorpusId": "44071489"
                },
                {
                    "start": 444,
                    "end": 448,
                    "matchedPaperCorpusId": "232478661"
                },
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "96438794"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "216036085"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "15817277"
                },
                {
                    "start": 531,
                    "end": 535,
                    "matchedPaperCorpusId": "44071489"
                },
                {
                    "start": 544,
                    "end": 548,
                    "matchedPaperCorpusId": "232478661"
                },
                {
                    "start": 560,
                    "end": 564,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 564,
                    "end": 568,
                    "matchedPaperCorpusId": "96438794"
                },
                {
                    "start": 568,
                    "end": 572,
                    "matchedPaperCorpusId": "216036085"
                },
                {
                    "start": 572,
                    "end": 576,
                    "matchedPaperCorpusId": "15817277"
                },
                {
                    "start": 635,
                    "end": 639,
                    "matchedPaperCorpusId": "210848001"
                },
                {
                    "start": 639,
                    "end": 643,
                    "matchedPaperCorpusId": "13996487"
                },
                {
                    "start": 711,
                    "end": 715,
                    "matchedPaperCorpusId": "44071489"
                },
                {
                    "start": 979,
                    "end": 983,
                    "matchedPaperCorpusId": "232478661"
                },
                {
                    "start": 994,
                    "end": 998,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 998,
                    "end": 1002,
                    "matchedPaperCorpusId": "96438794"
                },
                {
                    "start": 1002,
                    "end": 1006,
                    "matchedPaperCorpusId": "216036085"
                },
                {
                    "start": 1006,
                    "end": 1010,
                    "matchedPaperCorpusId": "15817277"
                },
                {
                    "start": 1010,
                    "end": 1013,
                    "matchedPaperCorpusId": "210848001"
                },
                {
                    "start": 1013,
                    "end": 1016,
                    "matchedPaperCorpusId": "13996487"
                },
                {
                    "start": 1294,
                    "end": 1298,
                    "matchedPaperCorpusId": "213346145"
                },
                {
                    "start": 1298,
                    "end": 1302,
                    "matchedPaperCorpusId": "221846252"
                },
                {
                    "start": 1302,
                    "end": 1306,
                    "matchedPaperCorpusId": "227227911"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.880859375
        },
        {
            "corpus_id": "211677681",
            "title": "Quantized Neural Network Inference with Precision Batching",
            "text": "Recent advances in deep learning have demonstrated the wide range of the applications of neural networks (Krizhevsky et al., 2012;Hochreiter & Schmidhuber, 1997;Sutskever et al., 2014;Graves et al., 2013;Bengio et al., 2003;Rajpurkar et al., 2016;Wang & Jiang, 2015;Dao et al., 2019). Applying a neural network to make predictions, termed inference, is computationally expensive, exacts high energy costs and often demands specific latency requirements (e.g: a speech recognition neural network must be fast enough in decoding human language for a real time virtual assistant). Research in quantization aims to reduce the computational costs and thus improve the speed of neural network inference (Hubara et al., 2016;2017;Xu et al., 2018;Sze et al., 2017). Generally, quantization reduces the precision of neural network weights / activations and speeds up inference by facilitating the use of high throughput low precision operations and by reducing the amount of memory transfers (Lin et al., 2015;Xu et al., 2018;Krishnan et al., 2019). \n\nImportantly, quantization incurs an increasingly larger accuracy penalty when quantizing to lower bitwidths (Hubara et al., 2017;Choi et al., 2018). For this reason, state of the art quantization methods often retrain or recalibrate their neural networks to achieve sufficient accuracy at lower bitwidths (Choi et al., 2018;Zhu et al., 2016;Xu et al., 2018;Han et al., 2015;2016b;Lam, 2018;Zhou et al., 2017). It is also common to require architectural changes to the neural network in addition to retraining (Zhu et al., 2016;Xu et al., 2018). At the time of writing, state of the art quantization methods for bitwidths < 8 typically involve retraining and recalibration.",
            "score": 0.6636936604062045,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1715
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 130,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 130,
                    "end": 161,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 204,
                    "end": 224,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 224,
                    "end": 247,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 266,
                    "end": 283,
                    "matchedPaperCorpusId": "76661331"
                },
                {
                    "start": 697,
                    "end": 718,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 723,
                    "end": 739,
                    "matchedPaperCorpusId": "8257350"
                },
                {
                    "start": 1001,
                    "end": 1017,
                    "matchedPaperCorpusId": "8257350"
                },
                {
                    "start": 1151,
                    "end": 1172,
                    "matchedPaperCorpusId": "15817277"
                },
                {
                    "start": 1384,
                    "end": 1400,
                    "matchedPaperCorpusId": "8257350"
                },
                {
                    "start": 1400,
                    "end": 1417,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1417,
                    "end": 1423,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 1433,
                    "end": 1451,
                    "matchedPaperCorpusId": "12130431"
                },
                {
                    "start": 1570,
                    "end": 1586,
                    "matchedPaperCorpusId": "8257350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82958984375
        },
        {
            "corpus_id": "259187584",
            "title": "Sparq: A Custom RISC-V Vector Processor for Efficient Sub-Byte Quantized Inference",
            "text": "Allowing to address both computation complexity and memory footprint, quantization has aroused a real interest. In fact, quantizing properly can achieve considerable memory savings with limited accuracy drop. On Deep Neural Networks such as ResNeXt [10] and EfficientNet [11], only a low accuracy degradation was observed when quantizing the fullprecision 32-bit weights and activations down to 8-bit [12]. In recent work, ultra-low-precision quantization (\u2264 8-bit), which we refer to as sub-byte quantization, has demonstrated that minimal accuracy degradation was achievable with new network structures and training strategies. In some cases subbyte Quantized Neural Networks (QNNs) perform better than their full-precision counterparts [13], [14] as shown in Table I. This was made possible by the introduction of architectures and methods focused on ultra-low-precision. LSQ [13], SSG [14] and DSQ [15] propose to learn the quantization parameters for both weights and activations by minimizing the quantization loss of the network during training. SAWB [3] focuses on the weight quantization parameter using the weight distribution to estimate the optimal quantization scale. To address the issue of unbounded activation range after ReLU, PACT [3] proposes to train a clipping parameter to find the balance point between clipping and quantization error. Focusing on binary or ternary quantization, CNN architectures tailored for 1-bit or 2-bit such as XNORNet [2] or BinaryNet [4] have been introduced. While achieving very high efficiency and low memory footprint, their accuracy suffers a significant drop when compared to full-precision models. All these methods try to minimize the accuracy drop of sub-byte QNN by minimizing the error due to quantization on weights and activations.",
            "score": 0.6630538884705801,
            "section_title": "A. Sub-byte Convolutional Neural Networks",
            "char_start_offset": 2685,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 275,
                    "matchedPaperCorpusId": "167217261"
                },
                {
                    "start": 401,
                    "end": 405,
                    "matchedPaperCorpusId": "235435934"
                },
                {
                    "start": 739,
                    "end": 743,
                    "matchedPaperCorpusId": "67788003"
                },
                {
                    "start": 879,
                    "end": 883,
                    "matchedPaperCorpusId": "67788003"
                },
                {
                    "start": 1058,
                    "end": 1061,
                    "matchedPaperCorpusId": "96438794"
                },
                {
                    "start": 1249,
                    "end": 1252,
                    "matchedPaperCorpusId": "96438794"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88427734375
        },
        {
            "corpus_id": "268889951",
            "title": "DNN Memory Footprint Reduction via Post-Training Intra-Layer Multi-Precision Quantization",
            "text": "Tables I and II present the outcomes of our proposed quantization method alongside other approaches, focusing on accuracy and the size of the neural network.We employ a segment of the ImageNet training dataset, referred to as the calibration dataset, for initializing the activation functions.Notably, our proposed method obviates the need for a retraining step after quantization, making it independent of a training dataset.The ImageNet test dataset serves as the testing benchmark for our method.\n\nIn Table I, \"MP\" denotes the use of mixed precision quantization, wherein 8-bit precision is applied to important layers and 2-bit precision to non-important layers.Additionally, we utilize 6-bit and 4-bit precision for important layers and 2-bit precision for non-important layers.The results demonstrate that our proposed quantization method allows for the quantization of non-important layers with reduced precision, effectively diminishing the size of the neural network while maintaining accuracy.In the case of 8-bit precision quantization mode, our method achieves a 37.74% reduction in neural network size compared to the PWLQ method, which employs a fixed number of quantization bits but relies on non-overlapping regions.This size reduction can be attributed to our approach's ability to allocate different bit precisions based on the identification of important layers and channels using Eq. 1 and Eq. 2. Utilizing varying bit precisions significantly impacts the size of the neural network, as non-important layers and channels are quantized with fewer bits, ultimately reducing the model size.It's important to note that the ZEROQ method considers quantization bit precisions for weights as 2, 4, 8; however, for assessing the number of neural network parameters postquantization, a 4-bit quantization precision is assumed.\n\nTo counteract the substantial accuracy dip observed in the MobileNetV2 model when subjected to 2-bit precision quantization, we opted for a more robust 3-bit precision approach to safeguard the DNN's accuracy.into the quantization processes executed on the MobileNetV2 model, utilizing 8-bit, 6-bit, and 4-bit precision for important layers, and 3-bit precision for non-important layers.",
            "score": 0.6620566762388258,
            "section_title": "A. Evaluating ResNet50 and MobileNet Testcases",
            "char_start_offset": 12984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 157,
                    "end": 293
                },
                {
                    "start": 293,
                    "end": 426
                },
                {
                    "start": 426,
                    "end": 499
                },
                {
                    "start": 501,
                    "end": 666
                },
                {
                    "start": 666,
                    "end": 783
                },
                {
                    "start": 783,
                    "end": 1003
                },
                {
                    "start": 1003,
                    "end": 1232
                },
                {
                    "start": 1232,
                    "end": 1607
                },
                {
                    "start": 1607,
                    "end": 1837
                },
                {
                    "start": 1839,
                    "end": 2048
                },
                {
                    "start": 2048,
                    "end": 2226
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94677734375
        },
        {
            "corpus_id": "211677681",
            "title": "Quantized Neural Network Inference with Precision Batching",
            "text": "At the time of writing, state of the art quantization methods for bitwidths < 8 typically involve retraining and recalibration. There are two key issues with this: 1) retraining a neural network for a target bitwidth is computationally expensive, often taking much longer to train than its full precision counterpart, requiring a separate hyperparameter tuning process for convergence and best results (Hubara et al., 2016;Choi et al., 2018;Hubara et al., 2017) and 2) retraining/recalibrating a neural network requires access to data that matches the distribution of the training/test set (Wu, 2018;Li et al., 2017), which may not be available (e.g: a machine-learning service provider like Google Cloud or Amazon Web Services may have access only to the model but not the data). Thus, a core motivation is the development of a quantized inference method that works out of the box and can extend to below 8 bits with corresponding speedups while maintaining accuracy. \n\nAdditionally, research on quantized neural networks often involve the development of specialized hardware and it is unclear how their techniques translate to inference speedups on traditional hardware architectures (e.g: GPUs and CPUs) (Choi et al., 2018;Albericio et al., 2017;Sharma et al., 2017). Concretely, traditional CPUs and GPUs lack the necessary datatypes for more unusual bitwidths (e.g: 2-bit, 3-bit). Thus, another core motivation is the development of a quantized inference algorithm that can be leveraged in context of existing hardware platforms to speed up inference at lower bitwidths (< 8 bits).",
            "score": 0.6611634929580942,
            "section_title": "Introduction",
            "char_start_offset": 1603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1586
                }
            ],
            "ref_mentions": [
                {
                    "start": 402,
                    "end": 423,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 441,
                    "end": 461,
                    "matchedPaperCorpusId": "15817277"
                },
                {
                    "start": 600,
                    "end": 616,
                    "matchedPaperCorpusId": "10817450"
                },
                {
                    "start": 1226,
                    "end": 1249,
                    "matchedPaperCorpusId": "9051642"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79248046875
        },
        {
            "corpus_id": "199472741",
            "title": "Cheetah: Mixed Low-Precision Hardware & Software Co-Design Framework for DNNs on the Edge",
            "text": "As mentioned before, quantization with rounding has less overhead when compared to the other quantization approaches, but it is not possible to perform DNN inference with 5-bit posits with similar performance of DNN inference as 32-bit floating point. To improve performance of DNN inference, the [5..8]-bit posit numerical format is combined with linear quantization approaches and evaluated for a 4-layer feedforward neural network on the MNIST and Fashion-MNIST datasets. The \u03b1 1 \u00d7 A i and \u03b1 2 \u00d7 W ij in ( 5) can be either implemented by constant multiplication or by a shift operation where the     IV, exhibit that 5-bit low-precision DNN inference achieves similar performance to 32-bit floating point DNN inference on the MNIST data set. Essentially, by deploying this approach, the quantization error produced by the values that lie outside of posit's dynamic range is zeroed out. The linear quantization approach also plays a key role in reducing the hardware complexity of posit EMACs used for DNN inference. Notably, the accuracy of DNN inference with posits is significantly enhanced by using the linear quantization approach in comparison to quantization with rounding. Therefore, the overhead of adding linear quantization is offset by reducing the hardware complexity, i.e. carrying out the posit EMAC operation with es = 0 instead of es = 1, which is explained in depth in the next section.",
            "score": 0.660983769440958,
            "section_title": "B. Exploiting Numerical Formats with Quantization Approaches for DNN Inference",
            "char_start_offset": 22265,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1406
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82470703125
        },
        {
            "corpus_id": "250526497",
            "title": "Sub 8-Bit Quantization of Streaming Keyword Spotting Models for Embedded Chipsets",
            "text": "In this segment, we present 3 sets of ablation studies: 1) quantizing different components of the network to 8-bits; 2) quantizing inputs and activations from 8-bit to 5-bit; 3) quantizing other components of the network from 8-bit to 5-bit (bias, gain, batch norm parameters). 1) 8-bit models: Fixing 8-bit quantization for all components of the network, we perform an ablation study for accuracy implications in terms of quantizing bias, gain, BN parameters, input and activations. The unquantized baseline model and the quantized models in this section were trained for 100K updates. Table 4 presents the results in terms of relative degradation in FDR at equal FRR compared to the baseline model on TEST data. With tanh(.) 8-bit weight quantization we obtain a 3.8 % relative degradation in FDR. With further quantization of input and activations (5.2%) or bias and gain (4.7%) or batch norm parameters (5.3%), we did not see a significant decrease in model performance. However, it is interesting to note from the Table 4 that the dynamic scaling of BN yields an improvement (i.e., from 9.0% to 5.3% relative FDR degradation). \n\n2) 8-bit to 5-bit change in input and activations: While compressing input and activations from 16-bit to 8 bits (from M1 to M2 in Table 3) only results in a minor performance degradation, reducing further from 8-bit to 5-bit input and activations leads to a significant drop in performance (see Fig. 2). \n\n3) 8-bit to 5-bit change in parameters: Similar to the small degradation in FDR in going from 8 bits to 5 bits for weights (Table 3), reducing the bit width from 8 to 5 for other parameters does not lead to a significant degradation in FDR (3.0% increase in relative FDR).",
            "score": 0.6606959765894918,
            "section_title": "Non-binary Sub-8 bit Models: 1 st -Stage Training",
            "char_start_offset": 16044,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1438
                },
                {
                    "start": 1441,
                    "end": 1713
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86279296875
        },
        {
            "corpus_id": "170078885",
            "title": "Memory-Driven Mixed Low Precision Quantization For Enabling Deep Network Inference On Microcontrollers",
            "text": "Quantized Neural Networks. Early works on quantization of deep networks targeted 16 bits fixed-point implementations [15], which result in an almost lossless approximation of full-precision trained networks, or extreme binarized networks, which, despite the fascinating low-computational and memory requirements, showed major accuracy losses when applied on image classification benchmarks [4,19]. Several studies demonstrated that 8 bit quantization of weights and activations results in a good trade-off between latency, compression and a near-zero accuracy degradation, also if applied to efficient Imagenet classification networks [11,18,12]. Among the employed methodologies, TensorRT [18] approximates the parameters tensor by the minimization of the KL divergence metric between quantized and full-precision values. On the contrary, [11] quantizes values within a range defined by the tensor min and max values. Concerning activations, the PACT approach [2] demonstrated the highest efficiency by leveraging backpropagation to learn the quantization ranges. Recently, to fit stringent memory requirements, more aggressive sub-byte precision quantization approaches, i.e. less than 8 bit, are under investigation [3,12,6,13,16]. The works [12,6] exploits learning-based approaches for determining the quantization ranges of activation and weights at low-bitwidth precision. State-of-the-art accuracy level on the efficient MobilenetV1 model has been reported by [13,16], by making use of per-channel quantization when moving to 4 bits precision. It is also worth to mention as non-uniform quantizers have resulted as the best approximators when reducing the bit precision [24,22,9]. However, a high-precision (floating point) arithmetic is needed on uncompressed values within the datapath, hence these methods results not suitable for the microcontroller domain. In this work, we leverage existing techniques and show the insights, concerning either computational and memory aspects, when bringing fake-quantized networks to the integer-only arithmetic domain, which is not taken into consideration by this class of works. \n\nMixed Low Precision Quantization.",
            "score": 0.660672233516383,
            "section_title": "Related Work",
            "char_start_offset": 5129,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2129
                },
                {
                    "start": 2132,
                    "end": 2165
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 121,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 393,
                    "end": 396,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 635,
                    "end": 639,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1678,
                    "end": 1682,
                    "matchedPaperCorpusId": "50784025"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92236328125
        },
        {
            "corpus_id": "276813486",
            "title": "Universality of Layer-Level Entropy-Weighted Quantization Beyond Model Architecture and Size",
            "text": "In more technical terms, model (neural network) quantization refers to the process of reducing the numerical precision of weights and activations from 32-bit floating point to lower bit-width representations (typically 8-bit or 4-bit), quantization achieves significant reductions in memory footprint and computational requirements while preserving model functionality [Choi et al., 2018, Hubara et al., 2021]. Modern quantization approaches can be broadly categorized into two paradigms: \n\n\u2022 Uniform quantization: Applies identical precision reduction across all model components, enabling straightforward implementation but often resulting in significant accuracy degradation for sensitive layers [Yao et al., 2022]. \n\n\u2022 Mixed-precision quantization: Allocates higher precision to critical layers identified through sensitivity analysis, achieving better accuracy preservation at the cost of increased implementation complexity [Dettmers et al., 2022a, Zhao et al., 2024]. \n\nRecent advances in post-training quantization (PTQ) have demonstrated particular promise for LLM deployment. Dettmers et al. [2022a] introduced layer-wise adaptive mixed precision for GPT-3 models, maintaining 16-bit precision only for outlier-dominated attention heads. Frantar et al. [2022] developed a second-order quantization approach that minimizes layer-wise reconstruction errors, enabling 4-bit quantization of LLaMA models with minimal accuracy loss. The BitLinear layer proposed by Ashkboos et al. [2023] achieves extreme 1.58-bit quantization through entropy-driven logarithmic representations, though with increased computational overhead.",
            "score": 0.6606556305525075,
            "section_title": "Model Quantization",
            "char_start_offset": 7631,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 488
                },
                {
                    "start": 491,
                    "end": 718
                },
                {
                    "start": 721,
                    "end": 974
                },
                {
                    "start": 977,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1629
                }
            ],
            "ref_mentions": [
                {
                    "start": 387,
                    "end": 409,
                    "matchedPaperCorpusId": "235825979"
                },
                {
                    "start": 699,
                    "end": 717,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 930,
                    "end": 953,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 1086,
                    "end": 1109,
                    "matchedPaperCorpusId": "251564521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "corpus_id": "52893258",
            "title": "On the Universal Approximability and Complexity Bounds of Quantized ReLU Neural Networks",
            "text": "In this section, we first introduce the optimal bit-width problem and then show how a theoretical bound could potentially be used to estimate the optimal bit-width of a neural network. \n\nBecause of the natural need and desire of comparison with competitive approaches, most quantization techniques are evaluated on some popular reference networks, without modification of the network topology. On the one hand, the advancement of lossless quantization almost stalls at a bit-width between two and six (Han et al., 2015a;Choi et al., 2016;Sze et al., 2017;Blott et al., 2017;Su et al., 2018;Faraone et al., 2018). A specific bit-width depends on the compactness of the reference network and the difficulty of the task. On the other hand, the design space, especially the different combinations of topology and bit-width, is largely underexplored because of the complexity, resulting in sub-optimal results. A recent work by Su et al. (2018) empirically validates the benefit of exploring flexible network topology during quantization. That work adds a simple variable of network expanding ratio, and shows that a bit-width of four achieves the best cost-accuracy trade-off among limited options in {1, 2, 4, 8, 16, 32}. Some recent effort on using reinforcement learning to optimize the network hyper-parameters (He et al., 2018) could potentially be used to address this issue. But the current design space is still limited to a single variable per layer (such as the pruning ratio based on a reference network). How to estimate an optimal bit-width for a target task without training could be an interesting research direction in the future. \n\nThe memory bound expression as derived in this paper helps us to determine whether there is an optimal \u03bb that would lead to the lowest bound and most compact network (which can be translated to computation cost in a fully connected structure) for a given target function. For example, by dropping the lower-order term and ignoring the rounding operator, our memory bound can be simplified as \n\nwhere \u03b8 1 is a constant determined by , n, and d. We can find an optimal \u03bb that minimizes M (\u03bb):",
            "score": 0.6605822447352814,
            "section_title": "BOUND-BASED OPTIMAL BIT-WIDTH",
            "char_start_offset": 21371,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 187,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1642
                },
                {
                    "start": 1645,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2036
                },
                {
                    "start": 2039,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2135
                }
            ],
            "ref_mentions": [
                {
                    "start": 555,
                    "end": 574,
                    "matchedPaperCorpusId": "5775331"
                },
                {
                    "start": 574,
                    "end": 590,
                    "matchedPaperCorpusId": "13828862"
                },
                {
                    "start": 590,
                    "end": 611,
                    "matchedPaperCorpusId": "49559664"
                },
                {
                    "start": 923,
                    "end": 939,
                    "matchedPaperCorpusId": "13828862"
                },
                {
                    "start": 1311,
                    "end": 1328,
                    "matchedPaperCorpusId": "52048008"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73828125
        },
        {
            "corpus_id": "261049460",
            "title": "FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs",
            "text": "We evaluate the performance of our weight-only quantization method on an MoE model and report the results in Table 6. We investigate the impact of different quantization precisions, ranging from 8-bit to 3-bit. Due to the robustness of the MoE FFN layers, the model's accuracy is preserved quite well even with 3-bit and 4-bit precision, when compared to the original fp16 accuracy. \n\nFigure 5 shows the end-to-end speed improvements with various batch size with 8-bit and 4-bit quantization.",
            "score": 0.6580596265818771,
            "section_title": "MoE model performance results",
            "char_start_offset": 21637,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 492
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.935546875
        },
        {
            "corpus_id": "210942717",
            "title": "Compact Recurrent Neural Networks for Acoustic Event Detection on Low-Energy Low-Complexity Platforms",
            "text": "Feed-forward propagation through a neural network requires vector/matrix/tensor multiplication and convolution. Therefore, all the core features employed in signal processing can be used for neural network computations. For this reason, ARM developed the CMSIS-NN framework for neural network propagation on top of DSP libraries [21]. The CMSIS-NN library maximizes performance and energy efficiency of common deep learning kernels on top of Cortex-M series cores. \n\nLike for DSP, truncation of floating point numbers to 8 or 16-bit fixed point numbers improves the execution time and reduces the memory footprint. According to [21], 8-bit quantization achieves 4.6X improvement in runtime/throughput and 4.9X improvement in energy efficiency in image classification with CIFAR10 dataset. On the other hand, since quantization implies loss of precision, we could expect a direct impact on the final prediction performance. However, the authors of [44] experimented different kinds of quantization in image classification, achieving a 20% drop in model size without significant loss of accuracy. In the following, we describe the quantization process from a 32-bit floating-point to an 8-bit fixed-point representation. From now on, we will assume that floating point numbers has infinite precision and we will use the nomenclature typically used for DSP. \n\nQuantization describes a real number with finite resolution. When a uniform quantization is applied, three parameters are used to define the fixed point representation: bit-width, stepsize (resolution) and dynamic range [44]. These parameters are correlated by the following expression: \n\nwhere bitwidth \u2212 1 accounts for the bit used to represent the sign. Stepsize is the minimum step between two fixed point numbers and will be always chosen as a power of two for convenience with binary representation. An equivalent formulation of Eq. 8 can be obtained by considering the number of bits used for the decimal and integer parts of numbers: \n\nwhere, integer is the number of bits used to represent the integer and decimal is the number of bits used for the decimal part. +1 in the last equation accounts for the sign bit.",
            "score": 0.658001467159217,
            "section_title": "A. Quantization",
            "char_start_offset": 27036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1643
                },
                {
                    "start": 1646,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1998
                },
                {
                    "start": 2001,
                    "end": 2128
                },
                {
                    "start": 2129,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 1577,
                    "end": 1581,
                    "matchedPaperCorpusId": "649645"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77099609375
        },
        {
            "corpus_id": "209515558",
            "title": "Towards Unified INT8 Training for Convolutional Neural Network",
            "text": "Compared to huge amount of studies on accelerating inference by model quantization [47,60,7,53,11,40], there are few works exploring quantized training including backward propagation comprehensively. DoReFa-Net [62] quantizes gradients to 4 and 6 bits, but only experiments AlexNet with low precision gradient. WAGE [56] and WAGEUBN [58] quantize gradient to 8-bit integer, but they both incur considerable loss of accuracy (greater than 5%). RangeBN [2] and FP8 training [54] achieve accuracy comparable to full-precision models, but they both use floatingpoint number in gradients, which is not beneficial for hardware optimization to boost the speed. Besides quantized training, most low-precision training research keeps gradient precision in 16-bit floating-point. Flexpoint [33], MPT [41] and DFP [9] all use 16-bit floating-point to train DNNs with accuracy comparable to full-precision model.",
            "score": 0.6575986900308433,
            "section_title": "Related Work",
            "char_start_offset": 4642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 900
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 87,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 87,
                    "end": 90,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 95,
                    "end": 98,
                    "matchedPaperCorpusId": "102352789"
                },
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "3603886"
                },
                {
                    "start": 472,
                    "end": 476,
                    "matchedPaperCorpusId": "53977760"
                },
                {
                    "start": 780,
                    "end": 784,
                    "matchedPaperCorpusId": "31312287"
                },
                {
                    "start": 803,
                    "end": 806,
                    "matchedPaperCorpusId": "3517431"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7763671875
        },
        {
            "corpus_id": "39867659",
            "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference",
            "text": "We conducted two set of experiments, one showcasing the effectiveness of quantized training (Section. 4.1), and the other illustrating the improved latency-vs-accuracy tradeoff of quantized models on common hardware (Section. 4.2). The most performance-critical part of the inference workload on the neural networks being benchmarked is matrix multiplication (GEMM). The 8-bit and 32-bit floating-point GEMM inference code uses the gemmlowp library [18] for 8-bit quantized inference, and the Eigen library [6] for 32-bit floating-point inference.",
            "score": 0.6572293116211487,
            "section_title": "Experiments",
            "char_start_offset": 19923,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 547
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8505859375
        },
        {
            "corpus_id": "246210442",
            "title": "Neural Network Quantization with AI Model Efficiency Toolkit (AIMET)",
            "text": "With the rise in popularity of deep learning as a general-purpose tool to inject intelligence into electronic devices, the necessity for small, low-latency and energy-efficient neural networks solutions has increased. Today neural networks can be found in many electronic devices and services, from smartphones, smart glasses and home appliances to drones, robots and self-driving cars. These devices are typically subject to strict time restrictions on the execution of neural networks or stringent power requirements for long-duration performance. \n\nOne of the most impactful ways to decrease the computational time and energy consumption of neural networks is quantization. In neural network quantization, the weights and activation tensors are stored in lower bit precision than the 16 or 32-bit precision they are usually trained in. When moving from 32 to 8 bit integer, the memory overhead of storing tensors decreases by a factor of 4, while the computational cost for matrix multiplication reduces quadratically by a factor of 16. Neural networks have been shown to be robust to quantization, meaning they can be quantized to lower bit-widths with a relatively small impact on the network's accuracy. Besides, neural network quantization can often be applied along with other common methods for neural network optimization, such as neural architecture search, compression and pruning. It is an essential step in the model efficiency pipeline for any practical use-case of deep learning. However, neural network quantization is not free. Low bit-width quantization introduces noise to the network that can lead to a drop in accuracy. While some networks are robust to this noise, other networks require extra work to exploit the benefits of quantization. \n\nIn literature, quantization issues are handled by using post-training quantization (PTQ) and quantization-aware Training (QAT) techniques. Some of these options are natively supported by frameworks, such as PyTorch and TensorFlow. To make neural network quantization easier for the broader AI ecosystem and enable energy-efficient, fixed-point inference, Qualcomm Innovation Center has open-sourced the AI Model Efficiency Toolkit or AIMET. AIMET is a library of state-ofthe-art neural network quantization and compression techniques based on the work of Qualcomm AI Research in this space.",
            "score": 0.657169222295295,
            "section_title": "Introduction",
            "char_start_offset": 27,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1762
                },
                {
                    "start": 1765,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2205
                },
                {
                    "start": 2206,
                    "end": 2355
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81982421875
        },
        {
            "corpus_id": "258685753",
            "title": "MBQuant: A novel multi-branch topology method for arbitrary bit-width network quantization",
            "text": "By converting the full-precision weights and activations in deep neural networks (DNNs) into lower-bit formats, the quantization technique has become one of the most predominant methods to show impressive capabilities of compressing DNNs in recent years. However, most network quantization methods [1,2,3] are designed with a fixed bit-width, limiting their scalability and adaptability to various computational resources in real-world applications. Therefore, recent work has focused on training the quantized neural networks (QNNs) of multiple precisions for adapting to a wide range of bit-width requirements in runtime, referred to as arbitrary bit-width QNNs [4,5,6,7,8]. \n\nTypically, as illustrated in Fig. 1a, these methods train the arbitrary bitwidth QNNs by optimizing the loss of all bit-width candidates for weights and activations [5,4]. Specifically, throughout the training process, the bitwidth is cyclically chosen from the candidate bit-widths. The weights and activations of the network are then quantized to the selected bit-width. Subsequently, the quantized network undergoes forward and backward propa-  gation to update weights. Previous methods improve the performance by utilizing mix-precision [6,7], training strategy [9], dynamic inference [8], and so on. Despite these early efforts, in this paper, we reveal that current methods for arbitrary bit-width QNNs struggle more with quantization errors, making their performance unsatisfactory. \n\nQuantization error, one of the most practical difficulties in QNNs [10,11,12], stems from the inherent uncertainty in digitizing an analog value as a result of the finite resolution of the conversion process. It depends typically on the number of bits in the converter, along with its error, noise, and nonlinearities [13,14]. In the case of fixed bit-width QNNs, the network only needs to account for quantization errors specific to one bit-width, which generally results in acceptable performance.",
            "score": 0.6560400802607858,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1469
                },
                {
                    "start": 1472,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1971
                }
            ],
            "ref_mentions": [
                {
                    "start": 298,
                    "end": 301,
                    "matchedPaperCorpusId": "199577815"
                },
                {
                    "start": 301,
                    "end": 303,
                    "matchedPaperCorpusId": "257575672"
                },
                {
                    "start": 303,
                    "end": 305,
                    "matchedPaperCorpusId": "220363587"
                },
                {
                    "start": 664,
                    "end": 667,
                    "matchedPaperCorpusId": "208138922"
                },
                {
                    "start": 667,
                    "end": 669,
                    "matchedPaperCorpusId": "209439896"
                },
                {
                    "start": 669,
                    "end": 671,
                    "matchedPaperCorpusId": "250628858"
                },
                {
                    "start": 671,
                    "end": 673,
                    "matchedPaperCorpusId": "232428391"
                },
                {
                    "start": 673,
                    "end": 675,
                    "matchedPaperCorpusId": "248299876"
                },
                {
                    "start": 844,
                    "end": 847,
                    "matchedPaperCorpusId": "209439896"
                },
                {
                    "start": 847,
                    "end": 849,
                    "matchedPaperCorpusId": "208138922"
                },
                {
                    "start": 1221,
                    "end": 1224,
                    "matchedPaperCorpusId": "250628858"
                },
                {
                    "start": 1224,
                    "end": 1226,
                    "matchedPaperCorpusId": "232428391"
                },
                {
                    "start": 1246,
                    "end": 1249,
                    "matchedPaperCorpusId": "260899867"
                },
                {
                    "start": 1269,
                    "end": 1272,
                    "matchedPaperCorpusId": "248299876"
                },
                {
                    "start": 1539,
                    "end": 1543,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 1546,
                    "end": 1549,
                    "matchedPaperCorpusId": "265593617"
                },
                {
                    "start": 1794,
                    "end": 1797,
                    "matchedPaperCorpusId": "132711236"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58837890625
        },
        {
            "corpus_id": "236087695",
            "title": "A High-Performance Adaptive Quantization Approach for Edge CNN Applications",
            "text": "Particularly, the storage requirement is substantially reduced. The memory communication efficiency is also greatly improved due to the use of low bit-width data. In addition, the circuit complexity of the low bit-width MAC (multiplyaccumulate) unit is much lower than that of the floating-point MAC unit. Thus the quantization models save both chip area and power consumption significantly. For instance, the floating-point adder consumes 30 times more energy and 116 times more area than the 8-bit adder; the floating-point multiplier consumes 18 times more energy and 27 times more area than the 8-bit multiplier [16]. \n\nDespite the high computational efficiency of low bit-width inference, the task accuracy degrades and needs to be recovered by retraining. The experimental results from recent research [17] [18] show a comparison of model accuracy under different bit-width quantization in the image classification task, and the results support that the 4-bit quantization models strike the best balance between minimal computing/memory complexity and minimal model accuracy degradation. \n\nGenerally, existing quantization approaches achieve good results in the traditional ReLU-based models, where the activations are passed through the ReLU function [19] and then restricted activations to be non-negative values. In contrast, contemporary network architectures adopt more complex activation functions, such as Swish [20], Leaky-ReLU [19], and Mish [21], with which the range of activation distributions is relatively more irregular than the weight distribution and hence the traditional quantization methods experience a significant information loss. Nevertheless, the indispensable benefit of reducing memory overhead strongly demands a more robust quantization method to manage the dynamic distribution range. \n\nThe traditional uniform quantization methods [17][22][23] simply quantize the normalized distribution using a simple scaling, and the results often suffer significant quantization errors, particularly when processing uneven and biased distributions. For instance, we show in Fig. 2(a) a sample original data distribution with the dashed box denoting the quantization range.",
            "score": 0.654641797487885,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2252,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 1093
                },
                {
                    "start": 1096,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1820
                },
                {
                    "start": 1823,
                    "end": 2072
                },
                {
                    "start": 2073,
                    "end": 2196
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9677734375
        },
        {
            "corpus_id": "218862856",
            "title": "Position-based Scaled Gradient for Model Quantization and Sparse Training",
            "text": "On the other hand, ImageNet experiment generally shows reasonable results until 6-bits but the accuracy drastically drops at 4-bits. PSGD targeting 8-bits and 6-bits marginally improves on all bits, yet also experiences drastic accuracy drop at 4-bits. In contrast, Gradient L1 (\u03bb = 0.05) and PSGD @ W4 maintain the performance of the quantized models even at 4-bits. Comparing with the second best method Gradient L1 (\u03bb = 0.05) [1], PSGD outperforms it at all bit-widths. At full precision (FP), 8-, 6-and 4-bits, the gap of performance between [1] and ours are about 4.2%, 3.8%, 1.8% and 8%, respectively. From Table 2, while the quantization noise may slightly degrade the accuracy in some cases, a general trend that using more bits leads to higher accuracy is demonstrated. Consequently, as our 4-bit targeted model (PSGD@W4) achieves comparable accuracy for all higher bits of the other methods, PSGD outperforms conventional methods in all bit-widths. Compared to other regularization methods, PSGD is able to maintain reasonable performance across all bits by constraining the distribution of the full precision weight to resemble that of the quantized weight. This quantization-friendliness is achieved by the appropriately designed scaling function. Also, unlike [1], PSGD does not need additional overhead of calculating the double-backpropagation. \n\nPost-training methods Table 3 shows that OCS, state-of-the-art post-training method, has a drastic accuracy drop at 4-bits. For OCS, following the original paper, we chose the best clipping method for both weights and activation. DFQ also has a similar tendency of showing drastic accuracy drop under the 6 bit-widths depicted in Fig. 1 at the original paper of DFQ [26]. This is due to the fundamental discrepancy between FP and quantized weight distributions as stated in Sec 1 and Fig. 1.",
            "score": 0.6524877117583656,
            "section_title": "Method",
            "char_start_offset": 20351,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1359
                },
                {
                    "start": 1362,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1853
                }
            ],
            "ref_mentions": [
                {
                    "start": 429,
                    "end": 432,
                    "matchedPaperCorpusId": "211724357"
                },
                {
                    "start": 546,
                    "end": 549,
                    "matchedPaperCorpusId": "211724357"
                },
                {
                    "start": 1273,
                    "end": 1276,
                    "matchedPaperCorpusId": "211724357"
                },
                {
                    "start": 1728,
                    "end": 1732,
                    "matchedPaperCorpusId": "184487878"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "219447096",
            "title": "Moving Deep Learning to the Edge",
            "text": "Different simplifications and optimizations were proposed to reduce the implementation complexity of deep learning models. The objective of the simplifications is to reduce the memory footprint and the computational complexity to run a network inference. There are two main classes of optimizations: (1) data quantization and (2) data reduction. Some optimizations reduce the complexity of the model with some accuracy reduction. Others can reduce the memory and computation footprints without accuracy degradation. All methods have been considered on resource-limited end devices, but only a few are considered on edge nodes, typically those that do not affect the accuracy. \n\nData quantization methods reduce the complexity of arithmetic operators and the number of bits (bitwidth) to represent parameters and activations. The complexity of hardware implementations of arithmetic operations depends on the type of data [112]. Operators for floating-point arithmetic are more complex than for fixed-point or integer arithmetic. The number of bits used to represent data also determines the complexity of the operators. Custom floating-point representations with 16 bits [113] and eight bits [114] considerably reduce the complexity of operators and achieve similar accuracies of networks implemented with single-precision floating-points. \n\nIn [115], the authors used 8 bit fixed-point data representations for parameters and activations. They concluded that the model achieved an accuracy close to that obtained with the same model using 32 bit floating-points. The same conclusion was found in [116][117][118]. All works showed that parameters and activations could be represented in fixed-points and with fewer bits with negligible accuracy reduction. Less than 8 bit quantizations were proposed in [119,120]. \n\nPrevious works considered a fixed quantization for all layers, but customized representations for different layers reduced further the complexity without incurring an accuracy reduction [121][122][123]. Studies with this hybrid quantization concluded that the first and last layers were the most sensitive to the size of data among all layers. Furthermore, different sizes could be adopted for weights and activations. In [122], the authors concluded that activations were more sensitive to data size reduction. \n\nData quantization can be taken to the limit with Binary Neural Networks (BNN).",
            "score": 0.6523683303956216,
            "section_title": "Hardware-Oriented Deep Neural Network Optimizations",
            "char_start_offset": 43117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1339
                },
                {
                    "start": 1342,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1813
                },
                {
                    "start": 1816,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2159
                },
                {
                    "start": 2160,
                    "end": 2234
                },
                {
                    "start": 2235,
                    "end": 2327
                },
                {
                    "start": 2330,
                    "end": 2408
                }
            ],
            "ref_mentions": [
                {
                    "start": 1345,
                    "end": 1350,
                    "matchedPaperCorpusId": "9455864"
                },
                {
                    "start": 1597,
                    "end": 1602,
                    "matchedPaperCorpusId": "2547043"
                },
                {
                    "start": 1602,
                    "end": 1607,
                    "matchedPaperCorpusId": "15178138"
                },
                {
                    "start": 1607,
                    "end": 1612,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 1808,
                    "end": 1812,
                    "matchedPaperCorpusId": "202773563"
                },
                {
                    "start": 2002,
                    "end": 2007,
                    "matchedPaperCorpusId": "207233292"
                },
                {
                    "start": 2007,
                    "end": 2012,
                    "matchedPaperCorpusId": "51974633"
                },
                {
                    "start": 2012,
                    "end": 2017,
                    "matchedPaperCorpusId": "207959781"
                },
                {
                    "start": 2238,
                    "end": 2243,
                    "matchedPaperCorpusId": "51974633"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.798828125
        },
        {
            "corpus_id": "244156385",
            "title": "Evaluation of Deep Neural Network Compression Methods for Edge Devices Using Weighted Score-Based Ranking Scheme",
            "text": "Bit reduction techniques have been around for quite a while [11,12]. These techniques aim to reduce the size of the model without a significant loss in the model performance. In practice, this is somewhat difficult to achieve due to the loss of information when approximating the 32-bit full precision weights and activations to a fixed point integer representation [13,14]. Quantization can be implemented using (16,8, or 4 bits); however, there can be extreme cases where 2 bits or 1 bit are used to represent the weights and/or activations. These are referred to as binarization and ternarization. Binary networks encode the weights and activations with 1 bit (-1, 1), in contrast to ternary, which uses 2 bits (-1, 0, 1) [15]. \n\nThe works of [11,12,16,17] showed the possibility of training deep networks with low bit-width weights. The weights and activations of the networks were discretized and a drastic reduction in model size with an insignificant drop in accuracy was achieved in [14]. An adaptive quantization framework that achieved a 20-40% higher compression rate, in contrast to non-adaptive quantization methods that relied on a uniform quantizer, was proposed in [18]. A vector-based quantization method that reduces the reconstruction in the network output was introduced in [19]. The work of [18] also showed that the network layers contribute differently to the model prediction result; therefore, it is logical not to use uniform bit-width quantization. \n\nQuantization techniques have shown promising results for large model compression. This breakthrough has caused different industries developing on-device/edge-based artificial intelligence solutions to adopt the methods. It is worth noting that the lower the bit-width used in quantization, the higher the compression rate and the model sensitivity to accuracy degradation.",
            "score": 0.651528466031656,
            "section_title": "Bit Reduction",
            "char_start_offset": 8498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1475
                },
                {
                    "start": 1478,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1850
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 67,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 370,
                    "end": 373,
                    "matchedPaperCorpusId": "212420024"
                },
                {
                    "start": 750,
                    "end": 753,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 991,
                    "end": 995,
                    "matchedPaperCorpusId": "212420024"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9013671875
        },
        {
            "corpus_id": "258959117",
            "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models",
            "text": "One important question for practitioners is whether to use a small model at full precision, or a larger quantized model of similar inference cost. While the exact trade-offs can vary based on several factors, we can make several recommendations based on our results. First, 8-bit quantization should be preferred over smaller full precision models, and PTQ methods are sufficient for this case. An 8-8-8 30B quantized model outperforms a 13B model of similar size, and should have lower latency and higher throughput in practice. This also holds for an 8-bit 13B model compared with a 16-bit 7B model. Furthermore, 4-bit models quantized using LLM-QAT should be preferred over 8-bit models of similar size. For instance a 4-8-4 LLM-QAT 30B outperforms an 8-bit LLaMA-13B, and a 4-8-8 LLM-QAT 13B is better than an 8-bit LLaMA-7B. As a result, we recommend 4-bit LLM-QAT models for the best efficiency-accuracy tradeoff.",
            "score": 0.6515126012161162,
            "section_title": "Main Results",
            "char_start_offset": 14824,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 919
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9541015625
        },
        {
            "corpus_id": "155100024",
            "title": "Derived Codebooks for High-Accuracy Nearest Neighbor Search",
            "text": "A product quantizer is fully characterized by two parameters: m, the number of quantizers and k the number of centroids per quantizer. We denote b = \u2308log 2 (k)\u2309 the number of bits per quantizer, and we denote m\u00d7b product quantizer a product quantizer employing m quantizers of b bits each. \n\nAn m\u00d7b product quantizer encodes high-dimensional vectors into short codes occupying m\u2022b bits of memory. The m\u2022b product impacts the accuracy of ANN search, and the memory use of the database. The higher the m\u2022b product, the higher the accuracy, but also the higher the memory use. In practice, 64-bit codes (i.e., m\u2022b = 64) are commonly used, but there has been a recent interest in using 32-bit codes [4]. \n\nFor a fixed m\u2022b product (e.g., m\u2022b = 64 or m\u2022b = 32), there is the option of using either 8-bit quantizers (b = 8) or 16-bit quantizers (b = 16). Thus, a product quantizer generating 64-bit codes (m\u2022b = 64) can be built using either 8 8-bit quantizers (8\u00d78 product quantizer) or 4 16-bit quantizers (4\u00d716 product quantizer). Any other arbitrary value of b than 8 or 16 would theoretically be possible. However, during a distance computation the m b-bit integers composing a compact code are accessed individually. For efficiency, it is important that these integers have a size that can be natively addressed by CPUs, i.e., b \u2208 {8, 16, 32, 64}. In addition, 32-bit or 64-bit vector quantizers are not tractable, leaving b = 8 and b = 16 as practical values. \n\nWe measure the impact of the use of 16-bit quantizers on accuracy (using the Recall@100 measure, denoted R@100), and on the time spent in the Index, Tables and Scan steps (Table 1).",
            "score": 0.6515101279906318,
            "section_title": "Impact of the use of 16-bit quantizers",
            "char_start_offset": 10417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 289
                },
                {
                    "start": 292,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 699
                },
                {
                    "start": 702,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1459
                },
                {
                    "start": 1462,
                    "end": 1643
                }
            ],
            "ref_mentions": [
                {
                    "start": 695,
                    "end": 698,
                    "matchedPaperCorpusId": "125463275"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78271484375
        },
        {
            "corpus_id": "250607637",
            "title": "Low-bit Shift Network for End-to-End Spoken Language Understanding",
            "text": "In Table 1, we present the performance in terms of accuracy and other quantization settings are explored. Results are based on the best model and not on the final model. For D3, for instance, we chose a model around epoch 100 which gave the best accuracy on the validation set. The same was done for the other experiments in Table 1. Results are compatible to the baseline results presented in [14], with the full-precision (FP32) even outperforming it with accuracy as high as 92.34 % and 98.97 % for the validation and test set, respectively, followed by our proposed solution quantized with 4 bits, S4, which provides 92.24 % and 98.76 %, respectively, for the validation and test sets as well. \n\nQuantization had a mild impact on the overall performance. For integer quantization, for example, quantizing with 16 bits showed to be slightly benefitial compared to quantizatizing with 2 bits with latter dropping accuracy only 0.06 % on the test set. Similar trend was found for the ternary shift network. While quantizing with 4 bits gives 98.76 % accuracy, quantizing with 2 bits provides 98.41 % accuracy, representing a decay of only 0.35 %. These results show that compressing a model with low-bit representation is feasible for the SLU task and with the ternary shift network fast inference will be possible with the absence of expensive multiplication operations.",
            "score": 0.6511918439500938,
            "section_title": "Results",
            "char_start_offset": 13655,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1372
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97998046875
        },
        {
            "corpus_id": "258841328",
            "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
            "text": "While the 4-bit NormalFloat (NF4) data type is informationtheoretically optimal, it still needs to be determined if this property translates to empirical advantages. We follow the setup from Dettmers and Zettlemoyer [13] where quantized LLMs (OPT [72], BLOOM [52], Pythia [7], LLaMA) of different sizes (125M to 65B) with different data types are evaluated on language modeling and a set of zero-shot tasks. In Figure 3 and Table 2 we see that NF4 improves performance significantly over FP4 and Int4 and that double quantization reduces the memory footprint without degrading performance. \n\nk-bit QLORA matches 16-bit full finetuning and 16-bit LoRA performance Recent findings have established that 4-bit quantization for inference is possible, but leads to performance degradation relative to 16-bit [13,18]. This raises the crucial question of whether the lost performance can be recovered by conducting 4-bit adapter finetuning. We test this for two setups. The first focuses on a comparison with full 16-bit finetuning of RoBERTA and T5 models sized 125M to 3B parameters on GLUE and the Super-NaturalInstructions dataset. Results are shown in Table 3. In both datasets, we observe that 16-bit, 8-bit, and 4-bit adapter methods replicate the performance of the fully finetuned 16-bit baseline. This suggests that the performance lost due to the imprecise quantization can be fully recovered through adapter finetuning after quantization. \n\nFor our second setup, since full finetuning models at and beyond 11B parameters requires more than one server of high memory GPUs, we continue to test whether 4-bit QLORA can match 16-bit LoRA at the 7B to 65B parameter scales. To this end, we finetune LLaMA 7B through 65B on two instruction following datasets, Alpaca and FLAN v2, and evaluate on the MMLU benchmark via 5-shot accuracy. Results are shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit LoRA MMLU performance.",
            "score": 0.6496070968287732,
            "section_title": "4-bit NormalFloat yields better performance than 4-bit Floating Point",
            "char_start_offset": 18538,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1443
                },
                {
                    "start": 1446,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1959
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.955078125
        },
        {
            "corpus_id": "273963038",
            "title": "When are 1.58 bits enough? A Bottom-up Exploration of BitNet Quantization",
            "text": "We have shown that BitNet models can be trained to yield commensurate accuracy with their standard 16/32-bit counterparts. In T5-like architectures, the BitNet models perform even better than the 16-bit models -which we attribute to a regularization effect. These results highlight that 1.58-bit quantization aware training methods could greatly reduce memory requirements and increase throughput at inference time for a wide range of models. \n\nThroughout Sections 3 and 4 we have shown that 1.58-bit models yield performance comparable to standard 16/32-bit models -both for nontransformer and transformer models. In some cases, the 1.58-bit models even outperform 16-bit models: on multilayer perceptrons (see Section 3.2), graph neural networks (Section 3.3, and encoder-decoder language models (Section 4.2). \n\nComparing the two weight quantization schemes for BitNet: AbsMedian and AbsMean, we have observed that AbsMedian is on par with AbsMean. Prior work has shown that AbsMedian performs better in some situations (Nielsen and Schneider-Kamp, 2024), conjecturing AbsMedian to be more resilient to weight-updates, which allows for higher variance without noticeable effect on the scaling factor. While this may be a factor for models with millions of parameters, the miniature models involved in solving the X-OR task seem to be extraordinarily sensitive when the scaling factor is computed on a much smaller sample. We conjecture that this is the source of the instability observed in some configurations (e.g, hidden size 8 and 16 with low learning rate), which we have observed to be dampened when increasing the hidden size or the learning rate. In the more practical scenarios, we see that AbsMean and AbsMedian quantization are close to each other with a 0.6% drop in accuracy for MLPs and within each other's confidence intervals for GNNs. This is in line with prior work for small language and vision models (Nielsen and Schneider-Kamp, 2024) and underlines the strong potential of b1.58 models.",
            "score": 0.6485341784859256,
            "section_title": "Discussion",
            "char_start_offset": 19207,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2011
                }
            ],
            "ref_mentions": [
                {
                    "start": 1023,
                    "end": 1057,
                    "matchedPaperCorpusId": "271212141"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57666015625
        },
        {
            "corpus_id": "237416732",
            "title": "On the Accuracy of Analog Neural Network Inference Accelerators [Feature]",
            "text": "As shown in Section 5, the accuracy loss due to accumulated cell errors can be minmized by using sufficiently precise memory cells and exploiting state-proportional errors. With less precise cells, some prior work has relied on ADC quantization to cut off the propagation of cell errors from layer to layer in a DNN [30,66,71]. Yang et al. [66] activated only a few rows per MVM, such that on average, the accumulated errors on a bit line fall below the separation of levels in an  ADC, as shown in Fig. 18(b). While this approach succeeds in suppressing error propagation, it reduces energy efficiency since many more analog MVMs (and ADC operations) are needed to process each layer. A coarse ADC can provide the same benefit without reducing the number of rows, as shown in Fig. 18(c), but the accuracy would suffer due to quantization errors, as discussed in Section 6. A purely hardware solution cannot solve this problem, but it is possible to eliminate the quantization errors by training a DNN to tolerate low-precision activations during inference. This would combine the benefits of low quantization errors, greater resilience to cell errors, and high energy efficiency. \n\nUnlike training techniques that are specialized for analog systems (see Section 2.5), quantization-aware training (QAT) benefits digital accelerators by reducing the computational load at inference time. Therefore, there has been much recent work on 4-bit or lower resolution networks with nearly no accuracy loss relative to floating-point networks [12,59,70]. Importantly, the broad applicability of low-precision networks increases the likelihood that QAT methods can be integrated into state-of-the-art training workflows. \n\nThis section evaluates a 4-bit QAT network with the ResNet50-v1.5 topology, submitted by Nvidia to the MLPerf Inference Benchmark [1]. The network uses 4-bit weights and activations in all layers except the first and last, which use 8-bit weights. Each ReLU output is multiplied by 16-bit scaling factors before quantizing to 4 bits. The digital software accuracy of this network is shown in Table 2.",
            "score": 0.6484805681660575,
            "section_title": "SUPPRESSING ERROR PROPAGATION",
            "char_start_offset": 53455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1709
                },
                {
                    "start": 1712,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2112
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "1589163"
                },
                {
                    "start": 320,
                    "end": 323,
                    "matchedPaperCorpusId": "189818963"
                },
                {
                    "start": 323,
                    "end": 326,
                    "matchedPaperCorpusId": "209405390"
                },
                {
                    "start": 340,
                    "end": 344,
                    "matchedPaperCorpusId": "189818963"
                },
                {
                    "start": 1540,
                    "end": 1543,
                    "matchedPaperCorpusId": "50784025"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "271902942",
            "title": "Efficient Onboard Multitask AI Architecture Based on Self-Supervised Learning",
            "text": "In addition to designing lightweight modules, neural network quantization [17] is also critical to improve memory requirements and inference speed. Our architecture underwent a post-training quantization process employing an 8-bit integer precision (INT8) scheme. In particular, static quantization is used to quantize both weights and activations and perform fully-integer inference. INT8 is particularly optimized for implementation on embedded devices and, generally, but not always, suffers from small penalties in inference accuracy. A different, task-specific calibration dataset has been used for quantization of each head.",
            "score": 0.6474342352707573,
            "section_title": "D. Quantization",
            "char_start_offset": 20906,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 630
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 78,
                    "matchedPaperCorpusId": "39867659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "273822014",
            "title": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM Quantization",
            "text": "The high computational cost of serving LLMs has driven extensive research into inference acceleration techniques, including quantization (Frantar et al., 2022;Dettmers and Zettlemoyer, 2022;Lin et al., 2024a), speculative decoding (Chen et al., 2023;Leviathan et al., 2023), and pruning (Xia et al., 2023;Muralidharan et al., 2024). Among these, quantization-reducing the bitwidth of weights, activations, or both-has emerged as the most widely used approach. However, its key challenge lies in balancing efficiency and accuracy. \n\nDespite progress, systematic benchmarks and practical deployment guidelines remain scarce. This uncertainty has fueled speculation around quantized models, exemplified by the initial skepticism toward the Llama-3.1-405B quantized model release (Dubey et al., 2024), which was later found to be near-lossless in LMSYS Arena user evaluations (Chiang et al., 2024). To address this gap, we pose the following core question: \n\nWhat are the practical accuracy-performance trade-offs for popular quantization formats? \n\nIn this study, we focus on widely supported, computationally efficient quantization formats. Specifically, we examine 8-bit weights and activations (W8A8), using integer (INT) precision for NVIDIA Ampere and older GPUs and floating-point (FP) precision for NVIDIA Hopper and Ada Lovelace. Additionally, we consider 4-bit integer weights with 16-bit activations (W4A16-INT), a competitive low-bit alternative. To evaluate accuracy, we implement a broad automated evaluation suite, spanning both academic and real-world benchmarks.",
            "score": 0.6465822193541128,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1575
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 190,
                    "matchedPaperCorpusId": "259076379"
                },
                {
                    "start": 190,
                    "end": 208,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 250,
                    "end": 273,
                    "matchedPaperCorpusId": "272694046"
                },
                {
                    "start": 305,
                    "end": 331,
                    "matchedPaperCorpusId": "271328221"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84228515625
        },
        {
            "corpus_id": "258657998",
            "title": "Systematic Literature Review on Cost-Efficient Deep Learning",
            "text": "Which bit-precision to use for quantization? 8-bit integers are a common choice for precision [S32], [S33], [S34]. 8-bit integers benefit from not requiring custom inference hardware but having a wide enough range to maintain model accuracy. Moderate accuracy loss has been achieved with 6bit precision [S35], 4-bit precision [S36], and ternary values [S37]. Quantization further reduces the model accuracy for simpler models [38]. The more complex the model and the more neurons the model has, the less quantization decreases the accuracy. When using low precision -binary or ternarymore complex models compensate for the accuracy loss. \n\nWhat is a good granularity level for bucketing? Bucketing can be performed for the entire model, by layer [S32] or by a CNN filter [S38]. Bucketing at the layer level can lead to fewer buckets, which requires smaller integer bit-width, which reduces the model size without significant accuracy loss [S32]. \n\nWhat to quantize: weights or activations? When quantizing the weights, the inference load is lowered. On the one hand, quantizing activations, including inputs, leads to inference-time bucketing, which adds computations. On the other hand, quantizing activations during inference can still be lighter than using floating point numbers for activations [S34]. \n\n3) BINARY NEURAL NETWORKS Binary neural networks (BNN) take the compression of weights and activations further by representing them with just one bit [S39]. Compared to 32-bit floating point DNNs, BNNs mean 32 times smaller models and radically reduced off-chip memory access during inference. Computations become light-weight as multiplications can be replaced by bit-wise operations, such as XNOR [39]. BNNs are promising for low latency and low energy inference, although optimal BNN execution benefits from custom hardware. Another significant drawback of BNNs is the reduced model accuracy. One suggested approach to increasing the BNN model accuracy is to combine ensemble methods with BNNs [S40].",
            "score": 0.6462228773337122,
            "section_title": "2) QUANTIZATION",
            "char_start_offset": 31297,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1305
                },
                {
                    "start": 1308,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2011
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93212890625
        },
        {
            "corpus_id": "271245049",
            "title": "ARTEMIS: A Mixed Analog-Stochastic In-DRAM Accelerator for Transformer Neural Networks",
            "text": "Given that SC demands 2  bits for each N-bit binary number, neural network model compression, particularly through quantization, can enhance the overall performance.Our analysis indicates that the utilization of 8-bit model quantization results in transformer inference accuracy levels comparable to those achieved with full precision (FP32), as depicted in Table IV.The % accuracy metric is used to assess transformer-base, BERT-base, Albert-base, and ViT models that are used for translation, sentiment analysis and image classification tasks respectively.Meanwhile the BLEU score metric is reported for the OPT-350 model that is used for a textgeneration task.Based on this analysis, we have opted for transformer models featuring 8-bit precision, where ARTEMIS represents parameter values stochastically with 128 bits plus one sign bit.Furthermore, we conducted detailed error analysis to assess the efficacy of each approximate computing operation in ARTEMIS as shown in Table V.The calibration accuracy represents the threshold in bits below which the computation results remain entirely accurate.For instance, in the case of stochastic multiplication, the output will begin to show small errors when the binary numbers involved exceed 4.68 bits in length.The mean absolute errors (MAEs) normalized to the maximum voltage supported by each operation, were accumulated and integrated into each transformer model inference.The resultant accuracy drop was found to be minimal as shown in table IV.Table IV presents the inference accuracies for the models employed in our experiments, for the baseline FP32, quantized 8-bit precision, and quantized 8-bit precision with SC multiplications cases.Through the avoidance of stochastic additions and the adoption of an optimized approach to stochastic multiplications, ARTEMIS demonstrates minimal accuracy degradation, averaging at 1.4% compared to FP32 and 0.5% compared to quantized 8-bit models.",
            "score": 0.6458567887986941,
            "section_title": "A. Computational Error and Accuracy Analysis",
            "char_start_offset": 44000,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 165,
                    "end": 367
                },
                {
                    "start": 367,
                    "end": 558
                },
                {
                    "start": 558,
                    "end": 663
                },
                {
                    "start": 663,
                    "end": 840
                },
                {
                    "start": 840,
                    "end": 984
                },
                {
                    "start": 984,
                    "end": 1103
                },
                {
                    "start": 1103,
                    "end": 1262
                },
                {
                    "start": 1262,
                    "end": 1427
                },
                {
                    "start": 1427,
                    "end": 1500
                },
                {
                    "start": 1500,
                    "end": 1697
                },
                {
                    "start": 1697,
                    "end": 1946
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89501953125
        },
        {
            "corpus_id": "273962606",
            "title": "Optimizing Large Language Models through Quantization: A Comparative Analysis of PTQ and QAT Techniques",
            "text": "Quantization involves reducing the precision of the model's weights and activations [10], typically from 32-bit floatingpoint (FP32) to lower bit-width representations such as 8bit integers (INT8) or even binary representations. Jacob et al. [11] introduced a pioneering approach to quantize neural networks for efficient integer-arithmetic-only inference, demonstrating significant speedups and memory savings with minimal loss in accuracy. Their work laid the foundation for subsequent advancements in post-training quantization (PTQ). Building on PTQ, Cheng et al. [12] explored fixedpoint quantization for deep neural networks, addressing the challenges of maintaining numerical stability and minimizing quantization errors. Their techniques have been instrumental in refining uniform quantization methods, ensuring reliable performance across various layers of neural networks.",
            "score": 0.6456005391432416,
            "section_title": "A. Quantization Techniques",
            "char_start_offset": 24141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 882
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 88,
                    "matchedPaperCorpusId": "273665467"
                },
                {
                    "start": 242,
                    "end": 246,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 568,
                    "end": 572,
                    "matchedPaperCorpusId": "32588614"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8154296875
        },
        {
            "corpus_id": "44071489",
            "title": "Scalable Methods for 8-bit Training of Neural Networks",
            "text": "While several works [8,6] have shown that training at 16-bit is sufficient for most networks, more aggressive quantization schemes were also suggested [29,21,19,13]. In the extreme case, the quantization process used only one bit which resulted in binarized neural networks (BNNs) [12] where both weights and activations were constrained to -1 and 1. However, for more complex models and challenging datasets, the extreme compression rate resulted in a loss of accuracy. Recently, Mishra et al. [20] showed that this accuracy loss can be prevented by merely increasing the number of filter maps in each layer, thus suggesting that quantized neural networks (QNNs) do not possess an inherent convergence problem. Nevertheless, increasing the number of filter maps enlarge quadratically the number of parameters, which raises questions about the efficiency of this approach. \n\nIn addition to the quantization of the forward pass, a growing interest is directed towards the quantization of the gradient propagation in neural networks. A fully quantized method, allowing both forward and backward low-precision operations will enable the use of dedicated hardware, with considerable computational, memory, and power benefits. Previous attempts to discretize the gradients managed to either reduce them to 16-bit without loss of accuracy [6] or apply a more aggressive approach and reduce the precision to 6-8 bit [29,13] with a noticeable degradation. Batch normalization is mentioned by [26] as a bottleneck for network quantization and is either replaced by a constant scaling layer kept in full precision, or avoided altogether; this clearly has some impact on performance (e.g., AlexNet trained over ImageNet resulted with top-1 error of 51.6%, where the state of the art is near 42%) and better ways to quantize normalization are explicitly called for. Recently L1 batch norm with only linear operations in both forward and backward propagation was suggested by [27,11] with improved numerical stability. Yet, our experiments show that with 8-bit training even L1 batch norm is prone to overflows when summing over many large positive values.",
            "score": 0.6453028872287185,
            "section_title": "Previous Work",
            "char_start_offset": 2758,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2143
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 23,
                    "matchedPaperCorpusId": "2547043"
                },
                {
                    "start": 158,
                    "end": 161,
                    "matchedPaperCorpusId": "10533533"
                },
                {
                    "start": 281,
                    "end": 285,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 1484,
                    "end": 1488,
                    "matchedPaperCorpusId": "3603886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84375
        },
        {
            "corpus_id": "234337594",
            "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit",
            "text": "Quantization has become a popular technique to compress neural networks and reduce compute cost, but most prior work focuses on studying quantization without changing the network size. Many real-world applications of neural networks have compute cost and memory budgets, which can be traded off with model quality by changing the number of parameters. In this work, we use ResNet as a case study to systematically investigate the effects of quantization on inference compute cost-quality tradeoff curves. Our results suggest that for each bfloat16 ResNet model, there are quantized models with lower cost and higher ac-curacy; in other words, the bfloat16 compute cost-quality tradeoff curve is Pareto-dominated by the 4-bit and 8-bit curves, with models primarily quantized to 4-bit yielding the best Pareto curve. Furthermore, we achieve state-of-the-art results on ImageNet for 4-bit ResNet-50 with quantization-aware training, obtaining a top-1 eval accuracy of 77.09%. We demonstrate the regularizing effect of quantization by measuring the generalization gap. The quantization method we used is optimized for practicality: It requires little tuning and is designed with hardware capabilities in mind. Our work motivates further research into optimal numeric formats for quantization, as well as the development of machine learning accelerators supporting these formats. As part of this work, we contribute a quantization library written in JAX, which is open-sourced at https://github.com/google-research/google-research/tree/master/aqt.",
            "score": 0.6450294116604723,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97705078125
        },
        {
            "corpus_id": "253157964",
            "title": "Too Brittle to Touch: Comparing the Stability of Quantization and Distillation towards Developing Low-Resource MT Models",
            "text": "Quantization is a common way to reduce the computational time and memory consumption of neural networks (Wu et al., 2020). Here, a lower-bit representation of weights and activation functions is used to achieve a lower memory footprint. In this work, we perform post-training quantization, where after training the base model with full precision of floating point 32 bits (fp-32), we convert the weights and activations of the model to 8 bit integers (int-8). Note that during inference, we still preserve the precision of the input and output encoder-decoder distributions as fp-32. In theory, this brings down the memory consumption of the model by nearly 4x times, though we see an effective reduction of about 3x in practice. More details on the memory-reductions achieved are specified in the Appendix A.4 \n\n3 Experimental Setup",
            "score": 0.6449909016331049,
            "section_title": "Size Adaptation: Quantization",
            "char_start_offset": 10875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 833
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6357421875
        },
        {
            "corpus_id": "237416732",
            "title": "On the Accuracy of Analog Neural Network Inference Accelerators [Feature]",
            "text": "This section describes the methodology for inference accuracy evaluation for the results presented in the remaining sections. Unless otherwise stated, neural networks are quantized to 8-bit precision, a common use case for inference [31,35].",
            "score": 0.6442727302785646,
            "section_title": "ACCURACY EVALUATION METHOD",
            "char_start_offset": 22789,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 241
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 237,
                    "matchedPaperCorpusId": "39867659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53955078125
        },
        {
            "corpus_id": "273185692",
            "title": "Differential Transformer",
            "text": "Figure 8, we quantize the attention logits to lower bits. We apply dynamic post-training quantization using absmax quantization (Wan et al., 2024). The 16-bit configuration represents the original results without quantization. The models are progressively quantized to 8 bits, 6 bits, and 4 bits. Figure 8 reports the zero-shot performance on HellaSwag (Gao et al., 2023). The other datasets follow a similar trend. DIFF Transformer retains high performance even at reduced bit-widths, ranging from 16 bits to 6 bits. In comparison, Transformer's accuracy significantly drops with 6-bit quantization. The 4bit DIFF Transformer achieves comparable accuracy as the 6-bit Transformer, and outperforms the 4-bit Transformer by about 25% in accuracy. The results indicate that DIFF Transformer natively mitigates activation outliers in attention scores, providing new opportunities for low-bit FlashAttention (Dao et al., 2022) 6: Ablation studies of 1.4B-size models. We report language modeling loss on the validation set. We also follow Arora et al. (2023) to report fine-grained metrics, where \"AR-Hit\" evaluates n-grams previously seen in the context. \"#Heads\" is number of heads. \"d\" is head dimension. \"GN\" indicates whether GroupNorm is used.",
            "score": 0.644234022412169,
            "section_title": "Quantization of Attention Logits As shown in",
            "char_start_offset": 21148,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 58,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1245
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 146,
                    "matchedPaperCorpusId": "266044196"
                },
                {
                    "start": 904,
                    "end": 922,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.888671875
        },
        {
            "corpus_id": "251272058",
            "title": "Convolutional Neural Networks Quantization with Attention",
            "text": "To enable neural networks to function on edge devices, researchers have developed various optimization methods in the past few years, such as quantization 1 , pruning 2,3 , and neural architecture search 4 . These methods optimize the network from different perspectives and are not mutually exclusive. Our work mainly studies network quantization. \n\nQuantization not only saves storage costs by several or even tens of times, but it also reduces computing complexity by switching from floating-point to integer operations. Both optimizations brought by quantization are especially beneficial for embedded devices that run neural networks with limited memory and computing resources. Most of the current deep learning libraries, such as TensorFlow 5 and Py-Torch 6 , only support mature 16/8-bit quantization without loss of accuracy. However, for applications targetting extremely low costs, 16/8-bit quantization is still insufficient. \n\nPrevious work [7][8][9] has shown that networks inference with low bit-width is feasible, and milestone progress has been made. On the other side, given the great success of the attention mechanism on CNNs, we found that it also helps to quantize the networks. Based on the previous work, ST quantization 10 , we further study the application of the attention mechanism in network quantization. We upgrade ST to double-stage ST, which is illustrated in Section 3. Section 4 demonstrates the experimental results. The conclusion part completes the paper. The proposed double-stage ST quantization overcomes the hardware-unfriendly-input-channel-wise quantization in ST and further improves the accuracy of low-precision networks. In particular, the accuracy of the 3-bit model exceeds the accuracy of the baseline model.",
            "score": 0.6440939016042224,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 348
                },
                {
                    "start": 351,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 937
                },
                {
                    "start": 940,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1759
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 169,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 169,
                    "end": 170,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 204,
                    "end": 205,
                    "matchedPaperCorpusId": "146808333"
                },
                {
                    "start": 763,
                    "end": 764,
                    "matchedPaperCorpusId": "202786778"
                },
                {
                    "start": 957,
                    "end": 960,
                    "matchedPaperCorpusId": "67788003"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91015625
        },
        {
            "corpus_id": "234337594",
            "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit",
            "text": "We ran experiments on ResNet50 with different number of parameters and quantization bits. To change the 2. 4-bit, with first/last in 8-bit: All layers in 4-bit, except first and last layers are in 8-bit. It is common practice to quantize these two layers to a lesser extent [6,33,6], since they tend to be most sensitive to quantization. We also found through our own layer sensitivity analysis that the first layer in ResNet is the most sensitive to quantization, followed by the last layer. Some solutions exist to address the challenge of quantizing first and last layers in neural networks, e.g. by remapping the input features [35]. We chose not to explore these special case methods, as it is not the focus of this work.\n\n3. 8-bit: All layers quantized to 8-bit. 4. bfloat16: Baseline, all layers in bfloat16.\n\nThe cross-product of the filter multipliers and quantization settings results in a total of 36 experiments. Figure 1 showcases compute cost and Top-1 eval accuracy for ResNet50 with different parameters and quantization settings. The x-axis shows the linear compute cost (see Section 4.1) normalized to that of the baseline bfloat16 model. In this figure, we are comparing the four quantization settings described above. Each curve corresponds to a quantization setting, the different points on the curve correspond to different filter multipliers. The Pareto curves show the tradeoff between the compute cost and the accuracy in our experiments (towards upper-left is better).\n\nAs the curves in Figure 1 show, the 4-bit models with the first and last layers quantized to 8 bits achieve the best compute cost-accuracy Pareto curve, outperforming bfloat16,  [34] 76.40% --75.10% -1.30% SSPS [28] 77.15% --76.22% c -0.93% PACT [6] 76.90% --76.50% b -0.40% HAQ [31] 76.15% --76.14% c -0.01% ZeroQ [5",
            "score": 0.6413778715916012,
            "section_title": "Compute Cost-Accuracy Tradeoff Experiments",
            "char_start_offset": 19493,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 632,
                    "end": 636,
                    "matchedPaperCorpusId": "229348728"
                },
                {
                    "start": 1674,
                    "end": 1678,
                    "matchedPaperCorpusId": "50784025"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94970703125
        },
        {
            "corpus_id": "221112343",
            "title": "Compression of Deep Learning Models for Text: A Survey",
            "text": "[15] were the first to quantize Transformers. They observed that in the last attention layer of the decoder over the encoder hidden states, the attention distribution of the original and 4-bit model are highly similar, indicating 4 bit weights, i.e weights that take on one of 16 values, is enough to get the full effects of attention. Attention distributions in the encoder layers of the Transformer for the original and 4-bit models are almost indistinguishable from one another. This again highlights the idea that self-attention is highly resistant to quantization and could be heavily compressed. Later Shen et al. [104] showed that comparable performance to full precision BERT can be achieved with at most 2.3% performance degradation across many tasks, even with ultra-low precision quantization down to 2 bits, corresponding up to 13x compression of the model parameters, and up to 4x compression of the embedding table as well as activations.\n\nOverall, quantization performs model compression by reducing the number of bits per weight value. Binary quantization does not work well by itself for text based neural models. But ternary and higher-bit quantization lead to significant model size reduction without loss in accuracy across tasks. One consideration for quantization is that 3-bit quantized execution is typically not supported in hardware. It is however possible to load 3-bit quantized values and cast them to higher bit precision such as 4 or 8 bits in the execution units. This would still have the benefit of reduced memory volume to/from DRAM. Non-uniform quantization methods like balanced quantization or KMeans based quantization methods are better than uniform quantization methods. Loss aware quantization done while training is better than static loss-unaware quantization. Mixed-precision quantization combined with pruning is highly effective for Transformer based models.",
            "score": 0.6405543971669646,
            "section_title": "Summary",
            "char_start_offset": 56918,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "218674432",
            "title": "VecQ: Minimal Loss DNN Model Compression With Vectorized Weight Quantization",
            "text": "D EEP Neural Networks (DNNs) have been widely adopted in machine learning based applications [1], [2]. However, besides DNN training, DNN inference is also a computation-intensive task which affects the effectiveness of DNN based solutions [3], [4], [5]. Neural network quantization employs low precision and low bitwidth data instead of high precision data for the model execution. Compared to the DNNs with floating point with 32-bit width (FP32), the quantized model can achieve up to 32\u00c2 compression rate with an extremely low-bitwidth quantization [6]. The lowbitwidth processing, which reduces the cost of the inference by using less memory and reducing the complexity of the multiply-accumulate operation, improves the efficiency of the execution of the model significantly [5], [7].\n\nHowever, lowering the bitwidth of the data often brings accuracy degradation [4], [8], [9]. This requires the quantization solution to balance between computing efficiency and final model accuracy. However, the quantitative trade-off is nonconvex and hard to optimizethe impact of the quantization to the final accuracy of the DNN models is hard to formulate.\n\nPrevious methods neglect the quantitative analysis of the Direct Quantization Loss (DQL) of the weight data and make the quantization decision empirically while directly evaluating the final model accuracy [6], [10], [11], [12], [13] thus only achieving unpredictable accuracy.\n\nIn order to achieve higher training accuracy, finding an optimal quantization solution with minimal loss during the training of the learning kernels is effective and practical. One way of finding a local optimal solution is to minimize the DQL of the weight data, which is widely used in the current quantization solutions [14], [15], [16], [17], [18].\n\nAs shown in Fig. 1, w f denotes the full-precision weight and w q is the value after quantization. Conventional quantization methods regard w f as a point (set as origin in Fig. 1) in Euclidean Space, and w q is a point which is close to w f in a",
            "score": 0.6405155033055708,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 96,
                    "matchedPaperCorpusId": "14124313"
                },
                {
                    "start": 98,
                    "end": 101,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 240,
                    "end": 243,
                    "matchedPaperCorpusId": "67871410"
                },
                {
                    "start": 245,
                    "end": 248,
                    "matchedPaperCorpusId": "104292236"
                },
                {
                    "start": 250,
                    "end": 253,
                    "matchedPaperCorpusId": "53243741"
                },
                {
                    "start": 553,
                    "end": 556,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 781,
                    "end": 784,
                    "matchedPaperCorpusId": "53243741"
                },
                {
                    "start": 786,
                    "end": 789,
                    "matchedPaperCorpusId": "51974633"
                },
                {
                    "start": 869,
                    "end": 872,
                    "matchedPaperCorpusId": "104292236"
                },
                {
                    "start": 874,
                    "end": 877,
                    "matchedPaperCorpusId": "202699949"
                },
                {
                    "start": 879,
                    "end": 882,
                    "matchedPaperCorpusId": "9455864"
                },
                {
                    "start": 1359,
                    "end": 1362,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 1364,
                    "end": 1368,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 1376,
                    "end": 1380,
                    "matchedPaperCorpusId": "14395129"
                },
                {
                    "start": 1382,
                    "end": 1386,
                    "matchedPaperCorpusId": "3500279"
                },
                {
                    "start": 1761,
                    "end": 1765,
                    "matchedPaperCorpusId": "13556195"
                },
                {
                    "start": 1767,
                    "end": 1771,
                    "matchedPaperCorpusId": "8840788"
                },
                {
                    "start": 1773,
                    "end": 1777,
                    "matchedPaperCorpusId": "52847187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8505859375
        },
        {
            "corpus_id": "240353756",
            "title": "RMSMP: A Novel Deep Neural Network Quantization Framework with Row-wise Mixed Schemes and Multiple Precisions",
            "text": "Non-linear quantization schemes use non-uniform quantization levels, such as the Power-of-Two (PoT) scheme, where quantization levels are power-of-two numbers. The multiplication of a weight (in PoT) and an input (in Fixed) can be replaced with the bit shifting operation. And therefore PoT can have higher speedup than the Fixed scheme for the DNN inference. PoT quantization was adopted first in LogQuant [24], and then with model accuracy enhancement techniques in INQ [37], extremely low bit neural network with ADMM [19], and LQ-Nets [36]. However, PoT suffers from the rigid resolution phenomenon, and cannot attain higher model accuracy even with increased bit-width. Particularly, 4-bit PoT quantization will result in accuracy loss of 1% \u223c 2%. \n\nTo overcome the limitation, Additive Power-of-Two (APoT) [21] represents quantization levels as the sum of multiple power-of-two numbers to mitigate the accuracy loss. Then the multiplication of an APoT quantized weight and a Fixed input can be replaced with bit shifting operations and additions. Furthermore, MSQ [2] leverages a mixture of a variant APoT and Fixed to maximize the resource utilization on FPGA devices, while in this work, we use mixed schemes of PoT and Fixed, achieving even higher accuracy and further reducing inference computation. All of the above works use the single-precision quantization.",
            "score": 0.6401121989195018,
            "section_title": "Non-Linear Quantization Schemes",
            "char_start_offset": 7903,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1371
                }
            ],
            "ref_mentions": [
                {
                    "start": 472,
                    "end": 476,
                    "matchedPaperCorpusId": "12130431"
                },
                {
                    "start": 521,
                    "end": 525,
                    "matchedPaperCorpusId": "8840788"
                },
                {
                    "start": 539,
                    "end": 543,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 812,
                    "end": 816,
                    "matchedPaperCorpusId": "210848001"
                },
                {
                    "start": 1070,
                    "end": 1073,
                    "matchedPaperCorpusId": "227744972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93115234375
        },
        {
            "corpus_id": "270870579",
            "title": "Joint Pruning and Channel-Wise Mixed-Precision Quantization for Efficient Deep Neural Networks",
            "text": "For all the three \"High\" DNNs (leftmost plot), most of the weight parameters are quantized at 8 bits, i.e. the highest possibile precision, as expected. In \"Medium\" models (central plot), very few parameters, if any, are quantized at 2 bits, since it is the least interesting precision due to the low representational capacity, but not negligible cost. In contrast, 4-bit channels represent a significant component, especially for the size and NE16 regularizers, costing less then twice the 2-bit ones while providing significant accuracy advantages. For what concerns \"Low\" models, except when trained with the MPIC regularizer, the 4-bit precision is assigned more often than 8-bit. In particular, with a more deepened analysis, it is possible to see that the 8-bit precision is favored only in the final layer, which is a common finding [7], [28]. Size is the only cost regularizer leading to some weights' channels being quantized at 2 bits. The MPIC cost model mainly favors pruning and keeps most of the other weights at 8 bits, since there is not a sufficient cost difference between this and smaller bit-widths. The NE16 cost model, instead, encourages a more spread-out distribution between 4-and 8-bit but entirely avoids 2-bit precision. The reason is that the NE16 cost model does not scale linearly with the output channels, as each processing element (PE) handles groups of 32 output channels (Sec. 4.3.3). Consequently, running a single channel at one precision incurs the same cost as running 32 channels, implying that to enhance latency, the NE16 accelerator should execute at least 32 2-bit filters. However, this would lead to suboptimal solutions from an accuracy standpoint, and is thus avoided by the optimization.",
            "score": 0.6394651388785003,
            "section_title": "Models analysis",
            "char_start_offset": 59907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1737
                }
            ],
            "ref_mentions": [
                {
                    "start": 840,
                    "end": 843,
                    "matchedPaperCorpusId": "215745195"
                },
                {
                    "start": 845,
                    "end": 849,
                    "matchedPaperCorpusId": "220363587"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.951171875
        },
        {
            "corpus_id": "234337594",
            "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit",
            "text": "In this work, we analyzed how quantization at different precisions influences the compute cost-quality Pareto curves on ResNet models. We found that quantization consistently improves the tradeoff regardless of where we are on the compute cost-quality tradeoff graph. In other words, for each bfloat16 model, we found quantized models with lower compute cost and higher accuracy. Additionally, models in 4-bit (with first and last layers in 8-bit) offer a consistent advantage over 8-bit, using both linear and quadratic cost models. This observation suggests that 4-bit may be a preferred numeric format for quantizing neural networks. Based on our results, we proposed a simple practical approach to compress models given compute cost and quality constraints, consisting of two steps: quantize the model to 4-bit, then multiply the number of parameters in each layer by a global factor to achieve the desired tradeoff. We invite further research into comparing different numeric formats for quantization, and hope that this line of work will inform the development of future hardware. We also encourage more works in model compression to compare methods on cost-quality tradeoff graphs, as this provides a more nuanced and thorough analysis. Lastly, we open-sourced our quantization library, in the hopes that it will accelerate quantization research and deployment with JAX.",
            "score": 0.6394526637336789,
            "section_title": "Conclusion",
            "char_start_offset": 23412,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98583984375
        },
        {
            "corpus_id": "85502181",
            "title": "BitSplit-Net: Multi-bit Deep Neural Network with Bitwise Activation Function",
            "text": "This indicates the proposed scheme has good bit-scalability. We further show that the BitSplit version of a network can be trained to have the same level of accuracy as that of its original counterpart. For example, BitSplit versions of AlexNet [12] and ResNet-18 [8] showed little accuracy difference (< \u00b10.5%) compared to state-of-theart quantized neural networks [2,3,10,16,19,21]. We also evaluate the hardware benefit of BitSplit scheme on GPU platforms. For the state-of-the-art GPU environment, we implemented custom CUDA kernel for BitSplit scheme. We evaluate the speed-up of BitSplit kernel on matrix-vector multiplication and forward propagation of MLP on MNIST dataset. As a result, BitSplit kernel achieved 4.26\u223c12.5x speed-up for matrix-vector multiplication and 2.65\u223c5.89x speed-up for inference on MNIST benchmark. Recently proposed quantized neural networks use limited bit precision for weights and/or activations. In order to extract meaningful information from high-precision data, the most significant bit (MSB) and the next few bits are often chosen. For example, quantization of an 8-bit unsigned integer number into 2-bit equals means selecting the MSB and the second-most significant bit (2nd MSB) then truncating all the other following bits. While MSBs are generally more important than 2nd MSBs for correct classification, 2nd MSBs often provide more detailed information than MSBs. Thus, in general, information from 2nd MSBs are added to that from MSBs in multi-bit neural networks. However, sometimes 2nd MSBs alone may have enough importance. Figure . 1 shows an image from ImageNet dataset [4]. Each image is the result of quantization to (a) 8-bit, (b) 1-bit (MSB), and (c) 1-bit (2nd MSB). MSBs in the image represent the boundary between the dog and the background (Figure 1b). On the other hand, 2nd MSBs in the image show more detailed information such as the eyes of the dog (Figure 1c).",
            "score": 0.6389444761558924,
            "section_title": "Introduction",
            "char_start_offset": 2121,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1926
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 249,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 264,
                    "end": 267,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 374,
                    "end": 377,
                    "matchedPaperCorpusId": "10533533"
                },
                {
                    "start": 377,
                    "end": 380,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 1623,
                    "end": 1626,
                    "matchedPaperCorpusId": "57246310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80419921875
        },
        {
            "corpus_id": "271050128",
            "title": "Real-Time Spacecraft Pose Estimation Using Mixed-Precision Quantized Neural Network on COTS Reconfigurable MPSoC",
            "text": "This results in the initial depthwise convolution having merely 288 parameters, as highlighted in Figure 3.This finding suggests that architectural modifications in recent designs [30], influenced by vision transformers that position depthwise convolutions before expansion layers, might be sensitive to low-bit quantization.Further experiments have revealed that activations are notably more sensitive to low-bit quantization, with quantization errors below 4 bits frequently causing orientation decoding errors and disrupting the neural network's training process.This significant constraint influences the layer-wise bit-width activation choices in the next paragraph.Our findings also indicate that the binarization of early layers diminishes the generalization capabilities of the neural network.However, it improves in later layers, suggesting that low-bit quantization compromises information in the initial layers while acting as regularization in the last layers.Additionally, the sensitivity of early layers to quantization predominantly affects orientation estimation, as opposed to position estimation, which is comparatively simpler [10].Therefore, quantization below 8 bits has a more pronounced impact on orientation accuracy than on position accuracy.\n\nThis analysis leads us to identify an optimal balance among layer-wise bit-width, FPGA resource utilization, and throughput.Given the limitations of FINN, we quantize only the neural network's backbone for FPGA deployment, leaving the heads in Float32.We observed that activations are highly sensitive to low-bit quantization, yet the hardware resources required increase exponentially with the activation bit-width due to the current FINN C++ HLS backend [25].Consequently, we quantize all activations to four bits to conserve FPGA resources, as higher bit-widths would disproportionately consume resources [27].Weight bit-width selection varies by layer index due to differing quantization sensitivities; the weights of the first layer are quantized at four bits, reflecting their higher sensitivity.The most sensitive layer, the first depthwise convolution, uses six-bit weights.Subsequently, the first projection convolution weights are set to four bits, and the weights of all remaining 49 convolutional layers are set to three bits.",
            "score": 0.6386364395345332,
            "section_title": "III. CO-DESIGN METHODOLOGY A. Methodology Overview",
            "char_start_offset": 8839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 671
                },
                {
                    "start": 671,
                    "end": 801
                },
                {
                    "start": 801,
                    "end": 972
                },
                {
                    "start": 972,
                    "end": 1151
                },
                {
                    "start": 1151,
                    "end": 1267
                },
                {
                    "start": 1269,
                    "end": 1393
                },
                {
                    "start": 1393,
                    "end": 1521
                },
                {
                    "start": 1521,
                    "end": 1730
                },
                {
                    "start": 1730,
                    "end": 1882
                },
                {
                    "start": 1882,
                    "end": 2071
                },
                {
                    "start": 2071,
                    "end": 2151
                },
                {
                    "start": 2151,
                    "end": 2307
                }
            ],
            "ref_mentions": [
                {
                    "start": 1146,
                    "end": 1150,
                    "matchedPaperCorpusId": "248512548"
                },
                {
                    "start": 1877,
                    "end": 1881,
                    "matchedPaperCorpusId": "219886219"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95703125
        },
        {
            "corpus_id": "257913374",
            "title": "RPTQ: Reorder-based Post-training Quantization for Large Language Models",
            "text": "Post-training quantization is a powerful technique for compressing neural networks. Although non-uniform quantization can achieve a relatively small quantization error, they require specialized hardware that is not widely accessible [16]. In contrast, uniform quantization is a more practical and feasible approach that can be efficiently executed on regular hardware devices. Therefore, our study focuses on uniform quantization techniques. Typically, we use uniform quantization function Q k to transform a float value x to k bits integer x q as follows: \n\nwhere s represents the scaling factor, z denotes the zero point, and the clamp function constrains the value within the range of a k-bit integer, specifically \n\n. For a 4-bit integer, the range is [-8, 7]. The integer x q can be de-quantized to x = s(x q \u2212 z) \u2248 x. The de-quantized value x is a float. The quantization parameters, scale factor s, and zero point z must be stored in memory for both quantization and de-quantization processes. To further reduce the storage and computational overhead of quantization, multiple weights or activation values X = {x 1 , ..., x n } share the same quantization parameters. \n\nThere are three steps in post-training quantization (PTQ). The first step is to specify the quantization settings, which include the bit-width k and quantization type. The bit-width determines the number of bits used to represent a numerical value in a quantized format. The quantization types include static and dynamic quantization. Static quantization is a method in which the quantization parameters of activations are computed prior to deployment, and all inputs during runtime are quantized using the same set of parameters. Dynamic quantization, on the other hand, set the quantization parameters during runtime [35]. Dynamic quantization is generally more accurate than static quantization, as it can adapt to the distribution of activations. However, it can result in slower inference times, as the quantization parameters should be computed on-the-fly for each input data. We focus on static quantization in this paper.",
            "score": 0.6372083419684624,
            "section_title": "Post-training Quantization",
            "char_start_offset": 8525,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 556
                },
                {
                    "start": 559,
                    "end": 717
                },
                {
                    "start": 720,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1174
                },
                {
                    "start": 1177,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2106
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 237,
                    "matchedPaperCorpusId": "251928917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.818359375
        },
        {
            "corpus_id": "221655691",
            "title": "Fast Implementation of 4-bit Convolutional Neural Networks for Mobile Devices",
            "text": "For example, Google's gemmlowp library [13] provides low-precision matrix multiplication for 8-bit QNNs. They use 8-bit unsigned integers for inputs and accumulate the result into 32-bit integers. In Facebook's QNNPACK [14], authors also use 8-bit unsigned integers for inputs but re-quantize 32-bit intermediates to get 8-bit outputs. \n\nOf course, there are many QNNs, which use 8-bit [15], 4-arXiv:2009.06488v1 [cs.CV] 14 Sep 2020 \n\nbit [16], 2-bit ternary [17] even binary [18] and other lowbit quantization schemes [19]. The main problem of such approaches is that they can achieve high performance basically on special chips [20] or FPGA [21] because modern central (CPU) and graphical (GPU) processors do not provide enough commands to work with separate bits: they only allow to access 8-bit (or multiple of 8-bit) blocks and perform computations on them. Therefore, efficient QNN implementations for CPUs mainly do not use less than 8-bit matrices and highly specialize for a target CPU architecture to achieve inference time speed up [22]. \n\nIn this paper, we provide an algorithm for fast inference of 4-bit quantized neural network on CPU. As was proven before, CNN models with 4-bit quantization are viable, and even that with just a few percent accuracy degradation, retraining CNN models may be unnecessary for 4-bit quantization [23]. But to our best knowledge, no research was focused on an efficient implementation of 4-bit QNNs. At first, we introduce our fast matrix multiplication algorithm for 4-bit quantized matrices with a 16-bit integer result matrix. It uses the advantages of modern mobile CPU architectures on the example of the ARM architecture. We compare its efficiency with an 8-bit multiplication method similar to one from gemmlowp library, with traditional floating-point and 32-bit integer matrix multiplication.",
            "score": 0.6368758697899671,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 335
                },
                {
                    "start": 338,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1048
                },
                {
                    "start": 1051,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1848
                }
            ],
            "ref_mentions": [
                {
                    "start": 439,
                    "end": 443,
                    "matchedPaperCorpusId": "11119748"
                },
                {
                    "start": 459,
                    "end": 463,
                    "matchedPaperCorpusId": "2951619"
                },
                {
                    "start": 476,
                    "end": 480,
                    "matchedPaperCorpusId": "53761197"
                },
                {
                    "start": 519,
                    "end": 523,
                    "matchedPaperCorpusId": "198903430"
                },
                {
                    "start": 630,
                    "end": 634,
                    "matchedPaperCorpusId": "25193768"
                },
                {
                    "start": 1043,
                    "end": 1047,
                    "matchedPaperCorpusId": "218930341"
                },
                {
                    "start": 1344,
                    "end": 1348,
                    "matchedPaperCorpusId": "59292009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88818359375
        },
        {
            "corpus_id": "52197199",
            "title": "Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference",
            "text": "Fine-tuning matches or exceeds the accuracy of the initial high-precision network and outperforms other methods FAQ trained 8-bit networks outperform all comparable quantization methods in all but one instance and exceeded pretrained fp32 network accuracy with only one epoch of training following quantization for all networks explored (Table 1). Immediately following quantization, network accuracy was nearly at the level of the pretrained networks (data not shown) with one exception, Inception-v3, which started at 72.34% top-1. Since the networks started close to a good solution, they did not require extensive fine-tuning to return to and surpass pretrained networks. \n\nFAQ trained 4-bit network accuracy outperforms all comparable quantization methods, exceeding the next closest approach by over 0.5% for ResNet-18 (Jung et al. 2018), and matched or exceeded pretrained fp32 network accuracy. FAQ 4-bit networks required significantly longer fine-tuning -110 epochs -for networks trained, ResNet-18, ResNet-34, and ResNet-50. In constrast to the 8-bit cases, immediately following quantization, network accuracy dropped precipitously, requiring significant fine-tuning to match and surpass the pretrained networks. \n\nFAQ trained 4-bit network accuracy is sensitive to several hyperparameters (Table 2). In summary, longer training duration, initialization from a pretrained model, larger batch size, lower weight decay, and activation calibration all contribute to improved accuracy when training the 4-bit ResNet-18 network, while the exact learning rate decay schedule contributed the least. We elaborate on some of these results subsequently.",
            "score": 0.6366464818617033,
            "section_title": "Experiments",
            "char_start_offset": 12749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1655
                }
            ],
            "ref_mentions": [
                {
                    "start": 825,
                    "end": 843,
                    "matchedPaperCorpusId": "195347490"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "275104652",
            "title": "Configurable Multi-Layer Perceptron-Based Soft Sensors on Embedded Field Programmable Gate Arrays: Targeting Diverse Deployment Goals in Fluid Flow Estimation",
            "text": "This lack of a clear correlation suggests that other factors, such as the interplay between layer count, neuron count, and the inherent noise resilience of the dataset, may play a significant role in determining the quantization impact. In general, these findings reaffirm the effectiveness of 8-bit quantization for most configurations, with precision losses being minimal and occasionally yielding improvements due to potential regularization effects introduced by quantization noise. \n\nAt 6-bit quantization, models generally maintain acceptable performance, but the reduction in bitwidth introduces a more pronounced degradation in precision compared to 8-bit quantization. As shown in Figure 14, the Test MSE and Test MAPE values increase across nearly all configurations relative to their 8-bit counterparts. For instance, the seven-layer, 120-neuron model now has a Test MSE of 68.91 and a Test MAPE of 1.76%, representing an increase of 19.65% in Test MSE compared to the 8-bit version. Smaller models are also obviously affected. The four-layer, 10-neuron model exhibits a Test MSE of 88.41 and a Test MAPE of 2.06%, a substantial increase from its performance at 8-bit quantization. Figure 13 further highlights the differences in Test MSE between the 6-bit models and their FP32 counterparts. Across all configurations, the percentage difference in Test MSE ranges from 12.48% to 32.36%. However, as with the 8-bit results, the effects of model complexity (i.e., layer count and neuron count) on quantization sensitivity remain inconsistent. Despite these challenges, Test MAPE values remain around 2% for all configurations, indicating that 6-bit quantization can still meet industrially acceptable precision requirements. This makes it a practical choice for scenarios where resource efficiency is a priority, providing a viable trade-off between precision and computational cost. At 4-bit quantization, the effects of reduced precision become far more pronounced, leading to significant increases in Test MSE and MAPE across configurations. Figure 15 reveals that most configurations exhibit considerable performance degradation.",
            "score": 0.636452930906023,
            "section_title": "Experiments 2: Quantized Models Analysis",
            "char_start_offset": 40969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 486
                },
                {
                    "start": 489,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2143
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "207870482",
            "title": "Adaptive Precision Training: Quantify Back Propagation in Neural Networks with Fixed-point Numbers",
            "text": "Using reduced precision for deep learning has been an active research topic. Prior efforts explore floatingpoints(e.g., 8-bit and 16-bit) for training [34,22] and maintain accuracy on a spectrum of deep learning models and datasets. However, as floating-point is more resourceintensive than fixed-point, the deployments always rely on quantization techniques. \n\nA branch of work explores the fixed-point for forward prorogation(FPROP) [16,17,6,33,35,35,37]. The weights and activations are quantified to 1-8 bits. However, the backward-pass, including gradient propagation (BPROP), and weight gradient computation (WTGRAD) still require float32. \n\nThere are recent attempts quantifying weight and activation on different layers with different bit-widths. For the inference of a trained network, there are some techniques that heuristically search the space of quantization bit-width combinations [35,33,37]. However, these inference techniques only need to consider single iteration, whose search space is much smaller than training. Hence, they are unsuitable for training. For training, some differentiable quantization methods [4,30,37] learn the quantization parameters (e.g., step size, dynamic range and bit-width) with gradient descent. However, the quantization parameters for backward propagation are hard to learn using differentiable    methods. [26] quantifies the backward propagation. Different from their method, which assigns layer-wise bit-width before training, our approach dynamically changes the bitwidth during training and we evaluate on widely used networks. \n\nResearchers have shown that 16-bit is sufficient for back propagation in most vision training tasks [7]. However, further quantization to 8-bit results in severe degradation [38,38,36,7,1]. WAGE [36] claims that first and last layers require higher precision. TBP [1] shows weight gradient computation (WTGRAD) needs more bits than gradient back propagation (BPROP). \n\nOur approach is different from others in three aspects. First, fixed-point is used in both forward-pass and backward-pass for training.",
            "score": 0.6361181725630624,
            "section_title": "Related Works",
            "char_start_offset": 4755,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 359
                },
                {
                    "start": 362,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1582
                },
                {
                    "start": 1585,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2089
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 155,
                    "matchedPaperCorpusId": "53977760"
                },
                {
                    "start": 435,
                    "end": 439,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 439,
                    "end": 442,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 444,
                    "end": 447,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 900,
                    "end": 903,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 1130,
                    "end": 1133,
                    "matchedPaperCorpusId": "57301568"
                },
                {
                    "start": 1766,
                    "end": 1769,
                    "matchedPaperCorpusId": "3603886"
                },
                {
                    "start": 1771,
                    "end": 1773,
                    "matchedPaperCorpusId": "44071489"
                },
                {
                    "start": 1780,
                    "end": 1784,
                    "matchedPaperCorpusId": "3603886"
                },
                {
                    "start": 1849,
                    "end": 1852,
                    "matchedPaperCorpusId": "44071489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7978515625
        },
        {
            "corpus_id": "202774886",
            "title": "Focused Quantization for Sparse CNNs",
            "text": "Table 1: The accuracies (%), sparsities (%) and CRs of focused compression on ImageNet models.The baseline models are dense models before compression and use 32-bit floating-point weights, and 5 bits and 7 bits denote the number of bits used by individual weights of the quantized models before Huffman encoding.\n\nModel  [26] 68.08 88.36 4.38 10.67 ADMM (2 bits) [14] 67.0 87.5 2.92 16.00 ADMM (3 bits) [14] 68.0 88.3 4.38 10.67 ABC-Net (5 bases, or 5 bits) [15] 67.30 87.90 7.30 6.4 LQ-Net (preact, 2 bits) [23] 68.00 88.00 2.92 16.00 D&Q (large) [20] 73.10 91.17 Figure 4 shows an accelerator design of the dot-products used in the convolutional layers with recentralized quantization for inference.Using this, in Table 4 we provide the logic usage required by the implementation to compute a convolution layer with 3 \u00d7 3 filters with a padding size of 1, which takes as input a 8 \u00d7 8 \u00d7 100 activation and produce a 8 \u00d7 8 \u00d7 100 tensor output.Additionally, we compare FQ to shift quantization, ABC-Net [15] and LQ-Net [23].The #Gates indicates the lower bound on the number of two-input logic gates required to implement the custom hardware accelerators for the convolution, assuming an unrolled architecture and the same throughput.Internally, a 5-bit FQ-based inference uses 3-bit unsigned shift quantized weights, with a minimal overhead for the added logic.Scaling constants \u03c3 \u2212 and \u03c3 + are equal and thus can be fused into \u03b1 l .Perhaps most surprisingly, a 5-bit FQ has more quantization levels yet uses fewer logic gates, when compared to ABC-Net and LQ-Net implementing the same convolution but with different quantizations.",
            "score": 0.6358444319735803,
            "section_title": "PREPRINT",
            "char_start_offset": 15857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 94,
                    "end": 312
                },
                {
                    "start": 314,
                    "end": 701
                },
                {
                    "start": 701,
                    "end": 944
                },
                {
                    "start": 944,
                    "end": 1024
                },
                {
                    "start": 1024,
                    "end": 1234
                },
                {
                    "start": 1234,
                    "end": 1362
                },
                {
                    "start": 1362,
                    "end": 1434
                },
                {
                    "start": 1434,
                    "end": 1632
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 325,
                    "matchedPaperCorpusId": "12130431"
                },
                {
                    "start": 363,
                    "end": 367,
                    "matchedPaperCorpusId": "8840788"
                },
                {
                    "start": 403,
                    "end": 407,
                    "matchedPaperCorpusId": "8840788"
                },
                {
                    "start": 458,
                    "end": 462,
                    "matchedPaperCorpusId": "10533533"
                },
                {
                    "start": 508,
                    "end": 512,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 548,
                    "end": 552,
                    "matchedPaperCorpusId": "3323727"
                },
                {
                    "start": 1003,
                    "end": 1007,
                    "matchedPaperCorpusId": "10533533"
                },
                {
                    "start": 1019,
                    "end": 1023,
                    "matchedPaperCorpusId": "50784025"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45849609375
        },
        {
            "corpus_id": "258187227",
            "title": "DeepGEMM: Accelerated Ultra Low-Precision Inference on CPU Architectures using Lookup Tables",
            "text": "Deep learning methods have achieved state-of-the-art performance on several computer vision tasks, but deploying these algorithms on mainstream CPU platforms is challenging due to cost and latency constraints [17]. Ultra lowbit quantization [2,5,7,10,15,23] presents an attractive option for reducing neural network inference costs. A 2bit quantized model offers a theoretical model compression rate of 16\u00d7 relative to the 32-bit floating-point (FP32) base- line, but achieving low latency inference with ultra low-bit models on general purpose processors (GPPs) remains an active area of research [8,11,19]. \n\nDeep learning workloads on CPUs are typically accelerated by exploiting data-level parallelism through SIMD programming. However, ultra low-bit deep learning operators can not be efficiently executed on these devices because sub-8-bit instructions are not generally supported in the vectorized instruction sets of mainstream CPU architectures including SSE/AVX instructions on x86 and Neon instructions on Arm. Therefore, to enable ultra low-precision model deployment on these GPPs, it is imperative to develop techniques that leverage the available SIMD instructions for ultra low-precision computations. \n\nUltra low-bit quantization methods are highly competitive with industry standard 8-bit (INT8) quantization techniques, especially on classification tasks. For example, as shown in Tab. 1, at 2 bits of precision, the state-of-the-art uniform quantization method LSQ [10] achieves a 2.3% and 2% accuracy drop on the ImageNet dataset when quantizing the networks ResNet50 and VGG16, respectively. In comparison, the same method at INT8 precision results in an accuracy loss of 0.1% and improvement of 0.1% over the 32-bit full-precision baseline, respectively. The improved compression and latency of the 2-bit model makes the mild accuracy degradation tolerable in many use cases. \n\nMoreover, ultra low-bit quantization can also be employed using a mixed precision approach to prevent the potentially significant accuracy loss resulting from quantizing all layers in a network.",
            "score": 0.6351067868468705,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1898
                },
                {
                    "start": 1901,
                    "end": 2095
                }
            ],
            "ref_mentions": [
                {
                    "start": 598,
                    "end": 601,
                    "matchedPaperCorpusId": "211521880"
                },
                {
                    "start": 601,
                    "end": 604,
                    "matchedPaperCorpusId": "221070245"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "276617790",
            "title": "Binary Neural Networks for Large Language Model: A Survey",
            "text": "In Section 4.1, we provided a detailed explanation of the activation method in BitNet [17]. While BitNet b1.58 [18] quantizes the weights to three values, it also makes certain changes to the activations. Specifically, BitNet scales the activation values to the range [0, Q b ] before the nonlinear function, while BitNet b1.58 scales the activations to [\u2212Q b , Q b ] for each token. This design helps avoid zero-point quantization. The activation values in both BitNet and BitNet b1.58 are 8 bits. \n\nIn contrast, BitNet a4.8 [19] introduces several changes to activation quantization. The BitNet a4.8 team incorporates hybrid quantization and sparsification strategies to support 4-bit activations in 1-bit LLMs, addressing quantization errors. They use 4-bit activations for the inputs to the attention and feed-forward networks while sparsifying intermediate states and quantizing them to 8 bits. BitNet a4.8 is trained from 8-bit activations to 4-bit activations using a two-stage training strategy. Compared to BitNet b1.58, BitNet a4.8 achieves faster inference speeds. \n\nAlthough BitNet b1.58, proposed by the BitNet team, significantly improves the performance of LLMs, we observe that its training is 60% slower than the baseline and relatively more costly. This is because RMSNorm has been activated. Considering this, FBI-LLM [21] and Bi-Mamba [22] only quantize the weights, leaving the activations as 16-bit. Figure 5 shows a comparison of activation quantization in BitNet [17], BitNet b1.58 [18], BitNet a4.8 [19], FBI-LLM [21], and Bi-Mamba [22].",
            "score": 0.6346599738467993,
            "section_title": "Optimization of Activation Quantization Methods",
            "char_start_offset": 12222,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 498
                },
                {
                    "start": 501,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1562
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88232421875
        },
        {
            "corpus_id": "259226792",
            "title": "Neural Network Exploration for Keyword Spotting on Edge Devices",
            "text": "The inference latency of the neural network models with the four quantization schemes is shown in Figure 9. Like the nonquantized versions, the FFCN architecture was the clear winner. Beyond FFCN, no architecture was unequivocally better than another. However, each architecture exhibited the same linear trend in every quantization method. The deviations from the linear trend also have the same hardware-related explanations (number of memory reads/writes, CPU clock rate, and CPU branch predictor hit/miss rate). \n\nThe average inference latency of the 16-bit floating point, integer-float, and the fullinteger quantization models was very close to the nonquantized models, presumably due to the lack of runtime quantization. However, inference latency increased significantly with dynamic range quantization compared with the other quantization methods. This increase in latency may be due to the additional operations required to quantize operations at runtime. This increase in inference time is the most significant in ResNet but comparatively less in CENet.",
            "score": 0.6345441020582461,
            "section_title": "Effect of Quantization on Inference Latency",
            "char_start_offset": 39349,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 515
                },
                {
                    "start": 518,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1064
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7216796875
        },
        {
            "corpus_id": "271212850",
            "title": "Quantized Prompt for Efficient Generalization of Vision-Language Models",
            "text": "Quantization is one of the most effective compression methods [4,8,10,21,38,47,70] for deep learning models.Generally, parameters such as weights and activations are typically stored as 32-bit floating-point numbers, which consume a significant amount of memory and require intensive computation during inference.Quantization [18,19,56] involves representing these parameters with reduced precision, such as 8-bit integers or even lower bit-widths.By doing so, quantization can significantly reduce the memory footprint and computational complexity of the model without significantly sacrificing accuracy.Quantization methods can be divided into two groups, Post-Training Quantization (PTQ) [2,16,26,41,46,53,55,72] that consumes few resources but suffers higher accuracy loss, and Quantization-Aware Training (QAT) [3,14,32,40,57] that relies on plenty of resources for training and shows better accuracy.Existing works aim to minimize quantization error to improve accuracy, while our work demonstrates that both excessive and insufficient errors are detrimental to model generalization.To achieve optimal generalization performance, a moderate error is required.\n\n3 Exploring Quantization in Model Generalization",
            "score": 0.6344907195177998,
            "section_title": "Quantization",
            "char_start_offset": 6113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 108,
                    "end": 313
                },
                {
                    "start": 313,
                    "end": 448
                },
                {
                    "start": 448,
                    "end": 605
                },
                {
                    "start": 605,
                    "end": 906
                },
                {
                    "start": 906,
                    "end": 1089
                },
                {
                    "start": 1089,
                    "end": 1165
                },
                {
                    "start": 1167,
                    "end": 1215
                }
            ],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 65,
                    "matchedPaperCorpusId": "252968113"
                },
                {
                    "start": 65,
                    "end": 67,
                    "matchedPaperCorpusId": "102350555"
                },
                {
                    "start": 67,
                    "end": 70,
                    "matchedPaperCorpusId": "237108363"
                },
                {
                    "start": 70,
                    "end": 73,
                    "matchedPaperCorpusId": "236635335"
                },
                {
                    "start": 76,
                    "end": 79,
                    "matchedPaperCorpusId": "5993328"
                },
                {
                    "start": 79,
                    "end": 82,
                    "matchedPaperCorpusId": "247475977"
                },
                {
                    "start": 691,
                    "end": 694,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 697,
                    "end": 700,
                    "matchedPaperCorpusId": "235825979"
                },
                {
                    "start": 700,
                    "end": 703,
                    "matchedPaperCorpusId": "254823125"
                },
                {
                    "start": 703,
                    "end": 706,
                    "matchedPaperCorpusId": "235658553"
                },
                {
                    "start": 706,
                    "end": 709,
                    "matchedPaperCorpusId": "59606175"
                },
                {
                    "start": 709,
                    "end": 712,
                    "matchedPaperCorpusId": "184487878"
                },
                {
                    "start": 712,
                    "end": 715,
                    "matchedPaperCorpusId": "244527659"
                },
                {
                    "start": 816,
                    "end": 819,
                    "matchedPaperCorpusId": "216036085"
                },
                {
                    "start": 819,
                    "end": 822,
                    "matchedPaperCorpusId": "67788003"
                },
                {
                    "start": 822,
                    "end": 825,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 825,
                    "end": 828,
                    "matchedPaperCorpusId": "252873138"
                },
                {
                    "start": 828,
                    "end": 831,
                    "matchedPaperCorpusId": "247595112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72900390625
        },
        {
            "corpus_id": "251644990",
            "title": "A compute-in-memory chip based on resistive random-access memory",
            "text": "In general, models trained with higher noises converge faster during inference. The model trained with 20% noise reaches the lowest error at the end of 100 Gibbs sampling steps. \n\nExtended Data Fig. 7e shows the effect of noise injection on weight distribution. Without noise injection, the weights have a Gaussian distribution. The neural-network outputs heavily depend on a small fraction of large weights, and thus become vulnerable to noise injection. With noise injection, the weights distribute more uniformly, making the model more noise resilient. \n\nTo efficiently implement the models on NeuRRAM, inputs to all convolutional and fully connected layers are quantized to 4-bit or below. The input bit-precisions of all the models are summarized in Table 1. We perform the quantized training using the parameterized clipping activation technique 46 . The accuracies of some of our quantized models are lower than that of the state-of-the-art quantized model because we apply <4-bit quantization to the most sensitive input and output layers of the neural networks, which have been reported to cause large accuracy degradation and are thus often excluded from low-precision quantization 46,54 . To obtain better accuracy for quantized models, one can use higher precision for sensitive input and output layers, apply more advanced quantization techniques, and use more optimized data preprocessing, data augmentation and regularization techniques during training. However, the focus of this work is to achieve comparable inference accuracy on hardware and on software while keeping all these variables the same, rather than to obtain state-of-the-art inference accuracy on all the tasks. The aforementioned quantization and training techniques will be equally beneficial for both our software baselines and hardware measurements.",
            "score": 0.6344116592785207,
            "section_title": "Noise-resilient neural-network training",
            "char_start_offset": 53016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 177
                },
                {
                    "start": 180,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1834
                }
            ],
            "ref_mentions": [
                {
                    "start": 1195,
                    "end": 1197,
                    "matchedPaperCorpusId": "67788003"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9560546875
        },
        {
            "corpus_id": "278129868",
            "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs",
            "text": "We present the detailed results of BitNet v2 and the baselines in Table 1. Introducing the Hadamard transformation before the quantization in attention and FFN layers results in minimal perplexity degradation. For 8-bit activations, BitNet v2 surpasses BitNet b1.58 with an average accuracy improvement of 0.16%, 0.49%, and 0.61% on end tasks for the 1.3B, 3B, and 7B model sizes, respectively. Additionally, BitNet v2 enables native 4-bit activations across all linear layers, enhancing efficiency for batched inference. With INT4 activations, BitNet v2 achieves perplexity comparable to BitNet a4.8 while demonstrating superior performance on downstream tasks for the 3B and 7B models. \n\nTable 2 and Table 3 summarize detailed results of BitNet v2 (a8) and BitNet v2 (a4) with low-bit attention, respectively. We adopt post-RoPE quantization for QKV states. The QKV heads were directly quantized to unsigned integers using the absmax function, without the need of any calibration dataset. We retain the KV heads of [BOS] token as 8-bit precision. As shown in Table 2 and Table 3, BitNet v2 with 3-bit KV Cache achieves accuracy comparable to its counterpart with full-precision KV cache in 3B and 7B models. transformations to mitigate outlier features, while SpinQuant uses the learnable rotary matrix. Then they adopt GPTQ and absmax function to quantize the weight and activations into 4-bit, respectively. Since the weights of BitNet b1.58 were already trained to be ternary values from scratch, we adopted absmean function used in the training of BitNet rather than GPTQ for weight quantization. For activation quantization of the baselines, we retained rotary transformations for activations as an online operation; Furthermore, we removed the fusion of RMS normalization scales into projections. Following [LZF + 24], we tuned the rotation of SpinQuant with 800 samples from WikiText2 dataset.",
            "score": 0.6340228327480717,
            "section_title": "Main Results",
            "char_start_offset": 6204,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 687
                },
                {
                    "start": 690,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1902
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82421875
        },
        {
            "corpus_id": "207870482",
            "title": "Adaptive Precision Training: Quantify Back Propagation in Neural Networks with Fixed-point Numbers",
            "text": "While deep neural networks have become state-of-theart techniques for a wide range of machine learning applications, such as image recognition [14], object detection [21], machine translation [32,8], the computation costs of deep neural networks are continuously increasing, which greatly hampers the development and deployment of deep neural networks. For example, 10,000 GPU hours are used to perform neural architecture search on ImageNet [2]. Quantization is a promising technique to reduce the computation cost of neural network training, which can replace high-cost floating-point numbers (e.g., float32) with low-cost fixedpoint numbers (e.g., int8/int16). Recently, both the software society [6,12,16,19,27,35] and the hardware society [11,24,23,31] have carried out extensive researches about quantization of deep neural network for inference tasks. \n\nThough various investigations have demonstrated that deep learning inference can be accurately performed with low bit-width fixed-point numbers through quantization, the quantified training remains an open challenge. Some ex-isting approaches quantify the backward-pass to low-bit (e.g., int8) but incur significant accuracy drop, for examples, 3\u02dc7% loss for AlexNet [38,36]. [7] uses int16 for both forward-pass and backward-pass to ensure accuracy. However, there is no guarantee that unified int16 precision works for all the tasks and networks. \n\nMost previous investigations on quantified training use unified precision (i.e., bit-width) for all network layers. Intuitively, using mixed precisions for different layers will promote the network performance. However, it is hard to find the most appropriate precisions for so many layers in so many training iterations. Considering a widely used ResNet50 model, with 4 candidate quantization bit-widths (e.g., 8, 16, 24, 32 for weights, activations and activation gradients), the size of quantization precision combination search space for 450,000 training iterations can achieve 4 3 * 50 * 450,000 .",
            "score": 0.6338666837973441,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1409
                },
                {
                    "start": 1412,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 166,
                    "end": 170,
                    "matchedPaperCorpusId": "2141740"
                },
                {
                    "start": 192,
                    "end": 196,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 196,
                    "end": 198,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 706,
                    "end": 709,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 709,
                    "end": 712,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 744,
                    "end": 748,
                    "matchedPaperCorpusId": "2547043"
                },
                {
                    "start": 754,
                    "end": 757,
                    "matchedPaperCorpusId": "49421250"
                },
                {
                    "start": 1232,
                    "end": 1235,
                    "matchedPaperCorpusId": "3603886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8076171875
        },
        {
            "corpus_id": "253367548",
            "title": "Energy-Efficient DNN Training Processors on Micro-AI Systems",
            "text": "The inference processor can take advantage of ultralow-bit quantization because the network can be retrained multiple times in advance to compensate for accuracy degradation. Ultralow-bit quantization enables look-up-table-based computing [3], [4], [10] for energy-efficient inference acceleration. Furthermore, an inference processor can utilize analogdomain computing [8] if it repeats retraining after adding noise components to a prewell-trained network. However, both ultralow-bit quantization and mixed-mode computing can cause accuracy degradation during the DNN training and slow down the training curve. For this reason, the majority of training processors adopt precision-configurable MAC units which can also support high-bit-precision FP. It sometimes uses BW binarization [34], [43] but only in limited applications.",
            "score": 0.6338142312402287,
            "section_title": "1) PE/CIRCUIT-LEVEL DIFFERENCES",
            "char_start_offset": 47628,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 829
                }
            ],
            "ref_mentions": [
                {
                    "start": 244,
                    "end": 247,
                    "matchedPaperCorpusId": "23067262"
                },
                {
                    "start": 249,
                    "end": 253,
                    "matchedPaperCorpusId": "53081184"
                },
                {
                    "start": 370,
                    "end": 373,
                    "matchedPaperCorpusId": "3839319"
                },
                {
                    "start": 785,
                    "end": 789,
                    "matchedPaperCorpusId": "233376906"
                },
                {
                    "start": 791,
                    "end": 795,
                    "matchedPaperCorpusId": "53083929"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "251655759",
            "title": "Design and Implementation of Convolutional Neural Networks Accelerator Based on Multidie",
            "text": "The parameters obtained after network model training are often in FP32 format, and when inferring convolutional neural networks on FPGAs, they are not suitable for floating-point calculations due to the circuit characteristics of FPGAs and their resource limitations, so quantization of floating-point models is required. Mittal [13] compared different quantization schemes and quantization bit widths and found that the accuracy loss of the network was stable within 1% when the quantization bit width was 8 bits and above, while the accuracy loss was significant when quantization of lower bit widths such as 6 bits and 4 bits was performed. Considering the above factors, the 8-bit quantization scheme is adopted in this design. It is shown that, compared with 32-bit floating-point numbers, when using 8-bit fixedpoint numbers for network inference, can greatly reduce the number of parameters, the bandwidth requirement, and the hardware computation pressure, thus increasing the inference speed, while guaranteeing almost no loss of accuracy. \n\nThis design performs asymmetric quantization of feature map data into UINT8 format and symmetric quantization of weight data into INT8 format. The API function of PyTorch is used to implement both feature data and weight data. In addition, after the convolution of two 8-bit data, the convolution calculation result is no longer 8 bits, and the convolution calculation result is to be used as the input feature map of the next layer, so the 8-bit quantization of the convolution calculation result is also performed. According to the quantization scheme of [14], we can obtain the mapping relation from the integer q to the real number r, as shown in Equation (1): \n\nwhere r is the real value, q is the quantized value, and s and z are both quantization parameters. The convolution calculation formula is shown in Equation (2): \n\nAmong them, r 3 is the result of the convolution calculation, r 1 is the input feature map, and r 2 is the input weight. Putting Equation (1) into Equation (2), we get Equation (3):",
            "score": 0.6334050426415488,
            "section_title": "C. 8-BIT QUANTIZATION",
            "char_start_offset": 6526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 1048
                },
                {
                    "start": 1051,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1715
                },
                {
                    "start": 1718,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1878
                },
                {
                    "start": 1881,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2062
                }
            ],
            "ref_mentions": [
                {
                    "start": 329,
                    "end": 333,
                    "matchedPaperCorpusId": "52925676"
                },
                {
                    "start": 1608,
                    "end": 1612,
                    "matchedPaperCorpusId": "39867659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93798828125
        },
        {
            "corpus_id": "250048704",
            "title": "QReg: On Regularization Effects of Quantization",
            "text": "Deep neural network (DNN) quantization has been an active research topic in the past few years. Quantization is the task of approximating high precision (floating point) weights and activations of a model with lower bit resolution counterparts (usually 1 to 8 bit integers). Quantization can be done after (post training quantization) or during training (quantization aware training). Although both methods can compress the model and reduce overall execution time, at the cost of model accuracy, post training quantization usually takes less time to quantize a model. On the other hand, quantization aware training usually requires a full training session and with a moderate quantization level (int8) can produce quantized models with little to no accuracy loss compared to their full precision counterparts. In fact, as we will discuss in Section 2, there are many existing works Objectiveness Loss Value Epoch Number Validation objective loss of 4-bit quantized , 8-bit quantized and full precision model for the YOLOv5n model on the VOC dataset. This figure illustrates that unlike the quantized model, the full precision model exhibits heavy overfitting. We believe that the quantized model has better generalization performance due to its regularization effect. \n\nclaiming that quantization aware training produced accuracy improvements compared to some of the best full precision models. In these studies, the authors usually attribute such accuracy gains to the regularization effect of quantization. This has motivated us to explore how quantization affects training. In this work, we first hypothesize the regularization effect of quantization. Moreover, we hypothesize that this regularization effect is correlated with the quantization level. To confirm our hypothesis, we analytically explore how quantization noise propagates in a network at training time, and how the quantization level is correlated with the amount of regularization. We then empirically explore the regularization effect of quantization by applying different quantization aware training methods on different models and datasets with different quantization levels. In both analytical and empirical studies, we confirm our initial hypothesis.",
            "score": 0.6328636991027249,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1267
                },
                {
                    "start": 1270,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2147
                },
                {
                    "start": 2148,
                    "end": 2224
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "233393900",
            "title": "Do All MobileNets Quantize Poorly? Gaining Insights into the Effect of Quantization on Depthwise Separable Convolutional Networks Through the Eyes of Multi-scale Distributional Dynamics",
            "text": "In conjunction with efficient CNN architecture design, low bit-width (16 bits and below, though most commonly 8 bits), fixed point quantization has enabled highly parallelized processors such as DSPs to run fast, low-power inference entirely with integer arithmetic. These methods [12,19,4] project the neural network weights and activations of each layer onto a low-dimensional, discretized space while minimizing loss of information. However, the noise induced by quantization error has complex interactions with the weights and activations of each layer and its impact on CNN output behaviour can be difficult to quantify. Thus, it can often be hard to predict which CNN architectures will quantize well (ie. are \"quantization friendly\") and are suitable for deployment.\n\nGiven the problem of quantization robustness, various research works [12,13,21] explore methods to increase the robustness of models to quantization noise. Methods such as quantization-aware training (QAT) [12] and trained quantization thresholds (TQT) [13] make use of simulated quantization and the straight-through estimator (STE) to train the network to adapt to quantization noise. Note that since these methods simulate quantization, the training is bit-width specific and a network must be retrained if one were to adapt a model for a different bit-width. Furthermore, quantization training can require hyperparameter tuning of its own, thus extending the design cycle. Oftentimes, the choice of how to optimize a model for quantization is based on available tools and trial-and-error. While many of the devised \"quantization fixes\" have shown remarkable results [21,20,13], they aren't necessarily guaranteed to transfer across applications. For example, image reconstruction and other continuous value prediction/regression tasks may have a much lower error tolerance than classification. Thus, we seek to use a systematic approach which can help delineate the various factors affecting quantization robustness. In this way, we can make more informed choices on how to improve the quantized behaviour of our CNN models. [23,18] perform a layerwise analysis of the signalto-quantization-noise-ratio (SQ",
            "score": 0.6325978821230878,
            "section_title": "Fixed-point Quantization For Efficient Mobile Inference",
            "char_start_offset": 7241,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 848,
                    "end": 851,
                    "matchedPaperCorpusId": "202542905"
                },
                {
                    "start": 851,
                    "end": 854,
                    "matchedPaperCorpusId": "184487878"
                },
                {
                    "start": 1028,
                    "end": 1032,
                    "matchedPaperCorpusId": "202542905"
                },
                {
                    "start": 1645,
                    "end": 1649,
                    "matchedPaperCorpusId": "184487878"
                },
                {
                    "start": 1652,
                    "end": 1655,
                    "matchedPaperCorpusId": "202542905"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.751953125
        },
        {
            "corpus_id": "264406079",
            "title": "Exploring the Potential of Flexible 8-bit Format: Design and Algorithm",
            "text": "Neural network quantization is widely used to reduce model inference complexity in real-world deployments. However, traditional integer quantization suffers from accuracy degradation when adapting to various dynamic ranges. Recent research has focused on a new 8-bit format, FP8, with hardware support for both training and inference of neural networks but lacks guidance for hardware design. In this paper, we analyze the benefits of using FP8 quantization and provide a comprehensive comparison of FP8 with INT quantization. Then we propose a flexible mixed-precision quantization framework that supports various number systems, enabling optimal selection of the most appropriate quantization format for different neural network architectures. Experimental results demonstrate that our proposed framework achieves competitive performance compared to full precision on various tasks, including image classification, object detection, segmentation, and natural language understanding. Our work furnishes critical insights into the tangible benefits and feasibility of employing FP8 quantization, paving the way for heightened neural network efficiency in tangible scenarios. Our code is available in the supplementary material.",
            "score": 0.632529521759586,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8486328125
        },
        {
            "corpus_id": "255996789",
            "title": "CNN Sensor Analytics With Hybrid-Float6 Quantization on Low-Power Embedded FPGAs",
            "text": "The training with iterative early stop obtains a model with enhanced accuracy than standard early stop. This method iteratively resets the moving averages of Adam's optimizer, which helps to iteratively search for better local minima. This iterative search is suitable for models with low computational cost. \n\nThe TensorFlow Lite 8-bit quantization preserves the overall model accuracy. In some cases, the associated regularization effect can improve the accuracy. However, the error distribution in CNN linear regressions gets slightly degraded. In particular, 8-bit quantized output layers incur in discrete-degradation patterns, Fig. 18(b) shows this effect on three different models. Vertical and horizontal patterns appear in the error distribution of 8-bit fixed-point quantization. We attribute this effect to the 8-bit resolution in the activation maps. In the case of HF6 quantization, the activation maps are represented by floating-point preventing this degradation. \n\nThe proposed 6-bit FP representation (E4M1) improves latency, hardware area, and power dissipation, while preserving model accuracy. For comparison, in our application, this number format produces better results than the 6-bit logarithmic representation (E5M0). This is demonstrated in Fig. 15(d) and Fig. 15(e). \n\nIn [22], Lai et al. demonstrated that 4-bit exponent and X-bit mantissa preserves accuracy on SqueezeNet, AlexNet, GoogLeNet, and VGG-16. To contribute on this, we investigate 4-bit exponent and 1-bit mantissa to ALL-CNN-C [36], this produces an accuracy degradation of 1.39% and 0.11% with QAT. While applying 6-bit logarithmic produces a degradation of 11.18% and 7.22% with QAT.",
            "score": 0.6318458181493465,
            "section_title": "D. DISCUSSION 1) TRAINING AND QUANTIZATION",
            "char_start_offset": 35879,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 308
                },
                {
                    "start": 311,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1293
                },
                {
                    "start": 1296,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1677
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8291015625
        },
        {
            "corpus_id": "49356451",
            "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
            "text": "We present an overview of techniques for quantizing convolutional neural networks for inference with integer weights and activations. Per-channel quantization of weights and per-layer quantization of activations to 8-bits of precision post-training produces classification accuracies within 2% of floating point networks for a wide variety of CNN architectures. Model sizes can be reduced by a factor of 4 by quantizing weights to 8-bits, even when 8-bit arithmetic is not supported. This can be achieved with simple, post training quantization of weights.We benchmark latencies of quantized networks on CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations compared to floating point on CPUs. Speedups of up to 10x are observed on specialized processors with fixed point SIMD capabilities, like the Qualcomm QDSPs with HVX. \nQuantization-aware training can provide further improvements, reducing the gap to floating point to 1% at 8-bit precision. Quantization-aware training also allows for reducing the precision of weights to four bits with accuracy losses ranging from 2% to 10%, with higher accuracy drop for smaller networks.We introduce tools in TensorFlow and TensorFlowLite for quantizing convolutional networks and review best practices for quantization-aware training to obtain high accuracy with quantized weights and activations. We recommend that per-channel quantization of weights and per-layer quantization of activations be the preferred quantization scheme for hardware acceleration and kernel optimization. We also propose that future processors and hardware accelerators for optimized inference support precisions of 4, 8 and 16 bits.",
            "score": 0.6317779514440182,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96826171875
        },
        {
            "corpus_id": "225067234",
            "title": "ExPAN(N)D: Exploring Posits for Efficient Artificial Neural Network Design in FPGA-Based Systems",
            "text": "The use of a specific quantized representation of the weights and biases will require the use of a compatible MAC design for inference. Hence, we performed a joint analysis of the performance of the various MAC designs and the errors induced in the parameters by the corresponding quantization scheme. The various MAC designs are grouped under three categories -PoFxbased, Posit-based (that includes both multiply and adder  combination and FMA-based designs) and FxP-based. For the PoFx-based and Posit-based designs, lower bit-width input designs were also considered. For example, for 8-bit quantization, N was varied from 5 to 8. Similarly, for 16-bit quantization, N was varied from 5 to 16. TABLE 3 shows the Pareto analysis results for 8and 16-bit MACs with the three objectives -PDP, average quantization-induced error and the LUT utilization. We report the number of dominating points for each of the three types of quantization schemes used for the parameters of each layer of VGG16. \n\nAs shown in the table, using PoFx-based designs contribute significantly to the number of points on the Pareto-front for 8-bit precision. We also report the percentage increase in the Pareto-front hypervolume due to the usage of PoFxbased designs over the collection of Posit and FxP-based designs only. As seen in the table, using PoFx-based designs we report up to 173% increase in the hypervolume for 8bits precision. Fig. 17 shows the dominating and dominated points for each of the three categories in the corresponding design space for 8-bit precision MACs for the first layer (Conv1_1) of VGG16. It can be observed that the Posit-and FxP-based designs contribute one point each to the resulting Pareto-front, compared to 9 PoFx-based points. 5 he improvements for 16-bit precision are lower compared to 8-bits. However, as shown in TABLE 4, if we also consider the bits-width of the parameters as a design objective in the analysis, we report consistent improvements using PoFx-based designs for both 8and 16-bits precision.",
            "score": 0.6316454551615451,
            "section_title": "Weights Quantization Error Analysis",
            "char_start_offset": 33670,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 993
                },
                {
                    "start": 996,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 2027
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78759765625
        },
        {
            "corpus_id": "273233193",
            "title": "A Survey: Collaborative Hardware and Software Design in the Era of Large Language Models",
            "text": "To improve the accuracy of quantized DNN models, numerous studies have proposed new architecture designs based on advanced quantization techniques. BitFusion [161] supports various bit-width quantizations by combining low-bit processing elements. OLAccel [162] and GOBO [162] quantizes outliers with higher precision, but these approaches often suffer from unaligned memory accesses, leading to additional overhead and limited computing speed. ANT [163] offers a fixed-length adaptive quantization framework, considering tensor distribution but overlooking outliers' importance. Mokey [164] uses narrow fixed-point inference for transformer models by converting values to 4-bit indices into dictionaries of 16-bit fixed-point centroids, improving hardware efficiency without fine-tuning. OliVe [165] employs an outlier-aware quantization method using an outlier-victim pair mechanism to address quantization challenges, reducing hardware overhead and aligning memory access, enabling efficient 4-bit quantization for weights and activations. These methods collectively advance the deployment of quantized LLMs in resource-constrained environments by improving performance, reducing memory usage, and maintaining model accuracy. \n\nNew Data Type. Many studies also focus on designing new numeric types with reduced precision to improve model compression and efficiency. Microsoft Floating Point (MSFP) [166] uses a shared exponent for groups of values, enabling efficient dot product computations and higher arithmetic density compared to formats like Bfloat16 or INT8, making it ideal for large-scale cloud deployments. FP6-LLM [167] introduces a 6-bit floating-point format that leverages TC-FPx, a GPU kernel design, to reduce inference costs and improve performance for large language models (LLMs). LLM-FP4 [168] utilizes 4-bit floating-point quantization, optimizing exponent bits and clipping ranges, achieving minimal accuracy loss while enabling efficient deployment in resourceconstrained environments. LLM.int8() [169] enables efficient 8-bit matrix multiplication by combining vector-wise quantization and mixed-precision decomposition, maintaining accuracy for models up to 175B parameters and reducing memory usage, facilitating inference on large models using consumergrade GPUs.",
            "score": 0.6316389544602848,
            "section_title": "C. Compression Methods and Accelerators",
            "char_start_offset": 52273,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1227
                },
                {
                    "start": 1230,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2292
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 163,
                    "matchedPaperCorpusId": "21681898"
                },
                {
                    "start": 255,
                    "end": 260,
                    "matchedPaperCorpusId": "218571099"
                },
                {
                    "start": 270,
                    "end": 275,
                    "matchedPaperCorpusId": "218571099"
                },
                {
                    "start": 448,
                    "end": 453,
                    "matchedPaperCorpusId": "251928917"
                },
                {
                    "start": 585,
                    "end": 590,
                    "matchedPaperCorpusId": "247627656"
                },
                {
                    "start": 794,
                    "end": 799,
                    "matchedPaperCorpusId": "258179335"
                },
                {
                    "start": 1400,
                    "end": 1405,
                    "matchedPaperCorpusId": "227276514"
                },
                {
                    "start": 1810,
                    "end": 1815,
                    "matchedPaperCorpusId": "264451753"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90771484375
        },
        {
            "corpus_id": "270227790",
            "title": "Quasar-ViT: Hardware-Oriented Quantization-Aware Architecture Search for Vision Transformers",
            "text": "2.2.1 Quantization Scheme.To compress model size and improve inference speed, model quantization has been widely explored for deep neural networks (DNNs).Existing quantization research can be categorized according to quantization schemes, such as binary [8,36], ternary [13], and low-bit-width fixed-point [7,7,68,68] quantize models with the same interval between each quantization level.Although binary and ternary quantization reduce operations and simplify hardware implementation to the extreme, they introduce large accuracy loss due to insufficient bit-width.For example, based on reports from the above works, accuracy typically degrades by > 5% under binary quantization and 2 \u2212 3% for ternary quantization.To overcome the large accuracy loss coming from insufficient bit-width, the fixed-point quantization is proposed, applying moderate and adjustable quantization bit-width, to maintain accuracy.This quantization scheme was implemented with different methods and algorithms, such as DoReFa-Net [68] and PACT [7].\n\nFinally, there are also non-linear quantization schemes, such as power-of-two (PoT) [17] and additive PoT [19].They replace the multiplication with shifting operations where the distribution of quantization levels becomes unbalanced, having higher precision around the mean and less precision at the two sides.\n\n2.2.2 Mixed-Precision/Scheme Quantization.To explore more quantization potential while preserving the model accuracy, Besides the single scheme quantization, some works [9,39,45,49,54] explore inter-layer mixed-precision quantization by assigning different precisions to layers.For example, HAQ [49] determines the bit-width of each layer by an agent trained with reinforcement learning.DNAS [54] used NAS to search layer-wise bit-width.Furthermore, [29] explored intra-layer mixed quantization to enable different precisions or schemes within each layer.Based on them, hardware designs [5,40] leveraged the intra-layer mixedprecision/mixed-scheme to enable uniformity within each layer, guaranteeing inference acceleration.",
            "score": 0.6312730706356853,
            "section_title": "Non-Transformer DNN Model Quantization",
            "char_start_offset": 9462,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 26,
                    "end": 154
                },
                {
                    "start": 154,
                    "end": 389
                },
                {
                    "start": 389,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 716
                },
                {
                    "start": 716,
                    "end": 908
                },
                {
                    "start": 908,
                    "end": 1025
                },
                {
                    "start": 1027,
                    "end": 1138
                },
                {
                    "start": 1138,
                    "end": 1337
                },
                {
                    "start": 1339,
                    "end": 1381
                },
                {
                    "start": 1381,
                    "end": 1617
                },
                {
                    "start": 1617,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1776
                },
                {
                    "start": 1776,
                    "end": 1894
                },
                {
                    "start": 1894,
                    "end": 2063
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 257,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 257,
                    "end": 260,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 270,
                    "end": 274,
                    "matchedPaperCorpusId": "52909828"
                },
                {
                    "start": 1111,
                    "end": 1115,
                    "matchedPaperCorpusId": "8840788"
                },
                {
                    "start": 1133,
                    "end": 1137,
                    "matchedPaperCorpusId": "210848001"
                },
                {
                    "start": 1508,
                    "end": 1511,
                    "matchedPaperCorpusId": "148571720"
                },
                {
                    "start": 1511,
                    "end": 1514,
                    "matchedPaperCorpusId": "202565587"
                },
                {
                    "start": 1514,
                    "end": 1517,
                    "matchedPaperCorpusId": "204803866"
                },
                {
                    "start": 1517,
                    "end": 1520,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 1634,
                    "end": 1638,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 1789,
                    "end": 1793,
                    "matchedPaperCorpusId": "211011309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "257913869",
            "title": "A Unified Compression Framework for Efficient Speech-Driven Talking-Face Generation",
            "text": "Quantization (Krishnamoorthi, 2018;Migacz, 2017) enables the use of lower-precision representations in neural networks and improves computational efficiency. When converting floating-point operations to 8-bit integer (INT8) operations for the entire generator, we observed a significant degradation in the visual quality (see Table 1). To overcome this issue, we adopt a hybrid-precision quantization approach (Cai et al., 2020;Li et al., 2021b) that uses 16-bit floatingpoint (FP16) compute units for the quantization-sensitive layers and INT8 for the other layers. Figure 3 shows a quantization sensitivity analysis that investigates the impact of switching the boundary layer index between INT8 and FP16 on FID performance through a layer-by-layer basis. We empirically find that applying FP16 precision to the decoder's output block performs well for the compact generator.",
            "score": 0.6312370107335556,
            "section_title": "Mixed-Precision Post-Training Quantization",
            "char_start_offset": 5982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 877
                }
            ],
            "ref_mentions": [
                {
                    "start": 410,
                    "end": 428,
                    "matchedPaperCorpusId": "209531713"
                },
                {
                    "start": 428,
                    "end": 444,
                    "matchedPaperCorpusId": "231861390"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59228515625
        },
        {
            "corpus_id": "243907144",
            "title": "Neural Architecture Search and Hardware Accelerator Co-Search: A Survey",
            "text": "CNNs are typically used for error-resilient applications in which a minor error introduced by inexact computing is often invisible to the end-user. Hence, CNNs can be simplified to reduce hardware resources, power consumption, or latency. \n\nBy pruning, some connections, neurons, filters, and channels can be removed [42]. By quantization, the most suitable number of bits and data format is assigned to selected weights, activations, and other intermediate results in the network instead of using the common 32-bit floating-point (FP) data type [42]- [44]. Recent studies have shown that with novel quantization methodologies, namely PACT and SAWB, and specialized number formats, DLFloat16 (16 bit) and Hybrid-FP8 (8 bit) for training, and INT4 for inference, no loss in accuracy can be reached for 4-bit inference for common CNN models on ImageNet [45]. Model compression tries to reduce the number of different weight values to minimize the CNN memory footprint [42], [46]. \n\nNAS algorithms that will be discussed in Sections V and VI implicitly perform pruning. Searching for the optimal bit widths is directly performed by, e.g., [47]- [51].",
            "score": 0.6311707909884102,
            "section_title": "D. CNN OPTIMIZATION",
            "char_start_offset": 16424,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 238
                },
                {
                    "start": 241,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 977
                },
                {
                    "start": 980,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1147
                }
            ],
            "ref_mentions": [
                {
                    "start": 317,
                    "end": 321,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 546,
                    "end": 550,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 851,
                    "end": 855,
                    "matchedPaperCorpusId": "227123172"
                },
                {
                    "start": 966,
                    "end": 970,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 972,
                    "end": 976,
                    "matchedPaperCorpusId": "215799572"
                },
                {
                    "start": 1142,
                    "end": 1146,
                    "matchedPaperCorpusId": "211106623"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9111328125
        },
        {
            "corpus_id": "266624782",
            "title": "Research on Efficient CNN Acceleration Through Mixed Precision Quantization: A Comprehensive Methodology",
            "text": "The maximum operating frequency is 200MHz, and the supported data bit-widths are 8bit and 16-bit. \n\nAdditionally, quantization [13] can be used to reduce model sizes and hardware resource consumption, such as replacing original 32-bit floating-point operations with lower precision fixed-point numbers like 8-bit or 16-bit. Jacob et al. [15] proposed an integer quantization method, which uniformly quantizes both weight and activation to 8-bit. Furthermore, there are ultra low-bit quantization methods, such as ternary quantization [16] which quantifies weights into {-w, 0, +w}, and even binary quantization neural networks [17], which quantize weights and activation values to 1 or -1. \n\nHowever, using a unified quantization bit-width in ultra low-bit-width situations would significantly affect CNN performance. A highly effective solution to this problem is through mixed precision quantization [20]. It allows each layer www.ijacsa.thesai.org of the CNN model to have different quantization bit-widths. which can greatly preserve the performance. Lin et al. [21] proposed an analytical solution to address the fixed-point quantization problem. It seeks an optimal bit-width allocation strategy across network layers by optimizing the Signal-to-Quantization-Noise Ratio (SQNR). Wang et al. [22] designed a Hardware Aware Quantization (HAQ) algorithm that incorporates inference speed information evaluated by a hardware simulator into the training process, which utilized reinforcement learning to automatically determine quantization strategies. It reduces latency by 1.4-1.95 times and energy consumption by 1.9 times. However, the current methods face challenges in their applicability to FPGA platforms or in terms of high time and space complexity when searching for mixed precision strategies. \n\nIn conclusion, if we have an efficient mixed precision search algorithm and can apply the strategies obtained by this algorithm to FPGA platforms; it will be greatly significant for the application of deep learning on AIoT devices. Therefore, we propose a method for implementing a mixed precision CNN model on FPGA, co-designing from software and hardware aspects.",
            "score": 0.6309694795544126,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 100,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1806
                },
                {
                    "start": 1809,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2174
                }
            ],
            "ref_mentions": [
                {
                    "start": 337,
                    "end": 341,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 534,
                    "end": 538,
                    "matchedPaperCorpusId": "51813470"
                },
                {
                    "start": 627,
                    "end": 631,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 902,
                    "end": 906,
                    "matchedPaperCorpusId": "238416197"
                },
                {
                    "start": 1066,
                    "end": 1070,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 1297,
                    "end": 1301,
                    "matchedPaperCorpusId": "102350477"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5751953125
        },
        {
            "corpus_id": "261494391",
            "title": "FPGA-based Deep Learning Inference Accelerators: Where Are We Standing?",
            "text": "Reducing the number of bits for DNN parameters lowers the computation requirements and increases the model's performance and scalability. Most state-of-the-art FPGA designs rely on quantization to replace the 32-bit loating-point arithmetics with ixed-point arithmetics. The community has widely employed 16-bit ixed-point arithmetics. For instance, designs in [55,92,125,169,179] have employed the 16-bit ixed-point precision to accelerate CNNs on an FPGA. Others in [19,46,160] leveraged the same precision to accommodate RNNs on FPGA devices. Some designs adopted even lower precision, such as 12-and 8-bit precision [56,98,126,171]. In [125], Qiu et al. adopt a dynamic quantization scheme for CNNs using 8 and 4 bits for convolutional and dense layers, reducing storage and saving bandwidth compared to 8 bits for both layers. Additionally, many researchers have investigated DNN parameter binarization to reduce the model storage space and increase scalability. A 2-Bit VGG16 accelerator proposed by Yonekawa et al. [175] achieved a throughput of 460.8 GOP/s at 22 W, yielding an energy eiciency of 20.94 GOP/s/W. Although BNNs and TNNs sufer from accuracy loss, many designs explore the beneit of using 1-or 2-bit precision for computation to achieve real-time inference capabilities [36,44,48,49,182].",
            "score": 0.6301369034614304,
            "section_title": "DNN Model Simplification",
            "char_start_offset": 35443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1309
                }
            ],
            "ref_mentions": [
                {
                    "start": 361,
                    "end": 365,
                    "matchedPaperCorpusId": "5023174"
                },
                {
                    "start": 365,
                    "end": 368,
                    "matchedPaperCorpusId": "206657365"
                },
                {
                    "start": 368,
                    "end": 372,
                    "matchedPaperCorpusId": "207233273"
                },
                {
                    "start": 372,
                    "end": 376,
                    "matchedPaperCorpusId": "21947343"
                },
                {
                    "start": 468,
                    "end": 472,
                    "matchedPaperCorpusId": "67871713"
                },
                {
                    "start": 475,
                    "end": 479,
                    "matchedPaperCorpusId": "3542448"
                },
                {
                    "start": 620,
                    "end": 624,
                    "matchedPaperCorpusId": "206628725"
                },
                {
                    "start": 624,
                    "end": 627,
                    "matchedPaperCorpusId": "207958902"
                },
                {
                    "start": 631,
                    "end": 635,
                    "matchedPaperCorpusId": "53791706"
                },
                {
                    "start": 640,
                    "end": 645,
                    "matchedPaperCorpusId": "207233273"
                },
                {
                    "start": 1022,
                    "end": 1027,
                    "matchedPaperCorpusId": "27667556"
                },
                {
                    "start": 1291,
                    "end": 1295,
                    "matchedPaperCorpusId": "207149245"
                },
                {
                    "start": 1301,
                    "end": 1304,
                    "matchedPaperCorpusId": "4392444"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7216796875
        },
        {
            "corpus_id": "220936526",
            "title": "High Throughput Matrix-Matrix Multiplication between Asymmetric Bit-Width Operands",
            "text": "In recent years, numerous research efforts have been devoted to quantizing neural network architectures to sub-byte values while preserving the accuracy of full-precision model [2]- [6], [10], [11], [13], [17]- [21]. Furthermore, several approaches were proposed on developing compressed neural networks through the use of weight pruning [7], tensor decomposition [8], [14]- [16], compact network architecture design, etc. Learning quantization for numerical precision of 4-bits has been shown to be effective in recent works [1], [2], [10], [11], [18], in turn creating demand for efficient execution of matrix multiplication kernel between 4-bit weights and 8-bit activations on existing CPUs and DNN hardware accelerators. However, the mismatch between read bandwidth of 4-bit weights, 8-bit activations, and write bandwidth of accumulators poses major obstacles in implementing such an instruction for matrix multiplication hardware in CPUs and DNN hardware accelerators. The use of existing instructions (that execute MAC operations between symmetric bit-width operands) to perform such matrix multiplication between asymmetric bit-width operands will not be able to fully exploit the benefit of 4-bit weight quantization. On the other hand, failure to match the vector width of weights, activations, and accumulators by a matrix multiply instruction will either under-utilize expensive CPU resources (e.g., register file port bandwidth, etc.) or require significant increase in the DNN hardware accelerator resident SRAM resources (e.g., size of accumulator buffers, etc.) to realize any throughput benefit from 4-bit quantization. None of the recent works on 4bit model quantization reports performance benefit on either existing CPUs or hardware accelerators.",
            "score": 0.6296065064723,
            "section_title": "II. BACKGROUND AND RELATED WORK",
            "char_start_offset": 3990,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1767
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "199577815"
                },
                {
                    "start": 182,
                    "end": 185,
                    "matchedPaperCorpusId": "11244259"
                },
                {
                    "start": 187,
                    "end": 191,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 193,
                    "end": 197,
                    "matchedPaperCorpusId": "52920066"
                },
                {
                    "start": 199,
                    "end": 203,
                    "matchedPaperCorpusId": "70055866"
                },
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "30684700"
                },
                {
                    "start": 211,
                    "end": 215,
                    "matchedPaperCorpusId": "53761197"
                },
                {
                    "start": 364,
                    "end": 367,
                    "matchedPaperCorpusId": "17864746"
                },
                {
                    "start": 369,
                    "end": 373,
                    "matchedPaperCorpusId": "186206998"
                },
                {
                    "start": 375,
                    "end": 379,
                    "matchedPaperCorpusId": "203836150"
                },
                {
                    "start": 526,
                    "end": 529,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 531,
                    "end": 534,
                    "matchedPaperCorpusId": "199577815"
                },
                {
                    "start": 536,
                    "end": 540,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 542,
                    "end": 546,
                    "matchedPaperCorpusId": "52920066"
                },
                {
                    "start": 548,
                    "end": 552,
                    "matchedPaperCorpusId": "198903430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86865234375
        },
        {
            "corpus_id": "272881356",
            "title": "Accumulator-Aware Post-Training Quantization",
            "text": "The majority of neural network quantization research targeting compute acceleration emphasizes low-precision weights and activations. While this can significantly reduce the costs of multiplications, the resulting products are often still accumulated using high-precision additions. As lower precision integer representations continue to increase in popularity [32,33], one can expect a focus skewed towards weight and activation quantization to yield diminishing returns as high-precision additions can bottleneck throughput, power, and area [7,8,9,13]. For example, Ni et al. [7] show that when constraining weights and activations to 3-bit \u00d7 1-bit multipliers, the cost of 32-bit accumulation consumes nearly 75% of the total power and 90% of the total area of their custom MAC unit; they report up to 4\u00d7 power savings and 5\u00d7 area reduction when reducing to 8-bit accumulation. \n\nReducing the accumulator bit width is non-trivial in practice as it exponentially increases the risk of numerical overflow [12], often introducing arithmetic errors that degrade model accuracy [7,12]. Existing methods to prepare quantized neural networks (QNNs) for low-precision accumulation often aim to either reduce the risk of numerical overflow [9,34,35] or mitigate its impact on model accuracy [7,36,37]. These empirical approaches rely on several assumptions that limit their real-world applicability. For one, as discussed in [13], empirical estimates of overflow rely on a priori knowledge of the input distribution, which is impractical to assume in many real-world scenarios and can even introduce vulnerabilities [38]. Furthermore, overflow behavior can vary across platforms and programming languages, so designing methods to mitigate the detrimental impact of one particular overflow behavior (e.g., wraparound two's complement arithmetic) limits portability across applications and accelerators. Finally, empirical approaches are unable to support applications that require guaranteed arithmetic correctness, such as encrypted inference [39,40], and are known to break down when overflows occur too frequently [7].",
            "score": 0.6292671969112404,
            "section_title": "Low-Precision Accumulation",
            "char_start_offset": 9795,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2114
                }
            ],
            "ref_mentions": [
                {
                    "start": 361,
                    "end": 365,
                    "matchedPaperCorpusId": "254853733"
                },
                {
                    "start": 546,
                    "end": 548,
                    "matchedPaperCorpusId": "201895129"
                },
                {
                    "start": 548,
                    "end": 550,
                    "matchedPaperCorpusId": "220484674"
                },
                {
                    "start": 550,
                    "end": 553,
                    "matchedPaperCorpusId": "267061256"
                },
                {
                    "start": 1006,
                    "end": 1010,
                    "matchedPaperCorpusId": "261214350"
                },
                {
                    "start": 1079,
                    "end": 1082,
                    "matchedPaperCorpusId": "261214350"
                },
                {
                    "start": 1234,
                    "end": 1237,
                    "matchedPaperCorpusId": "220484674"
                },
                {
                    "start": 1237,
                    "end": 1240,
                    "matchedPaperCorpusId": "254100756"
                },
                {
                    "start": 1240,
                    "end": 1243,
                    "matchedPaperCorpusId": "254927690"
                },
                {
                    "start": 1419,
                    "end": 1423,
                    "matchedPaperCorpusId": "267061256"
                },
                {
                    "start": 1610,
                    "end": 1614,
                    "matchedPaperCorpusId": "174776079"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85205078125
        },
        {
            "corpus_id": "1423567",
            "title": "Resiliency of Deep Neural Networks under Quantization",
            "text": "So far we have examined the effect of direct and retraining-based quantization to the final classification error rates. As the number of quantization level decreases, more memory space can be saved at the cost of sacrificing the accuracy. Therefore, there is a trade-off between the total memory space for storing weights and the final classification accuracy. In practice, investigating this trade-off is important for deciding the optimal bit-widths for representing weights and implementing the most efficient neural network hardware. \n\nIn this section, we propose a guideline for finding the optimal bit-widths in terms of the total number of bits consumed by the network weights when the desired accuracy or the network size is given. Note that we assume 2n \u2212 1 quantization levels are represented by n bits (i.e. 2 bits are required for representing a ternary weight). For simplicity, all layers are quantized with the same number of quantization levels. However, the similar approach can be applied to the layer-wise quantization analysis. The optimal combination of the bit-width and layer size can be found when the number of total bits or the accuracy is given as shown in Figure 7. The figure shows the framewise phoneme error rate on TIMIT with respect to the number of total bits, while varying the layer size of DNNs with various number of quantization bits from 2 to 8 bits. The network has 4 hidden layers with the uniform sizes. With direct quantization, the optimal hardware design can be achieved with about 5 bits. On the other hand, the weight representation with only 2 bits shows the best performance after retraining. The remaining question is how much memory space can be saved by quantization while maintaining the accuracy. To examine this, we introduce a metric called effective compression ratio (ECR), which is defined as follows: \n\nThe compressed size is the total memory bits required for storing all weights with quantization. The effective uncompressed size is the total memory size with 32-bit floating point representation when the network achieves the same accuracy as that of the quantized network. \n\nFigure 8 describes how to obtain the effective number of parameters for uncompressed networks.",
            "score": 0.6292201918121875,
            "section_title": "EFFECTIVE COMPRESSION RATIO",
            "char_start_offset": 18813,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 537
                },
                {
                    "start": 540,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1860
                },
                {
                    "start": 1863,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2136
                },
                {
                    "start": 2139,
                    "end": 2233
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "272796227",
            "title": "An FPGA-Based YOLOv5 Accelerator for Real-Time Industrial Vision Applications",
            "text": "Previous researchers have conducted some fruitful explorations on accelerators for YOLO networks. Valadanzoj et al. [12] proposed an accelerator with 5-bit weights and 8-bit activations for the YOLOv4-tiny, which resulted in a 3% accuracy drop and a DSP packing scheme that was as effective as pure 8-bit accelerators [7]. Zhang et al. [13] developed a quantization method for YOLOv3-tiny with 2-bit weights and 8-bit activations, achieving a 2% accuracy decrease and implementing a density of 0.44 GOPS/DSP on an FPGA. Javed et al. [14] quantized the YOLOv2-tiny based on 1-bit weights and 4-bit activations and deployed it on an FPGA using a scalable binary accelerator framework. While the accelerator demonstrated good energy and resource efficiency, it incurred an accuracy loss of 5.6% compared with the float-point model. Zhao et al. [15] proposed a 4-bit quantization method and designed a LUT-based FPGA accelerator, achieving a high hardware utilization of 6.7 GOPS/DSP on the YOLOv2 network. However, their proposed quantization method only maintained minimal accuracy degradation on the classification network and no experiments were reported on the YOLO network. Sommer et al. [16] promoted Xilinx's DSP packaging [17] and explained the sources and correction methods of errors in the original scheme. More importantly, they proposed \"Overpacking\", which compresses six 4-bit multiplications into one DSP at the cost of small errors. However, in many neural network scenarios, activations can also be signed, limiting the general applicability of this method. Furthermore, applying \"Overpacking\" during neural network inference could introduce additional accuracy degradation, which may be difficult to mitigate through other compensatory techniques. \n\nIn summary, existing low-bit (\u22644-bit) YOLO accelerator designs exhibit two key limitations. First, there is a scarcity of quantization schemes that can simultaneously ensure hardware efficiency and high accuracy.",
            "score": 0.6291024926087547,
            "section_title": "Related Works",
            "char_start_offset": 4075,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1763
                },
                {
                    "start": 1766,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 1978
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 120,
                    "matchedPaperCorpusId": "264581638"
                },
                {
                    "start": 318,
                    "end": 321,
                    "matchedPaperCorpusId": "265202080"
                },
                {
                    "start": 336,
                    "end": 340,
                    "matchedPaperCorpusId": "247647967"
                },
                {
                    "start": 533,
                    "end": 537,
                    "matchedPaperCorpusId": "245446879"
                },
                {
                    "start": 841,
                    "end": 845,
                    "matchedPaperCorpusId": "264107478"
                },
                {
                    "start": 1190,
                    "end": 1194,
                    "matchedPaperCorpusId": "247595156"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8720703125
        },
        {
            "corpus_id": "273350594",
            "title": "Efficiera Residual Networks: Hardware-Friendly Fully Binary Weight with 2-bit Activation Model Achieves Practical ImageNet Accuracy",
            "text": "Typically, the input and output layers of the competitive ultra-low-bit quantized models are int8-quantized or float16 to mitigate the accuracy drop. Some recent work introduces advanced network designs using multiple bit-level quantization such as the downsampling by the average pooling with float 1x1 convolution (Liu et al., 2018;2020), 8-bit depth-wise separable convolution (DWConv) and 4-bit squeeze-and-excitation (SE) structures (Zhang et al., 2022) to lessen the performance degradation due to the ultra-low-bit quantization with a carefully designed small fraction of non-low-bit computation. They successfully improve the accuracy with a minimal increase in the cost of arithmetic computation. \n\nHowever, from the hardware accelerator design's point of view, non-low-bit or even float-valued computation and variations of CNNs such as DWConv and SE should be avoided for the following reasons. \n\nFirstly, the support of various operations and different-bit pair computation on the hardware accelerator requires the implementation of additional arithmetic circuits. It leads to an increase in the circuit area for ASIC or the use of FPGA resources regardless of their computational cost. For the existing ultra-low-bit quantized models, because the input and output layers are 8-bit or float, they need an additional 8-bit or float CNNs execution circuit or the operations are outsourced to the CPU (Zhang et al., 2021). \n\nSecondly, if the model involves operations that can not be executed on the hardware accelerator, it needs to be executed on the CPU and data transfer between the CPU and hardware accelerator occurs. It consumes the DRAM bandwidth and may become the bottleneck of the inference time. \n\nThirdly, the variations of CNN architectures such as DW-Conv and SE is likely to decrease the utilization of the hardware accelerator compared to the theoretical arithmetic operation cost. Even within the hardware accelerator, the data transfer is more likely to be the bottleneck than the arithmetic operation (Radosavovic et al., 2020). The support of such variations may just increase the circuit area and does not contribute to the reduction of inference time.",
            "score": 0.6280091049017411,
            "section_title": "Introduction",
            "char_start_offset": 1382,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 905
                },
                {
                    "start": 908,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1431
                },
                {
                    "start": 1434,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2183
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 334,
                    "matchedPaperCorpusId": "51892264"
                },
                {
                    "start": 438,
                    "end": 458,
                    "matchedPaperCorpusId": "244773000"
                },
                {
                    "start": 1410,
                    "end": 1430,
                    "matchedPaperCorpusId": "229348728"
                },
                {
                    "start": 2030,
                    "end": 2056,
                    "matchedPaperCorpusId": "214714446"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7744140625
        },
        {
            "corpus_id": "246210139",
            "title": "Dustin: A 16-Cores Parallel Ultra-Low-Power Cluster With 2b-to-32b Fully Flexible Bit-Precision and Vector Lockstep Execution Mode",
            "text": "Quantized inference and training of DNNs are both computationally intensive. In this context, an efficient representation of numerical values is particularly important since state of the art DNN models are heavily over-parameterized, providing ample opportunity for reducing bit precision without impacting accuracy [10], [11]. \n\nThe use of lower precision quantization improves hardware performance but can lead to significant accuracy degradation. Mixed-precision quantization, in which each layer is quantized with a different bit precision, addresses this issue but presents a challenge in selecting the appropriate bit setting for each layer. Different methods have been proposed to address the large search space for mixed-precision quantization, including reinforcement learning [12], Neural Architecture Search [13], and regularization-based approaches [14]. HAWQ [15] is an automatic method that uses second-order sensitivity to find mixed-precision settings, which has been shown to be faster than other methods. HAWQv2 and HAWQv3 [16], [17] have been proposed to improve this method with integer-only and hardware-aware quantization, respectively, and have been shown to be efficient on T4 GPUs, with up to 50% speedup compared to INT8 quantization. \n\nExtreme quantized neural networks are another trend in recent deep neural networks design for embedded computing. For example, Choi et. al. [18] demonstrate the use of a 2-bit CNN using a uniform quantization method, where different techniques are employed for quantizing activations and weights. The PACT technique, which finds an optimal value for the clipping threshold of the RELU function, is used for inputs during training, and SAWB, a scheme that aims to minimize quantization error without extensive search, is used for weights. The authors show that a 2-bit quantization can result in 3% accuracy degradation on the ResNet18 and ResNet50 model for Imagenet datasets compared to a full precision network (8-bit).",
            "score": 0.6273113775567217,
            "section_title": "A. Quantization",
            "char_start_offset": 6230,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 327
                },
                {
                    "start": 330,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1260
                },
                {
                    "start": 1263,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1984
                }
            ],
            "ref_mentions": [
                {
                    "start": 322,
                    "end": 326,
                    "matchedPaperCorpusId": "4555207"
                },
                {
                    "start": 786,
                    "end": 790,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 872,
                    "end": 876,
                    "matchedPaperCorpusId": "148571720"
                },
                {
                    "start": 1041,
                    "end": 1045,
                    "matchedPaperCorpusId": "207852310"
                },
                {
                    "start": 1047,
                    "end": 1051,
                    "matchedPaperCorpusId": "265038491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87890625
        },
        {
            "corpus_id": "216553191",
            "title": "Deploying Image Deblurring across Mobile Devices: A Perspective of Quality and Latency",
            "text": "For 8-bit quantization, post-training quantization suffers a destructive 2 dB PSNR drop. Even with quantization-aware training, there exists at least noticeable 0.8 dB PSNR drop. In contrast, 16-bit post-training quantization, is capable to preserve almost the same quality as floating-point network. \n\nIn our experiments, most devices have latency improvement with quantized networks except for Huawei Mate30 Pro. This is due to the lack of support for quantized RE-SIZE BILINEAR operation in its accelerator. We suggest 6 DEPTH TO SPACE is also known as pixel shuffle in some papers or frameworks future works to consider quantization configuration during the stage of architecture search. Note that NNAPI does not support 16-bit fixed-point inference. Hence, the evaluation requires proprietary SDK provided by platform providers. Qualcomm's SNPE [13] supports 16-bit fixed-point inference with HTA hardware. However, the corresponding software (HTA runtime library) is not available in Google Pixel 4. For Huawei's HiAI SDK [8], we cannot find appropriate information for its support of 16-bit fixed-point inference. Therefore, we only report the results of 16-bit fixed-point inference for MediaTek's NeuroPilot SDK [11] in Table 4.",
            "score": 0.6266040068621976,
            "section_title": "Quantization.",
            "char_start_offset": 18272,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1237
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85693359375
        },
        {
            "corpus_id": "236150092",
            "title": "Bipolar Morphological Neural Networks: Gate-Efficient Architecture for Computer Vision",
            "text": "There are many different methods for inference time reduction. They can be roughly divided into two groups. The methods of the first group reduce the number of weight coefficients and computational operations in the network, so it works faster. These include tensor decompositions of convolutions, model compression, and neural network architectures search. Methods of the second group are oriented to hardware characteristics and replace some computational operations and structures with others that and require less time to calculate. For example, these are low-precision computations or efficiently computable neuron/layer models that replace traditional ones. \n\nLow-precision neural networks store coefficients and use low-bit data types for computations (typically integers). Such computations are faster than in single-precision floatingpoint data types. Neural networks with 8-bit coefficients are widely used; there are a number of libraries developed for various device architectures [48]- [50]. There are both end-to-end systems and systems of mixed precision that use low-precision quantization, and integer computations [51]. For example, Z. Cai and N. Vasconcelos [52] note that using the same quantization method for all filters is not optimal and suggest performing a search for sufficient accuracy quantization parameters. They show that it can significantly improve the classification results compared to uniformly quantized networks. \n\nAlso, many researchers try to reduce the bit depth further. Since standard approaches to weight quantizing are not differentiable and direct conversion does not provide sufficient accuracy, accuracy improvement methods are desired. In [53], the authors propose an approach to finding weights, which improved the accuracy for a large set of bit-widths (1, 2, 4 bits for weights and 1, 2, 4, 8, 32 bits for activations). Zhuang et al. [54] add a full-precision auxiliary module to the network during the training stage. They demonstrate noticeable accuracy improvement on ImageNet, CIFAR-100, and COCO datasets. In [55], a model ternary weights and a novel training method for it are proposed. Their approach allows to create a network without multiplication and control its sparseness, i.e., the number of zero weights.",
            "score": 0.6264122620647496,
            "section_title": "A. METHODS FOR IMPROVING NEURAL NETWORK EFFICIENCY",
            "char_start_offset": 30302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1451
                },
                {
                    "start": 1454,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2272
                }
            ],
            "ref_mentions": [
                {
                    "start": 1132,
                    "end": 1136,
                    "matchedPaperCorpusId": "199489896"
                },
                {
                    "start": 1177,
                    "end": 1181,
                    "matchedPaperCorpusId": "215745195"
                },
                {
                    "start": 1887,
                    "end": 1891,
                    "matchedPaperCorpusId": "208309971"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7763671875
        },
        {
            "corpus_id": "231699188",
            "title": "Pruning and Quantization for Deep Neural Network Acceleration: A Survey",
            "text": "In addition to accelerating neural networks, quantization has also been found in some cases to result in higher accuracy. As examples: 1) 3-bit weights VGG-16 outperforms its full precision counterpart by 1.1% top-1 [144], 2) AlexNet reduces 1.0% top-1 error of the reference with 2-bit weights and 8-bit activations [66], 3) ResNet-34 with 4-bit weights and activation obtained 74.52% top-1 accuracy while the 32-bit version is 73.59% [174], 4) Zhou showed a quantized model reduced the classification error by 0.15%, 2.28%, 0.13%, 0.71%, and 1.59% on AlexNet, VGG-16, GoogLeNet, ResNet-18 and ResNet-50, respectively [269], and 5) Xu showed reduced bit quantized networks help to reduce over-fitting on Fully Connected Networks (FCNs). By taking advantage of strict constraints in biomedical image segmentation they improved segmentation accuracy by 1% combined with a 6.4\u00d7 memory usage reduction [251].",
            "score": 0.6263135165136311,
            "section_title": "Quantization Reduces Over-fitting",
            "char_start_offset": 114628,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 905
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 221,
                    "matchedPaperCorpusId": "8840788"
                },
                {
                    "start": 317,
                    "end": 321,
                    "matchedPaperCorpusId": "49559664"
                },
                {
                    "start": 436,
                    "end": 441,
                    "matchedPaperCorpusId": "3455952"
                },
                {
                    "start": 619,
                    "end": 624,
                    "matchedPaperCorpusId": "12130431"
                },
                {
                    "start": 899,
                    "end": 904,
                    "matchedPaperCorpusId": "3860571"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "220514471",
            "title": "A Survey of Numerical Methods Utilizing Mixed Precision Arithmetic",
            "text": "Modern high-performance computing (HPC) hardware continues to experience an ongoing shift towards supporting a variety reduced-precision formats for representing floating-point numbers in order to offer a much increased performance rate. However, portability is often of little concern as the hardware tends to serve only a specific set of workloads that are of special interest to the particular vendor. In general, the machine learning community has been more aggressive in evaluating multiple precision to the extent that even a 1-bit Stochastic Gradient Descent has been considered [98]. The typical use case in machine learning is to use the training with 32-bit arithmetic and use different precision for the inference task. The quantization for the inference is supported in popular frameworks like TensorFlow [99] and pyTorch [100]. Quantization is the approach to store the tensors and compute on them using bitwidths lower than floating point bitwidths. Even in machine learning frameworks, the support for quantizations is limited to just the key functionality needed for a convolutional neural networks or recurrent neural networks with some limited hardware support. For example, pyTorch and TensorFlow supports 8-bit quantization for activation and weights. This allows using 8-bits for inference where the additional 2-4x performance is necessary. On the training front, it has been shown that 16-bit training is sufficient for certain tasks [1,101]. The recent Gordon Bell winner demonstrated that lower-precison training can be used for scientific machine learning tasks as well [102]. \n\nThe analogous effort to the work in deep learning to the examples of our interest in scientific computing involves training the network in lower precision and performing inference in a higher one [103,104]. The compute imbalance between training and inference is even higher than that of factorization and the subsequent iterative refinement. Another difference is that in the context of neural network training, lowering the precision may be incorporated into the model as a regularizer. \n\n9 Multiprecision capabilities of xSDK Math Libraries and Interoperability",
            "score": 0.6256849695865263,
            "section_title": "Low precision and multiprecision technology for Machine Learning",
            "char_start_offset": 90044,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1602
                },
                {
                    "start": 1605,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2093
                },
                {
                    "start": 2096,
                    "end": 2169
                }
            ],
            "ref_mentions": [
                {
                    "start": 586,
                    "end": 590,
                    "matchedPaperCorpusId": "2189412"
                },
                {
                    "start": 817,
                    "end": 821,
                    "matchedPaperCorpusId": "6287870"
                },
                {
                    "start": 1457,
                    "end": 1460,
                    "matchedPaperCorpusId": "2547043"
                },
                {
                    "start": 1596,
                    "end": 1601,
                    "matchedPaperCorpusId": "52922914"
                },
                {
                    "start": 1806,
                    "end": 1810,
                    "matchedPaperCorpusId": "2547043"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5908203125
        },
        {
            "corpus_id": "266364140",
            "title": "Smart-DNN+: A Memory-efficient Neural Networks Compression Framework for the Model Inference",
            "text": "In the DNN inference stage, achieving high compression while maintaining low decompressing overhead is crucial for user experience. To this end, we propose a binary-quantizer as the first compression module, which compresses the original 32-bit floating-point model weights W \u2208 R n to the 1-bit integer version Q \u2208 R n through a lightweight linear quantization-based algorithm. \n\nConventionally, k-bit linear quantization maps the value range [f min , f max ] of the floating-point set f to the quantized value range [0, 2 k \u2212 1] of integer set Q, where 0 and 2 k \u2212 1 are the maximum and minimum values that can be represented after quantization (e.g., [0, 15] for 4-bit quantization). The quantized integer Q can be calculated as \n\nIn conventional 1-bit linear quantization (e.g., k = 1), parameters close to f min are quantized to 0, whereas those close to f max are quantized to 1. However, as discussed in Section 3, DNN parameters are approximately normally distributed and most of the parameters are concentrated around a small range (see Figure 2). In this case, the conventional 1-bit linear quantization causes a large quantization error (a.k.a., residual) to the majority parameters with a small magnitude (i.e., parameters close to zero). The residual is calculated as Delta = f * \u2212 f , where f * is the decompressed lossy parameters. Residual has the same size as the full-precise model and reflects the \"gap\" between the Q l \u2190 Quan.(W l ) by Equation ( 5) \n\n3: \n\n8: \n\nabs (v \u2212 S j 1 ) 11: end for 12: Return the quantized model Q; quantization constant list V 0 and V 1 quantized model and the original model. The upper bound of residuals E u is theoretically given by \n\nin the conventional 1-bit linear quantization. Large E u reduces parameter precision, potentially degrading the model inference accuracy. \n\nTo reduce the quantization error as much as possible, we introduce a binary-quantizer with a remapping mechanism to change the distribution of residuals.",
            "score": 0.6253533520079614,
            "section_title": "Layerwise Binary-quantizer for Model Parameters.",
            "char_start_offset": 18255,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 377
                },
                {
                    "start": 380,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1468
                },
                {
                    "start": 1471,
                    "end": 1473
                },
                {
                    "start": 1476,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1821
                },
                {
                    "start": 1824,
                    "end": 1977
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9033203125
        },
        {
            "corpus_id": "261065149",
            "title": "Analyzing Quantization in TVM",
            "text": "There has been many papers in academic literature on quantizing weight tensors in deep learning models to reduce inference latency and memory footprint. TVM also has the ability to quantize weights and support low-bit computations. Although quantization is typically expected to improve inference time, in TVM, the performance of 8-bit quantization does not meet the expectations. \n\nTypically, when applying 8-bit quantization to a deep learning model, it is usually expected to achieve around 50% of the full-precision inference time. However, in this particular case, not only does the quantized version fail to achieve the desired performance boost, but it actually performs worse, resulting in an inference time that is about 2 times as slow as the non-quantized version.",
            "score": 0.6249997060757391,
            "section_title": "The Problem",
            "char_start_offset": 1240,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 775
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70361328125
        },
        {
            "corpus_id": "240547382",
            "title": "Efficient Shot Detector: Lightweight Network Based on Deep Learning Using Feature Pyramid",
            "text": "Forward propagation and backpropagation operations in deep learning generally perform weighting in a single precision floating point format (FP32), which usually occupies 32 bits in the computer memory. Several computational gain can be observed by performing the lowerbit operation when performing sum and multiplication operations on the graphics processing unit (GPU) under the same conditions. In addition, the quantized neural network can use a relatively low bandwidth because it reduces the frequency of the memory access. The quantization is possible with 8-bit to 16-bit real number types, depending on the quantization strength. However, the network accuracy is lowered because the expressive power is lost after quantization. In the case of a CNN affected by the previous layer, the more the network has fewer parameters, the greater the decrease in accuracy. Quantization techniques have been proposed to minimize the lost value. Specifically, the static quantization method combines the network weights and activation functions in advance, and the dynamic quantization method adjusts only the weight values. Recently, a technique for quantizing weights and activation functions of the network during quantization aware training was proposed. Moreover, the mixed precision method uses both 16-bit and 32-bit floating point during training. When changing a 32-bit value to 16-bit, 16-bit will be used except for the part where accuracy decreases rapidly.",
            "score": 0.6247727368360643,
            "section_title": "Weight Quantization",
            "char_start_offset": 6011,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1465
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80712890625
        },
        {
            "corpus_id": "9413876",
            "title": "Exploiting Modern Hardware for High-Dimensional Nearest Neighbor Search. (Exploitation du mat\u00e9riel moderne pour la recherche de plus proche voisin en haute dimensionnalit\u00e9)",
            "text": "For most current ANN search use cases, product quantization with 8-bit subquantizers offers a sufficient accuracy. With Quick ADC, we have shown that even 4-bit sub-quantizers can be used. We have proposed two highly efficient scan procedures for these use cases, PQ Fast Scan and Quick ADC. However, some emerging uses cases require a higher accuracy. Thus, descriptors generated by deep neural networks are increasingly popular for multimedia similarity search. When they are not PCA-compressed, such descriptors have several thousand dimensions (usually 4096). For these very high-dimensional vectors, a higher quantization accuracy is required. Therefore, 16-bit quantizers are used to increase product quantization accuracy. In addition, there has been a recent interest in using 32-bit codes instead of the usual 64-bit codes [8]. For 32-bit codes, 16-bit quantizers are required as 8-bit quantizers have a too low accuracy. However, using 16-bit quantizer causes a threefold increase in search time, which often outweighs their accuracy advantage. We have shown that this threefold increase in response time is caused by the fact that lookup tables used for distance computations are stored in the L3 cache when using 16-bit sub-quantizers. On the contrary, when using 8-bit sub-quantizers, lookup tables are stored in the much faster L1 cache (Section 3.1).\n\nIn this chapter, we introduce a novel approach, derived quantizers, that makes 16bit quantizers as fast as 8-bit quantizers, while retaining their accuracy. The key idea behind our approach is to derive 8-bit quantizers, named derived quantizers, from the 16-bit quantizers so that they share the same short codes. Therefore, our approach does not incur any increase in memory usage. The codebook of derived quantizers can be seen as an approximate version of the codebook of the high-resolution 16-bit quantizers. This allows us to design a two-pass nearest neighbor search procedure that provides both a low response time and a high-accuracy. More specifically, this chapter addresses the following points:\n\n\u2022 We present in detail the",
            "score": 0.6246485232980805,
            "section_title": "Motivation",
            "char_start_offset": 163188,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 832,
                    "end": 835,
                    "matchedPaperCorpusId": "125463275"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9423828125
        },
        {
            "corpus_id": "264451753",
            "title": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers",
            "text": "In general, all methods, except for the na\u00efve Min-Max INT Quantization, produce comparable outcomes in the 8-bit setting on both LLaMA-7B and LLaMA-13B. Additionally, we observe that the na\u00efve MinMax FP Quantization achieves nearly lossless results and even surpasses the state-ofthe-art integer post-training quantization method, SmoothQuant (Xiao et al., 2022), which indicates that floating-point quantization naturally has a strong capability in handling the distributions in transformers. However, both MinMax FP Quant and FPQ baseline fail when pushing the quan-1 https://github.com/EleutherAI/lm-evaluation-harness tization precision to ultra-low 4/4/4 bit setting, with 28.9% and 23.8% accuracy degradation on LLaMA-7B, respectively. In this extreme case, the previous state-of-the-art PTQ and QAT methods, SmoothQuant (Xiao et al., 2022) and LLM-QAT (Liu et al., 2023) also suffer severe accuracy downgrade. In comparison, FPQ demonstrates a strong capability of handling extra-low bit settings and achieves only 8.2/5.8% accuracy drop on LLaMA-7B/13B with 4/4/4 bit-width, outperforming SmoothQuant (Xiao et al., 2022) by a large margin, yet with less bit-width and smaller calibration size. Moreover, FPQ even achieves 5.3% accuracy improvements compared to LLM-QAT (Liu et al., 2023) in the 4/4/4 setting and 1.5% over GPTQ (Frantar et al., 2023) in the 4/4/16 configuration on LLaMA-7B. \n\nFor practitioners, a crucial consideration is determining the appropriate quantization methods for various bit-widths. Therefore, based on our findings, we offer two recommendations that balance the trade-off between accuracy and search/optimization efficiency.",
            "score": 0.6245971487887779,
            "section_title": "LLaMA-7B",
            "char_start_offset": 19663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1663
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 362,
                    "matchedPaperCorpusId": "244527659"
                },
                {
                    "start": 827,
                    "end": 846,
                    "matchedPaperCorpusId": "244527659"
                },
                {
                    "start": 1109,
                    "end": 1127,
                    "matchedPaperCorpusId": "244527659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "277272262",
            "title": "Improving Quantization with Post-Training Model Expansion",
            "text": "Quantization plays a critical role in the deployment of large language models (LLMs), offering a means of bridging the gap between full-precision reference models and their low-precision counterparts. The primary objective of quantization is to minimize inference costs while maintaining model quality by reducing the bit width requirements of weights and activations. To this end, recent advancements in post-training quantization (PTQ) have enabled the practical use of 4-bit weights and activations during inference, effectively addressing challenges that are particularly pronounced in LLMs, such as mitigating the impact of outliers [1,2] and managing non-uniform error sensitivity [3,4]. \n\nTraditionally, the quantization design space has been navigated with a fixed set of strategies; namely, the selection of bit width and data format [5,6,7], the addition or removal of zero-points [8,9], and the granularity or optimization of scaling factors [10,11]. Interestingly, parameter volume (i.e., model size \u00d7 bit width) is commonly viewed in one dimension; volume is primarily reduced via bit width reductions, and model size is rarely considered outside of scaling analyses [12,13,14]. However, in real-world applications where maintaining model quality is paramount, a modest model size increase of 5-10% is often an acceptable trade-off. For example, when deploying on hardware accelerators restricted to power-of-2 bit widths (e.g., GPUs), failing to meet an accuracy requirement at 4 bits necessitates reverting to 8-bit precision, a significant step size increase in parameter volume. While increasing model size typically requires either flexible supernetworks [15] or end-to-end re-training [14], we present post-training model expansion as an emerging strategy to reduce the gap between full-precision reference models and their quantized counterparts. \n\nOur proposal aligns with existing literature, which consistently demonstrates that increasing model size is generally an effective strategy for improving accuracy [16,17,18]. Moreover, a new trend is emerging; as further discussed in Section 2, recent studies have already introduced post-training quantization techniques that incidentally increase model size while still decreasing parameter volume.",
            "score": 0.6240854652326995,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1866
                },
                {
                    "start": 1869,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2269
                }
            ],
            "ref_mentions": [
                {
                    "start": 687,
                    "end": 690,
                    "matchedPaperCorpusId": "59413897"
                },
                {
                    "start": 843,
                    "end": 846,
                    "matchedPaperCorpusId": "218628799"
                },
                {
                    "start": 846,
                    "end": 848,
                    "matchedPaperCorpusId": "259501508"
                },
                {
                    "start": 891,
                    "end": 894,
                    "matchedPaperCorpusId": "251432134"
                },
                {
                    "start": 894,
                    "end": 896,
                    "matchedPaperCorpusId": "232352683"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "252198916",
            "title": "FP8 Formats for Deep Learning",
            "text": "FP8 is a natural progression for accelerating deep learning training inference beyond the 16-bit formats common in modern processors. In this paper we propose an 8-bit \ufb02oating point (FP8) binary interchange format consisting of two encodings - E4M3 (4-bit exponent and 3-bit mantissa) and E5M2 (5-bit exponent and 2-bit mantissa). While E5M2 follows IEEE 754 conventions for representatio of special values, E4M3\u2019s dynamic range is extended by not representing in\ufb01nities and having only one mantissa bit-pattern for NaNs. We demonstrate the ef\ufb01cacy of the FP8 format on a variety of image and language tasks, effectively matching the result quality achieved by 16-bit training sessions. Our study covers the main modern neural network architectures - CNNs, RNNs, and Transformer-based models, leaving all the hyperparameters unchanged from the 16-bit baseline training sessions. Our training experiments include large, up to 175B parameter, language models. We also examine FP8 post-training-quantization of language models trained using 16-bit formats that resisted \ufb01xed point int8 quantization.",
            "score": 0.6235635586270589,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64404296875
        },
        {
            "corpus_id": "267211596",
            "title": "FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design",
            "text": "8-bit [4,38] and 4-bit quantization [7,14,42] are the most widely applied schemes for the current post-training LLMs. However, recent algorithmic research [30,35] has demonstrated that superior trade-offs between inference cost and model quality can be achieved with FP6 quantization, compared to 8-bit and 4-bit quantization. \n\n(I) Lower inference cost than 8-bit quantization. Compared to the 8-bit quantization, the cost of deploying LLMs can be further reduced through more aggressive 6-bit quantization without a visible accuracy drop. On one hand, the size of LLM weights can be significantly reduced, nearly 2.7\u00d7 smaller than the FP16 baseline. Less GPU memory is required to store model weights, thereby requiring fewer GPUs and reducing the serving cost of deploying LLMs. On the other hand, 6-bit quantization can also more effectively accelerate the inference of LLMs. Given that the LLM inference is usually memory-bounded2 during token generation, faster LLM inference can be achieved through reducing GPU DRAM access of the model weights. As shown in Figure 1, the execution of the linear layer within llama-65b model [32] is consistently faster (up to 1.42\u00d7 faster) with our newly proposed 6-bit quantization system design (TC-FPx_W6A16) compared to the state-of-the-art 8-bit quantization support (TensorRT-LLM_W8A16 [26]). Given that linear layers are the most time-consuming part of the large language models, this speedup will directly translate to performance improvements for end-to-end inference scenarios (See Section 7.3). \n\n(II) Better model quality than 4-bit quantization. Although 4-bit quantization more aggressively reduces memory footprint and DRAM access, it unavoidably causes degradation in model quality. In contrast, near-lossless model compression can be achieved with 6-bit quantization. As shown in Table 1 and Table 2, FP6 displays strong and consistent performance across various tasks including code generation and zero-shot perplexity performance.",
            "score": 0.6233196866097193,
            "section_title": "Motivations",
            "char_start_offset": 7403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 326
                },
                {
                    "start": 329,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1546
                },
                {
                    "start": 1549,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 6,
                    "end": 9,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 36,
                    "end": 39,
                    "matchedPaperCorpusId": "259298689"
                },
                {
                    "start": 155,
                    "end": 159,
                    "matchedPaperCorpusId": "264146384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9462890625
        },
        {
            "corpus_id": "199577815",
            "title": "Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks",
            "text": "Deep convolution neural networks have achieved great success in many fields, such as computer vision, natural language processing, information retrieval, etc. However, the expensive memory and computation costs seriously impede their deployment on the widespread resource-limited devices, especially for real-time applications. To address * corresponding author this problem, the quantization technique has emerged as a promising network compression solution and achieved substantial progress in recent years. It can largely reduce the network storage and meanwhile accelerate the inference speed using different types of quantizers, mainly including binary/ternary [8,9,16,23,45,25,33], uniform [46,28,6,26,43,18,36,42,20,7,2,39,21] and nonuniform [44,35,13,5,41,30,38,37,3].\n\nLimited by the specific hardware features like the instruction sets, most quantization methods can hardly accomplish the goal of network acceleration and may still heavily depend on the special hardware design and longterm hardware development. For example, a special inference engine EIE [12] has been developed to speed up the method in [13]. Fortunately, the recent studies have proved that both the binary and uniform quantization models enjoy the hardware-friendly property [18,17,11,27], which enables us to accelerate the inference directly on off-theshelf hardware with the efficient bit operation or integeronly arithmetic.\n\nDespite the attractive benefits, when quantizing into extremely low bit, existing binary and uniform quantization models still face the severe performance degradation, due to the limited and discrete quantization levels [6]. First, based on the discrete quantized representation, the backward propagation can hardly access the accurate gradients, and thus has to resort to the appropriate approximation. In the literature, straight through estimation (STE) [4] has been widely used for approximation. But it ignores the influence of quantization, and when the data is quantized to extremely low bit, its error will be amplified, causing an obvious instability of optimization. Experiments and analysis in [26,8,24] have shown that the gradient error caused by quantization and STE greatly harms the accuracy of quantized models.\n\ndeviations between the original data and their",
            "score": 0.6232863612884931,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 680,
                    "end": 683,
                    "matchedPaperCorpusId": "10533533"
                },
                {
                    "start": 683,
                    "end": 686,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 696,
                    "end": 700,
                    "matchedPaperCorpusId": "52966467"
                },
                {
                    "start": 711,
                    "end": 714,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 749,
                    "end": 753,
                    "matchedPaperCorpusId": "2716753"
                },
                {
                    "start": 753,
                    "end": 756,
                    "matchedPaperCorpusId": "49241074"
                },
                {
                    "start": 759,
                    "end": 761,
                    "matchedPaperCorpusId": "1875476"
                },
                {
                    "start": 761,
                    "end": 764,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 767,
                    "end": 770,
                    "matchedPaperCorpusId": "3847056"
                },
                {
                    "start": 770,
                    "end": 773,
                    "matchedPaperCorpusId": "52847187"
                },
                {
                    "start": 1257,
                    "end": 1261,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1264,
                    "end": 1267,
                    "matchedPaperCorpusId": "46899409"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78759765625
        },
        {
            "corpus_id": "234499804",
            "title": "Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence",
            "text": "Methods for quantizing or compressing neural networks weights or feature tensors generally fall into two categories: quantization-aware training and post-training quantization. In quantization-aware training, the network weights are trained with quantization applied or simulated in floating point, often in the forward pass, with higher-precision arithmetic used during stochastic gradient descent to assist with convergence of the training process. For example, Jacob, et al. [9] uses that approach to quantize weights and activations to 8 bits, while quantizing bias parameters to 32 bits. Mishra, et al. [8] find that when increasing the batch size during training of a low-precision network, the proportion of memory used by activation maps increases significantly. To reduce the precision of weights and activations while maintaining the overall accuracy of the network, they insert additional filter maps into each layer. They find that for networks such as ResNet-34 [12] and others that near full-precision performance can be achieved with 4-bit activations and 2-bit weights. In [13], weights, activations, and some of the gradient and back-propagation computations are quantized to 8 bits, along with a scaling modification made to batch normalization. In [14], networks are trained with weights and activations quantized to two bits. Good performance is achievable even with extremely coarse quantization, as shown in [15], [16] where filters and the inputs to convolutional layers are one-bit values. \n\nFor quantizing neural networks that have already been trained, including those trained without considering quantization, post-training quantization can be applied during inference. If this quantization is sufficiently fine or if the quantizer design method is tailored to the characteristics of the weights or activations being quantized, then the performance of this inference can become quite good, and in some cases can match the performance of a system designed using quantizationaware training or even no quantization at all. For example, [17] applies straightforward 8-bit quantization to weights and activations while maintaining nearly the same accuracy as obtained with floating point, and with 4-bit quantization, a 5% loss in accuracy is observed.",
            "score": 0.623174448806479,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 4072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1513
                },
                {
                    "start": 1516,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2274
                }
            ],
            "ref_mentions": [
                {
                    "start": 478,
                    "end": 481,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 608,
                    "end": 611,
                    "matchedPaperCorpusId": "3455952"
                },
                {
                    "start": 975,
                    "end": 979,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 1089,
                    "end": 1093,
                    "matchedPaperCorpusId": "44071489"
                },
                {
                    "start": 1267,
                    "end": 1271,
                    "matchedPaperCorpusId": "96438794"
                },
                {
                    "start": 1430,
                    "end": 1434,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 1436,
                    "end": 1440,
                    "matchedPaperCorpusId": "14925907"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9501953125
        },
        {
            "corpus_id": "259937594",
            "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study",
            "text": "For the first question, we evaluate the LLaMA models at four sizes (i.e., 7B, 13B, 30B, and 65B), examining their performance across a range of precision levels: 2-bit, 4-bit, 8-bit, and 16-bit. Our experiments indicate that 4-bit precision yields the most favorable trade-off between model performance and memory footprint, achieving superior results with the same amount of allocated total bits. However, all models at different sizes suffer from a severe decline at 2-bit precision. \n\nRegarding the second question, we carefully examine the quantization sensitivity of different model components (or substructures), specifically attention and feed-forward networks (FFN). In our experiments, we find that FFN plays a crucial role in retaining the model performance for low-bit quantization. We also evaluated the effects of outlier dimensions, which are specific dimensions that exhibit significantly higher values compared to others in feature activations. We find the outlier dimensions affecting most Transformer layers are primarily responsible for the decline in the quantization performance, and they mainly concentrate on the down projections of FFN. These observa-tions motivate us to design more fine-grained substructure quantization strategies for improving the performance of low-bit models. \n\nFurthermore, we study how to enhance the performance of quantization models through finetuning. We evaluate the impacts of different finetuning methods executed before and after quantization. Our experimental results reveal that parameter-efficient fine-tuning after quantization can achieve commendable performance with significantly reduced computational resources. Our approach can fine-tune a 2-bit LLaMA-65B model on a single NVIDIA A100, surpassing the performance of a 16-bit LLaMA-13B model on zero-shot MMLU dataset.",
            "score": 0.6226931614542933,
            "section_title": "Introduction",
            "char_start_offset": 3712,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1306
                },
                {
                    "start": 1309,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1834
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9453125
        },
        {
            "corpus_id": "221738932",
            "title": "Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation",
            "text": "We compare our quantization strategy to our full precision (FP) baseline and quantization baselines in terms of translation quality and inference efficiency. Note that for the 2-bit baselines and 3-bit baselines, we respectively assign quantization bits of 2 and 3 to all Transformer parameters, and as for the 2-bit Emb. baseline, we assign 2 quantization bits to all word vectors in embedding block.\n\nOur quantized models are notated as (average # bits in an embedding parameter, average # bits in a decoder parameter, average # bits in an encoder parameter).  translation quality, and we assign the number of bits for each sub-layer accordingly. Each type of sublayers in the decoder block are assigned 2, 3, and 1 bits to Dec dd , Dec ed , and Dec f f n respectively. In this case, the average of quantization bits for the decoder block is 1.8. For (2.5, 1.8, FP) model, considering that we quantize the embedding and decoder blocks, which account for large number of parameters (69.0%), into the average of under 3-bit, BLEU score degradation is moderate (within -1 BLEU from the FP baseline). As we mentioned in Section 2.1, computations for encoder can be easily parallelizable, and thus, we assign slightly higher number of bits to the encoder block. We can improve quantization result of encoder block to be 3.7-bits per weight by assigning 3 bits to Enc ee sub-layers and 4 bits to more sensitive Enc f f n sub-layers. It is interesting that (2.5, 1.8, 3.7) models in various directions show higher BLEU score than (2.5, 1.8, FP) models which are of previous retraining phases with higher number of bits to represent the models. Our 2.6-bit Transformer models (2.5, 1.8, 3.7) attain 11.8\u00d7 model compression ratio with reasonable -0.5 BLEU or less in 3 different translation directions. Our quantized models outperform",
            "score": 0.622286944796841,
            "section_title": "Results",
            "char_start_offset": 18327,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7060546875
        },
        {
            "corpus_id": "259165162",
            "title": "INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation",
            "text": "Quantization serves as a valuable approach for reducing the size of deep networks. This method diverges from the standard 32-bit floating-point representation for each parameter, instead employing a fewer number of bits for each parameter in the compressed model. Widely adopted quantization standards include FP16 and INT8, which utilize 16-bit floating point and 8-bit integer number formats respectively [6,7,22]. However, either the reduction in size is often accompanied by a trade-off in accuracy or the solution is inference-only. Due to the reduced bit usage for parameters, the model's output starts to deviate from its original floating-point baseline. Notably, the methods developed in [14] tackle most of these issues, but the solution is not easily scalable to the massive amounts of parameters in Large Language Models. \n\nA more recent approach, GPTQ [9], has emerged as a state-of-the-art method for quantization. GPTQ is a post-training quantization method that requires no retraining or access to the full training dataset, yet significantly enhances the accuracy of models compressed to 4 bits or lower. GPTQ employs a one-shot weight quantization method that uses approximate second-order (Hessian) information to achieve high accuracy and efficiency. It can quantize decoder-only transformer models with 175 billion parameters in about four GPU hours, reducing the bandwidth to 3 or 4 bits per weight, whilst maintaining accuracy levels comparable to the uncompressed baseline in the case of 4 bits. Crucially, GPTQ allows for the execution of a 175 billion-parameter model on a single GPU and offers inference speedups of approximately 3.25\u00d7 on high-end GPUs and 4.5\u00d7 on more cost-effective ones. However, GPTQ primarily functions as an effective strategy for considerable memory reduction during inference workloads. Its quantization methodology fundamentally disrupts the backward gradient propagation, attributable to its reliance on the INT4 data format. Consequently, the memory conservation benefits it affords do not extend to either the fine-tuning or the training process.",
            "score": 0.6213405600592633,
            "section_title": "Quantization for Deep Networks",
            "char_start_offset": 2500,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 833
                },
                {
                    "start": 836,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 407,
                    "end": 410,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 410,
                    "end": 412,
                    "matchedPaperCorpusId": "238408308"
                },
                {
                    "start": 697,
                    "end": 701,
                    "matchedPaperCorpusId": "231861390"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91650390625
        },
        {
            "corpus_id": "254853733",
            "title": "The case for 4-bit precision: k-bit Inference Scaling Laws",
            "text": "Quantization methods reduce the number of bits required to represent each parameter in a model, trading accuracy for smaller memory footprints and inference latencies. However, the final model size depends on both the number of parameters of the original model and the rate of compression. For example, a 30B 8-bit model and a 60B 4-bit model have the same number of bits but may have very different zero-shot accuracies. In this work, we study this trade-off by developing inference scaling laws of zero-shot performance in Large Language Models (LLMs) to determine the bit-precision and model size that maximizes zero-shot performance. We run more than 35,000 experiments with 16-bit inputs and k-bit parameters to examine which zero-shot quantization methods improve scaling for 3 to 8-bit precision at scales of 19M to 176B parameters across the LLM families BLOOM, OPT, NeoX/Pythia, and GPT-2. We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float). Overall, our findings show that {4-bit} precision is almost universally optimal for total model bits and zero-shot accuracy.",
            "score": 0.6211987533973806,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.982421875
        },
        {
            "corpus_id": "211677681",
            "title": "Quantized Neural Network Inference with Precision Batching",
            "text": "The importance of activations in quantization quality has been noted in research. Specifically, PACT (Parameterized Clipping Activation for Quantized Neural Networks) (Choi et al., 2018) demonstrated that neural network weights and activations may be quantized to very low bitwidths (< 4) if an activation scale is optimized during training. Although PACT requires changes to the training process (and hence does not work out of the box), their research demonstrates the importance and difficulty of quantizing activations in maintaining quantization quality. Motivated by their findings, PrecisionBatching opts to keep activations in higher precision (8,16,32 bit) to maintain accuracy at very low quantization level. This comes at minimal cost during inference as compute is dominated by memory access times. Thus, PrecisionBatching circumvents the need to maintain a quantization scale at training time by giving more bits of precision to activations at inference time.",
            "score": 0.6211245155373698,
            "section_title": "PACT",
            "char_start_offset": 8781,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 972
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9287109375
        },
        {
            "corpus_id": "231918377",
            "title": "Dynamic Precision Analog Computing for Neural Networks",
            "text": "Neural networks are able to perform accurate inference at low bit precision in digital hardware. Empirical research has demonstrated that neural network accuracy degrades minimally when quantizing to 4 to 8-bit fixed point integer representations, despite the networks being trained using 32bit floating point numbers [10]- [12]. \n\nA common method for quantizing floating point values to low precision is affine quantization [10]. In affine quantization, floating point inputs x (l) (or weights) in the range \n\nmax ] are mapped to fixed point integers of B bits from 0 to 2 B\u22121 by scaling, translating, and rounding the inputs. Mathematically, this is \n\nThe average precision required by commonly deployed neural networks can be lowered by using mixed precision. Different layers of neural networks are tolerant to different degrees of precision, and uniformly quantizing all layers of a neural network to the same bit precision leads to accuracy degradation [14]. There are many approaches for determining the bit precision of each layer, which requires searching an exponentially large space in the number of layers [12]- [18]. The method most similar to the one presented in this paper learns the bitwidth of each layer via gradient descent [26]. Other works perform post-training mixed precision quantization by making theoretical assumptions about signal-to-quantization noise ratio (SQNR) [27].",
            "score": 0.6199172367921485,
            "section_title": "B. Low Precision Neural Networks",
            "char_start_offset": 7398,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 329
                },
                {
                    "start": 332,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 508
                },
                {
                    "start": 511,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1400
                }
            ],
            "ref_mentions": [
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 425,
                    "end": 429,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1118,
                    "end": 1122,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 1244,
                    "end": 1248,
                    "matchedPaperCorpusId": "204803866"
                },
                {
                    "start": 1395,
                    "end": 1399,
                    "matchedPaperCorpusId": "649645"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.931640625
        },
        {
            "corpus_id": "267320710",
            "title": "Effect of Weight Quantization on Learning Models by Typical Case Analysis",
            "text": "The development of data-driven science, which involves analyzing large-scale observational data using models with a vast number of parameters, has led to a significant increase in computational costs. A prime example is the large-scale neural networks used in artificial intelligence technologies. For instance, GPT (Generative Pre-trained Transformer)-3 [BMR + 20] possesses tens of billions of parameters, requiring substantial computational resources for storage and updates in computer memory. Moreover, with the growing need to implement these models in small-edge devices, it's crucial to minimize their computational resource requirements. Practical applications, such as implementing neural networks in smartphones and in-vehicle sensors, are examples of this trend [LGW + 21]. Reducing computational costs is essential to achieve accurate and sustainable inference on these devices. \n\nQuantization is one of the most common techniques for compressing large models (for a survey, see [GKD + 22]). Quantization involves rounding the parameters or activation values of a model to discrete values with lower bit precision. Specifically, it converts original values stored with higher bits, e.g., 32 bits, to lower bits, such as 4 or 8 bits. This operation not only reduces the amount of memory required to store parameters but also decreases the computational resources needed for operations like matrix products. In practice, it is empirically known that quantization minimally impacts the prediction accuracy of neural networks, thereby enhancing the utility of this technology (see [BHHS18] for an example). Additionally, quantization can be integrated with other model compression techniques, such as distillation and pruning [LGW + 21]. Its significance is anticipated to grow as models, including new versions of GPT, continue to increase in size. \n\nAn ongoing challenge in quantization is the selection of hyperparameters, including the number of bits and the quantization range. While neural networks exhibit some robustness to quantization, a significant reduction in the number of bits can make them susceptible to outliers and data shifts. Consequently, it is crucial to choose appropriate hyperparameters that balance accuracy with computational load. Nevertheless, the comprehension of quantization remains an evolving issue. The characteristics of optimal hyperparameters and their very existence are still not fully understood.",
            "score": 0.619087262762103,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2155
                },
                {
                    "start": 2156,
                    "end": 2268
                },
                {
                    "start": 2269,
                    "end": 2343
                },
                {
                    "start": 2344,
                    "end": 2447
                }
            ],
            "ref_mentions": [
                {
                    "start": 1590,
                    "end": 1598,
                    "matchedPaperCorpusId": "44071489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7939453125
        },
        {
            "corpus_id": "10405700",
            "title": "Personalized speech recognition on mobile devices",
            "text": "Since the 11.9 MB floating point neural network acoustic model described above consumes a significant chunk of the memory and processing-time, we quantize the model parameters (i.e. weights, biases) into a more compact 8-bit integer-based representation. This quantization has an immediate impact on the memory usage, reducing the acoustic model's footprint to a fourth of the original size. The final footprint of our AM is 3 MB as shown in Table 1. Using 8-bit integers also has the advantage that we can also achieve 8-way parallelism in many matrix operations on most mobile platforms. \n\nAlthough we could have applied a number of compression schemes [20,21], with simplicity and performance in mind, and validated by previous work [22], we adopt a uniform linear quantizer that assumes a uniform distribution of the values within a given range. First, we find the minimum and maximum values of the original parameters. We then use a simple mapping formula which determines a scaling factor that when multiplied by the parameters spreads the values evenly in the smaller precision scale, thus obtaining a quantized version of the original parameters. The inverse operation is used when converting a quantized value back to its 32-bit floating point equivalent. \n\nDuring neural network inference, we operate in 8-bit integers everywhere except in the activation functions and the final output of the network, which remain in floating point precision (by converting between quantized 8-bit values and their 32-bit equivalents as needed). Our quantization scheme and the inference computation approach provides a 2\u00d7 speed-up in evaluating our acoustic models as compared to the unquantized model, with only a small performance degredation (compare 'adaptation' vs. 'quantization' in Table 1).",
            "score": 0.6189879267085265,
            "section_title": "Efficient Representation and Fast Execution",
            "char_start_offset": 10843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1264
                },
                {
                    "start": 1267,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1793
                }
            ],
            "ref_mentions": [
                {
                    "start": 736,
                    "end": 740,
                    "matchedPaperCorpusId": "15196840"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70263671875
        },
        {
            "corpus_id": "238634729",
            "title": "MoRS: An Approximate Fault Modeling Framework for Reduced-Voltage SRAMs",
            "text": "Deep Neural Networks (DNNs) are widely used as an effective solution for object recognition, classification and segmentation, and many other areas. MNIST [46] and CIFAR-10 [47] datasets are widely used by the ML community to showcase the latest technology advancements in DNNs. DNNs perform in two phases: training and prediction (inference). The first process is training that weights of DNNs are fitted to data. The training phase is mainly performed in high-performance computing platforms like CPUs or GPUs. The second phase is the inference phase, DNN models, trained in the first phase, are used to extract unknown output from input data. Usually, the training phase performs once whereas the prediction phase performs repeatedly. The inference phase consists of mainly three elements: input, weights, and output. Figure 1 shows a high-level abstraction of one inference layer. Each inference layer of the network consists of weights, an input matrix, and an output matrix. Weights of these layers are stored in on-chip memories. Due to their large size and computational intensity, DNNs consume power; while they are fault-tolerant to some extend. Some studies improve the power-efficiency and reduce the total power consumption of DNNs by applying voltage underscaling [3], [12], architectural techniques [48]- [55], and hardware-level techniques [56]- [58]. \n\nQuantization is an architectural optimization technique that reduces the precision of data types. This technique can improve performance and energy-efficiency. Recent studies [16], [59]- [62] show that quantization to some extent does not significantly affect the accuracy of DNN models. In our study, we reduce weights precisions to 16-bit half-precision floating point, 8-bit(Q 4.4 ), 4-bit(Q 2.2 ) in fixed-point format, and 1-bit binary values.",
            "score": 0.6187470923559172,
            "section_title": "C. Deep Neural Networks",
            "char_start_offset": 10400,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1817
                }
            ],
            "ref_mentions": [
                {
                    "start": 1277,
                    "end": 1280,
                    "matchedPaperCorpusId": "56173881"
                },
                {
                    "start": 1361,
                    "end": 1365,
                    "matchedPaperCorpusId": "50771180"
                },
                {
                    "start": 1544,
                    "end": 1548,
                    "matchedPaperCorpusId": "44529396"
                },
                {
                    "start": 1550,
                    "end": 1554,
                    "matchedPaperCorpusId": "6564560"
                },
                {
                    "start": 1556,
                    "end": 1560,
                    "matchedPaperCorpusId": "3827557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75
        },
        {
            "corpus_id": "5100161",
            "title": "Understanding the impact of precision quantization on the accuracy and energy of neural networks",
            "text": "We consider a broad range of numerical precisions and quantizations, from 32-bit floating-point arithmetic to binary nets, as well as several precision points in between. We summarize them below: \n\n1) Floating-Point Arithmetic: This is the most commonly used precision as it generates the state-of-the-art results in accuracy. However, floating-point arithmetic requires complicated circuitry for the computational logic such as adders and multipliers as well as large bit-width, necessitating ample memory usage. As a result, this precision is not suitable for low-power and embedded devices. \n\n2) Fixed-Point Arithmetic: Fix-point arithmetic is less computationally demanding as it simplifies the logic by fixing the location of the radix point. This arithmetic also provides the flexibility of a wide range of accuracy-power trade-offs by changing the number of bits used in the representation. In this work, we evaluate 4-, 8-, 16-and 32-bit precisions. To improve accuracy, we allow a different radix point location between data and parameters [9]. However, we refrain from evaluating bit precisions that are not powers of 2 since they result in inefficient memory usage that might nullify the benefits. \n\n3) Power-of-Two Quantization: Multipliers are the most demanding computational unit for neural networks. As proposed by Lin [16], limiting the weights to be in the form of 2 i , enables the network to replace expensive, frequent, and power-hungry multiplications with much smaller and less complex shifts. In our evaluations, we consider power of two quantization of the weights while representing the inputs with 16-bit fixed-point arithmetic. \n\n4) Binary Representation: Recent work suggests that neural networks can generate acceptable results using just 1bit weight representation [5]. While work by Courbariaux suggests binarizing activation between network layers, it does not binarize the input layer [4]. For this reason, our accelerator would still need to support multi-bit inputs. Thus, we evaluate the binary net using one bit for weights, while using 16-bit fixed-point representation for the inputs and feature maps.",
            "score": 0.6182050941966399,
            "section_title": "A. Evaluated Precisions and Train-Time Techniques",
            "char_start_offset": 7864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 195
                },
                {
                    "start": 198,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 593
                },
                {
                    "start": 596,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1208
                },
                {
                    "start": 1211,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1655
                },
                {
                    "start": 1658,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2141
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89306640625
        },
        {
            "corpus_id": "263310765",
            "title": "MixQuant: Mixed Precision Quantization with a Bit-width Optimization Search",
            "text": "Neural network quantization can be applied to training [2,[8][9][10][11] or inference. There are two paradigms in quantized DNN inference: post-training quantization (PTQ) and quantization-aware training (QAT) [12,13]. In contrast to PTQ, QAT requires that the f32 model is retrained while simulating quantized inference in the forward pass. While MixQuant can be integrated with either, we focus on PTQ which does not require any re-training. \n\n[14] and [3] are amongst the recent state-of-the-art post training quantization works. [14] introduce AdaQuant, which finds optimal quantization for both weights and activations and is based on minimizing the error between quantized layer outputs and f32 layer outputs. This approach is similar to MixQuant ; however, MixQuant finds the optimal quantization bit-widths based on quantization error (QE) minimization, while AdaQuant treats the bit-width as a constant and quantizes all weights and activations using the same bit-width (either int8 or int4 ). [3] propose BRECQ, a quantization method based on DNN block reconstruction. [5] propose AdaRound, adaptive rounding for weights, which achieves better accuracy than rounding to the nearest. They formulate the rounding procedure as an optimization problem that minimizes the expected difference between model loss with and without weights quantization perturbation. [15] develop a method based on constraining all quantization levels as the sum of Powers-of-Two terms, [7] propose a Bit-Split and Stitching framework (Bitsplit), [16] study the effect of quantization on the structure of the loss landscape, [17] develop ACIQ-Mix, a 4 bit convolutional neural network quantization, and [18] perform zero-shot quantization ZeroQ based on distilling a dataset that matches the input data distribution. \n\nQuantization originated with convolutional neural networks, but it has been extended to natural language processing neural networks as well.",
            "score": 0.6180402453756051,
            "section_title": "Neural Network Quantization",
            "char_start_offset": 2930,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 443
                },
                {
                    "start": 446,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1800
                },
                {
                    "start": 1803,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 214,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1368,
                    "end": 1372,
                    "matchedPaperCorpusId": "210848001"
                },
                {
                    "start": 1531,
                    "end": 1535,
                    "matchedPaperCorpusId": "208139197"
                },
                {
                    "start": 1609,
                    "end": 1613,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 1687,
                    "end": 1691,
                    "matchedPaperCorpusId": "209531713"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84423828125
        },
        {
            "corpus_id": "257353315",
            "title": "Ternary Quantization: A Survey",
            "text": "Deep neural network (DNN) models involve millions of parameters. Model size and inference efficiency are major challenges when deploying DNN models under latency, memory, and power restrictions. DNN model quantization is very attractive as it can accelerate the training and inference process and lower resource consumption by reducing bit width. Despite these advantages, quantization also poses challenges, especially in training efficiency and performance: i) it takes much more effort to optimize and longer to train than conventional methods; ii) models trained on large datasets usually require pretraining before quantization. \n\nThe most significant disadvantage of quantization is that quantized models have lower accuracy than their fullprecision counterparts: the lower the bit width, the more noticeable the difference. Binary/ternary quantization are the two extreme cases that can completely replace the multiplication operation with the bit shift operation combined with hardware implementation. Compared with other higher bitwidth quantization works (2 to 16-bit), this can bring considerable energy efficiency gains. However, it is hard to maintain a higher accuracy when using binary quantization. The ternary models can achieve much better performance at a small additional bit-width cost. Their performance is comparable with full-precision models. In addition, unlike binary quantization, it is easy to turn a symmetrically higher bitwidth quantization into ternary quantization. In other words, ternary quantization is a special case among existing higher bit-width (2-16 bit) symmetric quantization works. Therefore, the summary of ternary quantization works would be instrumental. This survey focuses on ternary weights quantization. Detailed introduction of binary and other quantization works can be found in the work of Amir et al. [2]. \n\nQuantization can be classified in several ways: uniform, nonuniform, symmetric, asymmetric, post-training, and quantization-aware methods. In general, quantization methods involve two steps: thresholding and projection. Therefore, this survey summarizes ternary quantization in terms of projection function and optimization methods. The projection function is categorized by quantization strategies. The optimization methods are categorized by how the gradient is used to optimize the model parameters.",
            "score": 0.6172567920232672,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 633
                },
                {
                    "start": 636,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1862
                },
                {
                    "start": 1865,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2197
                },
                {
                    "start": 2198,
                    "end": 2264
                },
                {
                    "start": 2265,
                    "end": 2367
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7392578125
        },
        {
            "corpus_id": "235166247",
            "title": "Post-Training Sparsity-Aware Quantization",
            "text": "Quantization is a technique used in deep neural networks (DNNs) to increase execution performance and hardware efficiency. Uniform post-training quantization (PTQ) methods are common, since they can be implemented efficiently in hardware and do not require extensive hardware resources or a training set. Mapping FP32 models to INT8 using uniform PTQ yields models with negligible accuracy degradation; however, reducing precision below 8 bits with PTQ is challenging, as accuracy degradation becomes noticeable, due to the increase in quantization noise. In this paper, we propose a sparsity-aware quantization (SPARQ) method, in which the unstructured and dynamic activation sparsity is leveraged in different representation granularities. 4-bit quantization, for example, is employed by dynamically examining the bits of 8-bit values and choosing a window of 4 bits, while first skipping zero-value bits. Moreover, instead of quantizing activation-by-activation to 4 bits, we focus on pairs of 8-bit activations and examine whether one of the two is equal to zero. If one is equal to zero, the second can opportunistically use the other's 4-bit budget; if both do not equal zero, then each is dynamically quantized to 4 bits, as described. SPARQ achieves minor accuracy degradation, 2x speedup over widely used hardware architectures, and a practical hardware implementation. The code is available at this https URL.",
            "score": 0.6166409231816529,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96826171875
        },
        {
            "corpus_id": "247595112",
            "title": "Overcoming Oscillations in Quantization-Aware Training",
            "text": "Quantization is one of the most successful methods for optimizing neural networks for efficient inference and ondevice execution while maintaining a high accuracy. By compressing the weights and activations from the regular 32-bit floating-point format to more efficient low bit fixedpoint representations, such as INT8, we can reduce power consumption and accelerate inference when deploying neural networks on edge devices (Horowitz, 2014). \n\nDespite its clear power and latency benefits, quantization comes at the cost of added noise due to the reduced precision. However, researchers in recent years have shown that neural networks are robust to this noise and can be quantized to 8-bits with a minimal drop in accuracy using post-training quantization techniques (PTQ) (Nagel et al., 2019;Banner et al., 2019;Nahshan et al., 2020). PTQ can be very efficient and, generally, only requires access to a small calibration dataset, but suffers when applied to low-bit quantization (\u2264 4-bits) of neural networks. Meanwhile, quantization-aware training (QAT) has become the de-facto standard method for achieving low-bit quantization while maintaining near full-precision accuracy (Krishnamoorthi, 2018;Esser et al., 2020;Nagel et al., 2021). By simulating the quantization operation during training or fine-tuning, the network can adapt to the quantization noise and reach better solutions than with PTQ. \n\nIn this paper, we focus on the oscillations of quantized weights that occur during quantization-aware training. This is a little-known and under-investigated phenomenon in the optimization of quantized neural networks, with significant consequences for the network during and after training. When using the popular straight-through estimator (STE) (Bengio et al., 2013) for QAT, weights seemingly randomly oscillate between adjacent quantization levels leading to detrimental noise during the optimization process. Equipped with this insight, we investigate recent advances in QAT that claim improved performance and assess their effectiveness in addressing this oscillatory behavior.",
            "score": 0.6162486150048258,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1403
                },
                {
                    "start": 1406,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2090
                }
            ],
            "ref_mentions": [
                {
                    "start": 425,
                    "end": 441,
                    "matchedPaperCorpusId": "22028726"
                },
                {
                    "start": 774,
                    "end": 794,
                    "matchedPaperCorpusId": "184487878"
                },
                {
                    "start": 794,
                    "end": 814,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 1201,
                    "end": 1220,
                    "matchedPaperCorpusId": "67788003"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9580078125
        },
        {
            "corpus_id": "231855779",
            "title": "Distribution Adaptive INT8 Quantization for Training CNNs",
            "text": "Generally speaking, quantization methods can be roughly divided into two major categories: inference quantization and training quantization, according to distinct quantization targets. \n\nInference Quantization. Inference quantization aims to quantize weight and/or activation to accelerate the forward pass. BNNs (Hubara et al. 2016) trains the neural networks with binary weight and activation successfully. TWNs (Li, Zhang, and Liu 2016) constrains weight to +1, 0 and -1 and optimizes a threshold-based ternary function. These low bitwidths (less than 4-bits) quantization techniques have significant degradation in performance. To improve the accuracy, more advanced quantization schemes (Cai et al. 2017;Zhang et al. 2018;Jung et al. 2019) have been proposed. In (Choi et al. 2019), the combination of range ReLU and statistics-aware weight binning make the 2-bit quantization get comparable performance with full precision networks. By training quantization thresholds, TQT (Jain et al. 2019) also achieves near-floating-point accuracy. \n\nTraining Quantization. Compared with inference quantization, training quantization needs to further quantize the gradient to accelerate the whole training process. (Banner et al. 2018) replaces batch-norm (BN) layer with RangeBN and adopt \"Gradients Bifurcation\" to quantize both gradient and BN layers to 8-bit, but it de-quantizes gradient to floating point number before calculation, which can not be speed up by INT8 operator. A similar case is (Wang et al. 2018a), which needs new hardware platforms to boost the speed. Besides quantizing gradient to INT8, WAGE (Wu et al. 2018) and WAGEUBN (Yang et al. 2020) further quantize other training modules (e.g., Error and Update), leading to large loss of accuracy. (Zhu et al. 2020) utilizes direction sensitive gradient clipping and counteractive learning rate scaling to guide the 8-bit gradient learning and updating, which help it succeed in many networks and tasks.",
            "score": 0.6160401807164549,
            "section_title": "Related Work",
            "char_start_offset": 2818,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 187,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1042
                },
                {
                    "start": 1045,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 332,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 414,
                    "end": 439,
                    "matchedPaperCorpusId": "13556195"
                },
                {
                    "start": 692,
                    "end": 709,
                    "matchedPaperCorpusId": "1875476"
                },
                {
                    "start": 709,
                    "end": 727,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 727,
                    "end": 743,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 980,
                    "end": 997,
                    "matchedPaperCorpusId": "202542905"
                },
                {
                    "start": 1209,
                    "end": 1228,
                    "matchedPaperCorpusId": "44071489"
                },
                {
                    "start": 1494,
                    "end": 1513,
                    "matchedPaperCorpusId": "53977760"
                },
                {
                    "start": 1612,
                    "end": 1628,
                    "matchedPaperCorpusId": "3603886"
                },
                {
                    "start": 1641,
                    "end": 1658,
                    "matchedPaperCorpusId": "202540941"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81201171875
        },
        {
            "corpus_id": "57573783",
            "title": "DSConv: Efficient Convolution Operator",
            "text": "The use of low-bit width networks saves a significant amount of memory and computation, especially when targeted to custom hardware. For example, for 8-bit operations, [26] reports up to 10x increase in speed, and [12] reports up to 30x in energy saving and chip area. We categorize the previous research that tries to increase the neural network efficiency in two groups: Quantization with labelled data. Most research in neural network quantization has focused on problems that involve retraining, by either starting from scratch or by adapting from an existing pre-trained network. BinaryConnect, BWN and TWN [11,32,28] use 1-bit and ternary weights to make the FP-MACS addition only. XNOR-Net and BNN [32,11] applied 1-bit quantized weight and activations to ImageNet for fast inference, at the cost of a significant drop in accuracy. WRPN [30] improved this accuracy by using wider versions of the same architectures. Early demonstrations of low-bit network acceleration in custom hardware include Ristretto [16], which also uses data to quantize the network to 8-bit models. Many other papers followed, by also training the quantization scheme, using binary basis vectors, such as in LQ-Nets [38], Halfway Gaussian Quantization (HWGQ) [6], and ABC-Net [29]. DoReFa-Net [40] also quantized gradients, alongside weights and activations. \n\nThe compression problem has also mostly been dealt with by using retraining with access to labelled data. In DeepCompression [17], two of the three steps of the algorithm require retraining (pruning and quantization), with Huffman Encoding being performed without the need for data. HashedNet [7] use the \"hashing trick\" to save significant amounts of memory when storing the network, but still require labelled data for tuning. The more recent approach [31], uses distillation [3,19] obtain compressed weights, but also requires the full labelled training set. \n\nSeveral approaches introduce novel float data formats.",
            "score": 0.6159944902578527,
            "section_title": "Related Work",
            "char_start_offset": 3056,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1340
                },
                {
                    "start": 1343,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1904
                },
                {
                    "start": 1907,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 616,
                    "end": 619,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 705,
                    "end": 709,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 709,
                    "end": 712,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 1013,
                    "end": 1017,
                    "matchedPaperCorpusId": "51610353"
                },
                {
                    "start": 1198,
                    "end": 1202,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 1241,
                    "end": 1244,
                    "matchedPaperCorpusId": "1875476"
                },
                {
                    "start": 1258,
                    "end": 1262,
                    "matchedPaperCorpusId": "10533533"
                },
                {
                    "start": 1636,
                    "end": 1639,
                    "matchedPaperCorpusId": "543597"
                },
                {
                    "start": 1821,
                    "end": 1824,
                    "matchedPaperCorpusId": "11536917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67822265625
        },
        {
            "corpus_id": "247362669",
            "title": "Power-of-Two Quantization for Low Bitwidth and Hardware Compliant Neural Networks",
            "text": "A recent survey [6] on efficient neural network inference summarizes different aspects of quantization, including uniform and nonuniform quantization, symmetric or asymmetric (for tailed weights' distributions), different granularity (quantizing channel wise, kernel wise, etc.), as well as Quantization Aware Training (QAT) or Post Training Static Quantization (PTSQ).Logarithmic quantization was first introduced in [9].The authors performed PTSQ experiments quantizing separately activations, convolution and fully connected layers for VGG16 and AlexNet, and showed little degradation in accuracy for low bit width precision.They also proposed training with logarithmic representation.In [4], to increase the resolution of quantized weights around zero, the authors proposed using additional quantization levels, as fractional exponents of 2. The reported PTSQ accuracy results were better than using the algorithm proposed in [9].However, introducing fractional weights makes it difficult to use bit shifting instead of multiplication.Similar ideas motivated the authors of [8], but led to a far more complex and effective QAT algorithm for Additive-Powersof-Two (APoT) networks, where the weights are represented by both PoT and sums of PoT values.The reported results are state-ofthe-art for low bit width precision networks, preserving or even surpassing the baseline floating point accuracies for various residual networks.In the proposed method both weights and activations are quantized, excluding first and last layers.In [5], the authors presented two approaches to train PoT (DeepShift) networks: by learning values of i) shifts and ii) weights.For both scenarios, all convolution and fully connected layers' multiplications are replaced with bit shifting.The reported results are close to floating point accuracies for 4 or 5 bit width precision.The authors also implemented GPU kernels and showed decreased inference time when using bit shifting instead of multiplication.Other papers showed more hardware-oriented approaches: in [10], the authors proposed the hardware-inspired ShiftAddNet, with layers based solely on bit shifting and add operations with an efficient training algorithm.",
            "score": 0.6155555919730605,
            "section_title": "RELATED WORK",
            "char_start_offset": 2270,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 369
                },
                {
                    "start": 369,
                    "end": 422
                },
                {
                    "start": 422,
                    "end": 628
                },
                {
                    "start": 628,
                    "end": 688
                },
                {
                    "start": 688,
                    "end": 934
                },
                {
                    "start": 934,
                    "end": 1039
                },
                {
                    "start": 1039,
                    "end": 1253
                },
                {
                    "start": 1253,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1530
                },
                {
                    "start": 1530,
                    "end": 1658
                },
                {
                    "start": 1658,
                    "end": 1769
                },
                {
                    "start": 1769,
                    "end": 1860
                },
                {
                    "start": 1860,
                    "end": 1987
                },
                {
                    "start": 1987,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 691,
                    "end": 694,
                    "matchedPaperCorpusId": "54462109"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.841796875
        },
        {
            "corpus_id": "10817450",
            "title": "Training Quantized Nets: A Deeper Understanding",
            "text": "The arithmetic operations of deep networks can be encoded down to 8-bit fixed-point without significant deterioration in inference performance [5][6][7][8][9]. The most extreme scenario of quantization is binarization, in which only 1-bit (two states) is available for weight representation [10,3,1,4,11,12]. \n\nPrevious work on obtaining a quantized neural network (NN) can be divided into two categories: quantizing pre-trained models with or without retraining [7,13,6,14,15], and training a quantized model from scratch [5,3,4,1,16]. We focus on approaches that belong to the second category, as they can be used for both training and inference under constrained resources. \n\nFor training quantized NNs from scratch, many authors suggest maintaining a high-precision copy of the weights while feeding quantized weights into backprop [3,11,4,16], which results in good empirical performance. There are limitations in using such methods on low-power devices, however, where floating-point arithmetic is not available or not desirable. Another widely used solution using only low-precision weights is stochastic rounding [17,5]. Experiments show that networks using 16-bit fixed-point representations with stochastic rounding can deliver results nearly identical to 32-bit floating-point computations [5], while lowering the precision down to 3-bit fixed-point often results in a significant performance degradation [18]. Bayesian learning has also been applied to train binary networks [19,20]. A more comprehensive review can be found in [4].",
            "score": 0.6154434399907747,
            "section_title": "Background and Related Work",
            "char_start_offset": 2721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 308
                },
                {
                    "start": 311,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1544
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 152,
                    "matchedPaperCorpusId": "16104422"
                },
                {
                    "start": 304,
                    "end": 307,
                    "matchedPaperCorpusId": "2595906"
                },
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "16104422"
                },
                {
                    "start": 466,
                    "end": 469,
                    "matchedPaperCorpusId": "15178138"
                },
                {
                    "start": 1121,
                    "end": 1125,
                    "matchedPaperCorpusId": "13282575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81591796875
        },
        {
            "corpus_id": "250334271",
            "title": "Attention Round for Post-Training Quantization",
            "text": "In recent years, deep neural networks have developed rapidly and achieved remarkable results in many domains, such as computer vision, autonomous driving, natural language processing and speech recognition. However, the implementation of deep neural networks often requires huge computational resources and expensive computational costs, severely limiting their application to resource-limited devices. At present, there are three main ways to solve this problem: model compression, lightweight model architecture design and model quantification. Model compression [1,2,3,4,5,6,7,8] trims the redundant parameters in the trained model to reduce model size and alleviate computational burden. Lightweight model architecture design generally uses neural architecture search [9,10,11,12,13] to directly design a small network structure. Model quantification [14,15,16,17,18,19,20,21,22,23] refers to mapping the model weights and activation values from 32 bit floating points to the fixed point number of lower bits, thus reducing the inference time and power consumption of model and realizing the acceleration. The current popular model quantification algorithms can be divided into two categories: quantization aware training algorithms and post-training quantization algorithms. Among them, the quantization aware training algorithm [24,25,26,27,28] generally require sufficient training on the complete training data set to obtain good quantitative results, which requires a lot of computing resources and a long training time. Adequate training data acquisition will also be a great challenge, which is severely limited in practical application. Compare to the quantization aware training algorithms, post-training quantization algorithms [29,30,31] has higher training efficiency and lower requirements for training data completeness. This quantization algorithm generally requires only a small amount of training data, and can basically complete the quantization process in a few hours. Although the posttraining quantization algorithms has the characteristics of high training efficiency, the quantization model will suffer from serious performance degradation when the quantization precision is low. For example, DFQ [29] quantify the ResNet18 to 8-bit, the quantified model accuracy is 69.7%, but when quantified to 4-bit, the accuracy is only 39%, suffering a large accuracy loss.",
            "score": 0.6154187175655927,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2206
                },
                {
                    "start": 2207,
                    "end": 2389
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88623046875
        },
        {
            "corpus_id": "199549009",
            "title": "Practical Considerations for Accuracy Evaluation in Sensor-Based Machine Learning and Deep Learning",
            "text": "First, what is the accuracy loss for each model when lowering the inference quantization? This should allow the designer to determine the trade-off between the possible ADC resolutions and accuracy loss in each model. It will also provide a practical accuracy evaluation when considering the ADC ENOB problem. Second, which model achieves the highest accuracy at the required quantization level? Third, is there a performance difference between training with high quantization, then applying inference with lower quantization vs. implementing both the training and the inference with lower quantization? Table 3 lists the models' inference accuracies using a range of simulated resolutions for the ADC. For the training stage, the quantization used the original dataset levels [9] at 16 bits, and the lower quantization was applied on the test set. Figure 5 demonstrates the simulation for 5 bits (32 levels) and 6 bits (64 levels) ADC for one accelerometer axis in one instance. In Figure 6 models' accuracy trend with lower inference quantization is illustrated. As can be seen from Table 3 and Figure 6, DNN has a very high tolerance to lower quantization. For the DNN model, lowering the inference quantization from 16 bits to 6 bits resulted in an accuracy loss of only 0.36%. DNN also achieved higher tolerance to thermal noise as per the previous section. However, for the other models, the accuracy loss trend with lower quantization was different compared to the thermal noise impact, which was simulated in the previous section. This is due to As can be seen from Table 3 and Figure 6, DNN has a very high tolerance to lower quantization. For the DNN model, lowering the inference quantization from 16 bits to 6 bits resulted in an accuracy loss of only 0.36%. DNN also achieved higher tolerance to thermal noise as per the previous section. However, for the other models, the accuracy loss trend with lower quantization was different compared to the thermal noise impact, which was simulated in the previous section. This is due to the fact that the nature of these problems is different. The thermal noise is uncorrelated [26], while the quantization noise is partly correlated [27].",
            "score": 0.6147326954242938,
            "section_title": "Quantization Levels Simulation",
            "char_start_offset": 30170,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2099
                },
                {
                    "start": 2100,
                    "end": 2195
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9404296875
        },
        {
            "corpus_id": "278602066",
            "title": "ITERA-LLM: Boosting Sub-8-Bit Large Language Model Inference via Iterative Tensor Decomposition",
            "text": "As quantizing both the weights and activations of LLMs to sub-8-bit fixed-point often results in severe accuracy degradation, various approaches have been proposed to address this challenge. \n\nOne such approach is Quantization-Aware Training (QAT). Q8BERT [8] emulated integer quantization operations during the training phase and employed the Straight-Through Estimator (STE) to approximate gradients for non-differentiable quantization operations. This method achieved 8-bit integer quantization on the BERT model, with no accuracy loss compared to the 32-bit floating-point representation. Similarly, Q-BERT [9] reduced precision to 4 bits with less than 1% accuracy loss. While QAT methods demonstrate promising sub-8-bit results, they also introduce significant challenges, including the high computational cost of training LLMs and the need for access to extensive datasets. \n\nAn alternative approach is to stay with post-training quantization while enhancing its performance through layer-wise optimization. For example, mixed-precision quantization [9], [10] can be employed, where different layers of the model are quantized at varying bit-widths to balance overall accuracy and compression. The per-layer bitwidth decisions can be informed by sensitivity analysis, such as evaluating the gradient of the loss function with respect to the weight parameters [11]. While layer-wise optimization can capture variations in each layer effectively, it often requires bit-serial hardware [12] to support varying precision levels. Such bit-serial designs are more favorable for ASICs but less efficient for FPGAs, as dedicated DSP cores cannot be fully utilized [13]. \n\nAnother promising direction is combining quantization with other orthogonal compression techniques, such as pruning [14], [15] and tensor decomposition [16], [17]. For this line of work, in addition to algorithmic integration, FPGA accelerators could serve as a suitable platform for efficiently implementing these combined techniques in hardware. However, current research in this combined direction has either focused on convolutional neural networks [17] or optimizations for LLMs using 16-bit precision [18], leaving sub-8-bit LLMs unaddressed.",
            "score": 0.6147247062660084,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 4349,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 193,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1668
                },
                {
                    "start": 1671,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 259,
                    "matchedPaperCorpusId": "204509218"
                },
                {
                    "start": 611,
                    "end": 614,
                    "matchedPaperCorpusId": "202565587"
                },
                {
                    "start": 1057,
                    "end": 1060,
                    "matchedPaperCorpusId": "202565587"
                },
                {
                    "start": 1062,
                    "end": 1066,
                    "matchedPaperCorpusId": "267095435"
                },
                {
                    "start": 1366,
                    "end": 1370,
                    "matchedPaperCorpusId": "148571720"
                },
                {
                    "start": 1663,
                    "end": 1667,
                    "matchedPaperCorpusId": "57373767"
                },
                {
                    "start": 1787,
                    "end": 1791,
                    "matchedPaperCorpusId": "272373470"
                },
                {
                    "start": 1823,
                    "end": 1827,
                    "matchedPaperCorpusId": "252383196"
                },
                {
                    "start": 1829,
                    "end": 1833,
                    "matchedPaperCorpusId": "244532096"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.912109375
        },
        {
            "corpus_id": "261214350",
            "title": "A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance",
            "text": "Quantization is the process of reducing the range and precision of the numerical representation of data. When applied to the weights and activations of neural networks, integer quantization reduces compute and memory requirements, usually in exchange for minor reductions in model accuracy [14,19,20,47]. During inference, most of the compute workload is concentrated in operators such as convolutions and matrix multiplications, whose products are typically accumulated into 32-bit registers that we refer to as accumulators. It has been shown that reducing accumulators to 16 bits on CPUs and ASICs can increase inference throughput and bandwidth efficiency by up to 2x [9,45], and reducing to 8 bits can improve energy efficiency by over 4x [32]. However, exploiting such an optimization is highly non-trivial as doing so incurs a high risk of overflow. Due to wraparound two's complement arithmetic, this can introduce numerical errors that degrade model accuracy [32]. \n\nPrevious works have sought to either reduce the risk of overflow [25,37,45] or mitigate its impact on model accuracy [32]. However, such approaches struggle to maintain accuracy when overflow occurs too frequently [32], and are unable to support applications that require guaranteed arithmetic correctness, such as finite-precision fully homomorphic encryption computations [28,40]. Thus, we are motivated to avoid overflow altogether. As the first principled approach to guarantee overflow avoidance, we provide theoretical motivation in Section 3, where we derive comprehensive accumulator bit width bounds with finer granularity than existing literature. In Section 4, we present accumulator-aware quantization (A2Q); a novel method designed to train quantized neural networks (QNNs) to use low-precision accumulators during inference without any risk of overflow. In Section 5, we show that our method not only prepares QNNs for low-precision accumulation, but also inherently increases the sparsity of the weights. \n\nWhile our results have implications for general-purpose platforms such as CPUs and GPUs, we primarily target model deployment on custom FPGA-based inference accelerators.",
            "score": 0.6147147875473047,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1995
                },
                {
                    "start": 1998,
                    "end": 2168
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "15817277"
                },
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 300,
                    "end": 303,
                    "matchedPaperCorpusId": "251432134"
                },
                {
                    "start": 672,
                    "end": 675,
                    "matchedPaperCorpusId": "201895129"
                },
                {
                    "start": 675,
                    "end": 678,
                    "matchedPaperCorpusId": "220484674"
                },
                {
                    "start": 744,
                    "end": 748,
                    "matchedPaperCorpusId": "235613509"
                },
                {
                    "start": 968,
                    "end": 972,
                    "matchedPaperCorpusId": "235613509"
                },
                {
                    "start": 1041,
                    "end": 1045,
                    "matchedPaperCorpusId": "254100756"
                },
                {
                    "start": 1048,
                    "end": 1051,
                    "matchedPaperCorpusId": "220484674"
                },
                {
                    "start": 1093,
                    "end": 1097,
                    "matchedPaperCorpusId": "235613509"
                },
                {
                    "start": 1190,
                    "end": 1194,
                    "matchedPaperCorpusId": "235613509"
                },
                {
                    "start": 1350,
                    "end": 1354,
                    "matchedPaperCorpusId": "202781343"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87451171875
        },
        {
            "corpus_id": "209515558",
            "title": "Towards Unified INT8 Training for Convolutional Neural Network",
            "text": "Deep convolutional neural networks (DCNNs) have achieved remarkable success in many fields, such as computer vision, natural language processing, information retrieval, etc. However, training and deploying DCNNs usually require a large amount of computational cost and power consumption, which is greatly challenging the extensive applications in industry. As a result, many recent studies have been focusing on how to accelerate the inference of neural networks by fixed-point quantization on weights or activations [6,3,23,29,27,62,42,53,61,55,20], and design dedicated hardware utilizing the efficient integer arithmetic [17,5,26,22]. The successful progress surprisingly shows that the bit-width can be reduced to extremely low such as 4-bit while bringing quite little hurt to the accuracy for inference [15,57,13]. \n\nBesides inference, low-bit training can also promise considerable acceleration, which further quantizes gradients and utilizes low-bit efficient compute kernel for both the forward and backward propagation. As analyzed in [24], the computation of backward propagation occupies more time than that of forward propagation. So accelerating the training utilizing low-bit quantization has greater potential when considering the backward process. There has existed 16-bit floating-point (FP16) training, which proves the feasibility of low-bit training [41,9,33]. But it is restricted to limited advanced GPUs based on Turing or Volta architecture. Compared with FP16, the 8-bit integer (INT8) operation is widely supported by general GPUs based on Turing, Volta and even low-end Pascal architectures. Besides, the 8-bit integer arithmetic is theoretically and practically 2\u00d7 faster than FP16 and 4\u00d7 faster than FP32. Therefore, INT8 training enjoys better efficiency, lower power consumption and better versatility on off-the-shelf hardware. \n\nDespite the attractive benefits, when quantizing gradients to 8-bit, the normal training tends to become unstable, since the distortion of gradients easily misleads the direction of training and causes crash of optimization. This definitely makes INT8 training very difficult, especially for the deep networks. Currently only a few studies have attempted to solve this problem [62,56,58,2,54,50].",
            "score": 0.6144809113012469,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1860
                },
                {
                    "start": 1863,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2259
                }
            ],
            "ref_mentions": [
                {
                    "start": 522,
                    "end": 525,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 528,
                    "end": 531,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 543,
                    "end": 546,
                    "matchedPaperCorpusId": "52847187"
                },
                {
                    "start": 546,
                    "end": 549,
                    "matchedPaperCorpusId": "3520830"
                },
                {
                    "start": 628,
                    "end": 630,
                    "matchedPaperCorpusId": "201905919"
                },
                {
                    "start": 630,
                    "end": 633,
                    "matchedPaperCorpusId": "4202768"
                },
                {
                    "start": 809,
                    "end": 813,
                    "matchedPaperCorpusId": "199577815"
                },
                {
                    "start": 1375,
                    "end": 1377,
                    "matchedPaperCorpusId": "3517431"
                },
                {
                    "start": 1377,
                    "end": 1380,
                    "matchedPaperCorpusId": "31312287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87451171875
        },
        {
            "corpus_id": "240353756",
            "title": "RMSMP: A Novel Deep Neural Network Quantization Framework with Row-wise Mixed Schemes and Multiple Precisions",
            "text": "This highly hardware-informative quantization strategy significantly reduces the search space of the DNN quantization problem, making our framework distinctive from existing multi-precision quantization works. This is the first effort to apply mixed quantization schemes and multiple precisions within layers, targeting for simplified operations in hardware inference, while preserving the accuracy. Specifically, two quantization schemes i.e., Power-of-Two (PoT) and Fixed-point (Fixed), and two precisions i.e., 4-bit and 8-bit are adopted and explored for quantization on weights and activations, to reduce inference computation and preserve accuracy. Furthermore, as demonstrated by the previous works that either implicitly use higher precisions for the first and the last layers, or explicitly assign multiple precisions to different layers, it is essential to use higher precisions at least for parts of the model, to boost the accuracy of quantized models close to that of the full-precision models. However, different from existing works, this paper makes the observation that this does not necessarily relate to layer-wise sensitivity, instead, as long as a certain portion of the weights in every layer use higher precisions, the quantization error can be mitigated. This observation enables layer-wise uniformality towards practical hardware implementation of quantized models, while still enjoying row-wise flexibility of mixed schemes and multiple precision. The contributions of our quantization framework are summarized as follows: \n\n\u2022 A novel row-wise mixed-scheme and multipleprecision quantization approach. \n\n\u2022 A highly hardware-informative solution strategy significantly reducing the problem search space. \n\n\u2022 The best accuracy performance when under the same equivalent precision as the existing works. \n\n\u2022 The significant inference speedup on real devices, comparing to the network-wise uniform low-bit quantization i.e., the speed upper bound of the popular layer-wise multi-precision approaches. or ternarized (with values of \u22121, 0, +1) levels. Representative binary quantization methods include Binaryconnect [5], Binarized Neural Network (BNN) [6], XNORnet [26], and ABC-Net [22].",
            "score": 0.6141933582573946,
            "section_title": "Introduction",
            "char_start_offset": 3991,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1547
                },
                {
                    "start": 1550,
                    "end": 1626
                },
                {
                    "start": 1629,
                    "end": 1727
                },
                {
                    "start": 1730,
                    "end": 1825
                },
                {
                    "start": 1828,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2208
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87109375
        },
        {
            "corpus_id": "1875476",
            "title": "Deep Learning with Low Precision by Half-Wave Gaussian Quantization",
            "text": "[24,28] have shown that ternary weight quantization into levels {\u22121, 0, 1} can achieve 16\u00d7 or 32\u00d7 model compression with slight accuracy loss, even on large-scale classification tasks. Finally, [3] has shown that filter weights can be quantized to \u00b11 without noticeable loss of classification accuracy on datasets such as CIFAR-10 [21]. \n\nBeyond weight binarization, the quantization of activations has two additional benefits: 1) further speed-ups by replacement of the expensive inner-products at the core of all network computations with logical and bit-counting operations; 2) training memory savings, by avoiding the large amounts of memory required to cache full-precision activations. Due to these, activation quantization has attracted some attention recently [38,26,4,32,40,25,28]. In [38], activations were quantized with 8 bits, to achieve speedups on CPUs. By performing the quantization after network training, this work avoided the issues of nondifferentiable optimization. [26] developed an optimal algorithm for bit-width allocation across layers, but did not propose a learning procedure for quantized neural networks. Recently, [4,32,40] tried to tackle the optimization of networks with nondifferentiable quantization units, by using a continuous approximation to the quantizer function in the backpropagation step. [25] proposed several potential so-lutions to the problem of gradient mismatch and [28,40] showed that gradients can be quantized with a small number of bits during the backpropagation step. While some of these methods have produced good results on datasets such as CIFAR-10, none has produced low precision networks competitive with full-precision models on large-scale classification tasks, such as ImageNet [35].",
            "score": 0.6141123479416466,
            "section_title": "Related Work",
            "char_start_offset": 7669,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 336
                },
                {
                    "start": 339,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1750
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 197,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 768,
                    "end": 772,
                    "matchedPaperCorpusId": "15196840"
                },
                {
                    "start": 772,
                    "end": 775,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 794,
                    "end": 798,
                    "matchedPaperCorpusId": "15196840"
                },
                {
                    "start": 988,
                    "end": 992,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 1745,
                    "end": 1749,
                    "matchedPaperCorpusId": "2930547"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.779296875
        },
        {
            "corpus_id": "102350477",
            "title": "HAQ: Hardware-Aware Automated Quantization With Mixed Precision",
            "text": "In many real-time machine learning applications (such as robotics, autonomous driving, and mobile VR/AR), deep neural networks is strictly constrained by the latency, energy, and model size. In order to improve the hardware efficiency, many researchers have proposed to quantize the weights and activations to low precision [8,18,34]. \n\nConventional quantization methods use the same number of bits for all layers [2,14], but as different layers have different redundancy and behave differently on the hardware (computation bounded or memory bounded), it is necessary to use mixed precision for different layers (as shown in Figure 1). This flexibility was originally not supported by chip vendors until recently the hardware manufacturers started to implement this feature: Apple released the A12 Bionic chip that supports mixed precision for the neural network inference [6]; NVIDIA recently introduced the Turing GPU architecture that supports 1-bit, 4-bit, 8-bit and 16-bit arithmetic operations [21]; Imagination launched a flexible neural network IP that supports per-layer bitwidth adjustment for  1: Inference latency of MobileNet-V1 [12] on three hardware architectures under different quantization policies. \n\nThe quantization policy that is optimized for one hardware is not optimal for the other. This suggests we need a specialized quantization solution for different hardware architectures. (HW1: BitFusion [25], HW2: BISMO [26] edge accelerator, HW3: BISMO cloud accelerator, batch = 16). \n\nboth weights and activations [13]. Besides industry, recently academia also works on the bit-level flexible hardware design: BISMO [26] proposed the bit-serial multiplier to support multiplications of 1 to 8 bits; BitFusion [25] supports multiplications of 2, 4, 8 and 16 bits in a spatial manner. However, a very missing part is how to determine the bitwidth of both weights and activations for each layer on different hardware accelerators. This is a vast design space: with M different neural network models, each with N layers, on H different hardware platforms, there are in total O(H \u00d7 M \u00d7 8 2N ) * possible solutions.",
            "score": 0.6134191617211734,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 334
                },
                {
                    "start": 337,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1503
                },
                {
                    "start": 1506,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2130
                }
            ],
            "ref_mentions": [
                {
                    "start": 324,
                    "end": 327,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 327,
                    "end": 330,
                    "matchedPaperCorpusId": "38486148"
                },
                {
                    "start": 330,
                    "end": 333,
                    "matchedPaperCorpusId": "224893"
                },
                {
                    "start": 417,
                    "end": 420,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1421,
                    "end": 1425,
                    "matchedPaperCorpusId": "21681898"
                },
                {
                    "start": 1438,
                    "end": 1442,
                    "matchedPaperCorpusId": "49421250"
                },
                {
                    "start": 1637,
                    "end": 1641,
                    "matchedPaperCorpusId": "49421250"
                },
                {
                    "start": 1730,
                    "end": 1734,
                    "matchedPaperCorpusId": "21681898"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6591796875
        },
        {
            "corpus_id": "256982085",
            "title": "FPGA-Based Vehicle Detection and Tracking Accelerator",
            "text": "When the model is trained in the GPU, it usually uses a 32-bit floating point to express the weight, gradient, and activation value of the network. Using floating-point operations increases the overhead of the computational unit; thus, currently, lower bit-width fixedpoint numbers are usually used for the inference process of neural networks. We carried out the dynamic 16-bit fixed-point quantization for weight bias parameters, feature mapping, and intermediate results in three quantization stages [36]. The conversion of fixed-point and floating-point numbers is shown in Equations ( 5) and (6), where x q is a fixed-point number, x is a floating-point number, and Q is used to specify the base point position of the fixed-point number. \n\nFor a given fixed-point number, its actual floating-point value is a function of the bit width w and the exponent character Q, as shown below: \n\nIn the stage of quantifying the weight values and bias values, the optimal Q values are analyzed for each layer dynamically using the approach shown in Equations ( 8) and ( 9), so that the absolute error sum of the original value of the weight bias and the quantized value is minimized. W l f loat and b l f loat are 32-bit floating-point values of the l-th layer weights and biases, respectively, and W l f ixed (w, Q) and b l f ixed (w, Q) are 16-bit fixed-point values of the l-th layer weights and biases, respectively. \n\nIn the stage of quantization of inputs and outputs between layers, we find the optimal Q value for each layer of the input-output feature map, and the optimal Q value is calculated as shown in Equations ( 10) and (11). For example, the RGB value of the input image is scaled to the [0,1] interval in the preprocessing stage, and Q = 14 can be used to quantize the input of the first layer when the bit width w = 16. \n\nIn the stage of quantifying the intermediate results, we find the best Q value for each layer of intermediate data by using the approach shown in (12).",
            "score": 0.6132193914941209,
            "section_title": "Dynamic 16-bit Fixed-Point Quantization",
            "char_start_offset": 16465,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 742
                },
                {
                    "start": 745,
                    "end": 887
                },
                {
                    "start": 890,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1831
                },
                {
                    "start": 1834,
                    "end": 1985
                }
            ],
            "ref_mentions": [
                {
                    "start": 503,
                    "end": 507,
                    "matchedPaperCorpusId": "51868101"
                },
                {
                    "start": 597,
                    "end": 600,
                    "matchedPaperCorpusId": "9128073"
                },
                {
                    "start": 1629,
                    "end": 1633,
                    "matchedPaperCorpusId": "2141740"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6328125
        },
        {
            "corpus_id": "245537442",
            "title": "HiKonv: High Throughput Quantized Convolution With Novel Bit-wise Management and Computation",
            "text": "Quantization is a frequently used technique in hardware implementation of Deep Neural Network (DNN) models in order to reduce both the memory consumption and execution time [1]- [6]. It is typically done by approximating highprecision floating point numbers to low-bitwidth integers or fixed-point numbers. This is particularly important for modern DNN models as many of them employ convolutional layers, which contain intensive multiplication and accumulation (MAC) operations [5]- [8]. Therefore, many novel quantization methods have been proposed in the literature to reduce the precision of weights, activations, and even gradients for DNNs while retaining their high accuracy [2], [3], [6], [9]. \n\nThe current hardware implementation of quantized DNNs is, however, not ideal as there is no general support for quantized MACs without changing the underlying hardware [10]- [15]. Most hardware units have a high-bitwidth (such as 32 or 64 bits) MAC for either floating point numbers or *Both authors contribute equally to this paper. \n\nintegers [16]. When they are used for quantized MACs, most of the bitwidths are left underutilized, wasting precious computing resources. Even with the 8-bit multi-data processing of the Advanced Vector Extensions (AVX) support in X86 64 architecture, processing a single 4-bit multiplication would still occupy the 8-bit data width with the remaining 4 bits simply wasted [17]. The waste becomes even more severe when either processing lower bitwidth (such as binary) data or utilizing a hardware unit with higher built-in bitwidths. \n\nReconfigurable hardware such as FPGA may alleviate some of the waste because of its bit-level flexibility for configuration, but it exhibits similar drawbacks, especially for FPGAs with high-precision Digital Signal Processing (DSP) units [8], [16], [18]. Without a careful bit-wise management of inputs and outputs, deploying quantized DNNs onto FPGAs with the given DSPs would still waste much of their computation capacity.",
            "score": 0.6130389565019525,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 700
                },
                {
                    "start": 703,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1036
                },
                {
                    "start": 1039,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1573
                },
                {
                    "start": 1576,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2002
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 176,
                    "matchedPaperCorpusId": "232352683"
                },
                {
                    "start": 178,
                    "end": 181,
                    "matchedPaperCorpusId": "104292236"
                },
                {
                    "start": 478,
                    "end": 481,
                    "matchedPaperCorpusId": "53243741"
                },
                {
                    "start": 483,
                    "end": 486,
                    "matchedPaperCorpusId": "202699949"
                },
                {
                    "start": 681,
                    "end": 684,
                    "matchedPaperCorpusId": "203605628"
                },
                {
                    "start": 686,
                    "end": 689,
                    "matchedPaperCorpusId": "218674432"
                },
                {
                    "start": 691,
                    "end": 694,
                    "matchedPaperCorpusId": "104292236"
                },
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "14395129"
                },
                {
                    "start": 871,
                    "end": 875,
                    "matchedPaperCorpusId": "21681898"
                },
                {
                    "start": 877,
                    "end": 881,
                    "matchedPaperCorpusId": "189824784"
                },
                {
                    "start": 1048,
                    "end": 1052,
                    "matchedPaperCorpusId": "4700762"
                },
                {
                    "start": 1412,
                    "end": 1416,
                    "matchedPaperCorpusId": "57376776"
                },
                {
                    "start": 1815,
                    "end": 1818,
                    "matchedPaperCorpusId": "202699949"
                },
                {
                    "start": 1820,
                    "end": 1824,
                    "matchedPaperCorpusId": "4700762"
                },
                {
                    "start": 1826,
                    "end": 1830,
                    "matchedPaperCorpusId": "18593767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6796875
        },
        {
            "corpus_id": "239690802",
            "title": "Super-Resolution Model Quantized in Multi-Precision",
            "text": "According to the number of quantization bits, it can be divided into floating-point quantization and fixed-point quantization. Floating-point quantization is to quantize the original high-precision number with 16-bit floating-point or 8-bit floating-point number or even lower-precision floating-point number. Fixed-point quantization means highprecision quantization into 16-bit fixed-point or 8-bit fixed-point quantization or even lower-precision quantization. The quantized data is inferenced or trained in the neural network, and the intermediate data can be calculated and stored with low precision. Generally speaking, this quantization method mainly considers the choice of the number of data mapping bits and the method, and the principle is the same as mentioned above. At present, 8-bit integer quantization has the most stable effect in a variety of tasks, so this article mainly studies 8-bit integer quantization. \n\nOn the basis of whether quantization is needed in the quantization process, it is divided into post-training quantization and quantization aware training. Post-training quantization is the quantization operation after the floating-point model training converges, and whether it is necessary to \"feed\" the data to the model for calibration, it is divided into static quantization and dynamic quantization. The calibration in static quantization is to \"feed\" data to the model in advance (usually about a few hundred data), and determine the zero point and the scaling factor. For the determination of quantization parameters, sometimes the training environment and training data cannot be obtained, so calibration cannot be used. In dynamic quantization, the weight is quantized in advance, and the activation is dynamically quantized in the forward inference process, that is, the scale value and zero point are calculated once for each layer of the actual floating-point data range, and then quantized. Therefore, calibration is not used, but the maximum and minimum values are determined directly and dynamically based on the input tensor value, and then other parameters are determined. Static quantization is shown in Figure 4.",
            "score": 0.6126437951137477,
            "section_title": "Quantization Method",
            "char_start_offset": 13628,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 927
                },
                {
                    "start": 930,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2161
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67626953125
        },
        {
            "corpus_id": "258048735",
            "title": "AutoQNN: An End-to-End Framework for Automatically Quantizing Neural Networks",
            "text": "The heavy computational burden immensely hinders the deployment of deep neural networks (DNNs) on resource-limited devices in real application scenarios. Quantization is a technique which compresses DNN weight and activation values from high-precision to lowprecision. The low-precision weights and activation occupy smaller memory bandwidth, registers, and computing units, thus significantly improving the computing performance. For example, AlexNet with 32-bit floating-point (FP32) can only achieve the performance of 1.93TFLOPS on RTX2080Ti since the bandwidth constraints. However, the low-precision AlexNet with 16-bit floating-point (FP16) can achieve the performance of 7.74TFLOPS on the same device because the required bandwidth is halved while the available computing units are doubled 1 . The FP16 AlexNet is four times faster than the FP32 one on RTX2080Ti GPU. Therefore, quantization can reduce the computing budgets in DNN inference phases and enable DNN model deployment on resource-limited devices. \n\nHowever, unreasonable quantizing strategies, such as binary [2] and ternary [3], tend to seriously affect DNN model accuracy [4,5] and lead to customer frustration. Lower bitwidth usually leads to higher computing performance but larger accuracy degradation. The quantization strategy selection dominates the computing performance and inference accuracy of models. In order to balance the computing performance and inference accuracy, many previous investigations have tried to select a unified bitwidth for all layers in DNNs [6,7] cautiously. However, many studies show that different layers of DNNs have different sensitivities [8,9] and computing budgets [10]. Using the same bitwidth for different layers is hard to obtain superior speed-up and accuracy. To strike a fine-grained balance between efficiency and accuracy, it is strongly demanded to explore desirable quantizing schemes [11] and reasonable mixed-precision policies [10,12] for various neural architectures. The brute force approaches are not feasible for this exploration since the search space grows exponentially with the number of layers [8]. Heuristic explorations work in some scenarios but greatly rely on the heavy workloads of domain experts [11].",
            "score": 0.612563198706936,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1017
                },
                {
                    "start": 1020,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2135
                },
                {
                    "start": 2136,
                    "end": 2245
                }
            ],
            "ref_mentions": [
                {
                    "start": 619,
                    "end": 621,
                    "matchedPaperCorpusId": "218614013"
                },
                {
                    "start": 1080,
                    "end": 1083,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 1148,
                    "end": 1150,
                    "matchedPaperCorpusId": "2753399"
                },
                {
                    "start": 1547,
                    "end": 1550,
                    "matchedPaperCorpusId": "218674432"
                },
                {
                    "start": 1550,
                    "end": 1552,
                    "matchedPaperCorpusId": "203605628"
                },
                {
                    "start": 1654,
                    "end": 1656,
                    "matchedPaperCorpusId": "207852310"
                },
                {
                    "start": 1679,
                    "end": 1683,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 1910,
                    "end": 1914,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 1955,
                    "end": 1959,
                    "matchedPaperCorpusId": "102350477"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82861328125
        },
        {
            "corpus_id": "216104766",
            "title": "Efficient Embedded Deep Neural-Network-based Object Detection Via Joint Quantization and Tiling",
            "text": "Quantizing neural networks can reduce weights, activations and sometimes gradients down to 8 or even 1 bit representation hence shrinking the model size and accelerate inference especially in customized hardware accelerators [28]. Compressing a neural network by quantization not only reduces memory requirements but also power since the network is doing millions of multiplications and additions at much lower precision and the on-and off-chip data movement is reduced with the use of lower-bit quantized data. In this work, we are using an 8-bit quantization technique on both the input image and the weights of the CNN.",
            "score": 0.6124842438052158,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 6555,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 622
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "1518846"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78369140625
        },
        {
            "corpus_id": "232110416",
            "title": "Effective and Fast: A Novel Sequential Single Path Search for Mixed-Precision Quantization",
            "text": "With the development of deep neural networks (DNNs), DNNs have achieved impressive performance in many applications. Generous computing resources and footprint memory are urgently required when deeper networks are used to solve various problems. Moreover, with the rapid development of chip technologies, especially GPU and TPU, the computational frequency and efficiency have been greatly improved. Most scholars use GPUs as the basic hardware platform for network training due to excellent acceleration capability. However, for low power consumption platforms (e.g., mobile phone, embedded devices and smart chips), whose resources are limited, it is hard to achieve satisfactory performance for industrial applications. As one of the typical methods for model compression and acceleration, model quantization methods usually quantize fullprecision (32-bit) parameters to low-bits of precision (e.g., 8-bit and 4-bit). Even more, we can extremely constrain weights and activation values to binary {-1, +1} [18,40] or ternary {-1, 0, +1} [61], which can be computed by bitwise operations. This logic calculation is more suitable for the implementation of FPGA and other service-oriented computing platforms. As implemented in [27], they achieved about 705\u00d7 speed up compared with CPU and are 70\u00d7 faster than GPU in the peak condition. \n\nIn recent years, many methods [8,22,26,30,46,58] have been proposed to improve the performance of lowprecision models. However, the bit-precisions of most quan-0 arXiv:2103.02904v1 [cs.CV] 4 Mar 2021 tization models are set manually based on experience, and all layers have the same quantization precision in general. Some studies [5,7,10,11] show that different layers have different sensitivity to quantization. Therefore, the mixedprecision quantization [36,48,6] can achieve better performance according to the characteristics of network. Besides, some recent smart chips also support mixed-precision for the DNN inference, e.g., Apple A12 Bionic [12], Nvidia Turing GPU [37], BitFusion [44] and BISMO [49].",
            "score": 0.6119723938257913,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1335
                },
                {
                    "start": 1338,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2049
                }
            ],
            "ref_mentions": [
                {
                    "start": 1008,
                    "end": 1012,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 1012,
                    "end": 1015,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 1374,
                    "end": 1377,
                    "matchedPaperCorpusId": "210848001"
                },
                {
                    "start": 1377,
                    "end": 1380,
                    "matchedPaperCorpusId": "10533533"
                },
                {
                    "start": 1380,
                    "end": 1383,
                    "matchedPaperCorpusId": "70055866"
                },
                {
                    "start": 1383,
                    "end": 1386,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 1672,
                    "end": 1674,
                    "matchedPaperCorpusId": "52195590"
                },
                {
                    "start": 1677,
                    "end": 1680,
                    "matchedPaperCorpusId": "148571720"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67626953125
        },
        {
            "corpus_id": "229181236",
            "title": "Exploring Neural Networks Quantization via Layer-Wise Quantization Analysis",
            "text": "Quantized neural networks are networks which have their weights and/or activations constrained to low-bit representation, usually 8-bit and below. Neural networks are resource intensive algorithms and quantization is an easy and efficient way to reduce both the computational and memory load of deployed networks. In recent years, quantization of deep neural networks has become ubiquitous: indeed, just 3 years ago the frontier for quantization was 8-bits for both The building block of our layer-wise analysis is the creation of a network with only a single quantized layer j. The input and output of the layer is quantized as well as the weights. \n\nThe rest of the network is kept at full precision. (b) Layer-wise analysis applied to Resent-18 . This breakdown gives insight into which layers preform poorly under quantization activations and weights (Jacob et al., 2017), while state-ofthe-art works operate at 4-bits and below (Banner et al., 2018;Uhlich et al., 2019;Fang et al., 2020;Jain et al., 2019;Stock et al., 2019;Dong et al., 2019a;Wang et al., 2018;Choukroun et al., 2019). \n\nAn existing gap in the literature is tools for quantization post-mortem. Once a quantization method has been applied it is evaluated solely on the basis of its degradation with respect to the full-precision network. We claim that an investigation into the sources of quantization degradation is valuable for (a) a better characterization of quantization methods in general and their fail cases in particular, (b) directing future research to address common fail-cases and (c) quick ad-hoc fixes of quantization problems in specific networks. \n\nIn this paper we take a step in addressing the above gap. We present layer-wise quantization analysis which is based on previously observed properties of quantization: The noise and degradation of individually quantized layers can be summed to get the overall noise and degradation of a fully quantized network (Zhou et al., 2017).",
            "score": 0.6101657118144004,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 649
                },
                {
                    "start": 652,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 1090
                },
                {
                    "start": 1093,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1634
                },
                {
                    "start": 1637,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1968
                }
            ],
            "ref_mentions": [
                {
                    "start": 1066,
                    "end": 1088,
                    "matchedPaperCorpusId": "67750088"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.796875
        },
        {
            "corpus_id": "73479694",
            "title": "FPGA-Based Hybrid-Type Implementation of Quantized Neural Networks for Remote Sensing Applications",
            "text": "Table 4 shows the classification accuracies of the fundamental model and quantized model in five data formats. The model with the floating-point data type obtains the greatest result of 98.43%. The classification accuracy of the 4-bit quantized model has the maximum loss of 4.0%. The other quantized models obtain satisfying performance and the average results are all around 97.2%, with a 1.2% accuracy degradation, compared to floating-point types. It can be seen that the classification accuracy degradation is reduced as the bit-width increases, which may be due to the increased quantization resolution resulting in a computing error reduction. However, the performance can hardly improve after reaching the maximum as the inherent redundancy within the network is limited. The standards deviations of 4-bit and 6-bit quantized models are higher than the others. This means that the stability of training is not good when adopting a low bit-width quantization scheme and multiple trainings are required to obtain a satisfying performance. The memory requirement for the weight storage of quantized models is shown in Table 5. These results indicate that the compression rate of 4, 6, 8, 10, and 12-bit quantization is 8.0\u00d7, 5.3\u00d7, 4.0\u00d7, 3.2\u00d7 and 2.7\u00d7, respectively. Since the memory requirement of scaling factors is negligible, the compression rate \u03b7 can be approximately calculated by the following: \n\nThe image classification performance will degrade when the network is quantized. However fortunately, it is within an acceptable range. More importantly, the requirement of on-chip Static Random-Access Memory (SRAM) and memory bandwidth sharply decreases. The 8-bit quantized model achieves a trade-off between classification accuracy degradation and this requirement; therefore, the 8-bit quantization scheme is used for the hardware optimization design in this paper. Figure 9 shows the test results for each epoch for all the aforementioned models. To clearly reflect the test results, the formula, described in the figure, is used to smooth the curve, and the smoothing rate is set to 0.65. As can be seen from the results, floating-point networks achieve nearly best performance at about 50 epochs, while all the quantized networks require about 130 epochs.",
            "score": 0.6101221161592826,
            "section_title": "Dataset Description and Data Preprocessing",
            "char_start_offset": 40382,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2271
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97509765625
        },
        {
            "corpus_id": "273821224",
            "title": "A Comprehensive Study on Quantization Techniques for Large Language Models",
            "text": "After thoroughly examining various popular quantization techniques, it is evident that their performance varies depending on the precision of the quantization. For example, GPTQ achieves optimal performance at 4-bit precision but experiences a decline at 3-bit precision. In addition, LLM-QAT demonstrates better accuracy with a configuration of 4-bit weights, 4-bit KV caches, and 8-bit activations, compared to a uniform 4-bit setting across all precision [35]. To maximize the performance of a given quantization technique, developers must carefully select appropriate precision settings. Future research on quantization techniques could further explore the impact of precision configuration, potentially leading to more refined and efficient quantization strategies.",
            "score": 0.6098403479191405,
            "section_title": "VII. CONCLUSION",
            "char_start_offset": 30229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 770
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9541015625
        },
        {
            "corpus_id": "267320710",
            "title": "Effect of Weight Quantization on Learning Models by Typical Case Analysis",
            "text": "A seminal study [BHHS18] demonstrated the utility of quantization in more modern deep learning and discussed the effectiveness of 8-bit quantization. This policy and the important role of bits in deep learning were actively discussed in [WCB + 18, WJZ + 20, CBD14, BLS + 21]. More practical use of the quantization is also investigated: a study [CBUS + 20] demonstrated the effectiveness of 6-bit quantization techniques in improving performance with natural gradient methods. These research streams of quantization are exhaustively summarized in [GKD + 22]. \n\nThe theoretical study of quantization is still a developing field. This study [DLXS19] investigated the universal approximability of neural network models with the quantization, i.e., any continuous function can be approximated under any accuracy, as well as the magnitude of overfitting when making predictions with the quantized neural network. Another paper [HTL + 23] discussed theoretically the impact of the training process with the quantization on this prediction performance. This paper [GBGR23] quantified the impact of quantization on the approximation ability of neural networks. This study [LDX + 17] analyzed theoretically the difficulty of learning by the quantization, and another study [CEKL16] proposed a new loss function and methodology to prevent performance degradation due to the quantization. Quantization has been a subject of investigation in statistical physics, particularly in classical spin models like the Ising model with binary variables and the Potts model with -state values. The variations in phase transition phenomena arising from distinct values of variables have been discussed [MPV87]. Statistical physics models and methods have been incorporated into statistical learning, and have successfully revealed conditions for both the failure and success of inference, as well as the development of the efficient algorithms [KWT09, KMS + 12a, KMS + 12b, ZK16]. In line with recent trends, quantization settings are incorporated in statistical physics-based machine learning studies, such as Hopfield model with discrete coupling [SA21], restricted Boltzmann machine with binary synapses [Hua17], and single layer neural network with multiple states [BGL + 16].",
            "score": 0.6097516521303192,
            "section_title": "Introduction",
            "char_start_offset": 4563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 558
                },
                {
                    "start": 561,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 24,
                    "matchedPaperCorpusId": "44071489"
                },
                {
                    "start": 639,
                    "end": 647,
                    "matchedPaperCorpusId": "52893258"
                },
                {
                    "start": 1057,
                    "end": 1065,
                    "matchedPaperCorpusId": "263695115"
                },
                {
                    "start": 1264,
                    "end": 1272,
                    "matchedPaperCorpusId": "17299045"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6484375
        },
        {
            "corpus_id": "237416732",
            "title": "On the Accuracy of Analog Neural Network Inference Accelerators [Feature]",
            "text": "The digital software accuracy of this network is shown in Table 2. When simulating the analog accuracy of this network, these scaling steps are processed digitally between in situ MVMs. \n\nFig. 19 compares the error sensitivity of the 4-bit QAT model with the floating-point ResNet50-v1.5 model, whose weights are quantized to 8 bits after training. For a fair comparison, an 8-bit ADC is included for both cases; in the 4-bit model, this higher-resolution ADC helps minimize errors prior to the 4-bit quantization step, which is performed digitally. Fig. 19(a) shows that the 4-bit model is substantially more resilient to state-independent errors than the 8-bit network. This results entirely from activation quantization, and not weight quantization. The large separation between the 16 activation levels effectively cuts off the propagation of accumulated cell errors from one layer to the next. The same cell error results in a smaller dot product error on average. Fig. 19(b) shows that the 4-bit network is also more resilient to state-proportional error, but here the benefit is smaller. This can be explained by the different weight value distributions of the two networks. With only 16 levels, the distribution of the 4-bit weights cannot peak as sharply at zero as the 8-bit weights, which have 256 levels. As a result, the memory cells in the 4-bit network have a significantly higher average conductance (7.52% of G max ) than the cells in the 8-bit network (1.95% of G max ). Thus, deeply quantized weights can actually be harmful for analog systems, as it leads to a higher state-proportional error per cell. This is ultimately outweighed by the benefit of quantized activations, so there is a net improvement in error sensitivity. \n\nNotably, for the 8-bit network, the sensitivity to both types of error with an 8-bit ADC remains largely unchanged from the case with no ADC quantization, shown in Fig. 9(b) and Fig. 10(b). As shown in Section 6, a calibrated 8-bit ADC on its own induces very little accuracy loss for this network.",
            "score": 0.60963431438721,
            "section_title": "SUPPRESSING ERROR PROPAGATION",
            "char_start_offset": 55501,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 185
                },
                {
                    "start": 188,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1745
                },
                {
                    "start": 1748,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2046
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.974609375
        },
        {
            "corpus_id": "235434086",
            "title": "MQBench: Towards Reproducible and Deployable Model Quantization Benchmark",
            "text": "Quantization Algorithms. As an appealing strategy for model compression and acceleration, quantization algorithm has driven much attention ever since 90s in last century. In (Holi & Hwang, 1993) [51], an empirical analysis on simple neural networks show 8-bit is sufficient for lossless quantization, Hoehfeld & Fahlman (1992) [52] developed a stochastic rounding scheme to further quantize artificial neural network below 8-bits. \n\nIn modern deep learning, low-bit quantization has driven lots of interests in academical research. DoReFa-Net [31] first proposes to quantize neural networks into multi-bit. [15,18,30] leverage the straight-through estimator (STE) to learn an appropriate clipping range for quantization. DSQ [28] studies the variant of STE and approximate the gradient of step functions. Works like [29,53,54] use non-uniform quantization to better adapt the distribution of the weights and activations. Recently, mixed-precision algorithm also made significant progress. For example, [44] uses reinforcement learning to study the bit-width allocation. The eigenvalue of Hessian matrix is employed in [55] to determine the bit for each layer. Differentiable methods like [56,57,58] directly learns the bit-width through gradient descent. \n\nAlong with quantization-aware training (QAT), a much easier way to perform quantization is called post-training quantization (PTQ). PTQ only requires 10-1000 training images and tiny computational resources to finish the quantization. [59,60,61] seek to find the optimal clipping range for 4-bit PTQ. OCS [62] uses channel-splitting to deal with outliers. [5,45] even do quantization without accessing any real data. [63,64] adopt intermediate feature-map reconstruction to optimize the rounding policy. \n\nQuantization Frameworks and Hardware. Generally speaking, hardware for quantization can be naturally divided to specialized hardware and general hardware. In 1990, Hammerstrom [65] already designed specialized VLSI hardware for 8-bit and 16-bit training.",
            "score": 0.6090996817593552,
            "section_title": "D Related Work",
            "char_start_offset": 42780,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 430
                },
                {
                    "start": 433,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1254
                },
                {
                    "start": 1257,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1760
                },
                {
                    "start": 1763,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2017
                }
            ],
            "ref_mentions": [
                {
                    "start": 195,
                    "end": 199,
                    "matchedPaperCorpusId": "1353891"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "67788003"
                },
                {
                    "start": 611,
                    "end": 614,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 816,
                    "end": 820,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 1002,
                    "end": 1006,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 1118,
                    "end": 1122,
                    "matchedPaperCorpusId": "148571720"
                },
                {
                    "start": 1492,
                    "end": 1496,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 1499,
                    "end": 1502,
                    "matchedPaperCorpusId": "67750088"
                },
                {
                    "start": 1562,
                    "end": 1566,
                    "matchedPaperCorpusId": "59413897"
                },
                {
                    "start": 1613,
                    "end": 1616,
                    "matchedPaperCorpusId": "184487878"
                },
                {
                    "start": 1616,
                    "end": 1619,
                    "matchedPaperCorpusId": "209531713"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60498046875
        },
        {
            "corpus_id": "53217792",
            "title": "Bi-Real Net: Binarizing Deep Network Towards Real-Network Performance",
            "text": "Parameter quantization. The study in [18] demonstrates that real-valued deep neural networks such as AlexNet [17], GoogLeNet [33] and VGG-16 [29] only encounter marginal accuracy degradation when quantizing 32-bit param-eters to 8-bit. In Incremental Network Quantization [41], Zhou et al. quantize the parameter incrementally and show that it is even possible to further reduce the weight precision to 2-5 bits with slightly higher accuracy on the Ima-geNet dataset than a full-precision network. Based on that, Zhou et al. further propose explicit loss-error-aware quantization [42], which obtains comparable accuracy as the real-valued network with very low-bit parameter values. In BinaryConnect, Courbariaux et al. more radically employ 1-bit precision weights (1 and -1) while maintaining sufficiently high accuracy on the MNIST, CIFAR10 and SVHN datasets. Ho et al. utilize a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss with regard to the binarized weights [10]. \n\nQuantizing weights properly can achieve considerable memory savings with little accuracy degradation. However, acceleration via weight quantization alone would be limited due to the real-valued activations (i.e., the input to convolutional layers). Several recent studies have been conducted to explore new network structures and/or training techniques for quantizing both weights and activations while minimizing accuracy degradation. Successful attempts include DoReFa-Net [44] and QNN [13], which explore neural networks trained with 1-bit weights and 2-bit activations, and the accuracy drops by 6.1% and 4.9%, respectively, on the ImageNet dataset compared to the real-valued AlexNet. Recently, Zhuang et al. [46] propose to jointly train a full-precision model alongside the low-precision one, which leads to no performance decrease in a 4-bit precision network compared with its full-precision counterpart.",
            "score": 0.6076472544997347,
            "section_title": "Related Work",
            "char_start_offset": 11610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1020
                },
                {
                    "start": 1023,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 113,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 125,
                    "end": 129,
                    "matchedPaperCorpusId": "206592484"
                },
                {
                    "start": 580,
                    "end": 584,
                    "matchedPaperCorpusId": "53872663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "221136343",
            "title": "Degree-Quant: Quantization-Aware Training for Graph Neural Networks",
            "text": "Quantization allows for model size reduction and inference speedup without changing the model architecture. While there exists extensive studies of the impact of quantization at different bitwidths [9,16,28] and data formats [29,6,23], it is 8-bit integer (INT8) quantization that has attracted the most attention. This is due to INT8 models reaching comparable accuracy levels to full-precision (FP32) models [26,19], offer a 4\u00d7 model compression, and result in inference speedups on off-the-shelf hardware as 8-bit arithmetic is widely supported. \n\nQuantization-aware training (QAT) has become the de facto approach towards designing robust quantized models with low error [42,48,42]. In their simplest forms, QAT schemes involve exposing the numerical errors introduced by quantization by simulating it on the forward pass and make use of a straight-through estimator (STE) [4] to compute the gradients-as if no quantization had been applied. For integer QAT, the quantization of a tensor x during the forward pass is often implemented as: x q = min(q max , max(q min , x/s + z )), where q min and q max are the minimum and maximum representable values at a given bit-width and signedness, s is the scaling factor making x span the [q min , q max ] range and, z is the zero-point, which allows for the real value 0 to be representable in x q . Both s and z are scalars obtained at training time. Then, the tensor is dequantized as: x = (x q \u2212 z)s, where the resulting tensor x \u223c x for a high enough bit-width. This similarity degrades at lower bit-widths. Other variants of integer QAT are presented in Jacob et al. [19]. \n\nReaching performance comparable to FP32 models at lower bit-widths is not trivial.",
            "score": 0.6075911481927829,
            "section_title": "Quantization for Non-Graph Neural Networks",
            "char_start_offset": 5496,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 548
                },
                {
                    "start": 551,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1624
                },
                {
                    "start": 1627,
                    "end": 1709
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8408203125
        },
        {
            "corpus_id": "208057880",
            "title": "An FPGA-Based Hardware Accelerator for CNNs Using On-Chip Memories Only: Design and Benchmarking with Intel Movidius Neural Compute Stick",
            "text": "In this section, the results obtained from the quantization process are presented and discussed. e SCNN model of this network has many degrees of freedom. For this reason, the first simulation step is finalised to identify a starting point for a more complex analysis, and it only focuses on the quantization of input layer words and weights. Figure 4 shows simulation results, in terms of accuracy and mean square error (MSE) in relation to the floatingpoint model, when only the number of bits for input words representation is quantized. A number of 4 or 5 bits optimize accuracy and minimize the MSE. is first analysis gives intuitions about a possible optimization for the input layer: in particular, the number of bits of every input can be forced to be multiple of 4 bits, so that several inputs might be contained in buses such as the Advanced eXtensible Interface 4 (AXI4) bus [26], whose size is usually a multiple of 8 bits. Figure 5 shows simulation results, in terms of accuracy and MSE, when only the number of bits for the representation of weights has been quantized. In this case, accuracy rapidly grows between 8 and 12 bits, reaching even higher values than the original ones in correspondence of 11 and 12 bits. Finally, accuracy saturates for 16 or more bits. is parameter is crucial because it influences the number of bits necessary to represent the result of MAC operations and, consequentially, the complexity of the entire network.\n\nis first analysis was the starting point for a more detailed design exploration, involving the number of bits for the representation of the output of each layer. Table 2 reports the best results obtained in terms of accuracy. Only the number of bits of SC layers and final_conv outputs are presented in the table, whereas data regarding temporal convolutional sublayers are omitted. e parameters listed in the table are as follows: Collected data show that it is possible to increase model accuracy through quantization. In fact, the best accuracy obtained for the floating-point model is 87.77%, whereas for the fixed-point representation, the highest accuracy is 90.23%. e second part of the simulation considers a different quantization for the final_conv layer due to the inclusion of the average pooling effects, as explained in Section 3.",
            "score": 0.6073916196188771,
            "section_title": "Results of the Quantization Analysis",
            "char_start_offset": 12767,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85302734375
        },
        {
            "corpus_id": "231918483",
            "title": "Confounding Tradeoffs for Neural Network Quantization",
            "text": "Different layers of neural networks are tolerant to different degrees of precision, and uniformly quantizing all layers of a neural network to the same low precision leads to accuracy degradation [12]. Mixed-precision quantization uses varying precision for different layers [34,13,12,35,24,37,32] or channels [3,39] of neural networks to decrease the average precision required by neural networks. \n\nMixed precision allows for different settings for the number of bits. For example, some works restrict the bitwidth for quantized operations to {2, 4, 8} bits [7], whereas other works allow any integer bitwidth [34,3]. While arithmetic on 3-bit integers may not be directly supported in hardware, and operations must be computed by casting these integers to 4-bits, such approaches can save memory traffic if values are efficiently packed. Describing the choice of bitwidth is critical because one mixed-precision method may outperform another solely because it has a larger selection of bit precision settings. \n\nMoreover, the activation bitwidth utilized by a mixedprecision network may be constrained by different desiderata. If memory capacity is the limiting constraint for acceleration, then activation bitwidths can be constrained solely by the maximum feature map size [34]. However, if memory pressure or computational throughput is the system bottleneck, mixed precision methods may be constrained by the average feature map size. The choice of constraint will impact accuracy.",
            "score": 0.6073264361957689,
            "section_title": "Mixed Precision",
            "char_start_offset": 9743,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 398
                },
                {
                    "start": 401,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1488
                }
            ],
            "ref_mentions": [
                {
                    "start": 275,
                    "end": 279,
                    "matchedPaperCorpusId": "204803866"
                },
                {
                    "start": 279,
                    "end": 282,
                    "matchedPaperCorpusId": "148571720"
                },
                {
                    "start": 285,
                    "end": 288,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 288,
                    "end": 291,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 310,
                    "end": 313,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 560,
                    "end": 563,
                    "matchedPaperCorpusId": "209531713"
                },
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "204803866"
                },
                {
                    "start": 616,
                    "end": 618,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 1278,
                    "end": 1282,
                    "matchedPaperCorpusId": "204803866"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89453125
        },
        {
            "corpus_id": "263237367",
            "title": "Differentiable Neural Architecture, Mixed Precision and Accelerator Co-Search",
            "text": "the goal is to automatically search for precision/bit-width of every weight and activation matrix, parameters of neural architecture: kernel size (k) and Number of filters/channels (C) and hardware accelerator: array height and array width. \n\nproven successful on many tasks such as Image Classification [11], Object Detection [16], etc., where searched networks surpassed the handcrafted models. \n\nNeural Network model reduction techniques, such as Pruning and Quantization, are crucial for optimized inference on various hardware platforms. The low precision inference is an effective approach to boost DNN performance on many hardware architectures. Quantization [17], [18], a wellknown and widely used method, converts high-precision floating point weight and/or activation parameters, such as Floating Point 32 (FP32), to low-precision values like Integer 8 (Int8) Integer 4 (Int4). Uniform Quantization assigns same precision or bit-width to all weight and activation matrices across all layers in a network. Nonetheless, quantizing the model with ultra-low precision, such as 1 or 2 bits, can result in significant accuracy degradation [17]. \n\nMixed Precision Quantization [19], [20] is a mechanism to allow different bit-width to different layers in a model to overcome the challenges faced by Uniform Quantization. Generally, mixed precision assignment enables the model to capture the layer information better by setting different weight and activation matrices to different range of precisions. However, this method's main problem is determining each layer's precision in the network so that the model attains acceptable accuracy while minimizing the model size. The algorithmic advancement of NAS facilitated the method to apply in other applications in Deep Learning which are combinatorial in nature. Therefore, Mixed Precision Quantization can be translated into a search problem, thereby utilizing similar NAS principles. We propose a Fast Hardware-aware Mixed Precision Search based on a search space pruning and tensor-sharing methodology. \n\nThe bit-width and the layer dimensions (kernel and channel sizes) of a neural network are often correlated, thus allowing us to co-design the hyperparameters together. The optimal combination of neural architecture and precision of each layer contributes to the model accuracy and efficiency on the hardware.",
            "score": 0.6071893981367368,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2062,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2057
                },
                {
                    "start": 2060,
                    "end": 2227
                },
                {
                    "start": 2228,
                    "end": 2368
                }
            ],
            "ref_mentions": [
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "184487125"
                },
                {
                    "start": 672,
                    "end": 676,
                    "matchedPaperCorpusId": "231699188"
                },
                {
                    "start": 1180,
                    "end": 1184,
                    "matchedPaperCorpusId": "102350477"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88134765625
        },
        {
            "corpus_id": "266374655",
            "title": "A Performance Evaluation of a Quantized Large Language Model on Various Smartphones",
            "text": "In the LLM domain, quantization can be defined as the process of reducing the bit-width of model parameters. LLMs typically store weight parameters as 32-bit floating points. Quantization attempts to maintain these with lower resolution with minimal quality loss. In the literature, LLM-specific quantization has been extensively studied for various resolutions, such as 8-bit and 4-bit integers [11,12,13]. Additionally, there exist distinct methodologies in the literature either focusing on the execution of quantization during the training phase, or quantizing the parameters of a trained model [8]. Depending on the quantization method employed, an LLM's memory footprint and processing requirements can be significantly reduced. However, it is essential to note that quantization introduces a precision loss, which affects inference performance. The evaluation of inference degradation due to quantization is a research area we wish to elaborate on in future work. Nonetheless, we would like to highlight that our preliminary benchmarks have yielded promising results. For our study, owing to accessibility and ease of integration, we have opted to conduct our tests using a quantized model.",
            "score": 0.6070571060527336,
            "section_title": "Quantization",
            "char_start_offset": 4742,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1197
                }
            ],
            "ref_mentions": [
                {
                    "start": 400,
                    "end": 403,
                    "matchedPaperCorpusId": "251564521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7001953125
        },
        {
            "corpus_id": "277272083",
            "title": "Energy-Aware LLMs: A step towards sustainable AI for downstream applications",
            "text": "In the detailed training loss, the 16-bit model and the 32bit model were almost identical, with only negligible changes throughout the epochs. That means 16-bit precision performance is close to 32-bit performance, with energy efficiency; the best case reaches up to 80% in energy efficiency. While the performances of the 8-bit and 4-bit models are also similar to each other during the same range of epochs, they tend to show a bit higher training loss compared to 16-bit and 32-bit models. Also, the performance difference did exist between the 16-bit/32-bit models and the 8-bit/4-bit models, though small, and the 32-bit model was better than its low-precision version. Therefore, it underlines that the 16-bit model gives the best trade-off considering the power consumption and performance for the current task and hardware in this phase. Turning to the results from the inference phase, Fig. 4 and Fig. 5 highlight the effects of quantization levels during the inference phase. Similar to the fine-tuning phase, the 16-bit model stands out as one of the top candidates for energy efficiency (reduction up to 40.5% overall), offering a good compromise with model performance. In detail, the BERT score drops slightly while other metrics show a slight improvement. The 8-bit model demonstrates an improvement across most metrics, but this comes with a substantial rise in both energy consumption and the number of tokens generated per second. Thus, this suggests that it can maintain accuracy while using significantly fewer bits in a downstream application. However, this comes at a significant cost to energy efficiency, resulting in higher energy consumption and slower token generation. The comparison underscores that while the 8-bit Fig. 5: Impact of quantization levels 3 . Fig. 6: LLAMA3: impact of unstructured-base pruning and structured pruning 3 . model offers improved performance over the 32-bit model in the same task, the trade-off in resource usage needs to be carefully evaluated, especially when aiming for energy efficiency. One of the reasons for this is quantization also introduces quantization errors into the model, which adds extra computational cost as the model works to recover the weights during inference [22].",
            "score": 0.6069949949347304,
            "section_title": "IV. RESULTS AND FINDINGS",
            "char_start_offset": 18371,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2247
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80810546875
        },
        {
            "corpus_id": "226237041",
            "title": "CUTIE: Beyond PetaOp/s/W Ternary DNN Inference Acceleration With Better-Than-Binary Energy Efficiency",
            "text": "Quantized inference at a bit-width of 8 bits has been shown to offer equivalent statistical accuracy while allowing for significant savings in computation energy as well as reducing the requirements for working memory space, memory bandwidth, and storage by a factor of 4 compared to traditional 32-bit data formats [5], [6], [7], [8]. \n\nPushing along the reduced bit-width direction, recently several methods to train neural networks with binary and ternary weights and activations have been proposed [9], [10], [11], [12], [13], [14], allowing for an even more significant decrease in the amount of memory required to run inference. In the context of neural networks, binary values refer to the set {-1, +1} and ternary values refer to the set {-1, 0, 1} [9], [15]. These methods have also been used to convert complex state-of-the-art models to their Binary Neural Network (BNN) or Ternary Neural Network (TNN) form. While this extreme quantization incurs sizeable losses in accuracy compared to the full-precision baselines, such networks have been shown to work well enough for many applications and the accuracy gap has been reducing quite rapidly over time [16], [17], [18]. \n\nAlthough quantization of networks does not affect the total number of operations for inference, it reduces the complexity of the required multipliers and adders, which leads to much lower energy consumption per operation. For binary networks, a multiplier can be implemented by a single XNOR-gate [19]. Further, the number of bit accesses per loaded value is minimized, which not only reduces the memory footprint but also the required wiring and memory access energy. \n\nWhile Binary Neural Networks (BNNs) in particular are fairly well-suited to run on modern general-purpose computing platforms, to take full advantage of the potential energy savings enabled by aggressively quantized, specialized, digital, low-power hardware accelerators have been developed [20], [19], [21], [22].",
            "score": 0.6061416938889453,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 1921,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 335
                },
                {
                    "start": 338,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1652
                },
                {
                    "start": 1655,
                    "end": 1969
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 319,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 321,
                    "end": 324,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 331,
                    "end": 334,
                    "matchedPaperCorpusId": "201698364"
                },
                {
                    "start": 502,
                    "end": 505,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 507,
                    "end": 511,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 513,
                    "end": 517,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 519,
                    "end": 523,
                    "matchedPaperCorpusId": "13556195"
                },
                {
                    "start": 525,
                    "end": 529,
                    "matchedPaperCorpusId": "224893"
                },
                {
                    "start": 531,
                    "end": 535,
                    "matchedPaperCorpusId": "10533533"
                },
                {
                    "start": 757,
                    "end": 760,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 762,
                    "end": 766,
                    "matchedPaperCorpusId": "2951619"
                },
                {
                    "start": 1164,
                    "end": 1168,
                    "matchedPaperCorpusId": "213346145"
                },
                {
                    "start": 1170,
                    "end": 1174,
                    "matchedPaperCorpusId": "3455952"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89111328125
        },
        {
            "corpus_id": "174801144",
            "title": "Fighting Quantization Bias With Bias",
            "text": "In the last years, an increasing amount of effort is invested into executing DNN inference in low-precision arithmetic. While very effective in cutting down on memory, compute and power usage, this comes at a price of degraded network performance caused by weight-and activationrounding errors. Quantization, the conversion of a net to its low-precision version, is performed according to a \"scheme\", a conceptual model of the hardware execution * Equal contribution environment. In this paper we follow the 8-bit integer quantization scheme used in [12] which is widely employed across both cloud-and edge-devices due to its easy and efficient implementation on hardware. For most network architectures, 8-bit quantization carries a minimal degradation penalty [7] leading recent research to double-down on more aggressive schemes, using 4 bits or less for activations, weights or both [14,1,4,13]. Nonetheless, some architectures, such as Mobilenet [10,20], Inception-V1 [23] and Densenet [11] still exhibit significant degradation when quantized to 8 bits. Mobilenets are of particular interest as they were designed specifically for embedded applications and as such are often deployed in their quantized form. In order to successfully deploy these networks a quantizationaware training phase [22,7,12] is usually introduced to regain the network accuracy. This phase is costly in time and requires access to the original dataset on which the network was trained. Moreover, while effective, the training offers little insight into why there was significant degradation in the first place. Recently, there has been more focus on methods that do not rely on an additional training phase [15,1,27,16] but an exhaustive 8-bit quantization scheme that excludes training remains an open challenge. \n\nIn this paper, we begin by showing that some networks exhibit a significant shift in the mean activation value following quantization, which we henceforth denote as MAS (mean activation shift). We note that in order for such a shift to have an impact on the performance of the network it must be of significant magnitude with respect to the mean activation itself.",
            "score": 0.6060528059043067,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1795
                },
                {
                    "start": 1798,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2162
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9169921875
        },
        {
            "corpus_id": "199441962",
            "title": "U-Net Fixed-Point Quantization for Medical Image Segmentation",
            "text": "For the EM dataset, using a weighted sum of cross entropy and dice loss produced the best results. \n\nFigure 1 along with Table 1 show different quantization methods on the aforementioned datasets. Considering NIH dataset, Figure 1(top) and Table 1 show that despite using only 1 and 2 bits to represent network parameters, Binary and TernaryNet quantizations produce results that are close to the full precision model. However, for other datasets, our fixed point Q a 6.0, Q w 0.4 quantization surpasses Binary and TernaryNet Table 1: Dice scores of the quantized U-Net models on EM (left) GM (middle) and NIH (right) datasets. The last two row shows results for Binary and TernaryNet quantizations. Other rows report results obtained for different weight and activation quantization precisions. For GM and EM datasets, we also report results when Tanh is used instead of ReLU as the activation function. quantization. The other important factor here is how efficient these quantization techniques can be implemented using the current CPU and GPU hardware. At the time of writing this paper, there is no commercially available CPU or GPU that can efficiently store and load sub-8bit parameters of a neural network, which leaves us to use custom functions to do bit manipulation to make sub-8bit quantization more efficient. Moreover, in the case of TernaryNet, to apply floating point scaling factor after ternary convolution, floating point operations are required. Our fixed point quantization uses only integer operations, which requires less hardware footprint and use less power compared to floating point operations. Finally, TernaryNet uses Tanh instead of ReLU for the activations. Using hyperbolic tangent as an activation function increases training time [24] and execution time at inference. To verify it, we evaluated the performance of ReLU and Tanh in a simple neural network with 3 fully connected layers. We used the Intel's OpenVino [25] inference engine together with high performance gemm_blas and avx2 instructions. Table 2 illustrates that using ReLU instead of Tanh at training and inference can increase performance by up to 8 times.",
            "score": 0.605617707731692,
            "section_title": "Results and Discussion",
            "char_start_offset": 13924,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 101,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 1765,
                    "end": 1769,
                    "matchedPaperCorpusId": "195908774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7080078125
        },
        {
            "corpus_id": "246455082",
            "title": "FPGA Implementation of 3-Bit Quantized Multi-Task CNN for Contour Detection and Disparity Estimation",
            "text": "Many works have already been done on low bit-width quantized CNNs and the dedicated hardware for the quantization network, and the results were summarized in survey papers [14], [15]. The inference accuracy of 8-bit quantization is almost the same as that of single-precision floatingpoint numbers, and it is known that the accuracy does not drop so much even if the bits are lower than 8-bit. Research on 1-bit quantization has been also in progress [16], [17]. Many of these studies target the category classification. \n\nAlthough it is smaller in number than category classification, low-bit quantization of semantic segmentation networks has also been studied [29]. There are some FPGAs for semantic segmentation [30], [31]. Xilinx provides xdnn, an 8-bit inference CNN circuit [32]. It has also announced a 4-bit quantized circuit for inference, which is used to realize semantic segmentation [33]. We have already implemented a 3-bit quantized CNN for semantic segmentation on an FPGA [34]. \n\nYOLO, a CNN for object detection, outputs a bounding box along with the class of the object and infers the size and aspect ratio of the bounding box as a regression problem [2]. TinyYOLO, a small version of YOLO, has been quantized with 1-bit weighs and 4-bit activations for all layers except the first and last layers, and implemented on an FPGA using FINN [35]. Here, FINN is a framework for implementing quantized DNN on FPGAs [36]. No report was found that quantized the CNNs for motion or disparity estimation, which was the same regression problem as bounding box estimation by YOLO.",
            "score": 0.6055272764802606,
            "section_title": "DNN Quantization and Hardware Implementation",
            "char_start_offset": 12461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 520
                },
                {
                    "start": 523,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 995
                },
                {
                    "start": 998,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1588
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 182,
                    "matchedPaperCorpusId": "197446325"
                },
                {
                    "start": 457,
                    "end": 461,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 716,
                    "end": 720,
                    "matchedPaperCorpusId": "67869917"
                },
                {
                    "start": 722,
                    "end": 726,
                    "matchedPaperCorpusId": "91185703"
                },
                {
                    "start": 990,
                    "end": 994,
                    "matchedPaperCorpusId": "234214414"
                },
                {
                    "start": 1171,
                    "end": 1174,
                    "matchedPaperCorpusId": "206594738"
                },
                {
                    "start": 1429,
                    "end": 1433,
                    "matchedPaperCorpusId": "219886219"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.802734375
        },
        {
            "corpus_id": "276116485",
            "title": "Exploring Concept Depth: How Large Language Models Acquire Knowledge and Concept at Different Layers?",
            "text": "The production values are of the highest and the performances attractive without being memorable. \n\nThe sentence above is a movie review and reflects the writer's overall intention for this review. According to the sentence, judge whether the emotion is Positive or Negative. \n\nAfter adding noise: aaa The production values are of the highest and the performances attractive without being memorable. The sentence above is a movie review and reflects the writer's overall intention for this review. According to the sentence, judge whether the emotion is Positive or Negative. \n\naaa can be substituted by bbb, both cases have equal probability. \n\nQuantization Settings. Quantization has been a rule of thumb for faster LLMs' inference. The general fact is that using a lower precision level allows LLMs to be run on less capable hardware with an acceptable reduction in their ability and accuracy. Our approach to applying quantization is to explore its effect on Concept Depth, whether it still maintains the previous Concept Depth or slows down the process of understanding knowledge. We quantize the model weights to 8, 16, and 32 bits. \n\nResults. converge to an optimal solution. We also see that there isn't much difference between 32 and 16 bits, and the convergence rate slows when we reduce the model to 8 bits. Therefore, we may compress the LLMs to 16 bits for future designs. \n\nRemark 4 \n\nNoises or 8-bit-quantization can cause the accuracy to converge more slowly. Compressing the LLMs to 16 bits doesn't harm the understanding process too much. The layer-wise representations of LLMs are susceptible to noise and high-ratio quantization. Therefore, it is crucial to proceed cautiously when conducting high-ratio quantization inference.",
            "score": 0.6055088505071275,
            "section_title": "Before adding noise:",
            "char_start_offset": 25243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 100,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 275
                },
                {
                    "start": 278,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 643
                },
                {
                    "start": 646,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1138
                },
                {
                    "start": 1141,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1385
                },
                {
                    "start": 1388,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1747
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45556640625
        },
        {
            "corpus_id": "276903421",
            "title": "Emergent Abilities in Large Language Models: A Survey",
            "text": "Liu et al. [53] investigate a critical but often overlooked factor affecting the emergent abilities of LLMs: i.e., quantization. As LLMs grow in size, their memory and computational requirements become increasingly demanding, prompting the need for quantization techniques that reduce precision and optimize efficiency. However, a key question remains: Does quantization compromise the emergent abilities that make these models so powerful? Their study systematically examines this trade-off, particularly in the context of in-context learning, chain-of-thought reasoning, and instruction following -three hallmarks of advanced LLM capabilities. \n\nHow Quantization Affects Performance. Quantization is a widely used technique that compresses neural networks by reducing the number of bits used to represent each parameter. While this drastically lowers memory usage and inference costs, it often comes at the expense of model performance. Liu et al. [53] conduct extensive empirical evaluations on a range of LLaMA models, spanning 7B to 65B parameters, across multiple quantization levels, including 2-bit, 4-bit, 8-bit, and 16-bit precision. Their findings offer a nuanced understanding of how quantization impacts emergent abilities. At higher bit levels, such as 8-bit and 16-bit, LLMs retain much of their original performance, with minimal degradation in reasoning and instruction-following tasks. However, as precision decreases, particularly at 4-bit and below, a clear pattern emerges: while 4-bit quantization manages to preserve most emergent abilities, 2-bit quantization severely degrades performance, often reducing accuracy to near-random levels. This suggests that there exists a critical threshold beyond which the model struggles to maintain the structured reasoning and learning dynamics that enable emergent behaviors. \n\nA particularly insightful aspect of the study is its componentwise analysis, which reveals that not all parts of the model are equally affected by quantization. The feed-forward networks within transformer blocks are found to be especially crucial for retaining performance. When these layers undergo aggressive quantization, the model experiences significant losses in reasoning ability and generalization. This highlights the fact that while weight compression is beneficial, indiscriminate quantization of all components can be detrimental. Can Emergent Abilities Be Preserved?",
            "score": 0.605274684466678,
            "section_title": "D. The Impact of Quantization on Emergent Abilities",
            "char_start_offset": 31198,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1838
                },
                {
                    "start": 1841,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2248
                },
                {
                    "start": 2249,
                    "end": 2384
                },
                {
                    "start": 2385,
                    "end": 2421
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96875
        },
        {
            "corpus_id": "274131005",
            "title": "Towards Accurate and Efficient Sub-8-Bit Integer Training",
            "text": "Neural network training is a memory- and compute-intensive task. Quantization, which enables low-bitwidth formats in training, can significantly mitigate the workload. To reduce quantization error, recent methods have developed new data formats and additional pre-processing operations on quantizers. However, it remains quite challenging to achieve high accuracy and efficiency simultaneously. In this paper, we explore sub-8-bit integer training from its essence of gradient descent optimization. Our integer training framework includes two components: ShiftQuant to realize accurate gradient estimation, and L1 normalization to smoothen the loss landscape. ShiftQuant attains performance that approaches the theoretical upper bound of group quantization. Furthermore, it liberates group quantization from inefficient memory rearrangement. The L1 normalization facilitates the implementation of fully quantized normalization layers with impressive convergence accuracy. Our method frees sub-8-bit integer training from pre-processing and supports general devices. This framework achieves negligible accuracy loss across various neural networks and tasks ($0.92\\%$ on 4-bit ResNets, $0.61\\%$ on 6-bit Transformers). The prototypical implementation of ShiftQuant achieves more than $1.85\\times/15.3\\%$ performance improvement on CPU/GPU compared to its FP16 counterparts, and $33.9\\%$ resource consumption reduction on FPGA than the FP16 counterparts. The proposed fully-quantized L1 normalization layers achieve more than $35.54\\%$ improvement in throughout on CPU compared to traditional L2 normalization layers. Moreover, theoretical analysis verifies the advancement of our method.",
            "score": 0.6050470800143668,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89404296875
        },
        {
            "corpus_id": "272368092",
            "title": "Foundations of Large Language Model Compression - Part 1: Weight Quantization",
            "text": "Early work on neural network model quantization can be attributed to Vanhoucke et al. (2011), who demonstrated that 8-bit integer arithmetic can be used for network training and inference without incurring a significant loss of accuracy. More generally, quantization-aware training (QAT) (Zhou et al., 2017;Jacob et al., 2018;D. Zhang et al., 2018;Esser et al., 2019;Y. Choi et al., 2017;Wang et al., 2019) integrates the quantization process into training by allowing the model to adapt to the reduced precision in weights (Esser et al., 2019;Jacob et al., 2018;D. Zhang et al., 2018;Zhou et al., 2017) and activations (Y. Choi et al., 2017;Wang et al., 2019) by determining the optimum bit depth (Wang et al., 2019;D. Zhang et al., 2018) and step size (Esser et al., 2019) using back-prop to facilitate the gradient to flow through quantization operators. One shortcoming of QAT methods is that model training needs to be repeated for different quantized model sizes and accuracy, which makes them less suitable for quantizing larger models that require substantial computation and time for training. \n\nMore recent quantization techniques for language and vision models aim to facilitate compression of already trained models for rapid deployment without further training (Dong et al., 2019;Chen et al., 2021;Dettmers et al., 2022;Yao et al., 2022;Frantar et al., 2022;Dettmers et al., 2023;Xiao et al., 2023;Lin et al., 2024;Kim et al., 2024;Shao et al., 2024;Lee et al., 2024). Vese approaches quantize model weights to 3-4 or 8 bits for integer-arithmetic-only inference (Jacob et al., 2018) using mixed bit depth quantization (Wang et al., 2019;Chen et al., 2021) or by a separate handling of outlier channels (Zhao et al., 2019) to improve the accuracy of the quantized model.",
            "score": 0.6046390576140943,
            "section_title": "PREVIOUS WORK",
            "char_start_offset": 3881,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1102
                },
                {
                    "start": 1105,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1783
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 92,
                    "matchedPaperCorpusId": "15196840"
                },
                {
                    "start": 307,
                    "end": 326,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 348,
                    "end": 367,
                    "matchedPaperCorpusId": "67788003"
                },
                {
                    "start": 388,
                    "end": 406,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 524,
                    "end": 544,
                    "matchedPaperCorpusId": "67788003"
                },
                {
                    "start": 544,
                    "end": 563,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 642,
                    "end": 660,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 698,
                    "end": 717,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 754,
                    "end": 774,
                    "matchedPaperCorpusId": "67788003"
                },
                {
                    "start": 1274,
                    "end": 1293,
                    "matchedPaperCorpusId": "148571720"
                },
                {
                    "start": 1293,
                    "end": 1311,
                    "matchedPaperCorpusId": "238744390"
                },
                {
                    "start": 1311,
                    "end": 1333,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 1333,
                    "end": 1350,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 1393,
                    "end": 1411,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 1411,
                    "end": 1428,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 1428,
                    "end": 1445,
                    "matchedPaperCorpusId": "267095435"
                },
                {
                    "start": 1445,
                    "end": 1463,
                    "matchedPaperCorpusId": "261214575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79443359375
        },
        {
            "corpus_id": "266364140",
            "title": "Smart-DNN+: A Memory-efficient Neural Networks Compression Framework for the Model Inference",
            "text": "Li et al. [36] proposed to simplify the model structure and reduce the number of parameters by pruning the unimportant filter of convolutional layers. As pruning reduces the capacity of the model, it can potentially degrade the model quality. To maintain the model accuracy, extra training overheads are incurred to fine-tune the pruned model (i.e., iterative pruning). \n\nQuantization is an effective solution for reducing the size of neural network parameters by lowering the bit-width of model parameters [4,6,17,62,63]. The typical 8-bit fixed-point technology is widely studied for X86 CPU [56,64]. The linear fixed-point method is used to speed up the network running without significantly degrading the testing accuracy. To achieve the higher compression ratio, PWLQ [16] and ShapShft [34] quantize layers with different bit-widths [16,34]. A non-linear quantization scheme of network parameters is proposed to use scaling factors and zeros to determine the quantization value corresponding to the actual value [28]. Due to quantization generating more redundancy for the network, DeepSZ [29] and Smart-DNN [24] further employ SZ lossy compressor and lossless coding scheme to reduce the size of quantized integer weights. In addition, the binary neural networks, such as XNOR [49], BinaryNet [10], and BinaryConnect [11] directly represent the parameters of the network as 1-bit, greatly reducing the network size during the neural network training stage. However, the main limitations of quantization are the accuracy degradation caused by the loss of parameter precision. Additionally, the acceleration effect usually requires the support of related hardware. \n\nKnowledge distillation is proposed to compress deep and wide networks into smaller ones, where the compressed model (i.e., student) mimicked the function learned by the complex model (i.e., teacher) [2,21]. The main idea of knowledge distillation-based approaches is to shift knowledge from a large teacher model into a small student model by learning the class distribution output via SoftMax.",
            "score": 0.6044887509511045,
            "section_title": "BACKGROUND AND RELATED WORK",
            "char_start_offset": 8736,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1668
                },
                {
                    "start": 1671,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 2065
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 14,
                    "matchedPaperCorpusId": "14089312"
                },
                {
                    "start": 507,
                    "end": 510,
                    "matchedPaperCorpusId": "52947747"
                },
                {
                    "start": 515,
                    "end": 518,
                    "matchedPaperCorpusId": "3603886"
                },
                {
                    "start": 518,
                    "end": 521,
                    "matchedPaperCorpusId": "218571099"
                },
                {
                    "start": 598,
                    "end": 601,
                    "matchedPaperCorpusId": "247627656"
                },
                {
                    "start": 773,
                    "end": 777,
                    "matchedPaperCorpusId": "213004198"
                },
                {
                    "start": 791,
                    "end": 795,
                    "matchedPaperCorpusId": "204716405"
                },
                {
                    "start": 838,
                    "end": 842,
                    "matchedPaperCorpusId": "213004198"
                },
                {
                    "start": 842,
                    "end": 845,
                    "matchedPaperCorpusId": "204716405"
                },
                {
                    "start": 1017,
                    "end": 1021,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1094,
                    "end": 1098,
                    "matchedPaperCorpusId": "59316609"
                },
                {
                    "start": 1113,
                    "end": 1117,
                    "matchedPaperCorpusId": "245388714"
                },
                {
                    "start": 1283,
                    "end": 1287,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 1323,
                    "end": 1327,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 1870,
                    "end": 1873,
                    "matchedPaperCorpusId": "84176918"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6171875
        },
        {
            "corpus_id": "221103917",
            "title": "Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance",
            "text": "see that 4-bit quantization, which has quantization SNR (signal to noise ratio) 6.02 \u00d7 4 \u2248 24 dB, led to output jitter of 0.957% which means SNR 20 log 10 (1/.00957) \u2248 40 dB. A 40 dB SNR means that the neuron was extremely robust to weight quantization.\n\nDeep SNN Robustness: To evaluate the robustness of deep SNNs, we experimented with both weight quantization and noise perturbation. For weight quantization, the weights were quantized to 32-bit, 8-bit, 4-bit, and 2-bit words. Thanks to our easy training models, we could retrain the deep SNNs simply following the procedure developed for conventional CNN quantization (Li, Zhang, and Liu 2016;Rastegari et al. 2016). Specifically, the forward inference used quantized weights while the backward gradient propagation used full-precision weights. We first trained with 32-bit quantization. After the training converged, we applied 8-bit quantization and retrain the SNNs. This procedure was repeated until the 2-bit quantization. Table 4 shows the testing accuracy under weight quantization. For the models over MNIST, CIFAR10 and Ima-geNet, weight quantization caused the worst accuracy loss of 0.22%, 1.75%, and 8.8%, respectively. As comparison, we listed two typical CNN weight quantization results (Cheng et al. 2018;Zhang et al. 2018), which indicated a similar performance degradation pace. The results demonstrated that the SNN models were robust to weight quantization for relatively small datasets and small networks. For larger networks, the weights would better be encoded in 4-bit or over.\n\nTo evaluate noise perturbation, we added random noise to the trained weights. Experiment results are shown in Fig. 3, together with the results of weight quantization in Table 4 (expressed in SNR). We find that 24dB quantization noise (4-bit quantization) reduced ImageNet classification accuracy to 65.2%. Noise at 24dB SNR reduced ImageNet clas-   sification accuracy to 65.43",
            "score": 0.6041269037276213,
            "section_title": "Robustness",
            "char_start_offset": 27448,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 648,
                    "end": 670,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 1275,
                    "end": 1293,
                    "matchedPaperCorpusId": "50784025"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96435546875
        },
        {
            "corpus_id": "244677268",
            "title": "A Survey on the Optimization of Neural Network Accelerators for Micro-AI On-Device Inference",
            "text": "The perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has been investigated in many DNN models in computer vision, natural language processing, and related areas. Even though binary and ternary networks reduce the model size significantly, there is also considerable accuracy loss due to the compression. In practice, intermediate formats such as 16-bit and 8-bit full precision to mixed-precision formats are comprehensively used. Table . II denotes the popular networks that utilize different quantization and sparsification strategies for image classification. For reference, the ResNet-18 network loses around 14% accuracy when it goes from ternary to binary networks as shown in Table . II. On the other hand, VGG-16 with 16-bit fixed point precision can reach near state-of-the-art accuracy on ImageNet albeit with 16\u00d7 the model size compared to binary networks. Moving from floating-point representations to low-precision fixed integer values represented in four bits or less holds the potential of reducing the memory footprint and latency by a factor of 16\u00d7 or more, and reductions of 4\u00d7 to 8\u00d7 are readily realized in practice in these applications. In this section, we survey approaches to the problem of quantizing the numerical values in DNN computations, covering the advantages/disadvantages of current methods. \n\n1) Low-Bit Width Networks: Micro-AI inference devices have extremely low memory space for storing the weights and data for DNN models. That is why reducing the data bit-width to the extreme extent is a very popular and demanding task during DNN inference. The most severe quantization method is binarization, which reduces the memory requirement by 32\u00d7 through constraining the quantized values to a 1-bit representation. A uniform binarization strategy, on the other hand, would result in severe accuracy loss. As a result, a substantial body of literature has presented several solutions to this problem. BinaryConnect [53], which constrains the weights to either +1 or \u22121, is an important study in this area. To replicate the binarization effect, the weights are preserved as actual values and binarized only during the forward and backward passes in this method.",
            "score": 0.6036156426224463,
            "section_title": "C. Quantization",
            "char_start_offset": 28306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2127
                },
                {
                    "start": 2128,
                    "end": 2282
                }
            ],
            "ref_mentions": [
                {
                    "start": 2037,
                    "end": 2041,
                    "matchedPaperCorpusId": "1518846"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "207870482",
            "title": "Adaptive Precision Training: Quantify Back Propagation in Neural Networks with Fixed-point Numbers",
            "text": "Considering a widely used ResNet50 model, with 4 candidate quantization bit-widths (e.g., 8, 16, 24, 32 for weights, activations and activation gradients), the size of quantization precision combination search space for 450,000 training iterations can achieve 4 3 * 50 * 450,000 . \n\nTo avoid prohibitively long space searching of quantization bit-width combinations, we propose an efficient and adaptive technique to determine the bit-width layer by layer separately, which is based on our observation about the relationship between the layer-wise bit-width and the training convergence. Take AlexNet as an example, Figure . 1(a-c) depict the distributions of activation gradients on AlexNet last layer when quantified with different bit-widths. Compared with the original float32, int8 introduces a significant change in data distribution, int12 introduces slightly change of data mean, and int16 shows almost the same distribution with float32. Figure. 1(d) depicts the corresponding training loss, which shows int8 quantization does not converge at beginning, int12 convergences slower than float32 and int16 behaves similar as float32. The above experimental results suggest if a quantization resolution does not change the data distribution of a layer (e.g., int16 for the last layer of AlexNet), quantified training with this resolution for the corresponding layer will almost keep the training accuracy. \n\nBased on the above observation, one can train largescale deep neural network using fixed-point numbers, with no change of hyper parameters and no accuracy degradation. For each layer in training, our approach automatically finds the best quantization resolution (i.e., the smallest bit-width which does not significantly change the data mean) for weights, activations and activation gradients respectively. Concretely, we first calculate the mean of the data before quantization. Then, we quantify the data using int8 and calculate the quantization error. If the ratio of quantization error exceeds a threshold (e.g., 3%), the quantization bit-width is increased by 8. The above process is looped until the quantization error ratio is below the threshold.",
            "score": 0.6031448184756258,
            "section_title": "Introduction",
            "char_start_offset": 1749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 280
                },
                {
                    "start": 283,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1410
                },
                {
                    "start": 1413,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2168
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87158203125
        },
        {
            "corpus_id": "247331384",
            "title": "FxP-QNet: A Post-Training Quantizer for the Design of Mixed Low-Precision DNNs with Dynamic Fixed-Point Representation",
            "text": "Thus, network compression and acceleration are important issues in deep learning research and application. \n\nA promising approach to overcome the aforementioned challenges is to reduce the bit-precision level of DNN datastructures by quantization [18]. With the network topology unchanged, quantization has the potential to reduce the model size to only a fraction of the original by utilizing low-precision representation of parameters. An example of quantizing the parameters is to represent them with a lightweight floating-point format that uses a configurable number of bits for the exponent and mantissa segments [19]. Furthermore, the weights and activations could also be quantized to discrete values in the fixed-point representation. Then, the network inference is significantly accelerated by increasing the arithmetic intensity, that is, the ratio of operations performed to each memory byte accessed. Additionally, converting expensive floating-point arithmetic to more effective fixed-point and bit-wise operations gives an additional boost by lowering the inference latency [20]. \n\nBesides its benefits on memory and computation, quantization also offers large power savings [21]- [24]. Reducing the wordlength of neurons also benefits DNN hardware accelerators such as application specific integrated circuits (ASICs) and field programmable gate arrays (FPGAs) [25], because it means significantly lessening the resources required for calculations such as area and power requirements for multipliers. Table 1 demonstrates the improvement in power consumption and area requirement when using 8-bit fixed-point multiply-accumulate units compared to their fixed/floating-point counterparts in 45nm technology (0.9V) [21]. The improvement in computation, power, and hardware efficiency through network quantization makes the deployment of DNNs on dedicated embedded systems feasible now-a-days. Thus, quantization has received considerable attention and has become an active research area [26]. \n\nHowever, an extremely low-precision representation of data-structures leads to a severe information loss, referred to as a quantization error. The substantial quantization error, in turn, leads to a significant degradation in accuracy.",
            "score": 0.6029410336247707,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 1820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 109,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1094
                },
                {
                    "start": 1097,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2006
                },
                {
                    "start": 2009,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 619,
                    "end": 623,
                    "matchedPaperCorpusId": "244169650"
                },
                {
                    "start": 1089,
                    "end": 1093,
                    "matchedPaperCorpusId": "219891210"
                },
                {
                    "start": 1190,
                    "end": 1194,
                    "matchedPaperCorpusId": "22028726"
                },
                {
                    "start": 1196,
                    "end": 1200,
                    "matchedPaperCorpusId": "3273340"
                },
                {
                    "start": 1377,
                    "end": 1381,
                    "matchedPaperCorpusId": "57373837"
                },
                {
                    "start": 1729,
                    "end": 1733,
                    "matchedPaperCorpusId": "22028726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77197265625
        },
        {
            "corpus_id": "250144268",
            "title": "Sub-8-Bit Quantization Aware Training for 8-Bit Neural Network Accelerator with On-Device Speech Recognition",
            "text": "Latency reduction without introducing noticeable accuracy degradation is critical to on-device automatic speech recognition (ASR) systems in various scenarios, such as in in-car units and on portable devices, where Internet connectivity can be intermittent. Ubiquitous as end-to-end ASR models are [1,2,3,4], deploying them on edge can be challenging due to the strict limit of bandwidth and memory of edge devices [5,6]. Hence, it is highly necessary to improve model efficiency, i.e. model memory size vs. model accuracy, through applying model optimization and compression techniques when running real-time ASR on-device. \n\nEfficient neural network inference can be approached by various ways. The teacher-student training paradigm for knowledge distillation simplifies the network topology, although the student may fail to mimic the teacher's behavior if the model capacity is too low [7]. Furthermore, it usually requires additional network compression techniques prior to the hardware deployment [8]. Alternatively, enforcing model sparsity can significantly reduce memory footprint [9,10]. However, one can only realize the inference speedup if the sparsity pattern matches the specific memory design of the hardware [11]. \n\nNeural network quantization can be effectively employed to compress 32-bit weights down to 8-bit, or activations to 5-bit [12,13], via applying a simple post-training quantization step [14,15] or a more involved QAT mechanism [16,17]. Several quantization methods have been proposed to lower the bit-depth to 4 [18,19,20], or even 1 for proof-of-concept bit-wise neural networks [21]. Nevertheless, these methods can be heuristic which rely on an extensive hyper-parameter tuning to maintain the accuracy level. Furthermore, such sub-8-bit quantization methods require sub-8-bit operators on neural network accelerators (NNAs), which often have inferior performance compared to their 8-bit counterpart due to the reduced numerical accuracy. Consequently, sub-8-bit NNAs are less adopted and thus there is no real latency measurement for existing sub-8bit approaches [16,18,20,21].",
            "score": 0.6026663164121299,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 624
                },
                {
                    "start": 627,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1230
                },
                {
                    "start": 1233,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 298,
                    "end": 301,
                    "matchedPaperCorpusId": "225094349"
                },
                {
                    "start": 303,
                    "end": 305,
                    "matchedPaperCorpusId": "231846963"
                },
                {
                    "start": 418,
                    "end": 420,
                    "matchedPaperCorpusId": "235659012"
                },
                {
                    "start": 890,
                    "end": 893,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1093,
                    "end": 1096,
                    "matchedPaperCorpusId": "231855472"
                },
                {
                    "start": 1355,
                    "end": 1359,
                    "matchedPaperCorpusId": "195069135"
                },
                {
                    "start": 1359,
                    "end": 1362,
                    "matchedPaperCorpusId": "221376937"
                },
                {
                    "start": 1418,
                    "end": 1422,
                    "matchedPaperCorpusId": "221506623"
                },
                {
                    "start": 1422,
                    "end": 1425,
                    "matchedPaperCorpusId": "235166247"
                },
                {
                    "start": 1544,
                    "end": 1548,
                    "matchedPaperCorpusId": "204509218"
                },
                {
                    "start": 1612,
                    "end": 1616,
                    "matchedPaperCorpusId": "145912428"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87890625
        },
        {
            "corpus_id": "246294849",
            "title": "Bit-serial Weight Pools: Compression and Arbitrary Precision Execution of Neural Networks on Resource Constrained Processors",
            "text": "There are a few software-focused works that develop algorithms to deploy sub-byte neural networks on CPUs. (Yu et al., 2019) utilizes a single multiplication instruction to implement multiple sub-byte multiplications through bitpacking, and is able to show performance improvement for four-bit input and ternary weight network over 16-bit baselines. (Cowan et al., 2018) proposes a software method and corresponding optimizations for CPUs to compute sub-byte precision more efficiently. However, as their method has a time complexity proportional to total number of weight bits times total number of activation bits, moderate speedup over 8-bit baseline can only be demonstrated on very low activation and weight bitwidth (2-3 bits). Current software methods for accelerating sub-byte neural networks have limited use cases due to their strict requirements on activation and weight bitwidth. For many applications, quantizing both activation and weight to 2-3 bits can severely impact the learning capability of neural networks. Besides, some versions require advanced instructions that are not available for low-power microcontrollers like ARM Cortex M0 and M3. We do not directly compare against these works as the target applications and platforms are not same and they do not offer arbitrary sub-byte precision acceleration.",
            "score": 0.6026365311603297,
            "section_title": "Software Based Convolution Acceleration for Sub-byte Precision",
            "char_start_offset": 37007,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1328
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6953125
        },
        {
            "corpus_id": "244677268",
            "title": "A Survey on the Optimization of Neural Network Accelerators for Micro-AI On-Device Inference",
            "text": "Replacing the floating point representation with low-bit and fixed-bit data reduces bandwidth utilization and memory storage space while also simplifying calculation and lowering the cost of each operation, albeit at the expense of accuracy. Nowadays, most of the general purpose processor architectures have adopted INT8 quantization. It has become a pre-requisite for the micro-AI devices to adopt at least INT8 bit precision, and further lower precision as the processing unit allows. \n\nFPGAs are more popular for supporting quantized networks with less than 8 bits. BNN and TNN have 1-bit and 2-bit activations and weights which enables them to be computed using bit-wise arithmetic. These networks are well suited for FPGAs, and their memory requirements are also greatly decreased. However, BNN and TNN produce much lower accuracy on realistic datasets (i.e. ImageNet). For example, Xilinx Zynq heterogeneous FPGA platform supports BNN [106] and FINN [107] inference framework deployed BNN on a Zynq ZC706 SOC FPGA acquired 12.36M image classification per seconds allowing around 4% accuracy degradation. Reference [108] introduces fractional activations and binarizes the input layer with thermometer encoding which improves accuracy of BNNs for ImageNet. \n\nTNN [109] uses optimized ternary value multiplication while deployed on Sakura-X FPGA and achieved 255k image classification per second with 98% average accuracy. Both [106] and [109] experimented on MNIST dataset. Reference [8] introduces a ternary hardware accelerator along with TNN training framework for efficient processing of ImageNet scale datasets. Table IV shows recent FPGA works that implemented low-precision DNNs. All of them are using 2-bits or even 1-bit data precision while still achieving comparable state-of-the-art performance. Another representative low precision work is [110] where the authors demonstrate the efficacy of low bit width design in the form of a 4-core AI chip in 7nm EUV (Extreme Ultraviolet Lithography) technology to facilitate both training and inference for a variety of fixed point and floating point precisions.",
            "score": 0.6025659596808598,
            "section_title": "B. Digital Accelerators",
            "char_start_offset": 57838,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1262
                },
                {
                    "start": 1265,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 942,
                    "end": 947,
                    "matchedPaperCorpusId": "17645936"
                },
                {
                    "start": 957,
                    "end": 962,
                    "matchedPaperCorpusId": "10530917"
                },
                {
                    "start": 1121,
                    "end": 1126,
                    "matchedPaperCorpusId": "229348728"
                },
                {
                    "start": 1269,
                    "end": 1274,
                    "matchedPaperCorpusId": "2951619"
                },
                {
                    "start": 1433,
                    "end": 1438,
                    "matchedPaperCorpusId": "17645936"
                },
                {
                    "start": 1443,
                    "end": 1448,
                    "matchedPaperCorpusId": "2951619"
                },
                {
                    "start": 1490,
                    "end": 1493,
                    "matchedPaperCorpusId": "202699949"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75
        },
        {
            "corpus_id": "220466828",
            "title": "ALigN: A Highly Accurate Adaptive Layerwise Log_2_Lead Quantization of Pre-Trained Neural Networks",
            "text": "Average decimal accuracy of the weights of Conv1_2 layer of VGG-16 [20] network using different quantization schemes. eqnarray -x_y x bits reserved for storing the leading 1 location and y bits allocated to store the following values. \n\nOur proposed quantization schemes efficiently utilize the available bit-width to comprehensively incorporate the dynamic range (\u22121 to +1) of trained parameters. To show the efficacy of our proposed quantization scheme for the 2 ALigN-14_1 and ALigN-6_1 schemes have overlapping lines. parameters of pre-trained DNNs, we compare its decimal accuracy in the range of \u22121 to +1 with bfloat16 and minifloat. These results are presented in Fig. 14. The decimal accuracy [21], defined in Eqn. 12, represents the capability of a number system to represent Float32-based numbers. As shown by the results in Fig. 14, the ALigN-1_14 scheme (1 bit reserved for storing the leading one location and 14 bits allocated to store the following bits) provides the highest decimal accuracy. The ALigN-7_8 scheme provides slightly better performance than the bfloat16. Similarly, for 8-bit quantization, the ALigN-1_6 provides the best results among all other 8-bit variations. Finally, Fig. 15 shows the average decimal accuracy of the pre-trained weights of a single layer, Conv1_2, of VGG-16 network using different quantization schemes. The ALigN-3_12 configuration represents the quantized weights with the highest accuracy. Similarly, for 8-bit quantization, the ALigN-3_4 scheme produces better results than other corresponding 8-bit schemes. These results are in line with the observations of Fig. 10, where the allocation of 3 bits for the storage of the leading 1 location was producing the minimum quantization-induced average error. We have shown the results only for the Conv-1_2 layer; however, similar results can be generated for other layers of the network. Nonetheless, for other layers of the network, a different configuration of the ALigN can offer higher decimal accuracy than the ALigN-3_12 configuration.",
            "score": 0.6025349135767444,
            "section_title": "FIGURE 15.",
            "char_start_offset": 20336,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2045
                }
            ],
            "ref_mentions": [
                {
                    "start": 701,
                    "end": 705,
                    "matchedPaperCorpusId": "39715445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6826171875
        },
        {
            "corpus_id": "52980396",
            "title": "Quantization for Rapid Deployment of Deep Neural Networks",
            "text": "Han quantized the network parameters after pruning for compression in (author?) [7]. Kim proposed on using Hessian-weighted clustering to achieve a better compression ratio in quantization [1]. However, in those works, only the network parameters were quantized to save storage space leaving the feature maps in full-precision. Both the activations and the network parameters were quantized layer-wise to accommodate the large variations in the dynamic range across the layers in (author?) [4,6]. The max values found in activations were used to decide on the fractional lengths, and intensive fine tuning were required to recover accuracies degraded by quantization in some networks. Lin used SQNR instead of the max value to minimize the bit-width for each layer and optimized DNNs for fixed-point operation [15]. Migacz achieved linear quantization for 8-bit integer operation without fine tuning by minimizing the information loss with Kullback-Leibler (KL) divergence [17]. \n\nUnfortunately, collection of the activation histograms were required from a large number of samples. All in all, these methods used the layer-wise quantization scheme. Aggressively lowering the precision to be under 4 bits for both the weights and the activations have been actively explored [3,10,13,14,16,19,28]. Although they revealed impressive results on small benchmarks, there is still a huge gap in accuracy on large benchmarks such as the ImageNet classification using state-ofthe-art networks trained in full precision. Recent progress shows that it is possible to reduce the precision of DNNs to 4 bits without sacrificing accuracy by increasing the network size [18] or training the networks in multiple stages with guided training [29]. These work focus on training DNNs for low-precision inference from scratch rather than quantizing pretrained full-precision networks.",
            "score": 0.6022395685251054,
            "section_title": "Related works",
            "char_start_offset": 17784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1864
                }
            ],
            "ref_mentions": [
                {
                    "start": 810,
                    "end": 814,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 1273,
                    "end": 1276,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 1285,
                    "end": 1288,
                    "matchedPaperCorpusId": "10533533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8935546875
        },
        {
            "corpus_id": "252734746",
            "title": "Inference Latency Prediction at the Edge",
            "text": "Effects of Quantization. On mobile devices with limited power and computing resources, neural architectures can be converted into lower-precision representations (e.g., 16-bit floating point or 8-bit integers) to reduce memory utilization and computational demand, without substantial accuracy loss. We focus on the approach of integer-arithmetic-only inference [31] available in TFLite, where both weights and activations are represented as 8-bit integers during inference. 1 Fig. 4 compares inference latency using an 8-bit integer representation and a floating point representation. \n\nSimilarly to Fig. 2, we omit outliers (only of a couple of points) for better visualization, and report complete data in Fig. 27 of the Appendix. As can be seen, quantization shows a distinct speedup on various combinations of cores on all devices. Fig. 5 depicts performance improvement of each type of operation after quantization. On all devices, most operations achieve significant speedup when using 8-bit integers; however, padding and element-wise operations show performance degradation after quantization. For example, the average latency of element-wise operations is increased to 2.55x and 2.60x on Snapdragon 855 and Exynos 9820 respectively. Previous work [31,41] suggests that this degradation is due to the overhead of matching quantization ranges (i.e., the scale) of all inputs of quantized operations (e.g., element-wise addition). Insight 2. Quantization can reduce latency and memory utilization of a model, significantly improving the performance of inference tasks on mobile CPUs. However, quantization can cause performance degradation for some operations due to the cost of scaling its inputs.",
            "score": 0.6015825944910579,
            "section_title": "Performance Characteristics of Mobile CPUs",
            "char_start_offset": 16958,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1705
                }
            ],
            "ref_mentions": [
                {
                    "start": 362,
                    "end": 366,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1257,
                    "end": 1261,
                    "matchedPaperCorpusId": "39867659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87939453125
        },
        {
            "corpus_id": "271571530",
            "title": "Optical Computing for Deep Neural Network Acceleration: Foundations, Recent Developments, and Emerging Directions",
            "text": "Another approach to realize optical accelerators for power and performance constrained edge and IoT platforms is to utilize quantization in DNNs and perform hardware/software co-design to devise an optical accelerator tuned for the chosen quantization level. In quantization, the bitwidths of DNN parameters (weights) and activations are reduced from the default (typically 32-bit floating point) to a lower value (e.g., 8 bit fixed-point). In [111], an optical accelerator for binary quantized CNNs was proposed. Here binary refers to utilizing a single bit to represent parameters and/or weights. Unfortunately, using such a small bitwidth significantly reduces CNN inference accuracy. Studies have shown that activations are more susceptible to bitwidth reduction than weight parameters. Therefore, an alternative for increasing inference accuracy is to not binarize activations, while keeping the weights binarized. To determine the appropriate activation bitwidth (precision), an analysis was performed in [111], where weights were restricted to binary (1-bit) values, but the bit precision level of the activations was altered from 1-bit to 16-bits, across various CNN models and learning tasks. During CNN model training, weights were binarized during the forward and backward propagations but not during the parameter update step, because keeping high precision weights during the updates is necessary for stochastic gradient descent (SGD) to work efficiently. After training, all weights were in binary format, while the precision of input activations was varied. Figure 9(a) shows the results of varying activation bitwidth across four different CNN models and their datasets. It was observed that the accuracy had notable change initially as activations bits were increased, but this gain in accuracy soon saturated. Based on the results, 1-bit weights were considered along with 4-bit activations, which allowed reducing bitwidths without significantly reducing model accuracy. \n\nThe reduction in bit-width for weights and activations, from a baseline of 32-bit to 1 and 4 bits, respectively, allowed significantly reducing the number of ADCs and DACs in the proposed binary optical accelerator called ROBIN [111].",
            "score": 0.6015380683619859,
            "section_title": "Quantization Approaches",
            "char_start_offset": 64602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1989
                },
                {
                    "start": 1992,
                    "end": 2226
                }
            ],
            "ref_mentions": [
                {
                    "start": 444,
                    "end": 449,
                    "matchedPaperCorpusId": "235795063"
                },
                {
                    "start": 1011,
                    "end": 1016,
                    "matchedPaperCorpusId": "235795063"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.953125
        },
        {
            "corpus_id": "231592724",
            "title": "Fast convolutional neural networks on FPGAs with hls4ml",
            "text": "To further limit the model footprint, we reduce the numerical precision of the model weights before FPGA deployment. \n\nDuring training, one typically relies on single-or double-precision floating-point arithmetic, i.e. 32 or 64 bit precision. However, when deploying a deep neural network on FPGA, reduced precision fixed-point arithmetic (quantization) is often used in order to minimize resource consumption and latency. It has been shown that deep neural networks experience little accuracy loss when QAT is applied, even up to binary quantization of weights [61]. \n\nWhen a quantized model is deployed on an FPGA, all its weights, biases, and activation functions are converted to fixed-point precision before being deployed. This is referred to as post-training quantization (PTQ). The chosen precision is a new tunable hyperparameter. The hls4ml library allows users to specify different numerical precisions for different components of the network (known as heterogeneous quantization). For instance, it is found that severe PTQ of the activation functions typically results in a greater reduction of accuracy than severe PTQ of the weights [8]. By default, hls4ml assumes 16 total bits for every layer, 6 of which are dedicated to the integer part ( 16, 6 precision). \n\nIn this paper, we consider two approaches to network quantization: PTQ of a floating-point model, and QAT, resulting in a model already optimized for fixed-point precision. Both methods will be described in the following and the result on hardware discussed in detail in Section 8. To summarize, we observe a significant reduction in accuracy using PTQ, with no prediction power remaining below a bit width of 14. Using QAT, however, high accuracy is maintained down to extremely narrow bit widths of 3-4. The latency and resource consumption are similar for the two methods (with certain caveats that will be discussed in Section 8), and QAT is therefore the preferred solution for model quantization before deployment with hls4ml.",
            "score": 0.6014869702057333,
            "section_title": "Compression by quantization",
            "char_start_offset": 23242,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 119,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 567
                },
                {
                    "start": 570,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1274
                },
                {
                    "start": 1277,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 2009
                }
            ],
            "ref_mentions": [
                {
                    "start": 562,
                    "end": 566,
                    "matchedPaperCorpusId": "15817277"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8984375
        },
        {
            "corpus_id": "255998383",
            "title": "ACQ: Improving Generative Data-free Quantization Via Attention Correction",
            "text": "In recent years, neural networks have achieved great success in many applications, such as image recognition [1], object detection [2], and semantic segmentation [3]. Nevertheless, it is challenging to deploy a neural network in a resource-constrained device due to the enormous memory usage and complicated computation of the neural network. This issue promotes the development of model compression and acceleration techniques, including knowledge distillation [4], lightweight network design [5], as well as pruning [6]. Network quantization represents the floating-point parameters and activations within the networks by low-bit integers. Since more and more hardware (such as GPU, FPGA, and AI chips) support low-bit calculation, neural network quantization has emerged as a popular model compression and acceleration technique. \n\nThe rounding and truncation operations can bring quantization noise to the original network in quantization calculation. This may weaken the performance of the quantized network to a certain extent. \n\nUnder ultralow bit width, some quantized networks may even be in complete failure. When the training data is accessible, quantization-aware training (QAT) [7] is proven to be a technique that can offer desirable accuracy. This method can reduce the quantization noise by adjusting the weight through backpropagation. Nevertheless, the dependence on training data also forms a defect of this method since the original training data (such as medical records, confidential commercial pictures, or military pictures) are inaccessible in many scenes for privacy and security reasons. \n\nGiven the above considerations, data-free quantization has attracted much attention from researchers [8][9][10]. This technique can quantize a network precisely without accessing any authentic data. It is often assumed in the early DFQ [10] and ACIQ [11] that the neural network activation satisfies normal or Laplace distribution. They focused on solving quantization coefficients and realized comparable 8-bit quantization precision. However, these methods cannot be adopted for ultralow-bit quantization because of their high precision loss. Some researchers obtained synthetic samples that resemble the distribution of the authentic sample by using information from the pre-trained fullprecision network, such as batch-normalization (BN) statistics.",
            "score": 0.6013645802730432,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1033
                },
                {
                    "start": 1036,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1614
                },
                {
                    "start": 1617,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2161
                },
                {
                    "start": 2162,
                    "end": 2370
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 112,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 131,
                    "end": 134,
                    "matchedPaperCorpusId": "206770307"
                },
                {
                    "start": 162,
                    "end": 165,
                    "matchedPaperCorpusId": "4246903"
                },
                {
                    "start": 462,
                    "end": 465,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 518,
                    "end": 521,
                    "matchedPaperCorpusId": "231740691"
                },
                {
                    "start": 1718,
                    "end": 1721,
                    "matchedPaperCorpusId": "209531713"
                },
                {
                    "start": 1721,
                    "end": 1724,
                    "matchedPaperCorpusId": "247839715"
                },
                {
                    "start": 1724,
                    "end": 1728,
                    "matchedPaperCorpusId": "184487878"
                },
                {
                    "start": 1853,
                    "end": 1857,
                    "matchedPaperCorpusId": "184487878"
                },
                {
                    "start": 1867,
                    "end": 1871,
                    "matchedPaperCorpusId": "53112855"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62060546875
        },
        {
            "corpus_id": "216035831",
            "title": "Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation",
            "text": "Quantization techniques can reduce the size of Deep Neural Networks and improve inference latency and throughput by taking advantage of high throughput integer instructions. In this paper we review the mathematical aspects of quantization parameters and evaluate their choices on a wide range of neural network models for different application domains, including vision, speech, and language. We focus on quantization techniques that are amenable to acceleration by processors with high-throughput integer math pipelines. We also present a workflow for 8-bit quantization that is able to maintain accuracy within 1% of the floating-point baseline on all networks studied, including models that are more difficult to quantize, such as MobileNets and BERT-large.",
            "score": 0.6013363956345554,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.869140625
        },
        {
            "corpus_id": "250708422",
            "title": "Quantization Framework for Fast Spiking Neural Networks",
            "text": "The results in the previous section show that the network accuracy is very sensitive to the bit precision in the output layer. For instance, changing the output layer from 2-bit to floating-point gives a 4% accuracy improvement on ImageNet. In this section, we provide more fine-grained results on the bit precision in the output layer. Also, we discuss improving inference accuracy without sacrificing biological plausibility, in particular without using analog neurons in the output layer of the SNNs. \n\nWe mentioned that the SNN converted from a 2-bit ANN needs about 7 time steps to reach the highest accuracy as shown in Figure 6, which means that most of the information has been transmitted to the output layer in 7 time steps. Seven time steps can represent 3 bits of information from a rate-coded spiking neuron in theory, while the bit precision of the output  layer is only 2 bits. This gap motivates us to further utilize the information representation ability of spiking neurons by adjusting the precision of the output layer. We keep the bit precision in all hidden layers as 2-bit, and adjust the bit precision in the output layer during quantization training. As shown in Figure 8, higher precision in the output layer brings higher accuracy but longer latency. What stands out in this figure is the case with 3-bit bit precision: its accuracy improves 1.49% while its latency is only extended by 1 time step compared with the case with 2-bit precision. Meanwhile, its latency is 2 times shorter than the case with 4-bit precision. With that in mind, we choose 2-bit in all hidden layers and 3-bit in the output layer during quantization training to build high-accuracy, low-latency SNNs.",
            "score": 0.6013112735350401,
            "section_title": "Bit Precision in the Output Layer",
            "char_start_offset": 33907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1704
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94287109375
        },
        {
            "corpus_id": "268041317",
            "title": "FlattenQuant: Breaking through the Inference Compute-bound for Large Language Models with Per-tensor Quantization",
            "text": "Figure 1: As the batch size increases, the determining factor for inference latency shifts from being memory-bound to compute-bound. GPTQ prioritizes memory optimization while utilizing FP16 computation, whereas W8A8's 8-bit computation prioritizes computation optimization. \n\nor sequence length increases. Figure 1 visually demonstrates this challenge, where 8-bit computation shows superior acceleration, particularly evident with a sequence length of 256, compared to GPTQ utilizing FP16 computation. This phenomenon is rooted in the compute-bound nature of the inference process, where latency is predominantly influenced by matrix computations rather than memory access. \n\nIn small-scale models such as CNNs, 8-bit quantization can ensure a small loss of accuracy and effectively reduce inference delay (Banner et al., 2019). However, LLMs present two challenging arXiv:2402.17985v1 [cs.LG] 28 Feb 2024 aspects when it comes to quantification: \n\n\u2022 In the inference process of LLMs, there is a compute-bound problem when dealing with large batch size or long sequence. To speed up matrix calculations, there is a practical demand for using 8 bits or even 4 bits. \n\n\u2022 Due to the significant difference in the value distribution between each channel, most existing schemes use per-channel quantization (Frantar et al., 2022) or group-wise quantization (Yao et al., 2022). However, these finegrained quantization approaches uses different scaling factors inside the tensor, which can prevent the calculation of the linear layer from using low-bit matrix multiplication directly, thus slowing down the inference speed. \n\nIn this paper, we propose a method called Flatten-Quant to achieve accurate and low-bit per-tensor quantization. This method involves flattening channels with large values and adding extra channels to accommodate these values. Through this process, the maximum value of the tensor is considerably reduced while preserving complete information. The flattened tensor can undergo per-tensor quantization while ensuring the preservation of accuracy. In summary, our contributions are as follows: \n\n\u2022 We analyze the relationship between quantization schemes of LLMs and inference latency, especially the requirement for quantization schemes to overcome the computebound.",
            "score": 0.6011677386488137,
            "section_title": "Memory bound Compute bound",
            "char_start_offset": 1602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 274
                },
                {
                    "start": 277,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1166
                },
                {
                    "start": 1169,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1618
                },
                {
                    "start": 1621,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2112
                },
                {
                    "start": 2115,
                    "end": 2286
                }
            ],
            "ref_mentions": [
                {
                    "start": 808,
                    "end": 829,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 1354,
                    "end": 1372,
                    "matchedPaperCorpusId": "249395624"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7900390625
        },
        {
            "corpus_id": "244850643",
            "title": "A Hybrid Vision Processing Unit with a Pipelined Workflow for Convolutional Neural Network Accelerating and Image Signal Processing",
            "text": "The aforementioned experiment results show that the proposed VPU can achieve higher performance and efficiency on the normalized hardware resources compared with the state-of-the-art works. Although this implementation of the proposed VPU may not outperform other works, it should be noted that the performance of the design can be significantly improved with increased hardware resources, such as MACs, on-chip memory and ALUs. The increase includes both a rise in the quantity and an extension in the bitwidth. For example, increasing the number of MACs in each unit to four or using the MACs with a 16-bit width can both improve the performance of our design twofold. \n\nTechnically, our design has shown a performance measured with the GOPS-/DSP-/Frequency that is only slightly lower than that in works [33,36]. That is because work [33] exploits the sparsity in the activations and the performance is even higher than the ideal maximum value of 15.36 GOPS. However, this technique will limit the flexibility of the NPU for varied CNNs. Moreover, unlike other works, this architecture is difficult to expand to the larger scale implementation with more hardware resources. Work [36] adopts mixed bit-widths including 2-bits to 8-bits. Although this technique can improve the throughput, lower bit-width may also cause a significant loss in the accuracy of some CNNs. \n\nIt should be noted that the only factor in the NPU that affects the accuracy of the CNNs is the quantization. A lot of previous works have discussed the relation between the bit-width of the activations and weights applied on the NPU and the accuracy of the CNNs. It has been proven that the 8-bit quantization for weights and activations is efficient enough for most CNNs. Thanks to the robustness of the CNNs against the bit-width reduction, the 8-bit quantization causes negligible loss in accuracy, usually less than 1%. Table 4 shows the evaluation of the accuracy for the CNNs with 8-bit quantization. It must be noticed that this relation is not among the research points in this work, since it has been thoroughly studied in previous works.",
            "score": 0.6010616466604326,
            "section_title": "The Discussion",
            "char_start_offset": 72671,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1370
                },
                {
                    "start": 1373,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 807,
                    "end": 811,
                    "matchedPaperCorpusId": "3746338"
                },
                {
                    "start": 837,
                    "end": 841,
                    "matchedPaperCorpusId": "3746338"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8369140625
        },
        {
            "corpus_id": "254275121",
            "title": "QEBVerif: Quantization Error Bound Verification of Neural Networks",
            "text": "In the past few years, the development of deep neural networks (DNNs) has grown at an impressive pace owing to their outstanding performance in solving various complicated tasks [23,28]. However, modern DNNs are often large in size and contain a great number of 32-bit floating-point parameters to achieve competitive performance. Thus, they often result in high computational costs and excessive storage requirements, hindering their deployment on resourceconstrained embedded devices, e.g., edge devices. A promising solution is to quantize the weights and/or activation tensors as fixed-point numbers of lower bit-width [17,21,25,35]. For example, TensorFlow Lite [18] supports quantization of weights and/or activation tensors to reduce model size and latency, and Tesla FSD-chip [61] stores all the data and weights of a network in the form of 8-bit integers. \n\nIn spite of the empirically impressive results which show there is only minor accuracy loss, quantization does not necessarily preserve properties such as robustness [16]. Even worse, input perturbation can be amplified by quantization [11,36], worsening the robustness of quantized neural networks (QNNs) compared to their DNN counterparts. Indeed, existing neural network quantization methods focus on minimizing its impact on model accuracy (e.g., by formulating it as an optimization problem that aims to maximize the accuracy [27,43]). However, they cannot guarantee that the final quantization error is always lower than a given error bound, especially when some specific safety-critical input regions are concerned. This is concerning as such errors may lead to catastrophes when the quantized networks are deployed in safety-critical applications [14,26]. Furthermore, analyzing (in particular, quantifying) such errors can also help us understand how quantization affect the network behaviors [33], and provide insights on, for instance, how to choose appropriate quantization bit sizes without introducing too much error. Therefore, a method that soundly quantifies the errors between DNNs and their quantized counterparts is highly desirable.",
            "score": 0.6005183419990092,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2120
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 182,
                    "matchedPaperCorpusId": "58623492"
                },
                {
                    "start": 623,
                    "end": 627,
                    "matchedPaperCorpusId": "199577815"
                },
                {
                    "start": 627,
                    "end": 630,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 630,
                    "end": 633,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 633,
                    "end": 636,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 1033,
                    "end": 1037,
                    "matchedPaperCorpusId": "181525935"
                },
                {
                    "start": 1103,
                    "end": 1107,
                    "matchedPaperCorpusId": "221616646"
                },
                {
                    "start": 1107,
                    "end": 1110,
                    "matchedPaperCorpusId": "53502621"
                },
                {
                    "start": 1398,
                    "end": 1402,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 1402,
                    "end": 1405,
                    "matchedPaperCorpusId": "216056295"
                },
                {
                    "start": 1722,
                    "end": 1726,
                    "matchedPaperCorpusId": "29162614"
                },
                {
                    "start": 1726,
                    "end": 1729,
                    "matchedPaperCorpusId": "52956175"
                },
                {
                    "start": 1869,
                    "end": 1873,
                    "matchedPaperCorpusId": "227239002"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79931640625
        },
        {
            "corpus_id": "258990120",
            "title": "Low Precision Quantization-aware Training in Spiking Neural Networks with Differentiable Quantization Function",
            "text": "12% and 1.18%, respectively). Hence, for this dataset, the model can be quantized down to 1-bit with an accuracy degradation of around 1%.\n\nFor the N-Caltech101 dataset, on the other hand, the binarized SNN model has a more significant accuracy drop compared to other bit-precision models. While 8-bit, 4-bit, and 2-bit quantized SNN models have 1.07 %, 1.39 %, and 1.8 % accuracy drops, the 1-bit model has 3.47 % accuracy degradation compared to its full-precision counterpart.\n\nFor the N-MNIST dataset, the accuracy degradation is the lowest among the presented datasets. The accuracy drop is negligible and fluctuates between different bit-precisions. The accuracy drop in the < 0.2% range is achieved compared to the full-precision model.",
            "score": 0.6000817864792887,
            "section_title": "4) N-Caltech101:",
            "char_start_offset": 21003,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7421875
        },
        {
            "corpus_id": "220793844",
            "title": "WrapNet: Neural Net Inference with Ultra-Low-Resolution Arithmetic",
            "text": "Significant progress has been made in quantizing (or even binarizing) neural networks, and numerous methods have been proposed that reduce the precision of weights, activations, and even gradients while retaining high accuracy [3, 4, 6, 8, 10, 12, 14, 16-18, 23, 26, 28-30]. Such quantization strategies make neural networks more hardware-friendly by leveraging fast, integer-only arithmetic, replacing multiplications with simple bit-wise operations, and reducing memory requirements and bandwidth. \n\nUnfortunately, the gains from quantization are limited as much of the computation in quantized networks still requires high-resolution arithmetic. Even if weights and activations are represented with just one bit, deep feature computation requires the summation of hundreds or even thousands of products. Performing these summations with low-resolution registers results in integer overflow, contaminating downstream computations and destroying accuracy. Moreover, as multiplication costs are slashed by quantization, high-resolution accumulation starts to dominate the arithmetic cost. Indeed, our own hardware implementations show that an 8-bit \u00d7 8-bit multiplier consumes comparable power and silicon area to a 32-bit accumulator. When reducing the resolution to a 3-bit\u00d7 1-bit multiplier, a 32-bit accumulator consumes more than 10\u00d7 higher power and area; see Section 4.5. Evidently, low-resolution accumulators are the key to further accelerating quantized nets. \n\nIn custom hardware, low-resolution accumulators reduce area and power requirements while boosting throughput. On general-purpose processors, where registers have fixed size, low-resolution accumulators are exploited through bit-packing, i.e., by representing multiple low-resolution integers side-by-side within a single high-resolution register [2,22,23]. Then, a single vector instruction is used to perform the same operation across all of the packed numbers. For example, a 64-bit register can be used to execute eight parallel 8-bit additions, thus increasing the throughput of software implementations. Hence, the use of low-resolution accumulators is advantageous for both hardware and software implementations, provided that integer overflow does not contaminate results.",
            "score": 0.5999692424272076,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 499
                },
                {
                    "start": 502,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1469
                },
                {
                    "start": 1472,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 1824,
                    "end": 1827,
                    "matchedPaperCorpusId": "14925907"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.880859375
        },
        {
            "corpus_id": "251741056",
            "title": "Design Automation for Fast, Lightweight, and Effective Deep Learning Models: A Survey",
            "text": "Unlike the above three compression techniques, which struggle to optimize network architectures, quantization ap-peals to reduce the representation precision of network weights and intermediate activation tensors. Neural networks are generally trained using the 32-bit floating-point precision; if we were to perform network inference in the 32-bit floating-point as well, MAC operations, data transfer, and data saving would have to be done all in 32-bit floating-point. Hence, using lower bit precision would substantially reduce the hardware overhead, including communication bandwidth, computation and memory usage [182]. For example, when moving from 32 to 8 bits, the memory cost and computation would decrease by a factor of 4 and 16 respectively. In addition, fixedpoint computation is more efficient than its floating-point counterpart [182]. The most commonly explored quantization scheme, uniform quantization, converts a floating point tensor x = {x 1 , ..., x N } with range (x min , x max ) into its integer coding x q with range [n, p] = [0, 2 b \u2212 1] via the following definition [183]: \n\nx = (x q \u2212 z)\u2206, \n\nwhere \u2206 is the scale factor that specifies the step size of the quantizer; z is the zero-point that represents the real value zero without any precision loss; b is the quantization bitwidth; \n\n\u2022 is the round-to-nearest operation. The clamp(\u2022) function is to truncate all values to fall between n and p. Through this procedure, two sorts of errors are induced: a clipping error induced by the clamp function and a rounding error induced by the round-to-nearest operation \u2022 . The quantized tensor x is then used for efficient but low precision computation. Since there are no theoretical correlations between the model accuracy and b, simple try-and-error is only feasible in the scenario where all weights and activation tensors have the same precision. However, different layers have different redundancy and behaviours on both the hardware and the task. Therefore, automatically determining the bitwidth of weights and activations for each layer is stunningly attractive, and this mixed-precision feature has recently been supported by hardware manufacturers [184], [185].",
            "score": 0.599908485858105,
            "section_title": "D. Automated Quantization",
            "char_start_offset": 97111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1119
                },
                {
                    "start": 1122,
                    "end": 1312
                },
                {
                    "start": 1315,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2195
                }
            ],
            "ref_mentions": [
                {
                    "start": 619,
                    "end": 624,
                    "matchedPaperCorpusId": "22028726"
                },
                {
                    "start": 845,
                    "end": 850,
                    "matchedPaperCorpusId": "22028726"
                },
                {
                    "start": 1095,
                    "end": 1100,
                    "matchedPaperCorpusId": "234778162"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.703125
        },
        {
            "corpus_id": "211146310",
            "title": "Robust Quantization: One Model to Rule Them All",
            "text": "Deep neural networks (DNNs) are a prominent choice for many machine learning applications. However, a significant drawback of these models is their computational costs. Low-precision arithmetic is one of the key techniques being actively studied to overcome this difficult. With appropriate hardware support, low-precision training and inference can perform more operations per second, reduce memory bandwidth and power consumption, and allow larger networks to fit into a device. \n\nNaively quantizing a single-precision floating point (FP32) model to 4 bits (INT4) or lower usually incurs a significant accuracy degradation. Many studies have tried to mitigate this accuracy decrease by offering different quantization methods. These methods differ in whether they require training or not. Methods that require training (known as quantization aware training or QAT (Choi et al., 2018;Baskin et al., 2018;Esser et al., 2019;Zhang et al., 2018;Zhou et al., 2016)) simulate the quantization arithmetic on the fly, while methods that avoid training (known as post-training quantization or PQT (Banner et al., 2019;Choukroun et al., 2019;Migacz, 2017;Gong et al., 2018;Finkelstein et al., 2019;Zhao et al., 2019)) minimize the quantization noise added to the model. \n\nUnfortunately, both approaches are not robust to common variations in the assumed quantization noise model. For example, (Krishnamoorthi, 2018) has observed that in order to avoid accuracy degradation at inference time, it is essential to ensure that all quantization-related artifacts are faithfully modeled at training time. Our experiments in this paper further asses this observation. For example, when quantizing ResNet-18 (He et al., 2015) with DoReFa (Zhou et al., 2016) to 4-bit precision, an error of less than 2% in the quantizer step size results in an accuracy drop of 58%. \n\nIn practice, there is almost always some degree of uncertainty related to how quantization is executed.",
            "score": 0.599844278342718,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 1261
                },
                {
                    "start": 1264,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1849
                },
                {
                    "start": 1852,
                    "end": 1955
                }
            ],
            "ref_mentions": [
                {
                    "start": 924,
                    "end": 943,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 1090,
                    "end": 1111,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 1147,
                    "end": 1165,
                    "matchedPaperCorpusId": "46899409"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9599609375
        },
        {
            "corpus_id": "202781343",
            "title": "SHE: A Fast and Accurate Deep Neural Network for Encrypted Data",
            "text": "Besides homomorphic shift operations, the logarithmically quantized CNN models also require accumulations to do inferences. The computing overhead of a TFHE adder is proportional to its bitwidth. For instance, compared to its 4-bit counterpart, an 8-bit TFHE adder doubles the computing overhead. We quantize the weights into their 5-bit power-of-2 representations, since recent works [12] show a 5-bit logarithmically quantized AlexNet model degrades the inference accuracy by only < 0.6%. However, the accumulation intermediate results have to be represented by 16 bits. \n\nOtherwise, the inference accuracy may dramatically decrease. Accumulating 5-bit shifted results through 16-bit TFHE adders is computationally expensive. \n\nTable 2: The multiplicative depth (MD) overhead of prior LHECNNs. In the column of topology, C means a convolution layer; B is a batch normalization layer; A indicates an activation layer; P denotes a pooling layer; and F is a fully-connected layer. Acc is the inference accuracy. \n\nTherefore, we propose a mixed bitwidth accumulator shown in Figure 3(c) to avoid the unnecessary computing overhead. A mixed bitwidth accumulator is an adder tree, where each node is a TFHE adder. And TFHE adders at different levels of the tree have distinctive bitwidths. At the bottom level (layer 0 ) of the tree, we use b-bit TFHE adders, where b is 5. Instead of 16-bits adders, layer 1 of the tree has (b + 1)-bit TFHE adders, since the sum of two 5-bit integers can have at most 6 bits. The n th level of the mixed bitwidth accumulator consists of (b + n)-bit TFHE adders.",
            "score": 0.5997340434712852,
            "section_title": "Mixed Bitwidth Accumulator",
            "char_start_offset": 11572,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1010
                },
                {
                    "start": 1013,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1592
                }
            ],
            "ref_mentions": [
                {
                    "start": 385,
                    "end": 389,
                    "matchedPaperCorpusId": "12040525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86083984375
        },
        {
            "corpus_id": "259309210",
            "title": "Designing Strong Baselines for Ternary Neural Network Quantization through Support and Mass Equalization",
            "text": "As the performance of deep neural network grows, so do their computational requirements: in computer vision, popular architectures such as ResNet [1], MobileNet V2 [2] and Effi-cientNet [3] rely on high expressivity from millions of parameters to effectively tackle challenging tasks such as classification [4], object detection [5] and segmentation [6]. \n\nIn order to deploy these models using lower power consumption and less expensive hardware, many compression techniques have been developed to reduce the latency and memory footprint of large convolutional neural networks (CNN). Quantization is one of the most efficient of these techniques and consists in converting floating values with large bit-width to fixed point values encoded on lower bit-width. Quantization can be performed in three major contexts. First, data-free quantization (DFQ) [7,8,9,10,11], where, quantization is applied without data, hopefully not degrading the model accuracy. Second, post training quantization (PTQ) [12,13,14]: in this setup, one seeks to tune the quantization operator given an already trained model and a calibration set (usually a fraction of the training set). Third, quantization aware training (QAT) [15,16,17,18,19,20,21], in which, given a model and a training set, custom gradient descent proxies are usually implemented so as to circumvent zero gradients that stems from the rounding operation. \n\nWhile noawadays most methods are successful in quantizing models to 8 or 4 bits while preserving the accuracy [22,13,23,11], lower bit-widths remain an open challenge. In particular, ternary quantization, where weights are mapped to {\u22121, 0, 1} [24,12] is an extreme form of neural network quantization, that comes with its specific challenges. While most quantization techniques rely on a rounding operation [22,7,8,10], they struggle to preserve high accuracies when applied in the ternary quantization regime.",
            "score": 0.5995280693001799,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 354
                },
                {
                    "start": 357,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1402
                },
                {
                    "start": 1405,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1916
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 149,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 164,
                    "end": 167,
                    "matchedPaperCorpusId": "4555207"
                },
                {
                    "start": 186,
                    "end": 189,
                    "matchedPaperCorpusId": "167217261"
                },
                {
                    "start": 307,
                    "end": 310,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 350,
                    "end": 353,
                    "matchedPaperCorpusId": "502946"
                },
                {
                    "start": 852,
                    "end": 855,
                    "matchedPaperCorpusId": "184487878"
                },
                {
                    "start": 855,
                    "end": 857,
                    "matchedPaperCorpusId": "59606175"
                },
                {
                    "start": 997,
                    "end": 1001,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 1001,
                    "end": 1004,
                    "matchedPaperCorpusId": "216056295"
                },
                {
                    "start": 1204,
                    "end": 1208,
                    "matchedPaperCorpusId": "3603886"
                },
                {
                    "start": 1208,
                    "end": 1211,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1217,
                    "end": 1220,
                    "matchedPaperCorpusId": "4303893"
                },
                {
                    "start": 1515,
                    "end": 1519,
                    "matchedPaperCorpusId": "5071254"
                },
                {
                    "start": 1519,
                    "end": 1522,
                    "matchedPaperCorpusId": "216056295"
                },
                {
                    "start": 1653,
                    "end": 1656,
                    "matchedPaperCorpusId": "59292009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78662109375
        },
        {
            "corpus_id": "49356451",
            "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
            "text": "In this section, we explore choices of activation functions and tradeoffs between precision and width of a network. \n\n\u2022 Do not constrain activation ranges: One can get slightly better accuracy by replacing ReLU6 non-linearity with a ReLU and let the training determine the  activation ranges (see figure 16) \n\nFigure 16: Accuracy improvement of training with ReLU over ReLU6 for floating point and quantized mobilenet-v1 networks. \n\n\u2022 Explore tradeoff of width vs quantization: An over-parameterized model is more amenable to quantization. Even for leaner architectures like mobilenet, one can tradeoff the depth multiplier with the precisions of the weights. We compare the accuracies obtained with 4 bit per-channel quantization of weights with 8-bit quantization across different depth multipliers in figure 17. Note that this comparison allows us to evaluate a depth vs quantization tradeoff (see [31]). \n\nIt is interesting to see that one can obtain a further 25% reduction in the model size for almost the same accuracy by moving to 4 bit precision for the weights.",
            "score": 0.5994118509918445,
            "section_title": "Model Architecture Recommendations",
            "char_start_offset": 23741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 118,
                    "end": 307
                },
                {
                    "start": 310,
                    "end": 430
                },
                {
                    "start": 433,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 1071
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "277635372",
            "title": "QuantAttack: Exploiting Dynamic Quantization to Attack Vision Transformers",
            "text": "DNNs, particularly transformers, have achieved great success in various ML tasks [19,43]. However, their com-putational complexity and large model size pose challenges for real-time applications and resource-limited devices. The size of a model is determined by the number of parameters and their precision. The precision relates to the number of bits each weight value is stored with, typically 16 bits (also referred to as float16 or f16) or 8 bits (also referred to as int8 or i8). To mitigate these challenges, techniques like quantization are employed. \n\nQuantization is used to reduce the computational time and memory consumption of neural networks. By quantizing the weights and activations into low-bit integers (e.g., a 16-bit float to an 8-bit integer), GPU memory usage can be reduced and inference can be accelerated, due to lowbit-precision matrix multiplication. Two main quantization approaches have been proposed: -Static quantization [3,11,23,29,40,44,46,47]: The weights are quantized to lower precision only once using calibration sets, after the model is trained. -Dynamic quantization [8]: The weights and activations are quantized during runtime based on specific rules. In this paper, we focus on PTQ techniques, highlighting that dynamic techniques possess an availabilitybased vulnerability due to their inherent dynamism, which uniquely impacts their security. Additionally, we demonstrate that both static and dynamic techniques are susceptible to integrity-based attacks.",
            "score": 0.5992538356250452,
            "section_title": "Quantization",
            "char_start_offset": 5815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 557
                },
                {
                    "start": 560,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1500
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 85,
                    "matchedPaperCorpusId": "230435805"
                },
                {
                    "start": 85,
                    "end": 88,
                    "matchedPaperCorpusId": "269498086"
                },
                {
                    "start": 955,
                    "end": 958,
                    "matchedPaperCorpusId": "213004198"
                },
                {
                    "start": 958,
                    "end": 961,
                    "matchedPaperCorpusId": "235658553"
                },
                {
                    "start": 961,
                    "end": 964,
                    "matchedPaperCorpusId": "216056295"
                },
                {
                    "start": 964,
                    "end": 967,
                    "matchedPaperCorpusId": "221506623"
                },
                {
                    "start": 967,
                    "end": 970,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 970,
                    "end": 973,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 1107,
                    "end": 1110,
                    "matchedPaperCorpusId": "251564521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.525390625
        },
        {
            "corpus_id": "44071489",
            "title": "Scalable Methods for 8-bit Training of Neural Networks",
            "text": "In this study, we investigate the internal representation of low precision neural networks and present guidelines for their quantization. Considering the preservation of direction during quantization, we analytically show that significant quantization is possible for vectors with a Gaussian distribution. \n\nOn the forward pass the inputs to each layer are known to be distributed according to a Gaussian distribution, but on the backward pass we observe that the layer gradients g l do not follow this distribution. Our experiments further assess that angle is not well preserved on the backward pass, and moreover final validation accuracy tightly follows that angle. Accordingly, we bifurcate the layer gradients g l and use it at 16-bits for the computation of the weight gradient g W while keeping the computation of next layer gradient g l\u22121 at 8-bit. This enables the (slower) 16-bits computation of g W to be be done in parallel with g l\u22121 , without interrupting the propagation the layer gradients. \n\nWe further show that Range BN is comparable to the traditional batch norm in terms of accuracy and convergence rate. This makes it a viable alternative for low precision training. During the forward-propagation phase computation of the square and square root operations are avoided and replaced by max(\u2022) and min(\u2022) operations. During the back-propagation phase, the derivative of max(\u2022) or min(\u2022) is set to one where the coordinates for which the maximal or minimal values are attained, and is set to zero otherwise. \n\nFinally, we combine the two novelties into a single training scheme and demonstrate, for the first time, that 8-bit training on a large scale dataset does not harm accuracy. Our quantization approach has major performance benefits in terms of speed, memory, and energy. By replacing float32 with int8, multiplications become 16 times faster and at least 15 times more energy efficient [13]. This impact is attained for 2/3 of all the multiplications, namely the forward pass and the calculations of the layer gradients g l . The weight gradients g W are computed as a product of 8-bit precision (layer input) with a 16-bit precision (unquantized version of g l ), resulting with a speedup of x8 for the rest of multiplications and at least x2 power savings.",
            "score": 0.5991895692658606,
            "section_title": "Discussion",
            "char_start_offset": 16703,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1527
                },
                {
                    "start": 1530,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2287
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85302734375
        },
        {
            "corpus_id": "53085328",
            "title": "Differentiable Fine-grained Quantization for Deep Neural Network Compression",
            "text": "Neural networks have shown great performance in cognitive tasks. When deploying network models on mobile devices with limited resources, weight quantization has been widely adopted. Binary quantization obtains the highest compression but usually results in big accuracy drop. In practice, 8-bit or 16-bit quantization is often used aiming at maintaining the same accuracy as the original 32-bit precision. We observe different layers have different accuracy sensitivity of quantization. Thus judiciously selecting different precision for different layers/structures can potentially produce more efficient models compared to traditional quantization methods by striking a better balance between accuracy and compression rate. In this work, we propose a fine-grained quantization approach for deep neural network compression by relaxing the search space of quantization bitwidth from discrete to a continuous domain. The proposed approach applies gradient descend based optimization to generate a mixed-precision quantization scheme that outperforms the accuracy of traditional quantization methods under the same compression rate.",
            "score": 0.5989119706746511,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85595703125
        },
        {
            "corpus_id": "271218731",
            "title": "NITRO-D: Native Integer-only Training of Deep Convolutional Neural Networks",
            "text": "In recent years, the quantization of Deep Neural Networks (DNNs) has emerged as a pivotal trend in both the scientific and technological landscapes [7]. This technique aims at lowering the computational demand and memory requirements of DNNs by reducing the number of bits to represent weights and activations (i.e., typically from 32-bit floating-point to 16-bit or 8-bit integers). However, quantization frameworks for DNNs retain dependence on floating-point (FP) operations during the training. In this perspective, the aim of this paper is to address the following research question: is it possible to train arbitrarily deep Convolutional Neural Networks (CNNs) by using integer-only values in both inference and training? \n\nThe literature in this field is quite wide but highly fragmented, and current frameworks for integer-only DNNs can be divided into three main categories. The first one consists of applying quantization solely during inference, while the training relies on FP operations (to support the computation of the The rest of this paper is organized as follows. Section 2 describes the related literature. Section 3 introduces the NITRO-D framework developed in this study, while Section 4 presents experimental results to evaluate its effectiveness. Conclusions and future works are finally drawn in Section 5.",
            "score": 0.5984539440256158,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1332
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52001953125
        },
        {
            "corpus_id": "261530433",
            "title": "On the fly Deep Neural Network Optimization Control for Low-Power Computer Vision",
            "text": "In this section, we highlight our methods to control the activation sparsity and precision in DNNs. \n\n1) AdaptiveActivation ReLU: In this section, we describe our method to control the sparsity in DNN activations. We specifically focus on methods that require no no-retraining because they can be used for on-the-fly adjustments to inference performance. \n\n2) Activation Precision Control: The default quantization format used in DNN libraries such as PyTorch is 32-bit floating point. Howver, many other quantization formats can be used for inference. Some quantization formats are: 16-bit floating point, 8-bit floating point, 4-bit integer, and 2-bit integer. Using fewer bits reduces the memory requirement, and also speeds up the arithmetic operations. However, using fewer bits reduces the accuracy of the DNN. \n\nIn this work, we use different quantization levels for different layers based on the impact on the accuracy. Cast operators are inserted at the output of the activations to convert values",
            "score": 0.5982105622558396,
            "section_title": "A. Controlling Activation Sparsity and Precision",
            "char_start_offset": 7352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 102,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 354
                },
                {
                    "start": 357,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1006
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8984375
        },
        {
            "corpus_id": "231786493",
            "title": "Fixed-point Quantization of Convolutional Neural Networks for Quantized Inference on Embedded Platforms",
            "text": "From the results gathered on the allocation scheme for acceptable loss D l,p in inference accuracy when performing dependent optimized search, we noted that linear allocation Fig. 11: Results for accuracy loss and the optimal bitwidths of the quantized CNN resulting from dependent optimized search using a linear acceptable loss allocation scheme on a 15-layer sequential CNN trained on MNIST. \n\nworked best, giving sufficiently low accuracy losses for the resulting quantized network with low bitwidths and therefore a low memory consumption. We now use this allocation scheme as part of our algorithm to quantize parameters of a 32-bit floating-point full precision CNN. We then evaluate the quantized network against the constraints in ( 7) and based on the given metrics, compare it to a simple baseline approach that is used by a lot of commercial tools. We first present the baseline approach, then present and discuss the results to compare it against our approach. \n\n1) Baseline: A simple approach to post-training quantization that is used by a number of commercial tools is that of quantizing the entire network to the same fixed-point representation. Given that hardware is restricted to 4, 8, 16, 32 bits, quantizing an entire network to the same representation is usually limited to 8 bits as lower bitwidths result in larger inference accuracy degradation. \n\nFor the fractional offset, the tools find a fractional offset given the bitwidth such that none of the parameter distributions are subjected to clipping. We used this approach as a starting point for our algorithm in OptSearchCNN as defined in Step 2 in Section V-A. Concretely, we find a fractional offset F for a given bitwidth BW by assigning as many of the available integer bits in the bitwidth to the largest absolute valued number in the parameter distribution. This ensures that the range of values is covered avoiding any possibilities of clipping. The remaining bits are then assigned to the fractional part F as is defined in (8). \n\nFor our baseline, we will use the fixed-bitwidth approach and quantize weights, biases and activations for all layers to a fixed-point representation with a bitwidth of 8 bits.",
            "score": 0.5981988728335044,
            "section_title": "D. Linear Allocation Dependent Optimized Search",
            "char_start_offset": 77077,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 394
                },
                {
                    "start": 397,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2015
                },
                {
                    "start": 2018,
                    "end": 2194
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88330078125
        },
        {
            "corpus_id": "235436203",
            "title": "S2 Engine: A Novel Systolic Architecture for Sparse Convolutional Neural Networks",
            "text": "Many recent designs of neural network accelerators and GPUs already support 8/16-bit fixed-point data in CNN inference. In addition to the one-precision-fits-all approaches, mixed-precision quantization algorithms have been also developed by assigning different precision to different layers of CNNs according to the different sensitivities of each layer [22]. A fine-granularity quantization approach is also proposed by [19], where most data is represented with low precision (i.e., 4-bit) while only a small portion of the data (i.e., 3%) is represented with high precision (i.e., 16bit). These mixed-precision approaches further reduce the CNN computation cost and memory consumption.",
            "score": 0.5979619583041111,
            "section_title": "CNN Sparsity and Quantization",
            "char_start_offset": 6511,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 688
                }
            ],
            "ref_mentions": [
                {
                    "start": 422,
                    "end": 426,
                    "matchedPaperCorpusId": "5071254"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8369140625
        },
        {
            "corpus_id": "247331384",
            "title": "FxP-QNet: A Post-Training Quantizer for the Design of Mixed Low-Precision DNNs with Dynamic Fixed-Point Representation",
            "text": "The design of a mixed low-precision DNN starts with the preprocessor that generates initial QPs for each datastructure based on a given wordlength and round strategy as discussed in Equation ( 9). Specifically, preprocessor adopts three rounding strategies for this purpose; the conservative, neutral, and aggressive strategies. In this section, we evaluate the use of each rounding strategy in the performance of quantized DNNs considering five bit-precision levels, namely, 16, 10, 8, 6, and 5 bits. Recall from Section IV-A3 that these bit-precision levels are used to quantize G 1 datastructures set defined in Equation ( 22), except for activations and weights in first and last layers, while all other datastructures are represented in the 32-bit fixed-point format. \n\nThe data ranges, which are used for the quantization of activations, are collected after forwarding the network on the calibration set comprising of 800 images. Then, the top-1 and top-5 accuracies are measured on 200 test images. After 5 runs of each experiment, the average test accuracies for AlexNet, VGG-16, and ResNet-18 architectures are shown in Figure 7. Note that we use very small sets in this experiment to demonstrate the efficacy of the quantization solution proposed by preprocessor even with limited data. \n\nFirst of all, we can observe that the rounding strategy plays a crucial role in determining network's performance. One can also notice that the conservative strategy achieves better performance in all the considered architectures for 8-, 10-, and 16-bit fixed-point quantization. Focusing on 8-bit results, it is clear that the bit-precision reduction framework can start the design of mixed low-precision networks with an initial solution that causes less than 0.4% accuracy drop. \n\nGoing down to 6-bit quantized data, the obtained QPs by the neutral strategy increases the top-1 accuracy gap with those of the second-best strategy to 14.6% and 5.4% for AlexNet and ResNet-18, respectively.",
            "score": 0.5978869029127821,
            "section_title": "B. ANALYSIS OF ROUNDING STRATEGIES USED TO DETERMINE THE INITIAL QUANTIZATION PARAMETERS",
            "char_start_offset": 73016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1780
                },
                {
                    "start": 1783,
                    "end": 1990
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "53977760",
            "title": "Training Deep Neural Networks with 8-bit Floating Point Numbers",
            "text": "There has been a tremendous body of research conducted towards DNN precision scaling over the past few years. However, a significant fraction of this quantization research has focused around reduction of bit-width for the forward path for inference applications. Recently, precision for weights and activations were scaled down to 1-2 bits ( [11,3]) with a small loss of accuracy, while keeping the gradients and errors in the backward path as well as the weight updates in full-precision. In comparison to inference, much of the recent work on low precision training often uses much higher precision -specifically on the errors and gradients in the backward path. DoReFa-Net [23] reduces the gradient precision down to 6 bits while using 1-bit weights and 2-bit activations for training. WAGE [20] quantizes weights, activations, errors and gradients to 2, 8, 8 and 8 bits respectively. However, all of these techniques incur significant accuracy degradation (> 5%) relative to full-precision models. \n\nTo maintain model accuracy for reduced-precision training, much of recent work keeps the data and computation precision in at least 16 bits. MPT [16] uses a IEEE half-precision floating point format (16 bits) accumulating results into 32-bit arrays and additionally proposes a loss-scaling method to preserve gradients with very small magnitudes. Flexpoint [13] and DFP [4] demonstrated a format with a 16-bit mantissa and a shared exponent to train large neural networks with full-precision accuracy. The shared exponents can be adjusted dynamically to minimize overflow. However, even with 16-bit data representations, these techniques require the partial products to be accumulated in 32-bits and subsequently rounded down to 16 bits for the following computation. In addition, in all cases, a 32-bit copy of the weights is maintained to preserve the fidelity of the weight update process. \n\nIn contrast, using the new ideas presented in this paper, we show that it is possible to train these networks using just 8-bit floating point representations for all of the arrays used in matrix and tensor computations -weights, activations, errors and gradients.",
            "score": 0.5978587387914196,
            "section_title": "8-bit floating point training 2.1 Related Work",
            "char_start_offset": 3296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1896
                },
                {
                    "start": 1899,
                    "end": 2162
                }
            ],
            "ref_mentions": [
                {
                    "start": 1361,
                    "end": 1365,
                    "matchedPaperCorpusId": "31312287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86181640625
        },
        {
            "corpus_id": "998481",
            "title": "NEURAghe",
            "text": "Among CNN FPGA architectures, the precision of arithmetic operands plays a crucial role in energy efficiency. Although most of the architectures available in literature feature a precision of 16-bit (fixed-point) [12,21,34] some reduced-precision implementations have been proposed recently, relying on 8-bit, 4-bit accuracy for both maps and weights, exploiting the resiliency of CNNs to quantization and approximation [29]. \n\nQiu et. al. [29] proposed a CNN accelerator implemented on a Xilinx Zynq platform exploiting specific hardware to support 8/4 bit dynamic precision quantization, at the cost of 0.4% loss of classification accuracy. To improve the performance of fully connected layers, mainly limited by the off-chip bandwidth, the architecture employs Single Value Decomposition (SVD) to reduce the memory footprint of the weights. The design was evaluated on a VGG-16 network featuring SVD on the first fully connected layer, and achieves a performance of 187.8 GOP/s and 137.0 GOP/s for CONV layers and full CNN under 150 MHz frequency respectively achieving 4.4 Frames Per Second (FPS). \n\nMost extreme approaches to quantization exploit ternary [28] or binary [33] neural-networks accelerators for FPGA. This approach significantly improves the computational efficiency of FPGA Accelerators, allowing to achieve performance level as big as 8 TOPS [28]. These improvements are due to the 32-bit multipliers that can be replaced by simpler multiplexer and 2's complement operators, while bandwidth for loading weights can be reduced drastically, by 8 to 16 times if we compare with widely used 16-bit fixed point accelerators. The main issue related to binary and ternary accelerator is related to the training. While small networks like MNIST, CIFAR10, SVHN, GTSRB can reach good classification accuracy, the training is still a big challenge for larger networks such as VGG or ResNet [8]. \n\nIn this work we target execution of state of the art CNNs leveraging 16-bit operands and weights hence not requiring retraining.",
            "score": 0.5977792589909673,
            "section_title": "RELATED WORK",
            "char_start_offset": 12756,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1903
                },
                {
                    "start": 1906,
                    "end": 2034
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 217,
                    "matchedPaperCorpusId": "20249920"
                },
                {
                    "start": 217,
                    "end": 220,
                    "matchedPaperCorpusId": "207380371"
                },
                {
                    "start": 220,
                    "end": 222,
                    "matchedPaperCorpusId": "37390163"
                },
                {
                    "start": 420,
                    "end": 424,
                    "matchedPaperCorpusId": "207233273"
                },
                {
                    "start": 440,
                    "end": 444,
                    "matchedPaperCorpusId": "207233273"
                },
                {
                    "start": 1160,
                    "end": 1164,
                    "matchedPaperCorpusId": "65661"
                },
                {
                    "start": 1175,
                    "end": 1179,
                    "matchedPaperCorpusId": "10530917"
                },
                {
                    "start": 1362,
                    "end": 1366,
                    "matchedPaperCorpusId": "65661"
                },
                {
                    "start": 1899,
                    "end": 1902,
                    "matchedPaperCorpusId": "1518846"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8974609375
        },
        {
            "corpus_id": "53291227",
            "title": "Iteratively Training Look-Up Tables for Network Quantization",
            "text": "One of the main advantages of LUT-Q is that it jointly learns both dictionary values and assignments. Traditional approaches 4 Neural Network Libraries by Sony: https://nnabla.org/ r Fixed-point quantization (\"fp\") quantizes a float weight\n\ni.e., uniformly quantizes w using the quantization step size \u03b4 and, hence, the dynamic range is r = (2 n\u22121 \u2212 1)\u03b4. For an efficient hardware implementation, \u03b4 needs to be a powerof-two as the multiplication of two fp quantized numbers can then be implemented by an integer multiplication and a bit shift by n bits.\n\nr Pow-2 quantization (\"pow-2\"): Similarly, the pow-2 quantization of the weight w = s|w| is given by\n\nwhere m \u2208 Z with 2 m = r being the dynamic range. We use the same scheme as [16] and [17] to train these networks. Note that this is equivalent to LUT-Q training but skipping the dictionary update in the initial k-means and in the \"Step 1\" of Algorithm 1.\n\nFor fp and pow-2 quantization, we need to choose the dynamic range [\u2212r, r] for the weight quantization. Preliminary experiments showed that the choice of the dynamic range drastically influences the performance, especially for very small bitwidth quantization. We follow the approach from [23] where the dynamic range is chosen for each layer using r = 2 log 2 max oi |W oi | . As explained in Section V, full precision activations dominate the memory requirements for very low bitwidth weights. Therefore, we trained the networks with activations quantized to 8-bit using fp quantization. Table II compares the results of 4-bit and 2-bit quantization of a ResNet-20 with either fixed point (fp) or LUT-Q (constrained to power-of-two). We observe that fp achieves near baseline performances, however, we lose performance when the weights are quantized to 2 bit. LUT-Q achieves similar performance for 4-bit networks and significantly outperformed the fixed quantization methods for 2-bit weights with 8.02%",
            "score": 0.5977141775337844,
            "section_title": "A. Fixed Quantization Versus LUT-Q",
            "char_start_offset": 23561,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 126,
                    "matchedPaperCorpusId": "16664844"
                },
                {
                    "start": 743,
                    "end": 747,
                    "matchedPaperCorpusId": "3643430"
                },
                {
                    "start": 1204,
                    "end": 1208,
                    "matchedPaperCorpusId": "12130431"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8505859375
        },
        {
            "corpus_id": "259274857",
            "title": "ReDy: A Novel ReRAM-centric Dynamic Quantization Approach for Energy-efficient CNN Inference",
            "text": "Similarly, when decreasing the numerical precision of the uniform quantization to 4-bits, the RRMSE increases to 28.81% and 7.89% for groups (a) and (b) respectively, which corresponds to an increment of 17.2x and 16x respect the 8-bit uniform quantization. Based on these observations, we can potentially reduce the bitwidth of groups of activations with a more uniform distribution without affecting the accuracy of the network. Consequently, ReDy analyses the distribution of activation groups on-the-fly to determine the optimal numerical precision of the dynamic quantization process of each group.",
            "score": 0.5975433608359267,
            "section_title": "ANALYSIS OF CNN ACTIVATION GROUPS",
            "char_start_offset": 19068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 603
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96533203125
        },
        {
            "corpus_id": "54625607",
            "title": "Deep neural decoders for near term fault-tolerant experiments",
            "text": "inference in Google Search, Google Photos and in DeepMind's AlphaGo against one of the the world's top Go player, Lee Sedol.\n\nIt is claimed that the reduction in precision of a trained neural network from 32-bit float point precision in weights, biases, and arithmetic operations, to only 8-bit fixed point preserves the quality of inference from trained models [88]. This procedure is called network quantization. There is no mathematical reason to believe that the inference quality should hold up under network quantization. However, the intuitive explanation has been that although the training mode is very sensitive to small variations of parameters and hyperparameters, and fluctuations of the high precision weights of the network in individual iterations of training is very small, the resulting trained network is in principle robust to noise in data and weights.\n\nThe challenge in our case is that in quantum error correction, the input data is already at the lowest possible precision (each neuron attains 0 or 1, therefore only using a single bit). Furthermore, an error in the input neurons results in moving from one input syndrome to a completely different one (for instance, as opposed to moving from a high resolution picture to a low resolution, or poorly communicated one in an image processing task). We therefore see the need to experimentally verify whether network quantization is a viable approach to high-performance inference from a DND. Fig. 26 demonstrates an experiment to validate network quantization on a trained DND. Using 32-bit floatpoint precision, the results of Fig. 14 show that the trained DND improves the logical fault rate from 1.95 \u00d7 10 \u22124 obtained by lookup table methods to 9.45 \u00d7 10 \u22125 obtained by the LU-DND with 2 hidden layers. We observe that this improvement is preserved by the quantized networks with 8 bits and even 7 bits of precision using fix-point arithmetic.\n\nWe now explain how the quantized network for this experiment was constructed. Let us assume the available precision is up to k bits. First, the weights and biases of the network are rescaled and rounded to nearest integers such that the resulting parameters are all integers between \u22122 k\u22121 + 1 and 2 k\u22121 stored as signed k-bit integers. Each individual input neuron only requires a single bit since they store",
            "score": 0.597330511352745,
            "section_title": "B. Fast inference from a trained neural network",
            "char_start_offset": 78131,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 362,
                    "end": 366,
                    "matchedPaperCorpusId": "15196840"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76220703125
        },
        {
            "corpus_id": "266551359",
            "title": "Understanding the Potential of FPGA-based Spatial Acceleration for Large Language Model Inference",
            "text": "Bandwidth. Lastly, we investigate how quantization impacts the required memory bandwidth. As shown in Figure 8(c), the low-bit weight quantization can significantly alleviate the demands of off-chip memory access. By reducing the volume of data needed in each cycle, it can achieve a larger compute power , thus leading to a better performance. In particular, quantizing the model to a 2-bit representation yields a performance boost exceeding an order of magnitude when compared to a 16-bit weight quantization scheme. Recent research [7,103] has demonstrated that 4-bit or even 2-bit quantization can be implemented without compromising model accuracy, which makes efficient LLM deployment on FPGAs possible. \n\nInsight V: Low-bit weight quantization can further help alleviate the demands of offchip memory access. For multiple devices, we use the Vicuna-13B model to estimate the performance of 2, 4, and 8 FPGAs based on our analytical model. As shown in Figure 9, the latency can scale well when the number of devices increases. Since we employ a non-blocking communication scheme in our dataflow design as discussed in \u00a7 3.4, communication will not be the bottleneck of the design.",
            "score": 0.5973294291145668,
            "section_title": "Memory",
            "char_start_offset": 37117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 710
                },
                {
                    "start": 713,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1187
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83056640625
        },
        {
            "corpus_id": "216036085",
            "title": "LSQ+: Improving low-bit quantization through learnable offsets and better initialization",
            "text": "A good overview of the basics of quantization is given in [15], where the differences between asymmetric and symmetric quantization are explained. In general, we can classify quantization methods into post-training methods that work without fine-tuning and quantization-aware training methods that need fine-tuning.\n\nPost-training quantization methods [2,29,6] optimize neural networks for quantization without full training and using a little amount of data. [18,4] do this better without using any data at all. Although these methods work well on typical 8-bit quantization, they were not able to achieve good accuracy on very low-bit (2, 3, 4-bit) quantization.\n\nQuantization-aware training generally outperforms these methods on low-bit tasks given enough time to optimize.\n\nSimulated quantization-aware training methods and improvements for these are discussed in Gupta2015,jacob2018cvpr,louizos2018relaxed.\n\nEssentially, operations are added to the neural network computational graph that simulate how quantization would be done on an actual device. Several recent papers improve over these methods by learning the quantization parameters, e.g. QIL [13], TQT [12] and LSQ [7]. This is the approach we build upon in our paper, but a similar asymmetric quantiza-tion scheme and initialization we suggest could be used for any other methods.\n\nIn a parallel line of research, some works [14,16,19] have tried to apply knowledge distillation to quantization resulting in improved performances. Also, some recent work [26] has been done on automatically learning the bit-width alongside of the ranges. Note that our proposed method is orthogonal to these works, and thus it can be jointly used with them. Lastly, several papers have introduced different quantization grids than uniform one we use. In [17] and [27], a logarithmic space or fully free-format quantization space are used to quantize the network. In this paper, we do not consider this, as the hardware implementations for these are simply inefficient, requiring costly lookup table or approximation on runtime",
            "score": 0.5972661310631457,
            "section_title": "Related Work",
            "char_start_offset": 3341,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 352,
                    "end": 355,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 355,
                    "end": 358,
                    "matchedPaperCorpusId": "59413897"
                },
                {
                    "start": 358,
                    "end": 360,
                    "matchedPaperCorpusId": "67750088"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "184487878"
                },
                {
                    "start": 1155,
                    "end": 1159,
                    "matchedPaperCorpusId": "53719799"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.796875
        },
        {
            "corpus_id": "267035013",
            "title": "Model Compression Techniques in Biometrics Applications: A Survey",
            "text": "Using a lower number of bits to represent the weights and activations of the layers that do not have a big negative impact on the performance can highly reduce the computational resources needed at inference time without significant performance degradation. The authors prove that model performance suffers a bigger impact when the first layers are quantized due to the forward propagation of the performed approximations, allowing for stronger quantization when progressing toward the last layers of the network. The final MPQ framework comprised 8, 4 and 2-bit quantization for the initial, middle and final layers, respectively, resulting in a network with only 10% of the computational footprint of the original model and a performance drop of 0.68 percentual points on LFW. Thus, MPQ can be extremely useful in establishing a compromise between achieving good performances and having computationally efficient models. However, determining the MPQ configuration manually will most likely lead to a suboptimal quantization, especially considering the exponential growth of possible configurations with the network size [14], revealing the need to develop methods that automatize the MPQ configuration selection during quantization. \n\nKolf et al. [3] focused on the compression of models trained on the periocular recognition (PR) task through a QAT procedure that performed WAQ at 8, 6 and 4-bit precision. The achieved results show that the more the number of bits of the final representation is reduced, the bigger the verified performance drop, as expected. When b = 8 or b = 6, the performance degradation was small in the identification and verification tasks and, in some cases, the quantized models even slightly surpassed the baseline. For b = 4, the performance suffered an extreme drop. As an example, a MobileFaceNet trained in the identification task suffered a reduction in accuracy from 99.80% in 6-bit quantization to 5.53% in 4-bit quantization. Hence, in this experimental setup, 4-bit quantization is not enough to achieve good performance, showing that this precision level does not allow the model to learn how to encode the relevant features properly. \n\nKolf et al. [41] also compressed PR models in a WAQ framework at 8, 6 and 4-bit precision. The FP models were trained on visible spectrum images, which are widely available and labeled.",
            "score": 0.5965343538980915,
            "section_title": "A. Quantization",
            "char_start_offset": 37214,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1234
                },
                {
                    "start": 1237,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2175
                },
                {
                    "start": 2178,
                    "end": 2268
                },
                {
                    "start": 2269,
                    "end": 2363
                }
            ],
            "ref_mentions": [
                {
                    "start": 1122,
                    "end": 1126,
                    "matchedPaperCorpusId": "232352683"
                },
                {
                    "start": 1249,
                    "end": 1252,
                    "matchedPaperCorpusId": "255997000"
                },
                {
                    "start": 2190,
                    "end": 2194,
                    "matchedPaperCorpusId": "258493730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "227228957",
            "title": "A Tiny CNN Architecture for Medical Face Mask Detection for Resource-Constrained Endpoints",
            "text": "Leveraging quantization techniques is necessary for implementing CNNs on resource-constrained devices. [3] introduced 4-bit training quantization on both activation and weights, achieving accuracies, a few percent less than state-ofthe-art baselines across CNNs. [20] proposed a method which quantizes layer parameters that improve accuracy over existing post-training quantization techniques. [25] proposed an outlier channel splitting (OCS) based method to improve quantization performance without retraining. [5] discussed low bit quantization of neural networks by optimization of constrained Mean Squared Error(MSE) problems for performing hardware-aware quantization. [11] proposed a quantization scheme along with a co-designed training procedure. The paper concluded that inference using integer-only arithmetic performs better than floating-point arithmetic on typical ARM CPUs. [9] proposed Differentiable Soft Quantization (DSQ) to bridge the gap between the full-precision and low-bit networks. The hybrid compression model in [8] uses four major modules, Approximation, Quantization, Pruning, and Coding, which provides 20-30x compression rate with negligible loss in accuracy. The research by [7], [6], and [24] proposed mixed-precision quantization techniques, where more sensitive layers are at higher precision.",
            "score": 0.5961317100356662,
            "section_title": "Quantization",
            "char_start_offset": 5305,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1328
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 106,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 394,
                    "end": 398,
                    "matchedPaperCorpusId": "59413897"
                },
                {
                    "start": 512,
                    "end": 515,
                    "matchedPaperCorpusId": "67750088"
                },
                {
                    "start": 674,
                    "end": 678,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 888,
                    "end": 891,
                    "matchedPaperCorpusId": "199577815"
                },
                {
                    "start": 1039,
                    "end": 1042,
                    "matchedPaperCorpusId": "8328020"
                },
                {
                    "start": 1207,
                    "end": 1210,
                    "matchedPaperCorpusId": "148571720"
                },
                {
                    "start": 1212,
                    "end": 1215,
                    "matchedPaperCorpusId": "207852310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8212890625
        },
        {
            "corpus_id": "277468258",
            "title": "QSViT: A Methodology for Quantizing Spiking Vision Transformers",
            "text": "This step evaluates the impact of applying different precision levels in each network layer on the accuracy, thereby providing useful insights regarding the sensitivity of each layer under quantization. It is important, since understanding layers' sensitivity will guide us to devise an effective quantization setting that leads to a quantized model that can preserve high accuracy with a relatively small memory cost. \n\nTo do this, we consider quantizing the state-of-the-art SViT model (i.e., SDTv2 [14]), thus providing the state-of-the-art study of quantization for SViT models. Specifically, we first identify the quantizable layers in each network stage, which are summarized in Table I. In each stage, there are several blocks, each encompassing several sub-blocks, and each subblock consists of several layers. For CONV-based and Linearbased layers, quantization can be performed on the weights of CONV, DW-CONV, PW-CONV, RepCONV, and Linear MLP layers. Meanwhile, for Transformer-based layers (i.e., SDSA), quantization can be performed on the weights for Query, Key, and Value parameters (i.e., W Q , W K , and W V , respectively). After identifying the quantizable layers, we apply different precision levels (i.e., 16-bit, 12-bit, 8-bit, and 4-bit of weight precision) for each layer, and evaluate their impact on the accuracy. This proposed exploration strategy is summarized in Alg. Process: 4: for (l = 1; l < (L+1); l++) do 5: \n\nN 0 = N et; 6: \n\nfor",
            "score": 0.5957119424495985,
            "section_title": "A. Investigate the Impact of Layer-wise Quantization",
            "char_start_offset": 9489,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 418
                },
                {
                    "start": 421,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1442
                },
                {
                    "start": 1445,
                    "end": 1459
                },
                {
                    "start": 1462,
                    "end": 1465
                }
            ],
            "ref_mentions": [
                {
                    "start": 501,
                    "end": 505,
                    "matchedPaperCorpusId": "268987335"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8759765625
        },
        {
            "corpus_id": "269148715",
            "title": "SNN4Agents: a framework for developing energy-efficient embodied spiking neural networks for autonomous agents",
            "text": "To effectively compress the model size, we perform weight quantization through PTQ with TR rounding scheme.To do this, we first train the given network without quantization, while employing baseline settings for timestep, attention window, and training epoch.For this scenario, we employ 32-bit precision, 20 timestep, 100\u00d7100 attention window, and 200 training epoch.Once the training phase is finished, we perform quantization process to the trained network.Afterward, we perform DSE under different weight precision levels (i.e., 32, 16, 12, 10, 8, 6, and 4 bit) to evaluate their impact on the accuracy; see the parameter settings for DSE in Table 4. Experimental results of DSE are shown in Figure 7, from which we draw the following key observations.\n\n\u2022 In the early of training phase (e.g., \u2264 60 training epoch), the network is still learning new information, hence the accuracy curve is increasing for 16-, 12-, and 10-bit precision levels, as shown by 1 .\n\n\u2022 Employing 16-, 12-, and 10-bit precision levels for SNN weights lead to comparable accuracy to the original SNN model with 32-bit precision (no quantization) after running at least 80 training epoch, as shown by 2 .\n\n\u2022 Employing 8-, 6-, and 4-bit precision levels for SNN weights lead to significant accuracy degradation, as they can only reach about 50% accuracy across training epochs, as shown by 3 .These results indicate that the network is not properly trained.\n\nThese observations expose several key design guides that we should consider when applying quantization.First, selecting the precision level should be performed carefully, so that it does not lead to a significant accuracy degradation which diminishes the benefits of quantization.Second, a 10-bit precision level offers a good trade-off between accuracy and memory footprint as it can achieve comparable accuracy to that of the larger precision levels after running at least 80 training epoch., 32, 16, 12, 10, 8, and 4 bit).",
            "score": 0.5956281474029046,
            "section_title": "Model Compression through Quantization",
            "char_start_offset": 14288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 259
                },
                {
                    "start": 259,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 460
                },
                {
                    "start": 460,
                    "end": 756
                },
                {
                    "start": 758,
                    "end": 964
                },
                {
                    "start": 966,
                    "end": 1183
                },
                {
                    "start": 1185,
                    "end": 1371
                },
                {
                    "start": 1371,
                    "end": 1435
                },
                {
                    "start": 1437,
                    "end": 1540
                },
                {
                    "start": 1540,
                    "end": 1717
                },
                {
                    "start": 1717,
                    "end": 1930
                },
                {
                    "start": 1930,
                    "end": 1962
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91796875
        },
        {
            "corpus_id": "221103745",
            "title": "Leveraging Automated Mixed-Low-Precision Quantization for tiny edge microcontrollers",
            "text": "An automated mixed-precision quantization framework for MCUs operates with a double scope. On one side, (i) the memory requirements of the quantized model must fit the on-chip memory resources of the target MCU and, at the same time, (ii) the quantized model must be linearly quantized to low-bitwidth to be efficiently processed on a general-purpose CPU featuring integer-only arithmetic. \n\nMemory Requirement MCU's on-chip memory distinguishes between RAM and ROM memories. In this context, embedded FLASH memories are classified as ROM memories, because writing operations does not typically occur at runtime. As a design choice, we store the weight parameters in the ROM memory, while intermediate activations values are stored in RAM [18,6,9]. Hence, we derive the following constraints: M1 The memory requirement due to weight parameters, including bias or other stationary values, must fit the system's ROM memory (M ROM ). M2 The RAM memory must store any intermediate activation tensors at runtime. In addition to the input and output tensors of any network layer, tensors of optional parallel branches (e.g. skip connections) must be also kept in memory (M RAM ). \n\nComputational Requirement Low-bitwidth fixed-point inference tasks typically operate on 8-or 16-bit data [15]. The leading software backends PULP-NN [2] (RISCV) and CMix-NN [15] (ARM Cortex M) for mixed-precision inference supports 2-and 4-bit datatypes in addition to the widely used 8-bit compression. Operations on sub-bytes datatypes are software-emulated using 8bit or 16-bit instructions with low overhead. On the other hand, efficient software implementations for other quantization levels, i.e. 3-, 5-, 6-, 7-bits, have not been presented. Hence, we restrict the quantization bitwidth selection to power-of-2 format (2-, 4-and 8-bit) and exploit Uniform linear quantization rules to enable the usage of integer-only arithmetic for computation. This latter implies that quantization levels are uniformly spaced across the quantization range [13].",
            "score": 0.5956130330066778,
            "section_title": "MCU-aware optimization objectives",
            "char_start_offset": 9572,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 389
                },
                {
                    "start": 392,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1173
                },
                {
                    "start": 1176,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2029
                }
            ],
            "ref_mentions": [
                {
                    "start": 1325,
                    "end": 1328,
                    "matchedPaperCorpusId": "218871617"
                },
                {
                    "start": 2024,
                    "end": 2028,
                    "matchedPaperCorpusId": "39867659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.611328125
        },
        {
            "corpus_id": "236087695",
            "title": "A High-Performance Adaptive Quantization Approach for Edge CNN Applications",
            "text": "Although for the l-th layer, both the quantized input activation matrix  \u0302 and weight matrix  \u0302 are in 4-bit integer format, after the layer convolution computations, the output activation may overflow the 4-bit integer range. Therefore, the output activation  +1 needs to be re-normalized and shifted as the following, \n\nNext, by combing Eq.( 2) and Eq.( 8), we have \n\nThen, based on Eq.( 3) and Eq.( 6), the left side of Eq.( 9) becomes \n\nwhere the new bias b \u0303 denotes b  +  w  z A   \u0305  . \n\nFinally, from Eq.( 9) and Eq.( 10), we have the normalized output activation matrix of l-th layer as the following, \n\nThen we obtain the quantized output activation  \u0302+1 of -th by normalizing and then quantizing to a 4-bit integer format as: \n\nAs shown in Fig. 3, we have a complete execution flow of each layer. In general, an intermediate output through convolution and activation function   is normalized by shifting and scaling and then re-quantized into the 4-bit integer format before passing as the input to the next layer. Note that the values of   +1 ,    ,    ,  \u0303 and   +1 are all in 16-bit fixed-point format for proper accuracy, while these parameters take only less than 0.1% of the total memory space. With the above computing procedure, the entire CNN inferencing process can be efficiently executed in integer arithmetic. The experimental results support that the 4-bit quantization strikes the best balance between minimal computing/memory complexity and minimal model accuracy degradation. Next, we discuss how to find the optimal quantization parameters.",
            "score": 0.595553171745305,
            "section_title": "\ud835\udc00 \u0302\ud835\udc59 \u2261 \ud835\udc5e \ud835\udc56\ud835\udc5b\ud835\udc614 (\ud835\udc00 \u0305 \ud835\udc59 ) (\uf037)",
            "char_start_offset": 16396,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 319
                },
                {
                    "start": 322,
                    "end": 367
                },
                {
                    "start": 370,
                    "end": 438
                },
                {
                    "start": 441,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1568
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93701171875
        },
        {
            "corpus_id": "257771898",
            "title": "Energy-efficient Task Adaptation for NLP Edge Inference Leveraging Heterogeneous Memory Architectures",
            "text": "While floating point values are still the most common data representation for DNN training, reduced-precision numerical formats are a good compromise for targeting efficient inference [20]- [22], since quantized operands can significantly decrease data storage and movement costs, as well as reduce the complexity of the hardware used for implementing MAC processing units [23]. The vanilla ALBERT model has been demonstrated to benefit from quantization down to 16-bit floating-point without negatively impacting its inference performance [16]. For our adapter-ALBERT model, we hypothesize that, given the adapter module's proven ability to compensate for accuracy loss caused by pruning, it may also exhibit similar trends for quantization. \n\nWe have designed experiments with quantization configurations in 16-bit and 8-bit fixed-point representations using the CSP-both backbone. Using the conventional notation, Q i,f , to represent the quantization scheme using i bits for integer and sign, and f fractional bits, we focus on Q 3,13 and Q 3,5 . To ensure data consistency, the adapter modules are quantized using the same settings as the backbone. The quantization example provides competitive results across all the considered datasets, reducing the number of bits per operand to Q 3,5 shows a drastic accuracy reduction for some of the QNLI and SST-2 tasks. \n\nmodules in improving or maintaining the inference accuracy is limited by using lower precision quantization. Moreover, we want to verify if adapter modules can learn reduced data information and recover the loss via retraining of the taskspecific parameters. As shown in Figure 5, the Q 3,13 configuration provides competitive accuracy results when compared with the singleprecision floating point (FP32) baseline. On the other hand, the results for Q 3,5 quantization are much less consistent, suggesting that the optimal quantization scheme will dependent on the subset of tasks used by an application.",
            "score": 0.5953250265711745,
            "section_title": "A. Quantization",
            "char_start_offset": 23166,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 742
                },
                {
                    "start": 745,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1972
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 188,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 190,
                    "end": 194,
                    "matchedPaperCorpusId": "44529396"
                },
                {
                    "start": 373,
                    "end": 377,
                    "matchedPaperCorpusId": "85517827"
                },
                {
                    "start": 540,
                    "end": 544,
                    "matchedPaperCorpusId": "237421361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7548828125
        },
        {
            "paperId": "273027f85faea92c1977804ffe33545f26c4199d",
            "corpusId": 267966328,
            "title": "4.6-Bit Quantization for Fast and Accurate Neural Network Inference on CPUs",
            "venue": "Mathematics",
            "year": 2024,
            "referenceCount": 19,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2227-7390/12/5/651/pdf?version=1708681029",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/math12050651?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/math12050651, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2209812418",
                    "name": "A. Trusov"
                },
                {
                    "authorId": "37239110",
                    "name": "E. Limonova"
                },
                {
                    "authorId": "2188704593",
                    "name": "Dmitry P. Nikolaev"
                },
                {
                    "authorId": "35343060",
                    "name": "V. Arlazarov"
                }
            ],
            "abstract": "Quantization is a widespread method for reducing the inference time of neural networks on mobile Central Processing Units (CPUs). Eight-bit quantized networks demonstrate similarly high quality as full precision models and perfectly fit the hardware architecture with one-byte coefficients and thirty-two-bit dot product accumulators. Lower precision quantizations usually suffer from noticeable quality loss and require specific computational algorithms to outperform eight-bit quantization. In this paper, we propose a novel 4.6-bit quantization scheme that allows for more efficient use of CPU resources. This scheme has more quantization bins than four-bit quantization and is more accurate while preserving the computational efficiency of the later (it runs only 4% slower). Our multiplication uses a combination of 16- and 32-bit accumulators and avoids multiplication depth limitation, which the previous 4-bit multiplication algorithm had. The experiments with different convolutional neural networks on CIFAR-10 and ImageNet datasets show that 4.6-bit quantized networks are 1.5\u20131.6 times faster than eight-bit networks on the ARMv8 CPU. Regarding the quality, the results of the 4.6-bit quantized network are close to the mean of four-bit and eight-bit networks of the same architecture. Therefore, 4.6-bit quantization may serve as an intermediate solution between fast and inaccurate low-bit network quantizations and accurate but relatively slow eight-bit ones.",
            "corpus_id": "267966328",
            "text": "Quantization is a widespread method for reducing the inference time of neural networks on mobile Central Processing Units (CPUs). Eight-bit quantized networks demonstrate similarly high quality as full precision models and perfectly fit the hardware architecture with one-byte coefficients and thirty-two-bit dot product accumulators. Lower precision quantizations usually suffer from noticeable quality loss and require specific computational algorithms to outperform eight-bit quantization. In this paper, we propose a novel 4.6-bit quantization scheme that allows for more efficient use of CPU resources. This scheme has more quantization bins than four-bit quantization and is more accurate while preserving the computational efficiency of the later (it runs only 4% slower). Our multiplication uses a combination of 16- and 32-bit accumulators and avoids multiplication depth limitation, which the previous 4-bit multiplication algorithm had. The experiments with different convolutional neural networks on CIFAR-10 and ImageNet datasets show that 4.6-bit quantized networks are 1.5\u20131.6 times faster than eight-bit networks on the ARMv8 CPU. Regarding the quality, the results of the 4.6-bit quantized network are close to the mean of four-bit and eight-bit networks of the same architecture. Therefore, 4.6-bit quantization may serve as an intermediate solution between fast and inaccurate low-bit network quantizations and accurate but relatively slow eight-bit ones.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.97265625
        },
        {
            "paperId": "84ed091b4f7875323e512f17f5fe46eecbf66174",
            "corpusId": 256163866,
            "title": "A 95.6-TOPS/W Deep Learning Inference Accelerator With Per-Vector Scaled 4-bit Quantization in 5 nm",
            "venue": "IEEE Journal of Solid-State Circuits",
            "year": 2023,
            "referenceCount": 66,
            "citationCount": 29,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSSC.2023.3234893?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSSC.2023.3234893, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "153384442",
                    "name": "Ben Keller"
                },
                {
                    "authorId": "3172075",
                    "name": "Rangharajan Venkatesan"
                },
                {
                    "authorId": "2121059",
                    "name": "Steve Dai"
                },
                {
                    "authorId": "2831691",
                    "name": "S. Tell"
                },
                {
                    "authorId": "145736583",
                    "name": "B. Zimmer"
                },
                {
                    "authorId": "2063588",
                    "name": "Charbel Sakr"
                },
                {
                    "authorId": "80724002",
                    "name": "W. Dally"
                },
                {
                    "authorId": "145495407",
                    "name": "C. T. Gray"
                },
                {
                    "authorId": "2125244",
                    "name": "Brucek Khailany"
                }
            ],
            "abstract": "The energy efficiency of deep neural network (DNN) inference can be improved with custom accelerators. DNN inference accelerators often employ specialized hardware techniques to improve energy efficiency, but many of these techniques result in catastrophic accuracy loss on transformer-based DNNs, which have become ubiquitous for natural language processing (NLP) tasks. This article presents a DNN accelerator designed for efficient execution of transformers. The proposed accelerator implements per-vector scaled quantization (VSQ), which employs an independent scale factor for each 64-element vector to enable the use of 4-bit arithmetic with little accuracy loss and low energy overhead. Using a multilevel dataflow to maximize reuse, the 5-nm prototype achieves 95.6 tera-operations per second per Watt (TOPS/W) at 0.46 V on a 4-bit benchmarking layer with VSQ. At a nominal voltage of 0.67 V, the accelerator achieves 1734 inferences/s/W (38.7 TOPS/W) with only 0.7% accuracy loss on BERT-Base and 4714 inferences/s/W (38.6 TOPS/W) with 0.15% accuracy loss on ResNet-50 by using quantization-aware fine-tuning to recover accuracy, demonstrating a practical accelerator design for energy-efficient DNN inference.",
            "corpus_id": "256163866",
            "text": "The energy efficiency of deep neural network (DNN) inference can be improved with custom accelerators. DNN inference accelerators often employ specialized hardware techniques to improve energy efficiency, but many of these techniques result in catastrophic accuracy loss on transformer-based DNNs, which have become ubiquitous for natural language processing (NLP) tasks. This article presents a DNN accelerator designed for efficient execution of transformers. The proposed accelerator implements per-vector scaled quantization (VSQ), which employs an independent scale factor for each 64-element vector to enable the use of 4-bit arithmetic with little accuracy loss and low energy overhead. Using a multilevel dataflow to maximize reuse, the 5-nm prototype achieves 95.6 tera-operations per second per Watt (TOPS/W) at 0.46 V on a 4-bit benchmarking layer with VSQ. At a nominal voltage of 0.67 V, the accelerator achieves 1734 inferences/s/W (38.7 TOPS/W) with only 0.7% accuracy loss on BERT-Base and 4714 inferences/s/W (38.6 TOPS/W) with 0.15% accuracy loss on ResNet-50 by using quantization-aware fine-tuning to recover accuracy, demonstrating a practical accelerator design for energy-efficient DNN inference.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.92822265625
        },
        {
            "paperId": "17813f4082947ec9e3194398f0e71b0e2d854064",
            "corpusId": 249712097,
            "title": "Accelerating Inference and Language Model Fusion of Recurrent Neural Network Transducers via End-to-End 4-bit Quantization",
            "venue": "Interspeech",
            "year": 2022,
            "referenceCount": 29,
            "citationCount": 6,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2206.07882",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.07882, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "152971821",
                    "name": "A. Fasoli"
                },
                {
                    "authorId": "48240124",
                    "name": "Chia-Yu Chen"
                },
                {
                    "authorId": "2140484359",
                    "name": "Mauricio Serrano"
                },
                {
                    "authorId": "1778215",
                    "name": "Swagath Venkataramani"
                },
                {
                    "authorId": "1698208",
                    "name": "G. Saon"
                },
                {
                    "authorId": "2357983",
                    "name": "Xiaodong Cui"
                },
                {
                    "authorId": "144707379",
                    "name": "Brian Kingsbury"
                },
                {
                    "authorId": "33678523",
                    "name": "K. Gopalakrishnan"
                }
            ],
            "abstract": "We report on aggressive quantization strategies that greatly accelerate inference of Recurrent Neural Network Transducers (RNN-T). We use a 4 bit integer representation for both weights and activations and apply Quantization Aware Training (QAT) to retrain the full model (acoustic encoder and language model) and achieve near-iso-accuracy. We show that customized quantization schemes that are tailored to the local properties of the network are essential to achieve good performance while limiting the computational overhead of QAT. Density ratio Language Model fusion has shown remarkable accuracy gains on RNN-T workloads but it severely increases the computational cost of inference. We show that our quantization strategies enable using large beam widths for hypothesis search while achieving streaming-compatible runtimes and a full model compression ratio of 7.6$\\times$ compared to the full precision model. Via hardware simulations, we estimate a 3.4$\\times$ acceleration from FP16 to INT4 for the end-to-end quantized RNN-T inclusive of LM fusion, resulting in a Real Time Factor (RTF) of 0.06. On the NIST Hub5 2000, Hub5 2001, and RT-03 test sets, we retain most of the gains associated with LM fusion, improving the average WER by $>$1.5%.",
            "corpus_id": "249712097",
            "text": "We report on aggressive quantization strategies that greatly accelerate inference of Recurrent Neural Network Transducers (RNN-T). We use a 4 bit integer representation for both weights and activations and apply Quantization Aware Training (QAT) to retrain the full model (acoustic encoder and language model) and achieve near-iso-accuracy. We show that customized quantization schemes that are tailored to the local properties of the network are essential to achieve good performance while limiting the computational overhead of QAT. Density ratio Language Model fusion has shown remarkable accuracy gains on RNN-T workloads but it severely increases the computational cost of inference. We show that our quantization strategies enable using large beam widths for hypothesis search while achieving streaming-compatible runtimes and a full model compression ratio of 7.6$\\times$ compared to the full precision model. Via hardware simulations, we estimate a 3.4$\\times$ acceleration from FP16 to INT4 for the end-to-end quantized RNN-T inclusive of LM fusion, resulting in a Real Time Factor (RTF) of 0.06. On the NIST Hub5 2000, Hub5 2001, and RT-03 test sets, we retain most of the gains associated with LM fusion, improving the average WER by $>$1.5%.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.97998046875
        },
        {
            "paperId": "5aebf4e92c5fd6cc6cf3d4631d9b2881c5f6d581",
            "corpusId": 275852824,
            "title": "Fully Neural Network Low-Bit Quantization and Adjustment of Activation Distributions",
            "venue": "International Conference on the Software Process",
            "year": 2024,
            "referenceCount": 34,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICSP62129.2024.10846012?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICSP62129.2024.10846012, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294335787",
                    "name": "Bing-jie Zhang"
                },
                {
                    "authorId": "2267164095",
                    "name": "Zhihong Yan"
                },
                {
                    "authorId": "2342016079",
                    "name": "Dong Wang"
                }
            ],
            "abstract": "Quantization Aware Training (QAT) can usually achieve better performance than Post-Training Quantization (PTQ) in low-bit quantization due to its parameter learnability. However, deploying the network after low-bit quantization to dedicated hardware is still challenging. In particular, convolutional neural networks maintain 8-bit quantization for some of their sensitive layers when quantized at low bit. To enable the underlying hardware circuitry to support two different computational bitwidth at the same time during hardware deployment leads to an increase in cost; In addition, some target detection networks using LeakyRelu, etc. as the activation function can have poor accuracy after quantization due to the asymmetric distribution of activation values. In this paper, we propose a static 4-bit activation value quantization method for the sensitive layer to unify the quantization bits and an Parameterizable LeakyReLU(PLReLU) activation function to adjust the distribution of activation values to make them symmetric, which reduce the hardware cost and improve the accuracy of the quantization network. Our method outperforms current low-bit quantization methods by 1.11% to 1.72% on ImageNet dataset and by 0.9% on PASCAL VOC dataset, demonstrating the contribution of the design in this paper.",
            "corpus_id": "275852824",
            "text": "Quantization Aware Training (QAT) can usually achieve better performance than Post-Training Quantization (PTQ) in low-bit quantization due to its parameter learnability. However, deploying the network after low-bit quantization to dedicated hardware is still challenging. In particular, convolutional neural networks maintain 8-bit quantization for some of their sensitive layers when quantized at low bit. To enable the underlying hardware circuitry to support two different computational bitwidth at the same time during hardware deployment leads to an increase in cost; In addition, some target detection networks using LeakyRelu, etc. as the activation function can have poor accuracy after quantization due to the asymmetric distribution of activation values. In this paper, we propose a static 4-bit activation value quantization method for the sensitive layer to unify the quantization bits and an Parameterizable LeakyReLU(PLReLU) activation function to adjust the distribution of activation values to make them symmetric, which reduce the hardware cost and improve the accuracy of the quantization network. Our method outperforms current low-bit quantization methods by 1.11% to 1.72% on ImageNet dataset and by 0.9% on PASCAL VOC dataset, demonstrating the contribution of the design in this paper.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9609375
        },
        {
            "paperId": "aca3055770c107c259fb7e98afeec4451fa0e13b",
            "corpusId": 257205844,
            "title": "DyBit: Dynamic Bit-Precision Numbers for Efficient Quantized Neural Network Inference",
            "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
            "year": 2023,
            "referenceCount": 18,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2302.12510",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.12510, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Jiajun Zhou"
                },
                {
                    "authorId": "2110436131",
                    "name": "Jiajun Wu"
                },
                {
                    "authorId": "1939358",
                    "name": "Yizhao Gao"
                },
                {
                    "authorId": "40879066",
                    "name": "Yuhao Ding"
                },
                {
                    "authorId": "144259094",
                    "name": "Chaofan Tao"
                },
                {
                    "authorId": "143771569",
                    "name": "Bo Li"
                },
                {
                    "authorId": "1910352",
                    "name": "Fengbin Tu"
                },
                {
                    "authorId": "145210800",
                    "name": "Kwang-Ting Cheng"
                },
                {
                    "authorId": "1710259",
                    "name": "Hayden Kwok-Hay So"
                },
                {
                    "authorId": "1873081",
                    "name": "Ngai Wong"
                }
            ],
            "abstract": "To accelerate the inference of deep neural networks (DNNs), quantization with low-bitwidth numbers is actively researched. A prominent challenge is to quantize the DNN models into low-bitwidth numbers without significant accuracy degradation, especially at very low bitwidths <inline-formula> <tex-math notation=\"LaTeX\">$( < 8$ </tex-math></inline-formula> bits). This work targets an adaptive data representation with variable-length encoding called DyBit. DyBit can dynamically adjust the precision and range of separate bit-fields to be adapted to the DNN weights/activations distribution. We also propose a hardware-aware quantization framework with a mixed-precision accelerator to tradeoff the inference accuracy and speedup. Experimental results demonstrate that the ImageNet inference accuracy via DyBit is 1.97% higher than the state-of-the-art at 4-bit quantization, and the proposed framework can achieve up to <inline-formula> <tex-math notation=\"LaTeX\">$8.1\\times $ </tex-math></inline-formula> speedup compared with the original ResNet-50 model.",
            "corpus_id": "257205844",
            "text": "To accelerate the inference of deep neural networks (DNNs), quantization with low-bitwidth numbers is actively researched. A prominent challenge is to quantize the DNN models into low-bitwidth numbers without significant accuracy degradation, especially at very low bitwidths <inline-formula> <tex-math notation=\"LaTeX\">$( < 8$ </tex-math></inline-formula> bits). This work targets an adaptive data representation with variable-length encoding called DyBit. DyBit can dynamically adjust the precision and range of separate bit-fields to be adapted to the DNN weights/activations distribution. We also propose a hardware-aware quantization framework with a mixed-precision accelerator to tradeoff the inference accuracy and speedup. Experimental results demonstrate that the ImageNet inference accuracy via DyBit is 1.97% higher than the state-of-the-art at 4-bit quantization, and the proposed framework can achieve up to <inline-formula> <tex-math notation=\"LaTeX\">$8.1\\times $ </tex-math></inline-formula> speedup compared with the original ResNet-50 model.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.96435546875
        },
        {
            "paperId": "370d601d349c55e38cc50fa8c621eedd0cf8a7e4",
            "corpusId": 269090388,
            "title": "Acceleration of Neural Network Inference for Embedded GPU Systems",
            "venue": "International Conference on Big Data and Smart Computing",
            "year": 2024,
            "referenceCount": 1,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/BigComp60711.2024.00069?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/BigComp60711.2024.00069, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2296337030",
                    "name": "Kei Terakura"
                },
                {
                    "authorId": "2164256482",
                    "name": "Qiong Chang"
                },
                {
                    "authorId": "2184332112",
                    "name": "Jun Miyazaki"
                }
            ],
            "abstract": "In this study, we propose a general method for accelerating neural network inference on GPUs for embedded systems. Recently, the TensorRT is widely used for neural network inference on GPUs for embedded systems. However, as an efficient optimization method, a 8-bit quantization is not supported by TensorRT on a Nvidia Jetson Nano GPU. To address this, we proposed a acceleration method that involving quantizing weights and activations without TensorRT. Comparative experiments with TensorRT-optimized frame-works demonstrate that our method effectively accelerate the inference, while maintaing the inference accuracy.",
            "corpus_id": "269090388",
            "text": "In this study, we propose a general method for accelerating neural network inference on GPUs for embedded systems. Recently, the TensorRT is widely used for neural network inference on GPUs for embedded systems. However, as an efficient optimization method, a 8-bit quantization is not supported by TensorRT on a Nvidia Jetson Nano GPU. To address this, we proposed a acceleration method that involving quantizing weights and activations without TensorRT. Comparative experiments with TensorRT-optimized frame-works demonstrate that our method effectively accelerate the inference, while maintaing the inference accuracy.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.6796875
        },
        {
            "paperId": "118d060868d877a32921668084fa8a233f068276",
            "corpusId": 272536140,
            "title": "LUTIN: Efficient Neural Network Inference with Table Lookup",
            "venue": "International Symposium on Low Power Electronics and Design",
            "year": 2024,
            "referenceCount": 10,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3665314.3670804?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3665314.3670804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2320464212",
                    "name": "Shi-Zhe Lin"
                },
                {
                    "authorId": "2144865357",
                    "name": "Yun-Chih Chen"
                },
                {
                    "authorId": "2241998884",
                    "name": "Yuan-Hao Chang"
                },
                {
                    "authorId": "2238678789",
                    "name": "Tei-Wei Kuo"
                },
                {
                    "authorId": "2108489760",
                    "name": "Hsiang-Pang Li"
                }
            ],
            "abstract": "DNN models are becoming increasingly large and complex, but they are also being deployed on commodity devices that require low power and latency but lack specialized accelerators. We introduce LUTIN (LUT-based INference), which reduces the amount of matrix multiplication in DNN inference by converting it into table lookups. LUTIN's innovation is its use of hyperparameter optimization to refine the quantization process and vector partitioning, allowing it to run efficiently on a variety of hardware. By reducing off-chip memory lookups and designing a cache-efficient data layout, LUTIN reduces energy consumption while increasing the use of available CPU cache, even on devices with limited processing power. Our approach goes beyond the traditional limitations of 8-bit quantization, investigating lower bit-widths to further reduce LUT size while meeting accuracy requirements. Experimental results show that LUTIN achieves up to a 2.34x speedup in latency and a 2.04x improvement in energy efficiency over full-precision models.",
            "corpus_id": "272536140",
            "text": "DNN models are becoming increasingly large and complex, but they are also being deployed on commodity devices that require low power and latency but lack specialized accelerators. We introduce LUTIN (LUT-based INference), which reduces the amount of matrix multiplication in DNN inference by converting it into table lookups. LUTIN's innovation is its use of hyperparameter optimization to refine the quantization process and vector partitioning, allowing it to run efficiently on a variety of hardware. By reducing off-chip memory lookups and designing a cache-efficient data layout, LUTIN reduces energy consumption while increasing the use of available CPU cache, even on devices with limited processing power. Our approach goes beyond the traditional limitations of 8-bit quantization, investigating lower bit-widths to further reduce LUT size while meeting accuracy requirements. Experimental results show that LUTIN achieves up to a 2.34x speedup in latency and a 2.04x improvement in energy efficiency over full-precision models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.875
        },
        {
            "paperId": "b07cd85c825014bb6ae81724c7c2e47a92da8de5",
            "corpusId": 268142399,
            "title": "CANET: Quantized Neural Network Inference With 8-bit Carry-Aware Accumulator",
            "venue": "IEEE Access",
            "year": 2024,
            "referenceCount": 32,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10445180.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2024.3369889?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2024.3369889, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2289524715",
                    "name": "Jingxuan Yang"
                },
                {
                    "authorId": "2289215538",
                    "name": "Xiaoqin Wang"
                },
                {
                    "authorId": "2290028987",
                    "name": "Yiying Jiang"
                }
            ],
            "abstract": "Neural network quantization represents weights and activations with few bits, greatly reducing the overhead of multiplications. However, due to the recursive accumulation operations, high-precision accumulators are still required in multiply-accumulate (MAC) units to avoid overflow, incurring significant computational overhead. This constraint limits the efficient deployment of quantized NNs on resource-constrained platforms. To address this problem, we present a novel framework named CANET, which adapts the 8-bit quantized model to execute MAC operations with 8-bit accumulators. CANET not only employs 8-bit carry-aware accumulators to represent overflow data correctly, but also adaptively learns the optimal format per layer to minimize truncation errors. Meanwhile, a weight-oriented reordering method is developed to reduce the transfer length of the carry. CANET is evaluated on three networks in the ImageNet classification task, where comparable performance with state-of-the-art methods is realized. Finally, we implement the proposed architecture on a custom hardware platform, demonstrating a reduction of 40% in power and 49% in area compared with the MAC unit with 32-bit accumulators.",
            "corpus_id": "268142399",
            "text": "Neural network quantization represents weights and activations with few bits, greatly reducing the overhead of multiplications. However, due to the recursive accumulation operations, high-precision accumulators are still required in multiply-accumulate (MAC) units to avoid overflow, incurring significant computational overhead. This constraint limits the efficient deployment of quantized NNs on resource-constrained platforms. To address this problem, we present a novel framework named CANET, which adapts the 8-bit quantized model to execute MAC operations with 8-bit accumulators. CANET not only employs 8-bit carry-aware accumulators to represent overflow data correctly, but also adaptively learns the optimal format per layer to minimize truncation errors. Meanwhile, a weight-oriented reordering method is developed to reduce the transfer length of the carry. CANET is evaluated on three networks in the ImageNet classification task, where comparable performance with state-of-the-art methods is realized. Finally, we implement the proposed architecture on a custom hardware platform, demonstrating a reduction of 40% in power and 49% in area compared with the MAC unit with 32-bit accumulators.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7890625
        },
        {
            "paperId": "6dfa59db86a0ddeef21af1a6a28d15228a28adf6",
            "corpusId": 275038383,
            "title": "The Impact of 8- and 4-Bit Quantization on the Accuracy and Silicon Area Footprint of Tiny Neural Networks",
            "venue": "Electronics",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics14010014?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics14010014, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2337212061",
                    "name": "Pawe\u0142 Tumialis"
                },
                {
                    "authorId": "2337207657",
                    "name": "Marcel Skierkowski"
                },
                {
                    "authorId": "2337211715",
                    "name": "Jakub Przychodny"
                },
                {
                    "authorId": "2337209535",
                    "name": "Pawe\u0142 Obszarski"
                }
            ],
            "abstract": "In the field of embedded and edge devices, efforts have been made to make deep neural network models smaller due to the limited size of the available memory and the low computational efficiency. Typical model footprints are under 100 KB. However, for some applications, models of this size are too large. In low-voltage sensors, signals must be processed, classified or predicted with an order of magnitude smaller memory. Model downsizing can be performed by limiting the number of model parameters or quantizing their weights. These types of operations have a negative impact on the accuracy of the deep network. This study tested the effect of model downscaling techniques on accuracy. The main idea was to reduce neural network models to 3 k parameters or less. Tests were conducted on three different neural network architectures in the context of three separate research problems, modeling real tasks for small networks. The impact of the reduction in the accuracy of the network depends mainly on its initial size. For a network reduced from 40 k parameters, a decrease in accuracy of 16 percentage points was achieved, and for a network with 20 k parameters, a decrease of 8 points was achieved. To obtain the best results, knowledge distillation and quantization-aware training methods were used for training. Thanks to this, the accuracy of the 4-bit networks did not differ significantly from the 8-bit ones and their results were approximately four percentage points worse than those of the full precision networks. For the fully connected network, synthesis to ASIC (application-specific integrated circuit) was also performed to demonstrate the reduction in the silicon area occupied by the model. The 4-bit quantization limits the silicon area footprint by 90%.",
            "corpus_id": "275038383",
            "text": "In the field of embedded and edge devices, efforts have been made to make deep neural network models smaller due to the limited size of the available memory and the low computational efficiency. Typical model footprints are under 100 KB. However, for some applications, models of this size are too large. In low-voltage sensors, signals must be processed, classified or predicted with an order of magnitude smaller memory. Model downsizing can be performed by limiting the number of model parameters or quantizing their weights. These types of operations have a negative impact on the accuracy of the deep network. This study tested the effect of model downscaling techniques on accuracy. The main idea was to reduce neural network models to 3 k parameters or less. Tests were conducted on three different neural network architectures in the context of three separate research problems, modeling real tasks for small networks. The impact of the reduction in the accuracy of the network depends mainly on its initial size. For a network reduced from 40 k parameters, a decrease in accuracy of 16 percentage points was achieved, and for a network with 20 k parameters, a decrease of 8 points was achieved. To obtain the best results, knowledge distillation and quantization-aware training methods were used for training. Thanks to this, the accuracy of the 4-bit networks did not differ significantly from the 8-bit ones and their results were approximately four percentage points worse than those of the full precision networks. For the fully connected network, synthesis to ASIC (application-specific integrated circuit) was also performed to demonstrate the reduction in the silicon area occupied by the model. The 4-bit quantization limits the silicon area footprint by 90%.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.96728515625
        },
        {
            "paperId": "2a682d3ecda1a3003d5b249867eae53c35c35e68",
            "corpusId": 251000444,
            "title": "A 17\u201395.6 TOPS/W Deep Learning Inference Accelerator with Per-Vector Scaled 4-bit Quantization for Transformers in 5nm",
            "venue": "2022 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits)",
            "year": 2022,
            "referenceCount": 0,
            "citationCount": 31,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/vlsitechnologyandcir46769.2022.9830277?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/vlsitechnologyandcir46769.2022.9830277, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "153384442",
                    "name": "Ben Keller"
                },
                {
                    "authorId": "3172075",
                    "name": "Rangharajan Venkatesan"
                },
                {
                    "authorId": "2121059",
                    "name": "Steve Dai"
                },
                {
                    "authorId": "2831691",
                    "name": "S. Tell"
                },
                {
                    "authorId": "145736583",
                    "name": "B. Zimmer"
                },
                {
                    "authorId": "80724002",
                    "name": "W. Dally"
                },
                {
                    "authorId": "145495407",
                    "name": "C. T. Gray"
                },
                {
                    "authorId": "2125244",
                    "name": "Brucek Khailany"
                }
            ],
            "abstract": "We present a deep neural network (DNN) accelerator designed for efficient execution of transformer-based DNNs, which have become ubiquitous for natural language processing tasks. DNN inference accelerators often employ specialized hardware techniques such as reduced precision to improve energy efficiency, but many of these techniques result in catastrophic accuracy loss on transformers. The proposed accelerator supports per-vector scaled quantization and approximate softmax to enable the use of 4-bit arithmetic with little accuracy loss. The 5nm prototype achieves 95.6 TOPS/W in benchmarking and 1711 inferences/s/W with only 0.7% accuracy loss on BERT, demonstrating a practical accelerator design for energy-efficient inference with transformers.",
            "corpus_id": "251000444",
            "text": "We present a deep neural network (DNN) accelerator designed for efficient execution of transformer-based DNNs, which have become ubiquitous for natural language processing tasks. DNN inference accelerators often employ specialized hardware techniques such as reduced precision to improve energy efficiency, but many of these techniques result in catastrophic accuracy loss on transformers. The proposed accelerator supports per-vector scaled quantization and approximate softmax to enable the use of 4-bit arithmetic with little accuracy loss. The 5nm prototype achieves 95.6 TOPS/W in benchmarking and 1711 inferences/s/W with only 0.7% accuracy loss on BERT, demonstrating a practical accelerator design for energy-efficient inference with transformers.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.92822265625
        },
        {
            "paperId": "aff7b7b5ed9af1ea5c9469ac5a274fcf1c486e25",
            "corpusId": 256416025,
            "title": "Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 53,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2301.13330",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.13330, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2064431971",
                    "name": "Deepika Bablani"
                },
                {
                    "authorId": "46571359",
                    "name": "J. McKinstry"
                },
                {
                    "authorId": "2357931",
                    "name": "S. K. Esser"
                },
                {
                    "authorId": "2730753",
                    "name": "R. Appuswamy"
                },
                {
                    "authorId": "1944330",
                    "name": "D. Modha"
                }
            ],
            "abstract": "For efficient neural network inference, it is desirable to achieve state-of-the-art accuracy with the simplest networks requiring the least computation, memory, and power. Quantizing networks to lower precision is a powerful technique for simplifying networks. As each layer of a network may have different sensitivity to quantization, mixed precision quantization methods selectively tune the precision of individual layers to achieve a minimum drop in task performance (e.g., accuracy). To estimate the impact of layer precision choice on task performance, two methods are introduced: i) Entropy Approximation Guided Layer selection (EAGL) is fast and uses the entropy of the weight distribution, and ii) Accuracy-aware Layer Precision Selection (ALPS) is straightforward and relies on single epoch fine-tuning after layer precision reduction. Using EAGL and ALPS for layer precision selection, full-precision accuracy is recovered with a mix of 4-bit and 2-bit layers for ResNet-50, ResNet-101 and BERT-base transformer networks, demonstrating enhanced performance across the entire accuracy-throughput frontier. The techniques demonstrate better performance than existing techniques in several commensurate comparisons. Notably, this is accomplished with significantly lesser computational time required to reach a solution.",
            "corpus_id": "256416025",
            "text": "For efficient neural network inference, it is desirable to achieve state-of-the-art accuracy with the simplest networks requiring the least computation, memory, and power. Quantizing networks to lower precision is a powerful technique for simplifying networks. As each layer of a network may have different sensitivity to quantization, mixed precision quantization methods selectively tune the precision of individual layers to achieve a minimum drop in task performance (e.g., accuracy). To estimate the impact of layer precision choice on task performance, two methods are introduced: i) Entropy Approximation Guided Layer selection (EAGL) is fast and uses the entropy of the weight distribution, and ii) Accuracy-aware Layer Precision Selection (ALPS) is straightforward and relies on single epoch fine-tuning after layer precision reduction. Using EAGL and ALPS for layer precision selection, full-precision accuracy is recovered with a mix of 4-bit and 2-bit layers for ResNet-50, ResNet-101 and BERT-base transformer networks, demonstrating enhanced performance across the entire accuracy-throughput frontier. The techniques demonstrate better performance than existing techniques in several commensurate comparisons. Notably, this is accomplished with significantly lesser computational time required to reach a solution.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.939453125
        },
        {
            "paperId": "e533999b924c117919396b2e41c0fc44b77e15fd",
            "corpusId": 268514417,
            "title": "Wino Vidi Vici: Conquering Numerical Instability of 8-bit Winograd Convolution for Accurate Inference Acceleration on Edge",
            "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
            "year": 2024,
            "referenceCount": 30,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/WACV57701.2024.00013?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/WACV57701.2024.00013, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2182418804",
                    "name": "Pierpaolo Mor\u00ec"
                },
                {
                    "authorId": "2045300041",
                    "name": "Lukas Frickenstein"
                },
                {
                    "authorId": "2218176080",
                    "name": "Shambhavi Balamuthu Sampath"
                },
                {
                    "authorId": "2292048692",
                    "name": "Moritz Thoma"
                },
                {
                    "authorId": "1752876377",
                    "name": "Nael Fasfous"
                },
                {
                    "authorId": "9261189",
                    "name": "M. Vemparala"
                },
                {
                    "authorId": "118715093",
                    "name": "Alexander Frickenstein"
                },
                {
                    "authorId": "2292045203",
                    "name": "Christian Unger"
                },
                {
                    "authorId": "2241645071",
                    "name": "Walter Stechele"
                },
                {
                    "authorId": "2292047523",
                    "name": "Daniel Mueller-Gritschneder"
                },
                {
                    "authorId": "2241620074",
                    "name": "Claudio Passerone"
                }
            ],
            "abstract": "Winograd-based convolution can reduce the total number of operations needed for convolutional neural network (CNN) inference on edge devices. Most edge hardware accelerators use low-precision, 8-bit integer arithmetic units to improve energy efficiency and latency. This makes CNN quantization a critical step before deploying the model on such an edge device. To extract the benefits of fast Winograd-based convolution and efficient integer quantization, the two approaches must be combined. Research has shown that the transform required to execute convolutions in the Winograd domain results in numerical instability and severe accuracy degradation when combined with quantization, making the two techniques incompatible on edge hardware. This paper proposes a novel training scheme to achieve efficient Winograd-accelerated, quantized CNNs. 8-bit quantization is applied to all the intermediate results of the Winograd convolution without sacrificing task-related accuracy. This is achieved by introducing clipping factors in the intermediate quantization stages as well as using the complex numerical system to improve the transform. We achieve 2.8\u00d7 and 2.1\u00d7 reduction in MAC operations on ResNet-20-CIFAR-10 and ResNet-18-ImageNet, respectively, with no accuracy degradation.",
            "corpus_id": "268514417",
            "text": "Winograd-based convolution can reduce the total number of operations needed for convolutional neural network (CNN) inference on edge devices. Most edge hardware accelerators use low-precision, 8-bit integer arithmetic units to improve energy efficiency and latency. This makes CNN quantization a critical step before deploying the model on such an edge device. To extract the benefits of fast Winograd-based convolution and efficient integer quantization, the two approaches must be combined. Research has shown that the transform required to execute convolutions in the Winograd domain results in numerical instability and severe accuracy degradation when combined with quantization, making the two techniques incompatible on edge hardware. This paper proposes a novel training scheme to achieve efficient Winograd-accelerated, quantized CNNs. 8-bit quantization is applied to all the intermediate results of the Winograd convolution without sacrificing task-related accuracy. This is achieved by introducing clipping factors in the intermediate quantization stages as well as using the complex numerical system to improve the transform. We achieve 2.8\u00d7 and 2.1\u00d7 reduction in MAC operations on ResNet-20-CIFAR-10 and ResNet-18-ImageNet, respectively, with no accuracy degradation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7314453125
        },
        {
            "paperId": "650a37564509213680e4f0eb2de2555479ca8e46",
            "corpusId": 275213566,
            "title": "SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 45,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2411.10958?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2411.10958, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324058513",
                    "name": "Jintao Zhang"
                },
                {
                    "authorId": "2331494647",
                    "name": "Haofeng Huang"
                },
                {
                    "authorId": "2324080276",
                    "name": "Pengle Zhang"
                },
                {
                    "authorId": "2324564942",
                    "name": "Jia Wei"
                },
                {
                    "authorId": "2287800407",
                    "name": "Jun Zhu"
                },
                {
                    "authorId": "2276707",
                    "name": "Jianfei Chen"
                }
            ],
            "abstract": "Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. SageAttention utilizes 8-bit matrix multiplication, 16-bit matrix multiplication with 16-bit accumulator, and precision-enhancing methods, implementing an accurate and 2x speedup kernel compared to FlashAttention2. To further enhance the efficiency of attention computation while maintaining precision, we propose SageAttention2 , which utilizes significantly faster 4-bit matrix multiplication (Matmul) alongside additional precision-enhancing techniques. First, we propose to quantize matrixes ( Q, K ) to INT4 in a warp-level granularity and quantize matrixes ( (cid:101) P, V ) to FP8. Second, we propose a method to smooth Q and V , enhancing the accuracy of attention with INT4 QK and FP8 PV . Third, we analyze the quantization accuracy across timesteps and layers, then propose an adaptive quantization method to ensure the end-to-end metrics over various models. The operations per second (OPS) of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on RTX4090, respectively. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics loss across diverse models\u2014including those for large language processing, image generation, and video generation. The codes are available at https://github.com/thu-ml/SageAttention .",
            "corpus_id": "275213566",
            "text": "Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. SageAttention utilizes 8-bit matrix multiplication, 16-bit matrix multiplication with 16-bit accumulator, and precision-enhancing methods, implementing an accurate and 2x speedup kernel compared to FlashAttention2. To further enhance the efficiency of attention computation while maintaining precision, we propose SageAttention2 , which utilizes significantly faster 4-bit matrix multiplication (Matmul) alongside additional precision-enhancing techniques. First, we propose to quantize matrixes ( Q, K ) to INT4 in a warp-level granularity and quantize matrixes ( (cid:101) P, V ) to FP8. Second, we propose a method to smooth Q and V , enhancing the accuracy of attention with INT4 QK and FP8 PV . Third, we analyze the quantization accuracy across timesteps and layers, then propose an adaptive quantization method to ensure the end-to-end metrics over various models. The operations per second (OPS) of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on RTX4090, respectively. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics loss across diverse models\u2014including those for large language processing, image generation, and video generation. The codes are available at https://github.com/thu-ml/SageAttention .",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.91455078125
        },
        {
            "paperId": "85e363c67a54acc97128da8c7b33c0c9df45c006",
            "corpusId": 278769955,
            "title": "Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference",
            "venue": "",
            "year": 2025,
            "referenceCount": 57,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.14638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51193964",
                    "name": "Tomer Gafni"
                },
                {
                    "authorId": "2160627835",
                    "name": "A. Karnieli"
                },
                {
                    "authorId": "48967743",
                    "name": "Yair Hanani"
                }
            ],
            "abstract": "Deep neural networks have achieved state-of-the-art results in a wide range of applications, from natural language processing and computer vision to speech recognition. However, as tasks become increasingly complex, model sizes continue to grow, posing challenges in latency and memory efficiency. To meet these constraints, post-training quantization has emerged as a promising solution. In this paper, we propose a novel hardware-efficient quantization and inference scheme that exploits hardware advantages with minimal accuracy degradation. Specifically, we introduce a W4A8 scheme, where weights are quantized and stored using 4-bit integer precision, and inference computations are performed using 8-bit floating-point arithmetic, demonstrating significant speedups and improved memory utilization compared to 16-bit operations, applicable on various modern accelerators. To mitigate accuracy loss, we develop a novel quantization algorithm, dubbed Dual Precision Quantization (DPQ), that leverages the unique structure of our scheme without introducing additional inference overhead. Experimental results demonstrate improved performance (i.e., increased throughput) while maintaining tolerable accuracy degradation relative to the full-precision model.",
            "corpus_id": "278769955",
            "text": "Deep neural networks have achieved state-of-the-art results in a wide range of applications, from natural language processing and computer vision to speech recognition. However, as tasks become increasingly complex, model sizes continue to grow, posing challenges in latency and memory efficiency. To meet these constraints, post-training quantization has emerged as a promising solution. In this paper, we propose a novel hardware-efficient quantization and inference scheme that exploits hardware advantages with minimal accuracy degradation. Specifically, we introduce a W4A8 scheme, where weights are quantized and stored using 4-bit integer precision, and inference computations are performed using 8-bit floating-point arithmetic, demonstrating significant speedups and improved memory utilization compared to 16-bit operations, applicable on various modern accelerators. To mitigate accuracy loss, we develop a novel quantization algorithm, dubbed Dual Precision Quantization (DPQ), that leverages the unique structure of our scheme without introducing additional inference overhead. Experimental results demonstrate improved performance (i.e., increased throughput) while maintaining tolerable accuracy degradation relative to the full-precision model.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.966796875
        },
        {
            "paperId": "f4773aee0fac0a8848861794b26d0244f8c5aabc",
            "corpusId": 264987945,
            "title": "Eciton: Very Low-power Recurrent Neural Network Accelerator for Real-time Inference at the Edge",
            "venue": "ACM Transactions on Reconfigurable Technology and Systems",
            "year": 2023,
            "referenceCount": 80,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3629979?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3629979, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2140965400",
                    "name": "Jeffrey Chen"
                },
                {
                    "authorId": "2265147022",
                    "name": "Sang-Woo Jun"
                },
                {
                    "authorId": "2154321011",
                    "name": "Sehwan Hong"
                },
                {
                    "authorId": "2139529029",
                    "name": "Warrick He"
                },
                {
                    "authorId": "1958798",
                    "name": "Jinyeong Moon"
                }
            ],
            "abstract": "This article presents Eciton, a very low-power recurrent neural network accelerator for time series data within low-power edge sensor nodes, achieving real-time inference with a power consumption of 17 mW under load. Eciton reduces memory and chip resource requirements via 8-bit quantization and hard sigmoid activation, allowing the accelerator as well as the recurrent neural network model parameters to fit in a low-cost, low-power Lattice iCE40 UP5K FPGA. We evaluate Eciton on multiple, established time-series classification applications including predictive maintenance of mechanical systems, sound classification, and intrusion detection for IoT nodes. Binary and multi-class classification edge models are explored, demonstrating that Eciton can adapt to a variety of deployable environments and remote use cases. Eciton demonstrates real-time processing at a very low power consumption with minimal loss of accuracy on multiple inference scenarios with differing characteristics, while achieving competitive power efficiency against the state-of-the-art of similar scale. We show that the addition of this accelerator actually reduces the power budget of the sensor node by reducing power-hungry wireless transmission. The resulting power budget of the sensor node is small enough to be powered by a power harvester, potentially allowing it to run indefinitely without a battery or periodic maintenance.",
            "corpus_id": "264987945",
            "text": "This article presents Eciton, a very low-power recurrent neural network accelerator for time series data within low-power edge sensor nodes, achieving real-time inference with a power consumption of 17 mW under load. Eciton reduces memory and chip resource requirements via 8-bit quantization and hard sigmoid activation, allowing the accelerator as well as the recurrent neural network model parameters to fit in a low-cost, low-power Lattice iCE40 UP5K FPGA. We evaluate Eciton on multiple, established time-series classification applications including predictive maintenance of mechanical systems, sound classification, and intrusion detection for IoT nodes. Binary and multi-class classification edge models are explored, demonstrating that Eciton can adapt to a variety of deployable environments and remote use cases. Eciton demonstrates real-time processing at a very low power consumption with minimal loss of accuracy on multiple inference scenarios with differing characteristics, while achieving competitive power efficiency against the state-of-the-art of similar scale. We show that the addition of this accelerator actually reduces the power budget of the sensor node by reducing power-hungry wireless transmission. The resulting power budget of the sensor node is small enough to be powered by a power harvester, potentially allowing it to run indefinitely without a battery or periodic maintenance.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.464111328125
        },
        {
            "paperId": "1061f069b1db521f64ab51af15870781a367dcec",
            "corpusId": 253461967,
            "title": "A 4-bit Integer-Only Neural Network Quantization Method Based on Shift Batch Normalization",
            "venue": "International Symposium on Circuits and Systems",
            "year": 2022,
            "referenceCount": 10,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ISCAS48785.2022.9938013?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ISCAS48785.2022.9938013, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2892749",
                    "name": "Qingyu Guo"
                },
                {
                    "authorId": "39658679",
                    "name": "Xiaoxin Cui"
                },
                {
                    "authorId": "2155126781",
                    "name": "Jian Zhang"
                },
                {
                    "authorId": "2190576149",
                    "name": "Aifei Zhang"
                },
                {
                    "authorId": "30944407",
                    "name": "Xinjie Guo"
                },
                {
                    "authorId": "2156166711",
                    "name": "Yuan Wang"
                }
            ],
            "abstract": "Neural networks are powerful, but at the cost of huge amounts of computation. Deploying neural networks on edge devices is especially challenging. Quantization is a possible solution to alleviate the huge cost, while most quantization methods are not sufficiently hardware-friendly. In this paper, we proposed an integer-only quantization method. With no division or big integer multiplication, this quantization method is suitable to be deployed on co-designed hardware platforms. We applied 4-bit quantization on some classical networks and corresponding datasets. On MNIST, CIFAR10 and CFAR100, quantization networks perform as well as original networks. On SpeechCommands, accuracy error induced by quantization is 0.16%. We also deployed quantized networks under OpenCL framework and on a flash-based in-memory-computing chip to verify this method\u2019s feasibility.",
            "corpus_id": "253461967",
            "text": "Neural networks are powerful, but at the cost of huge amounts of computation. Deploying neural networks on edge devices is especially challenging. Quantization is a possible solution to alleviate the huge cost, while most quantization methods are not sufficiently hardware-friendly. In this paper, we proposed an integer-only quantization method. With no division or big integer multiplication, this quantization method is suitable to be deployed on co-designed hardware platforms. We applied 4-bit quantization on some classical networks and corresponding datasets. On MNIST, CIFAR10 and CFAR100, quantization networks perform as well as original networks. On SpeechCommands, accuracy error induced by quantization is 0.16%. We also deployed quantized networks under OpenCL framework and on a flash-based in-memory-computing chip to verify this method\u2019s feasibility.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.884765625
        },
        {
            "paperId": "8c87b602e25b4d586db9c5d2f5dad80fba3487a4",
            "corpusId": 273105172,
            "title": "Enhancing CNN Inference Time and Reducing Latency on Edge and Resource-Constrained Systems through Quantization",
            "venue": "2024 Second International Conference on Networks, Multimedia and Information Technology (NMITCON)",
            "year": 2024,
            "referenceCount": 21,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/NMITCON62075.2024.10699069?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/NMITCON62075.2024.10699069, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2320856387",
                    "name": "Gaurav Upadhyay"
                },
                {
                    "authorId": "2324125310",
                    "name": "C. S. Prasad"
                },
                {
                    "authorId": "2324123378",
                    "name": "V. Akash"
                },
                {
                    "authorId": "2260279511",
                    "name": "Ayush Kumar Singh"
                },
                {
                    "authorId": "2324126236",
                    "name": "R. Monish Sai"
                },
                {
                    "authorId": "2320856080",
                    "name": "Srijan Ghosal"
                },
                {
                    "authorId": "2285920010",
                    "name": "Subrat Kar"
                },
                {
                    "authorId": "2324121550",
                    "name": "S. H. Shantala"
                },
                {
                    "authorId": "2324124330",
                    "name": "J. Thakur LalitKrushna"
                },
                {
                    "authorId": "2324124433",
                    "name": "S. Srividhya"
                },
                {
                    "authorId": "2320859157",
                    "name": "Sunil Alias Balwantrao"
                }
            ],
            "abstract": "Systems that use Deep Learning (DL) models extensively utilize cloud computing for inference tasks in various domains such as traffic monitoring, healthcare, and IoT. However, applications like autonomous vehicles, surveillance systems, and spacecraft are transitioning towards edge computing due to band-width limitations, transmission delays, and network connectivity issues. Edge computing mitigates these challenges by reducing latency through local data and model processing on the device. Implementing Deep Neural Networks (DNNs) on edge devices faces resource constraints, such as limited memory, computing power, etc. DNNs employ 32-bit floating-point precision for accuracy, leading to inflated model sizes. Quantization offers a solution by converting high-precision floating-point (FP) values to lower-precision or integer (INT) values, focusing on throughput and improving latency. This paper presents a comparative study of the accuracy and performance of 64-bit, 32-bit, and 16-bit floating-point instructions, along with 8-bit integer instructions, using Post Training Quantization (PTQ) and Quantization-Aware Training (QAT), on multiple Nets including CustomNets, which was inferenced on a GPU as well as a Xilinx Deep Processing Unit (DPU). The models were evaluated on a sample of the EuroSat Remote Sensing dataset. Quantizing models to FP16 and INT8 resulted in $2-3 x$ and $4 x$ faster inferencing, respectively, with a negligible decrease in accuracy of $1-4 \\%$. FP64 exhibited a 2 $3 x$ decrease in speed but a slight accuracy improvement ($2 \\%$). On the DPU, models showed minimal accuracy degradation of about $1 \\%$. Overall, model size decreased by a constant $2 x$ and 4x from FP32 to FP16 and INT8, respectively, while increasing by $2 x$ for FP64. This reduction in size, with negligible loss in accuracy enables onboard storage along withfaster and accurate inferencing on resource constraint systems.",
            "corpus_id": "273105172",
            "text": "Systems that use Deep Learning (DL) models extensively utilize cloud computing for inference tasks in various domains such as traffic monitoring, healthcare, and IoT. However, applications like autonomous vehicles, surveillance systems, and spacecraft are transitioning towards edge computing due to band-width limitations, transmission delays, and network connectivity issues. Edge computing mitigates these challenges by reducing latency through local data and model processing on the device. Implementing Deep Neural Networks (DNNs) on edge devices faces resource constraints, such as limited memory, computing power, etc. DNNs employ 32-bit floating-point precision for accuracy, leading to inflated model sizes. Quantization offers a solution by converting high-precision floating-point (FP) values to lower-precision or integer (INT) values, focusing on throughput and improving latency. This paper presents a comparative study of the accuracy and performance of 64-bit, 32-bit, and 16-bit floating-point instructions, along with 8-bit integer instructions, using Post Training Quantization (PTQ) and Quantization-Aware Training (QAT), on multiple Nets including CustomNets, which was inferenced on a GPU as well as a Xilinx Deep Processing Unit (DPU). The models were evaluated on a sample of the EuroSat Remote Sensing dataset. Quantizing models to FP16 and INT8 resulted in $2-3 x$ and $4 x$ faster inferencing, respectively, with a negligible decrease in accuracy of $1-4 \\%$. FP64 exhibited a 2 $3 x$ decrease in speed but a slight accuracy improvement ($2 \\%$). On the DPU, models showed minimal accuracy degradation of about $1 \\%$. Overall, model size decreased by a constant $2 x$ and 4x from FP32 to FP16 and INT8, respectively, while increasing by $2 x$ for FP64. This reduction in size, with negligible loss in accuracy enables onboard storage along withfaster and accurate inferencing on resource constraint systems.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.82421875
        },
        {
            "paperId": "bf5f5e3075e70d2e3b07560e715de84cd645c6e9",
            "corpusId": 266085518,
            "title": "AQ2PNN: Enabling Two-party Privacy-Preserving Deep Neural Network Inference with Adaptive Quantization",
            "venue": "Micro",
            "year": 2023,
            "referenceCount": 58,
            "citationCount": 15,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3613424.3614297",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3613424.3614297?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3613424.3614297, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1491625900",
                    "name": "Yukui Luo"
                },
                {
                    "authorId": "2271735604",
                    "name": "Nuo Xu"
                },
                {
                    "authorId": "2266280214",
                    "name": "Hongwu Peng"
                },
                {
                    "authorId": "2144523645",
                    "name": "Chenghong Wang"
                },
                {
                    "authorId": "2125026562",
                    "name": "Shijin Duan"
                },
                {
                    "authorId": "145460048",
                    "name": "Kaleel Mahmood"
                },
                {
                    "authorId": "2246993163",
                    "name": "Wujie Wen"
                },
                {
                    "authorId": "2271995525",
                    "name": "Caiwen Ding"
                },
                {
                    "authorId": "2143359817",
                    "name": "Xiaolin Xu"
                }
            ],
            "abstract": "The growing prevalence of Machine Learning as a Service (MLaaS) enables a wide range of applications but simultaneously raises numerous security and privacy concerns. A key issue involves the potential privacy exposure of involved parties, such as the customer\u2019s input data and the vendor\u2019s model. Consequently, two-party computing (2PC) has emerged as a promising solution to safeguard the privacy of different parties during deep neural network (DNN) inference. However, the state-of-the-art (SOTA) 2PC-DNN techniques are tailored explicitly to traditional instruction set architecture (ISA) systems like CPUs and CPU+GPU. This reliance on ISA systems significantly constrains their energy efficiency, as these architectures typically employ 32- or 64-bit instruction sets. In contrast, the possibilities of harnessing dynamic and adaptive quantization to build high-performance 2PC-DNNs remain largely unexplored due to the lack of compatible algorithms and hardware accelerators.To mitigate the bottleneck of SOTA solutions and fill the existing research gaps, this work investigates the construction of 2PC-DNNs on field programmable gate arrays (FPGAs). We introduce AQ2PNN, an end-to-end framework that effectively employs adaptive quantization schemes to develop high-performance 2PC-DNNs on FPGAs. From an algorithmic perspective, AQ2PNN introduces an innovative 2PC-ReLU method to replace Yao\u2019s Garbled Circuits (GC). Regarding hardware, AQ2PNN employs an extensive set of building blocks for linear operators, non-linear operators, and a specialized Oblivious Transfer (OT) module for secure data exchange, respectively. These algorithm-hardware co-designed modules extremely utilize the fine-grained reconfigurability of FPGAs, to adapt the data bit-width of different DNN layers in the ciphertext domain, thereby reducing communication overhead between parties without compromising DNN performance, such as accuracy. We thoroughly assess AQ2PNN using widely adopted DNN architectures, including ResNet18, ResNet50, and VGG16, all trained on ImageNet and producing quantized models. Experimental results demonstrate that AQ2PNN outperforms SOTA solutions, achieving significantly reduced communication overhead by 25%, improved energy efficiency by 26.3\u00d7, and comparable or even superior throughput and accuracy.",
            "corpus_id": "266085518",
            "text": "The growing prevalence of Machine Learning as a Service (MLaaS) enables a wide range of applications but simultaneously raises numerous security and privacy concerns. A key issue involves the potential privacy exposure of involved parties, such as the customer\u2019s input data and the vendor\u2019s model. Consequently, two-party computing (2PC) has emerged as a promising solution to safeguard the privacy of different parties during deep neural network (DNN) inference. However, the state-of-the-art (SOTA) 2PC-DNN techniques are tailored explicitly to traditional instruction set architecture (ISA) systems like CPUs and CPU+GPU. This reliance on ISA systems significantly constrains their energy efficiency, as these architectures typically employ 32- or 64-bit instruction sets. In contrast, the possibilities of harnessing dynamic and adaptive quantization to build high-performance 2PC-DNNs remain largely unexplored due to the lack of compatible algorithms and hardware accelerators.To mitigate the bottleneck of SOTA solutions and fill the existing research gaps, this work investigates the construction of 2PC-DNNs on field programmable gate arrays (FPGAs). We introduce AQ2PNN, an end-to-end framework that effectively employs adaptive quantization schemes to develop high-performance 2PC-DNNs on FPGAs. From an algorithmic perspective, AQ2PNN introduces an innovative 2PC-ReLU method to replace Yao\u2019s Garbled Circuits (GC). Regarding hardware, AQ2PNN employs an extensive set of building blocks for linear operators, non-linear operators, and a specialized Oblivious Transfer (OT) module for secure data exchange, respectively. These algorithm-hardware co-designed modules extremely utilize the fine-grained reconfigurability of FPGAs, to adapt the data bit-width of different DNN layers in the ciphertext domain, thereby reducing communication overhead between parties without compromising DNN performance, such as accuracy. We thoroughly assess AQ2PNN using widely adopted DNN architectures, including ResNet18, ResNet50, and VGG16, all trained on ImageNet and producing quantized models. Experimental results demonstrate that AQ2PNN outperforms SOTA solutions, achieving significantly reduced communication overhead by 25%, improved energy efficiency by 26.3\u00d7, and comparable or even superior throughput and accuracy.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.41748046875
        },
        {
            "paperId": "0e51079938fac1abb89c97abb7998958dfcbceb6",
            "corpusId": 269312420,
            "title": "An Efficient Residual-Based Neural Network for Super Resolution and High-precision Quantization Method Design",
            "venue": "2024 4th International Conference on Neural Networks, Information and Communication (NNICE)",
            "year": 2024,
            "referenceCount": 11,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/NNICE61279.2024.10498806?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/NNICE61279.2024.10498806, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256901192",
                    "name": "Zhihao Ruan"
                },
                {
                    "authorId": "66028506",
                    "name": "Qiuqun Deng"
                },
                {
                    "authorId": "9115832",
                    "name": "Shanzhu Xiao"
                },
                {
                    "authorId": "2256905047",
                    "name": "Huamin Tao"
                }
            ],
            "abstract": "In response to the current problems of deep layers and large model parameters in CNN for super-resolution, we propose a lightweight and efficient network model (LRSPCN) based on residuals. An efficient feature extraction module is built in model to learn effective information from row images, we induce residual connection units to achieve deep and shallow feature combination. To reduce model computational complexity, sub-pixel convolution layer is inserted in to obtain image upscale reconstruction. In our model, the amount of convolution layers and model parameters are enormously decreased. Furthermore, this paper proposes an efficient quantization method, which has the advantage of higher accuracy and lower bit-width compared to commonly used quantization methods. In our design, weight, bias and feature map are quantized uniformly to the same bit-width, which greatly compressed the storage requirements of model parameters and improved the accuracy and efficiency of hardware in edge-inference reconstruction implementation.",
            "corpus_id": "269312420",
            "text": "In response to the current problems of deep layers and large model parameters in CNN for super-resolution, we propose a lightweight and efficient network model (LRSPCN) based on residuals. An efficient feature extraction module is built in model to learn effective information from row images, we induce residual connection units to achieve deep and shallow feature combination. To reduce model computational complexity, sub-pixel convolution layer is inserted in to obtain image upscale reconstruction. In our model, the amount of convolution layers and model parameters are enormously decreased. Furthermore, this paper proposes an efficient quantization method, which has the advantage of higher accuracy and lower bit-width compared to commonly used quantization methods. In our design, weight, bias and feature map are quantized uniformly to the same bit-width, which greatly compressed the storage requirements of model parameters and improved the accuracy and efficiency of hardware in edge-inference reconstruction implementation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.6171875
        }
    ],
    "quotes": {
        "cost": 0.23330700000000001,
        "quotes": [
            {
                "idx": 0,
                "key": "[211146310 | Shkolnik et al. | 2020 | Citations: 76]",
                "snippets": "Naively quantizing a single-precision floating point (FP32) model to 4 bits (INT4) or lower usually incurs a significant accuracy degradation. Many studies have tried to mitigate this accuracy decrease by offering different quantization methods.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 483,
                        "end": 728,
                        "sentence_offsets": [
                            {
                                "start": 483,
                                "end": 625
                            },
                            {
                                "start": 626,
                                "end": 728
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Naively quantizing a single-precision floating point (FP32) model to 4 bits (INT4) or lower usually incurs a significant accuracy degradation. Many studies have tried to mitigate this accuracy decrease by offering different quantization methods."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[218788236 | Charan et al. | 2020 | Citations: 30]",
                "snippets": "We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. DEVICE QUANTIZATION",
                        "pdf_hash": "",
                        "start": 543,
                        "end": 894,
                        "sentence_offsets": [
                            {
                                "start": 543,
                                "end": 894
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[218862856 | Kim et al. | 2020 | Citations: 7]",
                "snippets": "On the other hand, ImageNet experiment generally shows reasonable results until 6-bits but the accuracy drastically drops at 4-bits. PSGD targeting 8-bits and 6-bits marginally improves on all bits, yet also experiences drastic accuracy drop at 4-bits. In contrast, Gradient L1 (\u03bb = 0.05) and PSGD @ W4 maintain the performance of the quantized models even at 4-bits. Comparing with the second best method Gradient L1 (\u03bb = 0.05) (Alizadeh et al., 2020), PSGD outperforms it at all bit-widths. At full precision (FP), 8-, 6-and 4-bits, the gap of performance between (Alizadeh et al., 2020) and ours are about 4.2%, 3.8%, 1.8% and 8%, respectively. From Table 2, while the quantization noise may slightly degrade the accuracy in some cases, a general trend that using more bits leads to higher accuracy is demonstrated.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Method",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 778,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 132
                            },
                            {
                                "start": 133,
                                "end": 252
                            },
                            {
                                "start": 253,
                                "end": 367
                            },
                            {
                                "start": 368,
                                "end": 472
                            },
                            {
                                "start": 473,
                                "end": 607
                            },
                            {
                                "start": 608,
                                "end": 778
                            }
                        ],
                        "ref_mentions": [
                            "211724357",
                            "211724357"
                        ],
                        "quote": "On the other hand, ImageNet experiment generally shows reasonable results until 6-bits but the accuracy drastically drops at 4-bits. PSGD targeting 8-bits and 6-bits marginally improves on all bits, yet also experiences drastic accuracy drop at 4-bits. In contrast, Gradient L1 (\u03bb = 0.05) and PSGD @ W4 maintain the performance of the quantized models even at 4-bits. Comparing with the second best method Gradient L1 (\u03bb = 0.05) (Alizadeh et al., 2020), PSGD outperforms it at all bit-widths. At full precision (FP), 8-, 6-and 4-bits, the gap of performance between (Alizadeh et al., 2020) and ours are about 4.2%, 3.8%, 1.8% and 8%, respectively. From Table 2, while the quantization noise may slightly degrade the accuracy in some cases, a general trend that using more bits leads to higher accuracy is demonstrated."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[220891847 | Cho et al. | 2020 | Citations: 41]",
                "snippets": "For CNNs, inferences with int8 precision has been widely adopted in an increasing number of real-world GPUs [17], [34]/NPUs (Medina et al., 2020), (Jouppi et al., 2017), as int8 precision has been shown to be sufficient for GEMM operation in the inference of various CNNs in recent studies. It has also been found that sub-8-bit quantization of the inference of a CNN does not lead to significant accuracy loss. For example, with the ImageNet classification dataset (Deng et al., 2009), [38] reports an activation 4-bit and weight 4-bit quantization of an ResNet-50 (He et al., 2015) with a 0.2% top-1 accuracy loss compared to the case of FP32, in which an activation 2-bit and weight 2-bit quantization was done and a 3.2% top-1 accuracy loss was obtained.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[206594692 | He et al. | 2015 | Citations: 194487]": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                    "[213621056 | Medina et al. | 2020 | Citations: 37]": "The growing computational requirements of AI applications are challenging today's general-purpose CPU and GPU architectures and driving the need for purpose-built, programmable AI solutions. Habana Labs designed its Goya processor to meet the high throughput/low latency demands of Inference workloads, and its Gaudi processor for throughput combined with massive scale up and scale out capability needed to speed training workloads efficiently. To address the need for scaling training, Habana is the first AI chip developer to integrate standard Ethernet onto a training processor.",
                    "[4202768 | Jouppi et al. | 2017 | Citations: 4643]": "Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC-called a Tensor Processing Unit (TPU)-deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X\u201330X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X\u201380X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU."
                },
                "metadata": [
                    {
                        "section_title": "B. LATEST QUANTIZATION RESULTS",
                        "pdf_hash": "",
                        "start": 726,
                        "end": 1422,
                        "sentence_offsets": [
                            {
                                "start": 726,
                                "end": 982
                            },
                            {
                                "start": 983,
                                "end": 1103
                            },
                            {
                                "start": 1104,
                                "end": 1422
                            }
                        ],
                        "ref_mentions": [
                            "213621056",
                            "4202768",
                            "57246310",
                            "206594692"
                        ],
                        "quote": "For CNNs, inferences with int8 precision has been widely adopted in an increasing number of real-world GPUs [17], [34]/NPUs (Medina et al., 2020), (Jouppi et al., 2017), as int8 precision has been shown to be sufficient for GEMM operation in the inference of various CNNs in recent studies. It has also been found that sub-8-bit quantization of the inference of a CNN does not lead to significant accuracy loss. For example, with the ImageNet classification dataset (Deng et al., 2009), [38] reports an activation 4-bit and weight 4-bit quantization of an ResNet-50 (He et al., 2015) with a 0.2% top-1 accuracy loss compared to the case of FP32, in which an activation 2-bit and weight 2-bit quantization was done and a 3.2% top-1 accuracy loss was obtained."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[221103917 | Zhou et al. | 2019 | Citations: 96]",
                "snippets": "For weight quantization, the weights were quantized to 32-bit, 8-bit, 4-bit, and 2-bit words. Thanks to our easy training models, we could retrain the deep SNNs simply following the procedure developed for conventional CNN quantization (Li, Zhang, and Liu 2016;(Rastegari et al., 2016). Specifically, the forward inference used quantized weights while the backward gradient propagation used full-precision weights. We first trained with 32-bit quantization. After the training converged, we applied 8-bit quantization and retrain the SNNs. This procedure was repeated until the 2-bit quantization. Table 4 shows the testing accuracy under weight quantization. For the models over MNIST, CIFAR10 and Ima-geNet, weight quantization caused the worst accuracy loss of 0.22%, 1.75%, and 8.8%, respectively. As comparison, we listed two typical CNN weight quantization results (Cheng et al. 2018;(Zhang et al., 2018), which indicated a similar performance degradation pace. The results demonstrated that the SNN models were robust to weight quantization for relatively small datasets and small networks. For larger networks, the weights would better be encoded in 4-bit or over.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[14925907 | Rastegari et al. | 2016 | Citations: 4369]": "We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32\\(\\times \\) memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58\\(\\times \\) faster convolutional operations (in terms of number of the high precision operations) and 32\\(\\times \\) memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is the same as the full-precision AlexNet. We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than \\(16\\,\\%\\) in top-1 accuracy. Our code is available at: http://allenai.org/plato/xnornet.",
                    "[50784025 | Zhang et al. | 2018 | Citations: 703]": "Although weight and activation quantization is an effective approach for Deep Neural Network (DNN) compression and has a lot of potentials to increase inference speed leveraging bit-operations, there is still a noticeable gap in terms of prediction accuracy between the quantized model and the full-precision model. To address this gap, we propose to jointly train a quantized, bit-operation-compatible DNN and its associated quantizers, as opposed to using fixed, handcrafted quantization schemes such as uniform or logarithmic quantization. Our method for learning the quantizers applies to both network weights and activations with arbitrary-bit precision, and our quantizers are easy to train. The comprehensive experiments on CIFAR-10 and ImageNet datasets show that our method works consistently well for various network structures such as AlexNet, VGG-Net, GoogLeNet, ResNet, and DenseNet, surpassing previous quantization methods in terms of accuracy by an appreciable margin. Code available at https://github.com/Microsoft/LQ-Nets."
                },
                "metadata": [
                    {
                        "section_title": "Robustness",
                        "pdf_hash": "",
                        "start": 387,
                        "end": 1555,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "14925907",
                            "50784025"
                        ],
                        "quote": "For weight quantization, the weights were quantized to 32-bit, 8-bit, 4-bit, and 2-bit words. Thanks to our easy training models, we could retrain the deep SNNs simply following the procedure developed for conventional CNN quantization (Li, Zhang, and Liu 2016;(Rastegari et al., 2016). Specifically, the forward inference used quantized weights while the backward gradient propagation used full-precision weights. We first trained with 32-bit quantization. After the training converged, we applied 8-bit quantization and retrain the SNNs. This procedure was repeated until the 2-bit quantization. Table 4 shows the testing accuracy under weight quantization. For the models over MNIST, CIFAR10 and Ima-geNet, weight quantization caused the worst accuracy loss of 0.22%, 1.75%, and 8.8%, respectively. As comparison, we listed two typical CNN weight quantization results (Cheng et al. 2018;(Zhang et al., 2018), which indicated a similar performance degradation pace. The results demonstrated that the SNN models were robust to weight quantization for relatively small datasets and small networks. For larger networks, the weights would better be encoded in 4-bit or over."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[221655691 | Trusov et al. | 2020 | Citations: 17]",
                "snippets": "4-bit quantization gives 95.0% accuracy and 48% overall inference speedup, while an 8-bit quantized network gives 95.4% accuracy and 39% speedup.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 1070,
                        "end": 1215,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "4-bit quantization gives 95.0% accuracy and 48% overall inference speedup, while an 8-bit quantized network gives 95.4% accuracy and 39% speedup."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[225583435 | Yee et al. | 2020 | Citations: 2]",
                "snippets": "For 4-bit fixed-point quantization, the accuracy drops to 72.5% and 85% from the original 90% after quantization on the weights in the layer 'fire8-expand3x3' and 'fire9expand3x3'. The accuracy drops the to the lowest which is 42.5% after 4-bit fixed point quantization is being applied on the weights in the layer 'fire6-expand3x3'. This might due to the large value changes in total weight after quantization. Although the model size is compressed the most after applying weight quantization in the layer 'fire8-expand3x3', but there is 17.5% accuracy drop. Hence, it is recommended to apply 4-bit quantization on weights in layer 'fire-9-expand3x3' when doing 4-bit fixed point quantization on the weights in a single layer of the network as it causes a slight drop in accuracy while reducing the model size. For 8-bit and 16-bit fixed-point quantization, there is no change even when weight quantization is applied on any of the layers.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RESULTS AND DISCUSSION",
                        "pdf_hash": "",
                        "start": 352,
                        "end": 1292,
                        "sentence_offsets": [
                            {
                                "start": 352,
                                "end": 532
                            },
                            {
                                "start": 533,
                                "end": 685
                            },
                            {
                                "start": 686,
                                "end": 763
                            },
                            {
                                "start": 764,
                                "end": 911
                            },
                            {
                                "start": 912,
                                "end": 1163
                            },
                            {
                                "start": 1164,
                                "end": 1292
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For 4-bit fixed-point quantization, the accuracy drops to 72.5% and 85% from the original 90% after quantization on the weights in the layer 'fire8-expand3x3' and 'fire9expand3x3'. The accuracy drops the to the lowest which is 42.5% after 4-bit fixed point quantization is being applied on the weights in the layer 'fire6-expand3x3'. This might due to the large value changes in total weight after quantization. Although the model size is compressed the most after applying weight quantization in the layer 'fire8-expand3x3', but there is 17.5% accuracy drop. Hence, it is recommended to apply 4-bit quantization on weights in layer 'fire-9-expand3x3' when doing 4-bit fixed point quantization on the weights in a single layer of the network as it causes a slight drop in accuracy while reducing the model size. For 8-bit and 16-bit fixed-point quantization, there is no change even when weight quantization is applied on any of the layers."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[231699188 | Liang et al. | 2021 | Citations: 700]",
                "snippets": "ResNet-34 with 4-bit weights and activation obtained 74.52% top-1 accuracy while the 32-bit version is 73.59% (Mishra et al., 2017)",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[3455952 | Mishra et al. | 2017 | Citations: 267]": "For computer vision applications, prior works have shown the efficacy of reducing numeric precision of model parameters (network weights) in deep neural networks. Activation maps, however, occupy a large memory footprint during both the training and inference step when using mini-batches of inputs. One way to reduce this large memory footprint is to reduce the precision of activations. However, past works have shown that reducing the precision of activations hurts model accuracy. We study schemes to train networks from scratch using reduced-precision activations without hurting accuracy. We reduce the precision of activation maps (along with model parameters) and increase the number of filter maps in a layer, and find that this scheme matches or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly improve the execution efficiency (e.g. reduce dynamic memory footprint, memory bandwidth and computational energy) and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN - wide reduced-precision networks. We report results and show that WRPN scheme is better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks."
                },
                "metadata": [
                    {
                        "section_title": "Quantization Reduces Over-fitting",
                        "pdf_hash": "",
                        "start": 326,
                        "end": 441,
                        "sentence_offsets": [
                            {
                                "start": 122,
                                "end": 737
                            }
                        ],
                        "ref_mentions": [
                            "3455952"
                        ],
                        "quote": "ResNet-34 with 4-bit weights and activation obtained 74.52% top-1 accuracy while the 32-bit version is 73.59% (Mishra et al., 2017)"
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[233443785 | Lui et al. | 2021 | Citations: 15]",
                "snippets": "As expected, the accuracy dropped when lowering the bit-precision of all 3 layers from 16 to 8 and 4 bits, from 98.1% to 96.3% and 81.1% respectively.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "EXPERIMENTS AND RESULTS",
                        "pdf_hash": "",
                        "start": 143,
                        "end": 293,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "As expected, the accuracy dropped when lowering the bit-precision of all 3 layers from 16 to 8 and 4 bits, from 98.1% to 96.3% and 81.1% respectively."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[234337594 | Abdolrashidi et al. | 2021 | Citations: 36]",
                "snippets": "In this work, we analyzed how quantization at different precisions influences the compute cost-quality Pareto curves on ResNet models...Additionally, models in 4-bit (with first and last layers in 8-bit) offer a consistent advantage over 8-bit, using both linear and quadratic cost models. This observation suggests that 4-bit may be a preferred numeric format for quantizing neural networks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 133,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In this work, we analyzed how quantization at different precisions influences the compute cost-quality Pareto curves on ResNet models"
                    },
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 380,
                        "end": 637,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Additionally, models in 4-bit (with first and last layers in 8-bit) offer a consistent advantage over 8-bit, using both linear and quadratic cost models. This observation suggests that 4-bit may be a preferred numeric format for quantizing neural networks."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[235166247 | Shomron et al. | 2021 | Citations: 34]",
                "snippets": "Mapping FP32 models to INT8 using uniform PTQ yields models with negligible accuracy degradation; however, reducing precision below 8 bits with PTQ is challenging, as accuracy degradation becomes noticeable, due to the increase in quantization noise.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 305,
                        "end": 555,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Mapping FP32 models to INT8 using uniform PTQ yields models with negligible accuracy degradation; however, reducing precision below 8 bits with PTQ is challenging, as accuracy degradation becomes noticeable, due to the increase in quantization noise."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[235266137 | Guo et al. | 2021 | Citations: 2]",
                "snippets": "We tried 4-bit quantization on some simple datasets(MNIST, CIFAR10, CIFAR100), and there was no quantization loss. We also tried 4-bit quantization on big dataset(ImageNet). The experiment result shows that there is about 12% performance loss, but due to the inference efficiency of 4-bit quantization, this loss may be tolerable in some application scenarios.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "IV. DISCUSSION",
                        "pdf_hash": "",
                        "start": 425,
                        "end": 785,
                        "sentence_offsets": [
                            {
                                "start": 425,
                                "end": 539
                            },
                            {
                                "start": 540,
                                "end": 598
                            },
                            {
                                "start": 599,
                                "end": 785
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We tried 4-bit quantization on some simple datasets(MNIST, CIFAR10, CIFAR100), and there was no quantization loss. We also tried 4-bit quantization on big dataset(ImageNet). The experiment result shows that there is about 12% performance loss, but due to the inference efficiency of 4-bit quantization, this loss may be tolerable in some application scenarios."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[235623747 | Lorenzen et al. | 2021 | Citations: 18]",
                "snippets": "For the 4-bit networks, as each neuron can take only 2^4 = 16 different values, the total number of possible states per layer decreases significantly in this setting, and as expected we see lower overall MI measurements...However, the network accuracy has now degraded compared to the non-quantized networks (see Supplementary Material F), which indicates that the binning used in the estimation of the MI in previous experiments has discarded information vital to the network.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C EFFECT OF THE QUANTIZATION PRECISION",
                        "pdf_hash": "",
                        "start": 212,
                        "end": 431,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "For the 4-bit networks, as each neuron can take only 2^4 = 16 different values, the total number of possible states per layer decreases significantly in this setting, and as expected we see lower overall MI measurements"
                    },
                    {
                        "section_title": "C EFFECT OF THE QUANTIZATION PRECISION",
                        "pdf_hash": "",
                        "start": 693,
                        "end": 949,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "However, the network accuracy has now degraded compared to the non-quantized networks (see Supplementary Material F), which indicates that the binning used in the estimation of the MI in previous experiments has discarded information vital to the network."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[236087695 | Chin et al. | 2021 | Citations: 5]",
                "snippets": "The experimental results from recent research [17] [18] show a comparison of model accuracy under different bit-width quantization in the image classification task, and the results support that the 4-bit quantization models strike the best balance between minimal computing/memory complexity and minimal model accuracy degradation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 762,
                        "end": 1093,
                        "sentence_offsets": [
                            {
                                "start": 762,
                                "end": 1093
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The experimental results from recent research [17] [18] show a comparison of model accuracy under different bit-width quantization in the image classification task, and the results support that the 4-bit quantization models strike the best balance between minimal computing/memory complexity and minimal model accuracy degradation."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[237416732 | Xiao et al. | 2021 | Citations: 37]",
                "snippets": "Fig. 19(a) shows that the 4-bit model is substantially more resilient to state-independent errors than the 8-bit network. This results entirely from activation quantization, and not weight quantization. The large separation between the 16 activation levels effectively cuts off the propagation of accumulated cell errors from one layer to the next. The same cell error results in a smaller dot product error on average. Fig. 19(b) shows that the 4-bit network is also more resilient to state-proportional error, but here the benefit is smaller. This can be explained by the different weight value distributions of the two networks. With only 16 levels, the distribution of the 4-bit weights cannot peak as sharply at zero as the 8-bit weights, which have 256 levels. As a result, the memory cells in the 4-bit network have a significantly higher average conductance (7.52% of G max ) than the cells in the 8-bit network (1.95% of G max ). Thus, deeply quantized weights can actually be harmful for analog systems, as it leads to a higher state-proportional error per cell. This is ultimately outweighed by the benefit of quantized activations, so there is a net improvement in error sensitivity.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "SUPPRESSING ERROR PROPAGATION",
                        "pdf_hash": "",
                        "start": 550,
                        "end": 1745,
                        "sentence_offsets": [
                            {
                                "start": 550,
                                "end": 671
                            },
                            {
                                "start": 672,
                                "end": 752
                            },
                            {
                                "start": 753,
                                "end": 898
                            },
                            {
                                "start": 899,
                                "end": 969
                            },
                            {
                                "start": 970,
                                "end": 1094
                            },
                            {
                                "start": 1095,
                                "end": 1181
                            },
                            {
                                "start": 1182,
                                "end": 1316
                            },
                            {
                                "start": 1317,
                                "end": 1488
                            },
                            {
                                "start": 1489,
                                "end": 1622
                            },
                            {
                                "start": 1623,
                                "end": 1745
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Fig. 19(a) shows that the 4-bit model is substantially more resilient to state-independent errors than the 8-bit network. This results entirely from activation quantization, and not weight quantization. The large separation between the 16 activation levels effectively cuts off the propagation of accumulated cell errors from one layer to the next. The same cell error results in a smaller dot product error on average. Fig. 19(b) shows that the 4-bit network is also more resilient to state-proportional error, but here the benefit is smaller. This can be explained by the different weight value distributions of the two networks. With only 16 levels, the distribution of the 4-bit weights cannot peak as sharply at zero as the 8-bit weights, which have 256 levels. As a result, the memory cells in the 4-bit network have a significantly higher average conductance (7.52% of G max ) than the cells in the 8-bit network (1.95% of G max ). Thus, deeply quantized weights can actually be harmful for analog systems, as it leads to a higher state-proportional error per cell. This is ultimately outweighed by the benefit of quantized activations, so there is a net improvement in error sensitivity."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[244531228 | Wang et al. | 2021 | Citations: 3]",
                "snippets": "The results in Table 3 show that the quantized model with 8-bit weights and activations only has a small drop of  2.2% in the mean MF score. The model size of the quantized network is around 4X smaller than the full precision network. In addition, QAT brings weight sparsity, where 2.65% of the parameters are zero after the quantization. The results for the 6-bit and 4-bit weight quantized models are also presented in Table 3. Compared with the 8-bit weight quantized model, the 6-bit weight quantized model shows a drop of 0.2% in the mean MF score but achieves around 4 times weight sparsity. The 4-bit weight quantized network shows a larger drop in the mean MF score but has more than 38% zero parameters. In addition, its model size is 13X smaller than the full precision model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Model compression study",
                        "pdf_hash": "",
                        "start": 480,
                        "end": 1266,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The results in Table 3 show that the quantized model with 8-bit weights and activations only has a small drop of  2.2% in the mean MF score. The model size of the quantized network is around 4X smaller than the full precision network. In addition, QAT brings weight sparsity, where 2.65% of the parameters are zero after the quantization. The results for the 6-bit and 4-bit weight quantized models are also presented in Table 3. Compared with the 8-bit weight quantized model, the 6-bit weight quantized model shows a drop of 0.2% in the mean MF score but achieves around 4 times weight sparsity. The 4-bit weight quantized network shows a larger drop in the mean MF score but has more than 38% zero parameters. In addition, its model size is 13X smaller than the full precision model."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[245351885 | Chudakov et al. | 2021 | Citations: 0]",
                "snippets": "At the moment, the standard is 8-bit uniform quantization. Nevertheless, the use of uniform low-bit quantization (4- and 6-bit quantization) has significant advantages in speed and resource requirements for inference.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 97,
                        "end": 314,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "At the moment, the standard is 8-bit uniform quantization. Nevertheless, the use of uniform low-bit quantization (4- and 6-bit quantization) has significant advantages in speed and resource requirements for inference."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[246077316 | Rezk et al. | 2021 | Citations: 1]",
                "snippets": "Since researchers have found that 16-bit and 8-bit quantization do not significantly affect accuracy (Zhao et al., 2019), we will focus on 4-bit quantization (8x compression). CNN ImageNet models have been used for quantization experiments in most of the work we have seen. The accuracy drop due to 4-bit post-training quantization on CNN models has varied in these papers as follows: LAPQ [6] (6.1 to 9.4 p.p.), ACIQ [11] (0.4 to 10.8 p.p.), OCS [10] (more than 5 p.p.), and ZeroQ [38] (1.6 p.p.).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[59413897 | Zhao et al. | 2019 | Citations: 311]": "Quantization can improve the execution latency and energy efficiency of neural networks on both commodity GPUs and specialized accelerators. The majority of existing literature focuses on training quantized DNNs, while this work examines the less-studied topic of quantizing a floating-point model without (re)training. DNN weights and activations follow a bell-shaped distribution post-training, while practical hardware uses a linear quantization grid. This leads to challenges in dealing with outliers in the distribution. Prior work has addressed this by clipping the outliers or using specialized hardware. In this work, we propose outlier channel splitting (OCS), which duplicates channels containing outliers, then halves the channel values. The network remains functionally identical, but affected outliers are moved toward the center of the distribution. OCS requires no additional training and works on commodity hardware. Experimental evaluation on ImageNet classification and language modeling shows that OCS can outperform state-of-the-art clipping techniques with only minor overhead."
                },
                "metadata": [
                    {
                        "section_title": "Multi-objective search to minimize two objectives: WER and memory size",
                        "pdf_hash": "",
                        "start": 1381,
                        "end": 1864,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "59413897"
                        ],
                        "quote": "Since researchers have found that 16-bit and 8-bit quantization do not significantly affect accuracy (Zhao et al., 2019), we will focus on 4-bit quantization (8x compression). CNN ImageNet models have been used for quantization experiments in most of the work we have seen. The accuracy drop due to 4-bit post-training quantization on CNN models has varied in these papers as follows: LAPQ [6] (6.1 to 9.4 p.p.), ACIQ [11] (0.4 to 10.8 p.p.), OCS [10] (more than 5 p.p.), and ZeroQ [38] (1.6 p.p.)."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[247595112 | Nagel et al. | 2022 | Citations: 108]",
                "snippets": "Despite its clear power and latency benefits, quantization comes at the cost of added noise due to the reduced precision. However, researchers in recent years have shown that neural networks are robust to this noise and can be quantized to 8-bits with a minimal drop in accuracy using post-training quantization techniques (PTQ) (Nagel et al., 2019)(Banner et al., 2018)Nahshan et al., 2020). PTQ can be very efficient and, generally, only requires access to a small calibration dataset, but suffers when applied to low-bit quantization (\u2264 4-bits) of neural networks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[184487878 | Nagel et al. | 2019 | Citations: 515]": "We introduce a data-free quantization method for deep neural networks that does not require fine-tuning or hyperparameter selection. It achieves near-original model performance on common computer vision architectures and tasks. 8-bit fixed-point quantization is essential for efficient inference on modern deep learning hardware. However, quantizing models to run in 8-bit is a non-trivial task, frequently leading to either significant performance reduction or engineering time spent on training a network to be amenable to quantization. Our approach relies on equalizing the weight ranges in the network by making use of a scale-equivariance property of activation functions. In addition the method corrects biases in the error that are introduced during quantization. This improves quantization accuracy performance, and can be applied to many common computer vision architectures with a straight forward API call. For common architectures, such as the MobileNet family, we achieve state-of-the-art quantized model performance. We further show that the method also extends to other computer vision architectures and tasks such as semantic segmentation and object detection."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 445,
                        "end": 1011,
                        "sentence_offsets": [
                            {
                                "start": 445,
                                "end": 566
                            },
                            {
                                "start": 567,
                                "end": 836
                            },
                            {
                                "start": 837,
                                "end": 1011
                            }
                        ],
                        "ref_mentions": [
                            "184487878",
                            "59292009"
                        ],
                        "quote": "Despite its clear power and latency benefits, quantization comes at the cost of added noise due to the reduced precision. However, researchers in recent years have shown that neural networks are robust to this noise and can be quantized to 8-bits with a minimal drop in accuracy using post-training quantization techniques (PTQ) (Nagel et al., 2019)(Banner et al., 2018)Nahshan et al., 2020). PTQ can be very efficient and, generally, only requires access to a small calibration dataset, but suffers when applied to low-bit quantization (\u2264 4-bits) of neural networks."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[250607637 | Avila et al. | 2022 | Citations: 4]",
                "snippets": "Quantization had a mild impact on the overall performance. For integer quantization, for example, quantizing with 16 bits showed to be slightly benefitial compared to quantizatizing with 2 bits with latter dropping accuracy only 0.06 % on the test set. Similar trend was found for the ternary shift network. While quantizing with 4 bits gives 98.76 % accuracy, quantizing with 2 bits provides 98.41 % accuracy, representing a decay of only 0.35 %.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Results",
                        "pdf_hash": "",
                        "start": 700,
                        "end": 1147,
                        "sentence_offsets": [
                            {
                                "start": 700,
                                "end": 758
                            },
                            {
                                "start": 759,
                                "end": 952
                            },
                            {
                                "start": 953,
                                "end": 1007
                            },
                            {
                                "start": 1008,
                                "end": 1147
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Quantization had a mild impact on the overall performance. For integer quantization, for example, quantizing with 16 bits showed to be slightly benefitial compared to quantizatizing with 2 bits with latter dropping accuracy only 0.06 % on the test set. Similar trend was found for the ternary shift network. While quantizing with 4 bits gives 98.76 % accuracy, quantizing with 2 bits provides 98.41 % accuracy, representing a decay of only 0.35 %."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[251644990 | Wan et al. | 2022 | Citations: 503]",
                "snippets": "To efficiently implement the models on NeuRRAM, inputs to all convolutional and fully connected layers are quantized to 4-bit or below. The input bit-precisions of all the models are summarized in Table 1. We perform the quantized training using the parameterized clipping activation technique 46. The accuracies of some of our quantized models are lower than that of the state-of-the-art quantized model because we apply <4-bit quantization to the most sensitive input and output layers of the neural networks, which have been reported to cause large accuracy degradation and are thus often excluded from low-precision quantization 46,54. To obtain better accuracy for quantized models, one can use higher precision for sensitive input and output layers, apply more advanced quantization techniques, and use more optimized data preprocessing, data augmentation and regularization techniques during training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[67788003 | Esser et al. | 2019 | Citations: 810]": "Deep networks run with low precision operations at inference time offer power and space advantages over high precision alternatives, but need to overcome the challenge of maintaining high accuracy as precision decreases. Here, we present a method for training such networks, Learned Step Size Quantization, that achieves the highest accuracy to date on the ImageNet dataset when using models, from a variety of architectures, with weights and activations quantized to 2-, 3- or 4-bits of precision, and that can train 3-bit models that reach full precision baseline accuracy. Our approach builds upon existing methods for learning weights in quantized networks by improving how the quantizer itself is configured. Specifically, we introduce a novel means to estimate and scale the task loss gradient at each weight and activation layer's quantizer step size, such that it can be learned in conjunction with other network parameters. This approach works using different levels of precision as needed for a given system and requires only a simple modification of existing training code."
                },
                "metadata": [
                    {
                        "section_title": "Noise-resilient neural-network training",
                        "pdf_hash": "",
                        "start": 450,
                        "end": 1358,
                        "sentence_offsets": [
                            {
                                "start": 329,
                                "end": 455
                            },
                            {
                                "start": 456,
                                "end": 555
                            },
                            {
                                "start": 558,
                                "end": 693
                            },
                            {
                                "start": 694,
                                "end": 763
                            },
                            {
                                "start": 764,
                                "end": 856
                            },
                            {
                                "start": 857,
                                "end": 1199
                            },
                            {
                                "start": 1200,
                                "end": 1468
                            }
                        ],
                        "ref_mentions": [
                            "67788003"
                        ],
                        "quote": "To efficiently implement the models on NeuRRAM, inputs to all convolutional and fully connected layers are quantized to 4-bit or below. The input bit-precisions of all the models are summarized in Table 1. We perform the quantized training using the parameterized clipping activation technique 46. The accuracies of some of our quantized models are lower than that of the state-of-the-art quantized model because we apply <4-bit quantization to the most sensitive input and output layers of the neural networks, which have been reported to cause large accuracy degradation and are thus often excluded from low-precision quantization 46,54. To obtain better accuracy for quantized models, one can use higher precision for sensitive input and output layers, apply more advanced quantization techniques, and use more optimized data preprocessing, data augmentation and regularization techniques during training."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[254853733 | Dettmers et al. | 2022 | Citations: 234]",
                "snippets": "We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float). Overall, our findings show that {4-bit} precision is almost universally optimal for total model bits and zero-shot accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 899,
                        "end": 1298,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float). Overall, our findings show that {4-bit} precision is almost universally optimal for total model bits and zero-shot accuracy."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[257205844 | Zhou et al. | 2023 | Citations: 7]",
                "snippets": "Experimental results demonstrate that the ImageNet inference accuracy via DyBit is 1.97% higher than the state-of-the-art at 4-bit quantization",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Experimental results demonstrate that the ImageNet inference accuracy via DyBit is 1.97% higher than the state-of-the-art at 4-bit quantization",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[258841328 | Dettmers et al. | 2023 | Citations: 2606]",
                "snippets": "Recent findings have established that 4-bit quantization for inference is possible, but leads to performance degradation relative to 16-bit [13]18]. This raises the crucial question of whether the lost performance can be recovered by conducting 4-bit adapter finetuning. We test this for two setups. The first focuses on a comparison with full 16-bit finetuning of RoBERTA and T5 models sized 125M to 3B parameters on GLUE and the Super-NaturalInstructions dataset. Results are shown in Table 3. In both datasets, we observe that 16-bit, 8-bit, and 4-bit adapter methods replicate the performance of the fully finetuned 16-bit baseline. This suggests that the performance lost due to the imprecise quantization can be fully recovered through adapter finetuning after quantization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "4-bit NormalFloat yields better performance than 4-bit Floating Point",
                        "pdf_hash": "",
                        "start": 663,
                        "end": 1443,
                        "sentence_offsets": [
                            {
                                "start": 592,
                                "end": 811
                            },
                            {
                                "start": 812,
                                "end": 933
                            },
                            {
                                "start": 934,
                                "end": 962
                            },
                            {
                                "start": 963,
                                "end": 1128
                            },
                            {
                                "start": 1129,
                                "end": 1158
                            },
                            {
                                "start": 1159,
                                "end": 1299
                            },
                            {
                                "start": 1300,
                                "end": 1443
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recent findings have established that 4-bit quantization for inference is possible, but leads to performance degradation relative to 16-bit [13]18]. This raises the crucial question of whether the lost performance can be recovered by conducting 4-bit adapter finetuning. We test this for two setups. The first focuses on a comparison with full 16-bit finetuning of RoBERTA and T5 models sized 125M to 3B parameters on GLUE and the Super-NaturalInstructions dataset. Results are shown in Table 3. In both datasets, we observe that 16-bit, 8-bit, and 4-bit adapter methods replicate the performance of the fully finetuned 16-bit baseline. This suggests that the performance lost due to the imprecise quantization can be fully recovered through adapter finetuning after quantization."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[258959117 | Liu et al. | 2023 | Citations: 209]",
                "snippets": "First, 8-bit quantization should be preferred over smaller full precision models, and PTQ methods are sufficient for this case. An 8-8-8 30B quantized model outperforms a 13B model of similar size, and should have lower latency and higher throughput in practice. This also holds for an 8-bit 13B model compared with a 16-bit 7B model. Furthermore, 4-bit models quantized using LLM-QAT should be preferred over 8-bit models of similar size. For instance a 4-8-4 LLM-QAT 30B outperforms an 8-bit LLaMA-13B, and a 4-8-8 LLM-QAT 13B is better than an 8-bit LLaMA-7B.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Main Results",
                        "pdf_hash": "",
                        "start": 267,
                        "end": 829,
                        "sentence_offsets": [
                            {
                                "start": 267,
                                "end": 394
                            },
                            {
                                "start": 395,
                                "end": 529
                            },
                            {
                                "start": 530,
                                "end": 601
                            },
                            {
                                "start": 602,
                                "end": 706
                            },
                            {
                                "start": 707,
                                "end": 829
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "First, 8-bit quantization should be preferred over smaller full precision models, and PTQ methods are sufficient for this case. An 8-8-8 30B quantized model outperforms a 13B model of similar size, and should have lower latency and higher throughput in practice. This also holds for an 8-bit 13B model compared with a 16-bit 7B model. Furthermore, 4-bit models quantized using LLM-QAT should be preferred over 8-bit models of similar size. For instance a 4-8-4 LLM-QAT 30B outperforms an 8-bit LLaMA-13B, and a 4-8-8 LLM-QAT 13B is better than an 8-bit LLaMA-7B."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[259274857 | Sabri et al. | 2023 | Citations: 2]",
                "snippets": "Similarly, when decreasing the numerical precision of the uniform quantization to 4-bits, the RRMSE increases to 28.81% and 7.89% for groups (a) and (b) respectively, which corresponds to an increment of 17.2x and 16x respect the 8-bit uniform quantization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "ANALYSIS OF CNN ACTIVATION GROUPS",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 257,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 257
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Similarly, when decreasing the numerical precision of the uniform quantization to 4-bits, the RRMSE increases to 28.81% and 7.89% for groups (a) and (b) respectively, which corresponds to an increment of 17.2x and 16x respect the 8-bit uniform quantization."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[259937594 | Liu et al. | 2023 | Citations: 35]",
                "snippets": "Overall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Results and Analysis",
                        "pdf_hash": "",
                        "start": 84,
                        "end": 780,
                        "sentence_offsets": [
                            {
                                "start": 84,
                                "end": 182
                            },
                            {
                                "start": 183,
                                "end": 415
                            },
                            {
                                "start": 416,
                                "end": 570
                            },
                            {
                                "start": 571,
                                "end": 780
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Overall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[261049574 | Frumkin et al. | 2023 | Citations: 17]",
                "snippets": "Moving from 8-bit to 4-bit weight quantization, we see an accuracy degradation of about 2 \u2212 5% across all models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "4-bit Quantization",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 113,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 113
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Moving from 8-bit to 4-bit weight quantization, we see an accuracy degradation of about 2 \u2212 5% across all models."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[267035013 | Caldeira et al. | 2024 | Citations: 10]",
                "snippets": "The authors prove that model performance suffers a bigger impact when the first layers are quantized due to the forward propagation of the performed approximations, allowing for stronger quantization when progressing toward the last layers of the network. The final MPQ framework comprised 8, 4 and 2-bit quantization for the initial, middle and final layers, respectively, resulting in a network with only 10% of the computational footprint of the original model and a performance drop of 0.68 percentual points on LFW. Thus, MPQ can be extremely useful in establishing a compromise between achieving good performances and having computationally efficient models....When b = 8 or b = 6, the performance degradation was small in the identification and verification tasks and, in some cases, the quantized models even slightly surpassed the baseline. For b = 4, the performance suffered an extreme drop. As an example, a MobileFaceNet trained in the identification task suffered a reduction in accuracy from 99.80% in 6-bit quantization to 5.53% in 4-bit quantization. Hence, in this experimental setup, 4-bit quantization is not enough to achieve good performance, showing that this precision level does not allow the model to learn how to encode the relevant features properly.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A. Quantization",
                        "pdf_hash": "",
                        "start": 258,
                        "end": 923,
                        "sentence_offsets": [
                            {
                                "start": 258,
                                "end": 513
                            },
                            {
                                "start": 514,
                                "end": 778
                            },
                            {
                                "start": 779,
                                "end": 922
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The authors prove that model performance suffers a bigger impact when the first layers are quantized due to the forward propagation of the performed approximations, allowing for stronger quantization when progressing toward the last layers of the network. The final MPQ framework comprised 8, 4 and 2-bit quantization for the initial, middle and final layers, respectively, resulting in a network with only 10% of the computational footprint of the original model and a performance drop of 0.68 percentual points on LFW. Thus, MPQ can be extremely useful in establishing a compromise between achieving good performances and having computationally efficient models"
                    },
                    {
                        "section_title": "A. Quantization",
                        "pdf_hash": "",
                        "start": 1564,
                        "end": 2176,
                        "sentence_offsets": [
                            {
                                "start": 1564,
                                "end": 1746
                            },
                            {
                                "start": 1747,
                                "end": 1799
                            },
                            {
                                "start": 1800,
                                "end": 1964
                            },
                            {
                                "start": 1965,
                                "end": 2175
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".When b = 8 or b = 6, the performance degradation was small in the identification and verification tasks and, in some cases, the quantized models even slightly surpassed the baseline. For b = 4, the performance suffered an extreme drop. As an example, a MobileFaceNet trained in the identification task suffered a reduction in accuracy from 99.80% in 6-bit quantization to 5.53% in 4-bit quantization. Hence, in this experimental setup, 4-bit quantization is not enough to achieve good performance, showing that this precision level does not allow the model to learn how to encode the relevant features properly."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[267966328 | Trusov et al. | 2024 | Citations: 3]",
                "snippets": "Eight-bit quantized networks demonstrate similarly high quality as full precision models and perfectly fit the hardware architecture with one-byte coefficients and thirty-two-bit dot product accumulators. Lower precision quantizations usually suffer from noticeable quality loss and require specific computational algorithms to outperform eight-bit quantization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Eight-bit quantized networks demonstrate similarly high quality as full precision models and perfectly fit the hardware architecture with one-byte coefficients and thirty-two-bit dot product accumulators. Lower precision quantizations usually suffer from noticeable quality loss and require specific computational algorithms to outperform eight-bit quantization.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[270870579 | Motetti et al. | 2024 | Citations: 4]",
                "snippets": "In \"Medium\" models (central plot), very few parameters, if any, are quantized at 2 bits, since it is the least interesting precision due to the low representational capacity, but not negligible cost. In contrast, 4-bit channels represent a significant component, especially for the size and NE16 regularizers, costing less then twice the 2-bit ones while providing significant accuracy advantages.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Models analysis",
                        "pdf_hash": "",
                        "start": 153,
                        "end": 550,
                        "sentence_offsets": [
                            {
                                "start": 153,
                                "end": 352
                            },
                            {
                                "start": 353,
                                "end": 550
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In \"Medium\" models (central plot), very few parameters, if any, are quantized at 2 bits, since it is the least interesting precision due to the low representational capacity, but not negligible cost. In contrast, 4-bit channels represent a significant component, especially for the size and NE16 regularizers, costing less then twice the 2-bit ones while providing significant accuracy advantages."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[271050128 | Posso et al. | 2024 | Citations: 0]",
                "snippets": "Further experiments have revealed that activations are notably more sensitive to low-bit quantization, with quantization errors below 4 bits frequently causing orientation decoding errors and disrupting the neural network's training process...Our findings also indicate that the binarization of early layers diminishes the generalization capabilities of the neural network...Therefore, quantization below 8 bits has a more pronounced impact on orientation accuracy than on position accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "III. CO-DESIGN METHODOLOGY A. Methodology Overview",
                        "pdf_hash": "",
                        "start": 325,
                        "end": 565,
                        "sentence_offsets": [
                            {
                                "start": 325,
                                "end": 566
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Further experiments have revealed that activations are notably more sensitive to low-bit quantization, with quantization errors below 4 bits frequently causing orientation decoding errors and disrupting the neural network's training process"
                    },
                    {
                        "section_title": "III. CO-DESIGN METHODOLOGY A. Methodology Overview",
                        "pdf_hash": "",
                        "start": 671,
                        "end": 800,
                        "sentence_offsets": [
                            {
                                "start": 671,
                                "end": 801
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our findings also indicate that the binarization of early layers diminishes the generalization capabilities of the neural network"
                    },
                    {
                        "section_title": "III. CO-DESIGN METHODOLOGY A. Methodology Overview",
                        "pdf_hash": "",
                        "start": 1151,
                        "end": 1267,
                        "sentence_offsets": [
                            {
                                "start": 1151,
                                "end": 1267
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Therefore, quantization below 8 bits has a more pronounced impact on orientation accuracy than on position accuracy."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[271571530 | Pasricha | 2024 | Citations: 0]",
                "snippets": "Studies have shown that activations are more susceptible to bitwidth reduction than weight parameters. Therefore, an alternative for increasing inference accuracy is to not binarize activations, while keeping the weights binarized. To determine the appropriate activation bitwidth (precision), an analysis was performed in (Sunny et al., 2021), where weights were restricted to binary (1-bit) values, but the bit precision level of the activations was altered from 1-bit to 16-bits, across various CNN models and learning tasks. During CNN model training, weights were binarized during the forward and backward propagations but not during the parameter update step, because keeping high precision weights during the updates is necessary for stochastic gradient descent (SGD) to work efficiently. After training, all weights were in binary format, while the precision of input activations was varied. Figure 9(a) shows the results of varying activation bitwidth across four different CNN models and their datasets. It was observed that the accuracy had notable change initially as activations bits were increased, but this gain in accuracy soon saturated. Based on the results, 1-bit weights were considered along with 4-bit activations, which allowed reducing bitwidths without significantly reducing model accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[235795063 | Sunny et al. | 2021 | Citations: 35]": "Domain specific neural network accelerators have garnered attention because of their improved energy efficiency and inference performance compared to CPUs and GPUs. Such accelerators are thus well suited for resource-constrained embedded systems. However, mapping sophisticated neural network models on these accelerators still entails significant energy and memory consumption, along with high inference time overhead. Binarized neural networks (BNNs), which utilize single-bit weights, represent an efficient way to implement and deploy neural network models on accelerators. In this paper, we present a novel optical-domain BNN accelerator, named ROBIN, which intelligently integrates heterogeneous microring resonator optical devices with complementary capabilities to efficiently implement the key functionalities in BNNs. We perform detailed fabrication-process variation analyses at the optical device level, explore efficient corrective tuning for these devices, and integrate circuit-level optimization to counter thermal variations. As a result, our proposed ROBIN architecture possesses the desirable traits of being robust, energy-efficient, low latency, and high throughput, when executing BNN models. Our analysis shows that ROBIN can outperform the best-known optical BNN accelerators and many electronic accelerators. Specifically, our energy-efficient ROBIN design exhibits energy-per-bit values that are \u223c4 \u00d7 lower than electronic BNN accelerators and \u223c933 \u00d7 lower than a recently proposed photonic BNN accelerator, while a performance-efficient ROBIN design shows \u223c3 \u00d7 and \u223c25 \u00d7 better performance than electronic and photonic BNN accelerators, respectively."
                },
                "metadata": [
                    {
                        "section_title": "Quantization Approaches",
                        "pdf_hash": "",
                        "start": 688,
                        "end": 1989,
                        "sentence_offsets": [
                            {
                                "start": 688,
                                "end": 790
                            },
                            {
                                "start": 791,
                                "end": 919
                            },
                            {
                                "start": 920,
                                "end": 1201
                            },
                            {
                                "start": 1202,
                                "end": 1468
                            },
                            {
                                "start": 1469,
                                "end": 1572
                            },
                            {
                                "start": 1573,
                                "end": 1686
                            },
                            {
                                "start": 1687,
                                "end": 1827
                            },
                            {
                                "start": 1828,
                                "end": 1989
                            }
                        ],
                        "ref_mentions": [
                            "235795063"
                        ],
                        "quote": "Studies have shown that activations are more susceptible to bitwidth reduction than weight parameters. Therefore, an alternative for increasing inference accuracy is to not binarize activations, while keeping the weights binarized. To determine the appropriate activation bitwidth (precision), an analysis was performed in (Sunny et al., 2021), where weights were restricted to binary (1-bit) values, but the bit precision level of the activations was altered from 1-bit to 16-bits, across various CNN models and learning tasks. During CNN model training, weights were binarized during the forward and backward propagations but not during the parameter update step, because keeping high precision weights during the updates is necessary for stochastic gradient descent (SGD) to work efficiently. After training, all weights were in binary format, while the precision of input activations was varied. Figure 9(a) shows the results of varying activation bitwidth across four different CNN models and their datasets. It was observed that the accuracy had notable change initially as activations bits were increased, but this gain in accuracy soon saturated. Based on the results, 1-bit weights were considered along with 4-bit activations, which allowed reducing bitwidths without significantly reducing model accuracy."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[273821224 | Lang et al. | 2024 | Citations: 12]",
                "snippets": "LLM-QAT demonstrates better accuracy with a configuration of 4-bit weights, 4-bit KV caches, and 8-bit activations, compared to a uniform 4-bit setting across all precision [35].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "VII. CONCLUSION",
                        "pdf_hash": "",
                        "start": 285,
                        "end": 463,
                        "sentence_offsets": [
                            {
                                "start": 272,
                                "end": 463
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "LLM-QAT demonstrates better accuracy with a configuration of 4-bit weights, 4-bit KV caches, and 8-bit activations, compared to a uniform 4-bit setting across all precision [35]."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[275038383 | Tumialis et al. | 2024 | Citations: 1]",
                "snippets": "Thanks to this, the accuracy of the 4-bit networks did not differ significantly from the 8-bit ones and their results were approximately four percentage points worse than those of the full precision networks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Thanks to this, the accuracy of the 4-bit networks did not differ significantly from the 8-bit ones and their results were approximately four percentage points worse than those of the full precision networks.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[275932373 | Wang et al. | 2025 | Citations: 12]",
                "snippets": "Directly quantizing LLMs to such a lowbit format often results in substantial accuracy degradation, as shown in Figure 1. This is primarily because low-bit formats are constrained by a limited dynamic range, which increases the risk of overflow and underflow. Even existing methods for 8-bit quantization experience some degree of accuracy loss, underscoring the difficulties of employing a 4-bit format, which provides only 16 distinct representable values.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 567,
                        "end": 1025,
                        "sentence_offsets": [
                            {
                                "start": 567,
                                "end": 688
                            },
                            {
                                "start": 689,
                                "end": 826
                            },
                            {
                                "start": 827,
                                "end": 1025
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Directly quantizing LLMs to such a lowbit format often results in substantial accuracy degradation, as shown in Figure 1. This is primarily because low-bit formats are constrained by a limited dynamic range, which increases the risk of overflow and underflow. Even existing methods for 8-bit quantization experience some degree of accuracy loss, underscoring the difficulties of employing a 4-bit format, which provides only 16 distinct representable values."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[276903421 | Berti et al. | 2025 | Citations: 6]",
                "snippets": "At higher bit levels, such as 8-bit and 16-bit, LLMs retain much of their original performance, with minimal degradation in reasoning and instruction-following tasks. However, as precision decreases, particularly at 4-bit and below, a clear pattern emerges: while 4-bit quantization manages to preserve most emergent abilities, 2-bit quantization severely degrades performance, often reducing accuracy to near-random levels.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "D. The Impact of Quantization on Emergent Abilities",
                        "pdf_hash": "",
                        "start": 1237,
                        "end": 1661,
                        "sentence_offsets": [
                            {
                                "start": 1237,
                                "end": 1403
                            },
                            {
                                "start": 1404,
                                "end": 1661
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "At higher bit levels, such as 8-bit and 16-bit, LLMs retain much of their original performance, with minimal degradation in reasoning and instruction-following tasks. However, as precision decreases, particularly at 4-bit and below, a clear pattern emerges: while 4-bit quantization manages to preserve most emergent abilities, 2-bit quantization severely degrades performance, often reducing accuracy to near-random levels."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[49356451 | Krishnamoorthi | 2018 | Citations: 1021]",
                "snippets": "We note that at 8-bits of precision, post training quantization schemes provide close to floating point accuracy. In order to better understand the benefits of quantization aware training, we perform experiments to assess performance at 4 bit quantization for weights and activations...It is interesting to see that for most networks, one can obtain accuracies within 5% of 8-bit quantization with fine tuning 4 bit weights (column 4 of Table 5). The improvements due to fine tuning are also more apparent at 4 bits. Note that activations are quantized to 8-bits in these experiments.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Lower Precision Networks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 283,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 113
                            },
                            {
                                "start": 114,
                                "end": 284
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We note that at 8-bits of precision, post training quantization schemes provide close to floating point accuracy. In order to better understand the benefits of quantization aware training, we perform experiments to assess performance at 4 bit quantization for weights and activations"
                    },
                    {
                        "section_title": "Lower Precision Networks",
                        "pdf_hash": "",
                        "start": 883,
                        "end": 1182,
                        "sentence_offsets": [
                            {
                                "start": 883,
                                "end": 1043
                            },
                            {
                                "start": 1044,
                                "end": 1113
                            },
                            {
                                "start": 1114,
                                "end": 1181
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "It is interesting to see that for most networks, one can obtain accuracies within 5% of 8-bit quantization with fine tuning 4 bit weights (column 4 of Table 5). The improvements due to fine tuning are also more apparent at 4 bits. Note that activations are quantized to 8-bits in these experiments."
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[52197199 | McKinstry et al. | 2018 | Citations: 94]",
                "snippets": "FAQ trained 8-bit networks outperform all comparable quantization methods in all but one instance and exceeded pretrained fp32 network accuracy with only one epoch of training following quantization for all networks explored (Table 1). Immediately following quantization, network accuracy was nearly at the level of the pretrained networks (data not shown) with one exception, Inception-v3, which started at 72.34% top-1. Since the networks started close to a good solution, they did not require extensive fine-tuning to return to and surpass pretrained networks. \n\nFAQ trained 4-bit network accuracy outperforms all comparable quantization methods, exceeding the next closest approach by over 0.5% for ResNet-18 (Jung et al., 2018), and matched or exceeded pretrained fp32 network accuracy. FAQ 4-bit networks required significantly longer fine-tuning -110 epochs -for networks trained, ResNet-18, ResNet-34, and ResNet-50. In constrast to the 8-bit cases, immediately following quantization, network accuracy dropped precipitously, requiring significant fine-tuning to match and surpass the pretrained networks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Experiments",
                        "pdf_hash": "",
                        "start": 112,
                        "end": 1224,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 347
                            },
                            {
                                "start": 348,
                                "end": 533
                            },
                            {
                                "start": 534,
                                "end": 675
                            },
                            {
                                "start": 678,
                                "end": 902
                            },
                            {
                                "start": 903,
                                "end": 1035
                            },
                            {
                                "start": 1036,
                                "end": 1224
                            }
                        ],
                        "ref_mentions": [
                            "195347490"
                        ],
                        "quote": "FAQ trained 8-bit networks outperform all comparable quantization methods in all but one instance and exceeded pretrained fp32 network accuracy with only one epoch of training following quantization for all networks explored (Table 1). Immediately following quantization, network accuracy was nearly at the level of the pretrained networks (data not shown) with one exception, Inception-v3, which started at 72.34% top-1. Since the networks started close to a good solution, they did not require extensive fine-tuning to return to and surpass pretrained networks. \n\nFAQ trained 4-bit network accuracy outperforms all comparable quantization methods, exceeding the next closest approach by over 0.5% for ResNet-18 (Jung et al., 2018), and matched or exceeded pretrained fp32 network accuracy. FAQ 4-bit networks required significantly longer fine-tuning -110 epochs -for networks trained, ResNet-18, ResNet-34, and ResNet-50. In constrast to the 8-bit cases, immediately following quantization, network accuracy dropped precipitously, requiring significant fine-tuning to match and surpass the pretrained networks."
                    }
                ]
            },
            {
                "idx": 39,
                "key": "[57573783 | Gennari et al. | 2019 | Citations: 67]",
                "snippets": "The results also shows that compression down to 4-bits (which in convolutions with channel size input of 256 would yield a 5x compression rate) results in an accuracy drop of only 1% to 2% depending on the architecture...Also as known in previous research papers [26](Gysel et al., 2018), models of 8/8 bits lose only around 0.1% accuracy. For models of 5/5 and 4/4, we get a drop of 1% to 3% in accuracy. To our knowledge, this is the smallest bit-width for fast inference that has been reported when models are neither retrained nor adapted.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[51610353 | Gysel et al. | 2018 | Citations: 229]": "Convolutional neural networks (CNNs) have led to remarkable progress in a number of key pattern recognition tasks, such as visual scene understanding and speech recognition, that potentially enable numerous applications. Consequently, there is a significant need to deploy trained CNNs to resource-constrained embedded systems. Inference using pretrained modern deep CNNs, however, requires significant system resources, including computation, energy, and memory space. To enable efficient implementation of trained CNNs, a viable approach is to approximate the network with an implementation-friendly model with only negligible degradation in classification accuracy. We present Ristretto, a CNN approximation framework that enables empirical investigation of the tradeoff between various number representation and word width choices and the classification accuracy of the model. Specifically, Ristretto analyzes a given CNN with respect to numerical range required to represent weights, activations, and intermediate results of convolutional and fully connected layers, and subsequently, it simulates the impact of reduced word width or lower precision arithmetic operators on the model accuracy. Moreover, Ristretto can fine-tune a quantized network to further improve its classification accuracy under a given number representation and word width configuration. Given a maximum classification accuracy degradation tolerance of 1%, we use Ristretto to demonstrate that three ImageNet networks can be condensed to use 8-bit dynamic fixed point for network weights and activations. Ristretto is available as a popular open-source software project1 and has already been viewed over 1 000 times on Github as of the submission of this brief.1https://github.com/pmgysel/caffe"
                },
                "metadata": [
                    {
                        "section_title": "Accuracy Before Retraining or Adaptation",
                        "pdf_hash": "",
                        "start": 404,
                        "end": 622,
                        "sentence_offsets": [
                            {
                                "start": 404,
                                "end": 623
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The results also shows that compression down to 4-bits (which in convolutions with channel size input of 256 would yield a 5x compression rate) results in an accuracy drop of only 1% to 2% depending on the architecture"
                    },
                    {
                        "section_title": "Accuracy Before Retraining or Adaptation",
                        "pdf_hash": "",
                        "start": 911,
                        "end": 1217,
                        "sentence_offsets": [
                            {
                                "start": 911,
                                "end": 1012
                            },
                            {
                                "start": 1013,
                                "end": 1078
                            },
                            {
                                "start": 1079,
                                "end": 1216
                            }
                        ],
                        "ref_mentions": [
                            "51610353"
                        ],
                        "quote": "Also as known in previous research papers [26](Gysel et al., 2018), models of 8/8 bits lose only around 0.1% accuracy. For models of 5/5 and 4/4, we get a drop of 1% to 3% in accuracy. To our knowledge, this is the smallest bit-width for fast inference that has been reported when models are neither retrained nor adapted."
                    }
                ]
            },
            {
                "idx": 40,
                "key": "[73479694 | Wei et al. | 2019 | Citations: 28]",
                "snippets": "The classification accuracy of the 4-bit quantized model has the maximum loss of 4.0%. The other quantized models obtain satisfying performance and the average results are all around 97.2%, with a 1.2% accuracy degradation, compared to floating-point types. It can be seen that the classification accuracy degradation is reduced as the bit-width increases, which may be due to the increased quantization resolution resulting in a computing error reduction. However, the performance can hardly improve after reaching the maximum as the inherent redundancy within the network is limited. The standards deviations of 4-bit and 6-bit quantized models are higher than the others. This means that the stability of training is not good when adopting a low bit-width quantization scheme and multiple trainings are required to obtain a satisfying performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Dataset Description and Data Preprocessing",
                        "pdf_hash": "",
                        "start": 194,
                        "end": 1044,
                        "sentence_offsets": [
                            {
                                "start": 194,
                                "end": 280
                            },
                            {
                                "start": 281,
                                "end": 451
                            },
                            {
                                "start": 452,
                                "end": 650
                            },
                            {
                                "start": 651,
                                "end": 779
                            },
                            {
                                "start": 780,
                                "end": 868
                            },
                            {
                                "start": 869,
                                "end": 1044
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The classification accuracy of the 4-bit quantized model has the maximum loss of 4.0%. The other quantized models obtain satisfying performance and the average results are all around 97.2%, with a 1.2% accuracy degradation, compared to floating-point types. It can be seen that the classification accuracy degradation is reduced as the bit-width increases, which may be due to the increased quantization resolution resulting in a computing error reduction. However, the performance can hardly improve after reaching the maximum as the inherent redundancy within the network is limited. The standards deviations of 4-bit and 6-bit quantized models are higher than the others. This means that the stability of training is not good when adopting a low bit-width quantization scheme and multiple trainings are required to obtain a satisfying performance."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.033213,
        "cot": "To answer the query about the impact of 4-bit quantization on neural network inference accuracy compared to 8-bit and 16-bit quantization, I'll organize the quotes into several key dimensions:\n\n1. Introduction/Background: This section will provide a brief overview of quantization and its importance for neural network inference. While there aren't direct quotes for this section, it's essential to provide context.\n\n2. Accuracy Impact of Different Quantization Levels: This should be a synthesis section that directly compares how accuracy changes across 16-bit, 8-bit, and 4-bit quantization. Many quotes provide specific numbers and comparisons.\n\n3. Factors Affecting Quantization Impact: This synthesis section will explain what factors influence how much accuracy is lost during quantization, including network architecture, dataset complexity, and which layers are quantized.\n\n4. Mitigation Techniques for 4-bit Quantization: This synthesis section will discuss methods to reduce accuracy loss when using 4-bit quantization, as several quotes mention different approaches.\n\n5. Trade-offs and Benefits of 4-bit Quantization: This synthesis section will explore the practical advantages of 4-bit quantization despite potential accuracy losses, including speed, memory, and efficiency improvements.\n\n6. Specific Application Cases and Performance Results: This should be a list format to enumerate concrete examples and measurements of 4-bit quantization impacts across different models and tasks.\n\nThis organization allows for a comprehensive comparison while keeping the information structured and accessible. The synthesis sections are appropriate for explaining relationships and trends, while the list section is perfect for the diverse performance results mentioned in the quotes.",
        "plan": {
            "Introduction to Neural Network Quantization (synthesis)": [],
            "Accuracy Impact of Different Quantization Levels (synthesis)": [
                1,
                2,
                3,
                4,
                5,
                6,
                8,
                10,
                11,
                17,
                18,
                19,
                25,
                26,
                27,
                34,
                35,
                36,
                37,
                38,
                39,
                40
            ],
            "Factors Affecting Quantization Impact (synthesis)": [
                0,
                1,
                3,
                12,
                14,
                20,
                28,
                31,
                32,
                33
            ],
            "Mitigation Techniques for 4-bit Quantization (synthesis)": [
                0,
                2,
                9,
                13,
                21,
                22,
                23,
                24,
                37,
                38
            ],
            "Trade-offs and Benefits of 4-bit Quantization (synthesis)": [
                5,
                9,
                13,
                14,
                15,
                16,
                21,
                24,
                30
            ],
            "Specific Application Cases and Performance Results (list)": [
                4,
                5,
                6,
                7,
                8,
                11,
                15,
                19,
                20,
                22,
                23,
                26,
                27,
                28,
                29,
                39,
                40
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Neural Network Quantization",
                "tldr": "Neural network quantization reduces model precision from higher-bit floating-point formats to lower-bit integer representations to improve inference efficiency. This process involves converting weights and activations to lower precision formats like 16-bit, 8-bit, or 4-bit to reduce memory requirements and computational costs. (LLM Memory)",
                "text": "\nNeural network quantization is a technique that reduces the numerical precision of a model's parameters (weights) and activations from high-precision floating-point formats (typically 32-bit) to lower-precision representations. This process helps in decreasing the memory footprint, power consumption, and computational requirements of neural networks while attempting to maintain acceptable accuracy levels. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nIn standard neural networks, weights and activation values are typically stored as 32-bit floating-point numbers (FP32), which provide high precision but require significant memory and computational resources. Quantization converts these values to lower precision formats such as 16-bit (FP16 or INT16), 8-bit (INT8), or 4-bit (INT4) representations. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe quantization process involves mapping the continuous range of floating-point values to a discrete set of values that can be represented with fewer bits. This mapping can be uniform (equal spacing between quantized values) or non-uniform (varying spacing based on the distribution of values). The process typically includes steps like calibration, where the range of values is analyzed to determine optimal scaling factors, and actual conversion to the target precision. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nQuantization can be applied at different stages: post-training quantization (PTQ) occurs after a model has been fully trained at high precision, while quantization-aware training (QAT) incorporates the effects of quantization during the training process to help the model adapt to reduced precision. Each approach offers different trade-offs between implementation complexity and final model accuracy. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Accuracy Impact of Different Quantization Levels",
                "tldr": "The accuracy impact of 4-bit quantization is significantly more pronounced than 8-bit or 16-bit quantization across most neural network architectures. While 8-bit and 16-bit quantization typically maintain accuracy within 1% of full precision models, 4-bit quantization can cause accuracy drops ranging from 1-12% depending on network architecture, dataset complexity, and whether mitigation techniques are employed. (18 sources)",
                "text": "\nNeural network quantization demonstrates varying degrees of accuracy impact depending on the precision level used. The most commonly studied quantization levels are 16-bit, 8-bit, and 4-bit, with each showing distinct patterns of accuracy degradation compared to full-precision (32-bit) models.\n\nAt the highest quantization level, 16-bit precision generally preserves model accuracy almost perfectly, with minimal to no measurable degradation in performance across various architectures and tasks <Paper corpusId=\"235166247\" paperTitle=\"(Shomron et al., 2021)\" isShortName></Paper> <Paper corpusId=\"225583435\" paperTitle=\"(Yee et al., 2020)\" isShortName></Paper>. This makes 16-bit quantization a safe choice when balancing efficiency and accuracy.\n\nThe 8-bit quantization level has become widely adopted in real-world implementations, as it consistently maintains accuracy very close to full-precision models. Multiple studies confirm that 8-bit quantized networks typically experience only 0.1-0.5% accuracy drop compared to their full-precision counterparts <Paper corpusId=\"57573783\" paperTitle=\"(Gennari et al., 2019)\" isShortName></Paper> <Paper corpusId=\"247595112\" paperTitle=\"(Nagel et al., 2022)\" isShortName></Paper>. This negligible accuracy loss explains why 8-bit inference has been extensively deployed in commercial GPUs and NPUs <Paper corpusId=\"220891847\" paperTitle=\"(Cho et al., 2020)\" isShortName></Paper> <Paper corpusId=\"213621056\" paperTitle=\"(Medina et al., 2020)\" isShortName></Paper> <Paper corpusId=\"4202768\" paperTitle=\"(Jouppi et al., 2017)\" isShortName></Paper>. Krishnamoorthi observed that \"at 8-bits of precision, post training quantization schemes provide close to floating point accuracy\" <Paper corpusId=\"49356451\" paperTitle=\"(Krishnamoorthi, 2018)\" isShortName></Paper>.\n\nThe transition to 4-bit quantization, however, reveals more significant accuracy challenges. While 4-bit quantization offers substantial memory and computational benefits, it typically introduces noticeable accuracy degradation compared to higher precision formats. The magnitude of this degradation varies considerably based on several factors:\n\n1. **Network Architecture**: Deeper networks generally experience more severe accuracy loss with 4-bit quantization. For instance, VGG-16 demonstrates a dramatic drop from 93.3% accuracy at full precision to approximately 20% when using 4-bit weights <Paper corpusId=\"218788236\" paperTitle=\"(Charan et al., 2020)\" isShortName></Paper>. The accuracy degradation becomes more pronounced as network depth increases.\n\n2. **Dataset Complexity**: Networks trained on simpler datasets like MNIST or CIFAR-10 tend to be more resilient to 4-bit quantization than those trained on complex datasets like ImageNet. Guo et al. found no quantization loss on simple datasets but observed approximately 12% performance loss on ImageNet <Paper corpusId=\"235266137\" paperTitle=\"(Guo et al., 2021)\" isShortName></Paper>.\n\n3. **Quantization Approach**: When comparing different quantization techniques, the accuracy impact varies significantly. Post-training quantization (PTQ) to 4-bit precision without additional techniques typically causes more substantial accuracy drops than quantization-aware training (QAT) <Paper corpusId=\"49356451\" paperTitle=\"(Krishnamoorthi, 2018)\" isShortName></Paper>. Several studies report accuracy drops of 2-5% across various models when moving from 8-bit to 4-bit weight quantization <Paper corpusId=\"261049574\" paperTitle=\"(Frumkin et al., 2023)\" isShortName></Paper>.\n\nFor specific model families, the impact of 4-bit quantization presents interesting patterns. Large language models (LLMs) appear to maintain most of their emergent abilities at 4-bit precision, showing only minimal degradation in reasoning and instruction-following tasks <Paper corpusId=\"259937594\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"276903421\" paperTitle=\"(Berti et al., 2025)\" isShortName></Paper>. However, when quantization precision drops to 2-bit, performance typically degrades to near-random levels.\n\nThe accuracy impact also depends on the specific layer being quantized. For example, in SqueezeNet, 4-bit quantization of weights in the 'fire6-expand3x3' layer caused accuracy to drop to 42.5%, while the same quantization applied to the 'fire9-expand3x3' layer resulted in only a 5% accuracy reduction <Paper corpusId=\"225583435\" paperTitle=\"(Yee et al., 2020)\" isShortName></Paper>.\n\nVarious studies have quantified the typical accuracy loss with 4-bit quantization:\n- McKinstry et al. found that 4-bit networks required significantly longer fine-tuning compared to 8-bit networks to match or exceed pretrained floating-point accuracy <Paper corpusId=\"52197199\" paperTitle=\"(McKinstry et al., 2018)\" isShortName></Paper>.\n- Wei et al. observed a maximum accuracy loss of 4.0% for 4-bit quantized models, with higher variance in training stability compared to higher-precision models <Paper corpusId=\"73479694\" paperTitle=\"(Wei et al., 2019)\" isShortName></Paper>.\n- Trusov et al. reported that 4-bit quantization resulted in 95.0% accuracy compared to 95.4% for 8-bit quantization <Paper corpusId=\"221655691\" paperTitle=\"(Trusov et al., 2020)\" isShortName></Paper>.\n- Tumialis et al. found that 4-bit networks performed approximately four percentage points worse than full-precision networks <Paper corpusId=\"275038383\" paperTitle=\"(Tumialis et al., 2024)\" isShortName></Paper>.\n\nIn quantitative terms, the relative root mean square error (RRMSE) when decreasing from 8-bit to 4-bit uniform quantization increases by factors of 17.2x and 16x for different network groups, highlighting the increased noise introduced at lower precision <Paper corpusId=\"259274857\" paperTitle=\"(Sabri et al., 2023)\" isShortName></Paper>.\n\nOverall, the empirical evidence demonstrates a clear pattern: while 8-bit and 16-bit quantization can be applied with minimal accuracy impact across most network architectures, 4-bit quantization introduces more significant accuracy challenges that may require additional techniques to mitigate, especially for complex models and datasets.",
                "citations": [
                    {
                        "id": "(Shomron et al., 2021)",
                        "snippets": [
                            "Mapping FP32 models to INT8 using uniform PTQ yields models with negligible accuracy degradation; however, reducing precision below 8 bits with PTQ is challenging, as accuracy degradation becomes noticeable, due to the increase in quantization noise."
                        ],
                        "paper": {
                            "corpus_id": 235166247,
                            "title": "Post-Training Sparsity-Aware Quantization",
                            "authors": [
                                {
                                    "authorId": "19190632",
                                    "name": "Gil Shomron"
                                },
                                {
                                    "authorId": "3147903",
                                    "name": "F. Gabbay"
                                },
                                {
                                    "authorId": "2104448639",
                                    "name": "Samer Kurzum"
                                },
                                {
                                    "authorId": "1948330",
                                    "name": "U. Weiser"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 34
                        },
                        "score": 0.96826171875
                    },
                    {
                        "id": "(Yee et al., 2020)",
                        "snippets": [
                            "For 4-bit fixed-point quantization, the accuracy drops to 72.5% and 85% from the original 90% after quantization on the weights in the layer 'fire8-expand3x3' and 'fire9expand3x3'. The accuracy drops the to the lowest which is 42.5% after 4-bit fixed point quantization is being applied on the weights in the layer 'fire6-expand3x3'. This might due to the large value changes in total weight after quantization. Although the model size is compressed the most after applying weight quantization in the layer 'fire8-expand3x3', but there is 17.5% accuracy drop. Hence, it is recommended to apply 4-bit quantization on weights in layer 'fire-9-expand3x3' when doing 4-bit fixed point quantization on the weights in a single layer of the network as it causes a slight drop in accuracy while reducing the model size. For 8-bit and 16-bit fixed-point quantization, there is no change even when weight quantization is applied on any of the layers."
                        ],
                        "paper": {
                            "corpus_id": 225583435,
                            "title": "Face Recognition and Machine Learning at the Edge",
                            "authors": [
                                {
                                    "authorId": "2004954858",
                                    "name": "Joanne Ling Sin Yee"
                                },
                                {
                                    "authorId": "2412102",
                                    "name": "U. U. Sheikh"
                                },
                                {
                                    "authorId": "1951977",
                                    "name": "M. Mokji"
                                },
                                {
                                    "authorId": "2111988471",
                                    "name": "S. Rahman"
                                }
                            ],
                            "year": 2020,
                            "venue": "IOP Conference Series: Materials Science and Engineering",
                            "n_citations": 2
                        },
                        "score": 0.98193359375
                    },
                    {
                        "id": "(Gennari et al., 2019)",
                        "snippets": [
                            "The results also shows that compression down to 4-bits (which in convolutions with channel size input of 256 would yield a 5x compression rate) results in an accuracy drop of only 1% to 2% depending on the architecture",
                            "Also as known in previous research papers [26](Gysel et al., 2018), models of 8/8 bits lose only around 0.1% accuracy. For models of 5/5 and 4/4, we get a drop of 1% to 3% in accuracy. To our knowledge, this is the smallest bit-width for fast inference that has been reported when models are neither retrained nor adapted."
                        ],
                        "paper": {
                            "corpus_id": 57573783,
                            "title": "DSConv: Efficient Convolution Operator",
                            "authors": [
                                {
                                    "authorId": "2065676996",
                                    "name": "Marcelo Gennari"
                                },
                                {
                                    "authorId": "144924636",
                                    "name": "Roger Fawcett"
                                },
                                {
                                    "authorId": "2824784",
                                    "name": "V. Prisacariu"
                                }
                            ],
                            "year": 2019,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 67
                        },
                        "score": 0.9765625
                    },
                    {
                        "id": "(Nagel et al., 2022)",
                        "snippets": [
                            "Despite its clear power and latency benefits, quantization comes at the cost of added noise due to the reduced precision. However, researchers in recent years have shown that neural networks are robust to this noise and can be quantized to 8-bits with a minimal drop in accuracy using post-training quantization techniques (PTQ) (Nagel et al., 2019)(Banner et al., 2018)Nahshan et al., 2020). PTQ can be very efficient and, generally, only requires access to a small calibration dataset, but suffers when applied to low-bit quantization (\u2264 4-bits) of neural networks."
                        ],
                        "paper": {
                            "corpus_id": 247595112,
                            "title": "Overcoming Oscillations in Quantization-Aware Training",
                            "authors": [
                                {
                                    "authorId": "41229153",
                                    "name": "Markus Nagel"
                                },
                                {
                                    "authorId": "2090358941",
                                    "name": "Marios Fournarakis"
                                },
                                {
                                    "authorId": "2112207572",
                                    "name": "Yelysei Bondarenko"
                                },
                                {
                                    "authorId": "83133279",
                                    "name": "Tijmen Blankevoort"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 108
                        },
                        "score": 0.9580078125
                    },
                    {
                        "id": "(Cho et al., 2020)",
                        "snippets": [
                            "For CNNs, inferences with int8 precision has been widely adopted in an increasing number of real-world GPUs [17], [34]/NPUs (Medina et al., 2020), (Jouppi et al., 2017), as int8 precision has been shown to be sufficient for GEMM operation in the inference of various CNNs in recent studies. It has also been found that sub-8-bit quantization of the inference of a CNN does not lead to significant accuracy loss. For example, with the ImageNet classification dataset (Deng et al., 2009), [38] reports an activation 4-bit and weight 4-bit quantization of an ResNet-50 (He et al., 2015) with a 0.2% top-1 accuracy loss compared to the case of FP32, in which an activation 2-bit and weight 2-bit quantization was done and a 3.2% top-1 accuracy loss was obtained."
                        ],
                        "paper": {
                            "corpus_id": 220891847,
                            "title": "McDRAM v2: In-Dynamic Random Access Memory Systolic Array Accelerator to Address the Large Model Problem in Deep Neural Networks on the Edge",
                            "authors": [
                                {
                                    "authorId": "2111878754",
                                    "name": "Seunghwan Cho"
                                },
                                {
                                    "authorId": "1716222897",
                                    "name": "Haerang Choi"
                                },
                                {
                                    "authorId": "2292315",
                                    "name": "Eunhyeok Park"
                                },
                                {
                                    "authorId": "10307183",
                                    "name": "Hyunsung Shin"
                                },
                                {
                                    "authorId": "1808405",
                                    "name": "Sungjoo Yoo"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE Access",
                            "n_citations": 41
                        },
                        "score": 0.962890625
                    },
                    {
                        "id": "(Medina et al., 2020)",
                        "snippets": [
                            "The growing computational requirements of AI applications are challenging today's general-purpose CPU and GPU architectures and driving the need for purpose-built, programmable AI solutions. Habana Labs designed its Goya processor to meet the high throughput/low latency demands of Inference workloads, and its Gaudi processor for throughput combined with massive scale up and scale out capability needed to speed training workloads efficiently. To address the need for scaling training, Habana is the first AI chip developer to integrate standard Ethernet onto a training processor."
                        ],
                        "paper": {
                            "corpus_id": 213621056,
                            "title": "Habana Labs Purpose-Built AI Inference and Training Processor Architectures: Scaling AI Training Systems Using Standard Ethernet With Gaudi Processor",
                            "authors": [
                                {
                                    "authorId": "1379959105",
                                    "name": "Eitan Medina"
                                },
                                {
                                    "authorId": "1581369413",
                                    "name": "Eran Dagan"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE Micro",
                            "n_citations": 37
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jouppi et al., 2017)",
                        "snippets": [
                            "Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC-called a Tensor Processing Unit (TPU)-deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X\u201330X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X\u201380X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU."
                        ],
                        "paper": {
                            "corpus_id": 4202768,
                            "title": "In-datacenter performance analysis of a tensor processing unit",
                            "authors": [
                                {
                                    "authorId": "2360527413",
                                    "name": "Norman P. Jouppi"
                                },
                                {
                                    "authorId": "39660914",
                                    "name": "C. Young"
                                },
                                {
                                    "authorId": "2056800684",
                                    "name": "Nishant Patil"
                                },
                                {
                                    "authorId": "2265549299",
                                    "name": "David Patterson"
                                },
                                {
                                    "authorId": "2265528597",
                                    "name": "Gaurav Agrawal"
                                },
                                {
                                    "authorId": "2265528365",
                                    "name": "Raminder Bajwa"
                                },
                                {
                                    "authorId": "2265528604",
                                    "name": "Sarah Bates"
                                },
                                {
                                    "authorId": "2265528570",
                                    "name": "Suresh Bhatia"
                                },
                                {
                                    "authorId": "2265528912",
                                    "name": "Nan Boden"
                                },
                                {
                                    "authorId": "2265529409",
                                    "name": "Al Borchers"
                                },
                                {
                                    "authorId": "2265529523",
                                    "name": "Rick Boyle"
                                },
                                {
                                    "authorId": "2265531532",
                                    "name": "Pierre-luc Cantin"
                                },
                                {
                                    "authorId": "2265528729",
                                    "name": "Clifford Chao"
                                },
                                {
                                    "authorId": "2265550963",
                                    "name": "Chris Clark"
                                },
                                {
                                    "authorId": "2265528756",
                                    "name": "Jeremy Coriell"
                                },
                                {
                                    "authorId": "2265529003",
                                    "name": "Mike Daley"
                                },
                                {
                                    "authorId": "2265528872",
                                    "name": "Matt Dau"
                                },
                                {
                                    "authorId": "2265529729",
                                    "name": "Jeffrey Dean"
                                },
                                {
                                    "authorId": "2265528537",
                                    "name": "Ben Gelb"
                                },
                                {
                                    "authorId": "1398213038",
                                    "name": "Taraneh Ghaemmaghami"
                                },
                                {
                                    "authorId": "2265528754",
                                    "name": "Rajendra Gottipati"
                                },
                                {
                                    "authorId": "2265528752",
                                    "name": "William Gulland"
                                },
                                {
                                    "authorId": "2265528749",
                                    "name": "Robert Hagmann"
                                },
                                {
                                    "authorId": "2361904494",
                                    "name": "C. Richard Ho"
                                },
                                {
                                    "authorId": "2265528842",
                                    "name": "Doug Hogberg"
                                },
                                {
                                    "authorId": "2265596858",
                                    "name": "John Hu"
                                },
                                {
                                    "authorId": "1701671",
                                    "name": "R. Hundt"
                                },
                                {
                                    "authorId": "2265528853",
                                    "name": "Dan Hurt"
                                },
                                {
                                    "authorId": "2265529394",
                                    "name": "Julian Ibarz"
                                },
                                {
                                    "authorId": "10433955",
                                    "name": "A. Jaffey"
                                },
                                {
                                    "authorId": "2265529292",
                                    "name": "Alek Jaworski"
                                },
                                {
                                    "authorId": "2265551723",
                                    "name": "Alexander Kaplan"
                                },
                                {
                                    "authorId": "2265529388",
                                    "name": "Harshit Khaitan"
                                },
                                {
                                    "authorId": "35002044",
                                    "name": "Daniel Killebrew"
                                },
                                {
                                    "authorId": "2265529856",
                                    "name": "Andy Koch"
                                },
                                {
                                    "authorId": "2265642844",
                                    "name": "Naveen Kumar"
                                },
                                {
                                    "authorId": "2265529386",
                                    "name": "Steve Lacy"
                                },
                                {
                                    "authorId": "2258707080",
                                    "name": "James Laudon"
                                },
                                {
                                    "authorId": "2265529416",
                                    "name": "James Law"
                                },
                                {
                                    "authorId": "2265529515",
                                    "name": "Diemthu Le"
                                },
                                {
                                    "authorId": "2265529268",
                                    "name": "Chris Leary"
                                },
                                {
                                    "authorId": "2265576530",
                                    "name": "Zhuyuan Liu"
                                },
                                {
                                    "authorId": "2265532187",
                                    "name": "Kyle Lucke"
                                },
                                {
                                    "authorId": "2265529249",
                                    "name": "Alan Lundin"
                                },
                                {
                                    "authorId": "2265528983",
                                    "name": "Gordon MacKean"
                                },
                                {
                                    "authorId": "2265528640",
                                    "name": "Adriana Maggiore"
                                },
                                {
                                    "authorId": "2265529551",
                                    "name": "Maire Mahony"
                                },
                                {
                                    "authorId": "2265619145",
                                    "name": "Kieran Miller"
                                },
                                {
                                    "authorId": "1395811464",
                                    "name": "R. Nagarajan"
                                },
                                {
                                    "authorId": "1398213032",
                                    "name": "Ravi Narayanaswami"
                                },
                                {
                                    "authorId": "2265529536",
                                    "name": "Ray Ni"
                                },
                                {
                                    "authorId": "2265531761",
                                    "name": "Kathy Nix"
                                },
                                {
                                    "authorId": "3222376",
                                    "name": "Thomas Norrie"
                                },
                                {
                                    "authorId": "3175815",
                                    "name": "Mark Omernick"
                                },
                                {
                                    "authorId": "1398212990",
                                    "name": "Narayana Penukonda"
                                },
                                {
                                    "authorId": "2265528744",
                                    "name": "Andy Phelps"
                                },
                                {
                                    "authorId": "2265552909",
                                    "name": "Jonathan Ross"
                                },
                                {
                                    "authorId": "2265552588",
                                    "name": "Matt Ross"
                                },
                                {
                                    "authorId": "2265528580",
                                    "name": "Amir Salek"
                                },
                                {
                                    "authorId": "9364218",
                                    "name": "Emad Samadiani"
                                },
                                {
                                    "authorId": "2265528562",
                                    "name": "Chris Severn"
                                },
                                {
                                    "authorId": "2265528974",
                                    "name": "Gregory Sizikov"
                                },
                                {
                                    "authorId": "2265528625",
                                    "name": "Matthew Snelham"
                                },
                                {
                                    "authorId": "2265529370",
                                    "name": "Jed Souter"
                                },
                                {
                                    "authorId": "2265532375",
                                    "name": "Dan Steinberg"
                                },
                                {
                                    "authorId": "1394189636",
                                    "name": "Andy Swing"
                                },
                                {
                                    "authorId": "2265616791",
                                    "name": "Mercedes Tan"
                                },
                                {
                                    "authorId": "2265532008",
                                    "name": "Gregory Thorson"
                                },
                                {
                                    "authorId": "2265529994",
                                    "name": "Bo Tian"
                                },
                                {
                                    "authorId": "2265529013",
                                    "name": "Horia Toma"
                                },
                                {
                                    "authorId": "2265528972",
                                    "name": "Erick Tuttle"
                                },
                                {
                                    "authorId": "2265528643",
                                    "name": "Vijay Vasudevan"
                                },
                                {
                                    "authorId": "2265529413",
                                    "name": "Richard Walter"
                                },
                                {
                                    "authorId": "2265617722",
                                    "name": "Walter Wang"
                                },
                                {
                                    "authorId": "2265529247",
                                    "name": "Eric Wilcox"
                                },
                                {
                                    "authorId": "2360529871",
                                    "name": "Doe Hyun Yoon"
                                }
                            ],
                            "year": 2017,
                            "venue": "International Symposium on Computer Architecture",
                            "n_citations": 4643
                        },
                        "score": 0
                    },
                    {
                        "id": "(Krishnamoorthi, 2018)",
                        "snippets": [
                            "We note that at 8-bits of precision, post training quantization schemes provide close to floating point accuracy. In order to better understand the benefits of quantization aware training, we perform experiments to assess performance at 4 bit quantization for weights and activations",
                            "It is interesting to see that for most networks, one can obtain accuracies within 5% of 8-bit quantization with fine tuning 4 bit weights (column 4 of Table 5). The improvements due to fine tuning are also more apparent at 4 bits. Note that activations are quantized to 8-bits in these experiments."
                        ],
                        "paper": {
                            "corpus_id": 49356451,
                            "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
                            "authors": [
                                {
                                    "authorId": "2065915235",
                                    "name": "Raghuraman Krishnamoorthi"
                                }
                            ],
                            "year": 2018,
                            "venue": "arXiv.org",
                            "n_citations": 1021
                        },
                        "score": 0.98876953125
                    },
                    {
                        "id": "(Charan et al., 2020)",
                        "snippets": [
                            "We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization."
                        ],
                        "paper": {
                            "corpus_id": 218788236,
                            "title": "Accurate Inference With Inaccurate RRAM Devices: A Joint Algorithm-Design Solution",
                            "authors": [
                                {
                                    "authorId": "91781917",
                                    "name": "Gouranga Charan"
                                },
                                {
                                    "authorId": "2942785",
                                    "name": "Abinash Mohanty"
                                },
                                {
                                    "authorId": "3457252",
                                    "name": "Xiaocong Du"
                                },
                                {
                                    "authorId": "144828251",
                                    "name": "Gokul Krishnan"
                                },
                                {
                                    "authorId": "145553390",
                                    "name": "R. Joshi"
                                },
                                {
                                    "authorId": "1965873861",
                                    "name": "Yu Cao"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE Journal on Exploratory Solid-State Computational Devices and Circuits",
                            "n_citations": 30
                        },
                        "score": 0.9853515625
                    },
                    {
                        "id": "(Guo et al., 2021)",
                        "snippets": [
                            "We tried 4-bit quantization on some simple datasets(MNIST, CIFAR10, CIFAR100), and there was no quantization loss. We also tried 4-bit quantization on big dataset(ImageNet). The experiment result shows that there is about 12% performance loss, but due to the inference efficiency of 4-bit quantization, this loss may be tolerable in some application scenarios."
                        ],
                        "paper": {
                            "corpus_id": 235266137,
                            "title": "Integer-Only Neural Network Quantization Scheme Based on Shift-Batch-Normalization",
                            "authors": [
                                {
                                    "authorId": "2892749",
                                    "name": "Qingyu Guo"
                                },
                                {
                                    "authorId": "2156166711",
                                    "name": "Yuan Wang"
                                },
                                {
                                    "authorId": "39658679",
                                    "name": "Xiaoxin Cui"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.9814453125
                    },
                    {
                        "id": "(Frumkin et al., 2023)",
                        "snippets": [
                            "Moving from 8-bit to 4-bit weight quantization, we see an accuracy degradation of about 2 \u2212 5% across all models."
                        ],
                        "paper": {
                            "corpus_id": 261049574,
                            "title": "Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers",
                            "authors": [
                                {
                                    "authorId": "2094690757",
                                    "name": "N. Frumkin"
                                },
                                {
                                    "authorId": "2193705",
                                    "name": "Dibakar Gope"
                                },
                                {
                                    "authorId": "92419662",
                                    "name": "D. Marculescu"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 17
                        },
                        "score": 0.9658203125
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "Overall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K."
                        ],
                        "paper": {
                            "corpus_id": 259937594,
                            "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study",
                            "authors": [
                                {
                                    "authorId": "2108129670",
                                    "name": "Peiyu Liu"
                                },
                                {
                                    "authorId": "2119618242",
                                    "name": "Zikang Liu"
                                },
                                {
                                    "authorId": "9136116",
                                    "name": "Ze-Feng Gao"
                                },
                                {
                                    "authorId": "2162036220",
                                    "name": "Dawei Gao"
                                },
                                {
                                    "authorId": "2542603",
                                    "name": "Wayne Xin Zhao"
                                },
                                {
                                    "authorId": "2110479359",
                                    "name": "Yaliang Li"
                                },
                                {
                                    "authorId": "1696332",
                                    "name": "Bolin Ding"
                                },
                                {
                                    "authorId": "153693432",
                                    "name": "Ji-rong Wen"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Language Resources and Evaluation",
                            "n_citations": 35
                        },
                        "score": 0.98291015625
                    },
                    {
                        "id": "(Berti et al., 2025)",
                        "snippets": [
                            "At higher bit levels, such as 8-bit and 16-bit, LLMs retain much of their original performance, with minimal degradation in reasoning and instruction-following tasks. However, as precision decreases, particularly at 4-bit and below, a clear pattern emerges: while 4-bit quantization manages to preserve most emergent abilities, 2-bit quantization severely degrades performance, often reducing accuracy to near-random levels."
                        ],
                        "paper": {
                            "corpus_id": 276903421,
                            "title": "Emergent Abilities in Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2229196235",
                                    "name": "Leonardo Berti"
                                },
                                {
                                    "authorId": "2325903265",
                                    "name": "Flavio Giorgi"
                                },
                                {
                                    "authorId": "1686448",
                                    "name": "Gjergji Kasneci"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.96875
                    },
                    {
                        "id": "(McKinstry et al., 2018)",
                        "snippets": [
                            "FAQ trained 8-bit networks outperform all comparable quantization methods in all but one instance and exceeded pretrained fp32 network accuracy with only one epoch of training following quantization for all networks explored (Table 1). Immediately following quantization, network accuracy was nearly at the level of the pretrained networks (data not shown) with one exception, Inception-v3, which started at 72.34% top-1. Since the networks started close to a good solution, they did not require extensive fine-tuning to return to and surpass pretrained networks. \n\nFAQ trained 4-bit network accuracy outperforms all comparable quantization methods, exceeding the next closest approach by over 0.5% for ResNet-18 (Jung et al., 2018), and matched or exceeded pretrained fp32 network accuracy. FAQ 4-bit networks required significantly longer fine-tuning -110 epochs -for networks trained, ResNet-18, ResNet-34, and ResNet-50. In constrast to the 8-bit cases, immediately following quantization, network accuracy dropped precipitously, requiring significant fine-tuning to match and surpass the pretrained networks."
                        ],
                        "paper": {
                            "corpus_id": 52197199,
                            "title": "Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference",
                            "authors": [
                                {
                                    "authorId": "46571359",
                                    "name": "J. McKinstry"
                                },
                                {
                                    "authorId": "2357931",
                                    "name": "S. K. Esser"
                                },
                                {
                                    "authorId": "2730753",
                                    "name": "R. Appuswamy"
                                },
                                {
                                    "authorId": "2064431971",
                                    "name": "Deepika Bablani"
                                },
                                {
                                    "authorId": "2248110488",
                                    "name": "John V. Arthur"
                                },
                                {
                                    "authorId": "3121907",
                                    "name": "Izzet B. Yildiz"
                                },
                                {
                                    "authorId": "1944330",
                                    "name": "D. Modha"
                                }
                            ],
                            "year": 2018,
                            "venue": "arXiv.org",
                            "n_citations": 94
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Wei et al., 2019)",
                        "snippets": [
                            "The classification accuracy of the 4-bit quantized model has the maximum loss of 4.0%. The other quantized models obtain satisfying performance and the average results are all around 97.2%, with a 1.2% accuracy degradation, compared to floating-point types. It can be seen that the classification accuracy degradation is reduced as the bit-width increases, which may be due to the increased quantization resolution resulting in a computing error reduction. However, the performance can hardly improve after reaching the maximum as the inherent redundancy within the network is limited. The standards deviations of 4-bit and 6-bit quantized models are higher than the others. This means that the stability of training is not good when adopting a low bit-width quantization scheme and multiple trainings are required to obtain a satisfying performance."
                        ],
                        "paper": {
                            "corpus_id": 73479694,
                            "title": "FPGA-Based Hybrid-Type Implementation of Quantized Neural Networks for Remote Sensing Applications",
                            "authors": [
                                {
                                    "authorId": "1382778048",
                                    "name": "Xin Wei"
                                },
                                {
                                    "authorId": "2109188137",
                                    "name": "Wenchao Liu"
                                },
                                {
                                    "authorId": "2146072742",
                                    "name": "Lei Chen"
                                },
                                {
                                    "authorId": "145044878",
                                    "name": "Long Ma"
                                },
                                {
                                    "authorId": "92779203",
                                    "name": "He Chen"
                                },
                                {
                                    "authorId": "2199301922",
                                    "name": "Zhuang Yin"
                                }
                            ],
                            "year": 2019,
                            "venue": "Italian National Conference on Sensors",
                            "n_citations": 28
                        },
                        "score": 0.97509765625
                    },
                    {
                        "id": "(Trusov et al., 2020)",
                        "snippets": [
                            "4-bit quantization gives 95.0% accuracy and 48% overall inference speedup, while an 8-bit quantized network gives 95.4% accuracy and 39% speedup."
                        ],
                        "paper": {
                            "corpus_id": 221655691,
                            "title": "Fast Implementation of 4-bit Convolutional Neural Networks for Mobile Devices",
                            "authors": [
                                {
                                    "authorId": "1441088616",
                                    "name": "A. Trusov"
                                },
                                {
                                    "authorId": "37239110",
                                    "name": "E. Limonova"
                                },
                                {
                                    "authorId": "89361905",
                                    "name": "Dmitry Slugin"
                                },
                                {
                                    "authorId": "35331830",
                                    "name": "D. Nikolaev"
                                },
                                {
                                    "authorId": "35343060",
                                    "name": "V. Arlazarov"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Pattern Recognition",
                            "n_citations": 17
                        },
                        "score": 0.98291015625
                    },
                    {
                        "id": "(Tumialis et al., 2024)",
                        "snippets": [
                            "Thanks to this, the accuracy of the 4-bit networks did not differ significantly from the 8-bit ones and their results were approximately four percentage points worse than those of the full precision networks."
                        ],
                        "paper": {
                            "corpus_id": 275038383,
                            "title": "The Impact of 8- and 4-Bit Quantization on the Accuracy and Silicon Area Footprint of Tiny Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2337212061",
                                    "name": "Pawe\u0142 Tumialis"
                                },
                                {
                                    "authorId": "2337207657",
                                    "name": "Marcel Skierkowski"
                                },
                                {
                                    "authorId": "2337211715",
                                    "name": "Jakub Przychodny"
                                },
                                {
                                    "authorId": "2337209535",
                                    "name": "Pawe\u0142 Obszarski"
                                }
                            ],
                            "year": 2024,
                            "venue": "Electronics",
                            "n_citations": 1
                        },
                        "score": 0.96728515625
                    },
                    {
                        "id": "(Sabri et al., 2023)",
                        "snippets": [
                            "Similarly, when decreasing the numerical precision of the uniform quantization to 4-bits, the RRMSE increases to 28.81% and 7.89% for groups (a) and (b) respectively, which corresponds to an increment of 17.2x and 16x respect the 8-bit uniform quantization."
                        ],
                        "paper": {
                            "corpus_id": 259274857,
                            "title": "ReDy: A Novel ReRAM-centric Dynamic Quantization Approach for Energy-efficient CNN Inference",
                            "authors": [
                                {
                                    "authorId": "2058852328",
                                    "name": "Mohammad Sabri"
                                },
                                {
                                    "authorId": "143640059",
                                    "name": "Marc Riera"
                                },
                                {
                                    "authorId": "2220761969",
                                    "name": "Antonio Gonz'alez"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.96533203125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Factors Affecting Quantization Impact",
                "tldr": "Several key factors determine how severely 4-bit quantization affects neural network accuracy, including network architecture complexity, layer position within the network, and whether weights or activations are being quantized. (9 sources)",
                "text": "\nThe impact of quantization on neural network accuracy varies significantly based on multiple factors that must be considered when implementing lower-precision models. Understanding these factors is crucial for predicting and mitigating potential accuracy degradation, particularly at 4-bit precision levels.\n\n## Network Architecture and Complexity\nThe architecture and depth of a neural network significantly influence its resilience to quantization. Deeper networks generally experience more severe accuracy degradation when quantized to lower bit-widths. As demonstrated in research by Charan et al., \"The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to ~20% in 4-bit (INT4) quantization\" <Paper corpusId=\"218788236\" paperTitle=\"(Charan et al., 2020)\" isShortName></Paper>. This dramatic reduction in accuracy highlights how architectural complexity can amplify quantization effects.\n\n## Layer Position and Sensitivity\nThe position of layers within a network plays a critical role in quantization sensitivity. Studies have shown that early layers are particularly sensitive to quantization errors. Caldeira et al. found that \"model performance suffers a bigger impact when the first layers are quantized due to the forward propagation of the performed approximations, allowing for stronger quantization when progressing toward the last layers of the network\" <Paper corpusId=\"267035013\" paperTitle=\"(Caldeira et al., 2024)\" isShortName></Paper>. This insight has led to the development of mixed-precision approaches that apply higher precision to sensitive layers while using lower precision for less sensitive layers.\n\nPosso et al. further confirmed that \"the binarization of early layers diminishes the generalization capabilities of the neural network\" <Paper corpusId=\"271050128\" paperTitle=\"(Posso et al., 2024)\" isShortName></Paper>, suggesting that maintaining higher precision in early layers is crucial for preserving model performance.\n\n## Differential Sensitivity: Weights vs. Activations\nA particularly important insight is that weights and activations respond differently to quantization. Multiple studies have demonstrated that activations are generally more sensitive to precision reduction than weights. Posso et al. observed that \"activations are notably more sensitive to low-bit quantization, with quantization errors below 4 bits frequently causing orientation decoding errors and disrupting the neural network's training process\" <Paper corpusId=\"271050128\" paperTitle=\"(Posso et al., 2024)\" isShortName></Paper>.\n\nThis differential sensitivity has led to asymmetric quantization approaches. Pasricha noted that \"Studies have shown that activations are more susceptible to bitwidth reduction than weight parameters\" <Paper corpusId=\"271571530\" paperTitle=\"(Pasricha, 2024)\" isShortName></Paper>. In an analysis where weights were restricted to binary values but activation precision was varied, researchers found that \"accuracy had notable change initially as activations bits were increased, but this gain in accuracy soon saturated,\" leading to the conclusion that \"4-bit activations... allowed reducing bitwidths without significantly reducing model accuracy\" <Paper corpusId=\"271571530\" paperTitle=\"(Pasricha, 2024)\" isShortName></Paper> <Paper corpusId=\"235795063\" paperTitle=\"(Sunny et al., 2021)\" isShortName></Paper>.\n\nRecent work on large language models supports this observation, with Lang et al. reporting that \"LLM-QAT demonstrates better accuracy with a configuration of 4-bit weights, 4-bit KV caches, and 8-bit activations, compared to a uniform 4-bit setting across all precision\" <Paper corpusId=\"273821224\" paperTitle=\"(Lang et al., 2024)\" isShortName></Paper>.\n\n## Task-Specific Sensitivity\nThe specific task a model performs can influence its quantization sensitivity. For instance, Posso et al. found that \"quantization below 8 bits has a more pronounced impact on orientation accuracy than on position accuracy\" <Paper corpusId=\"271050128\" paperTitle=\"(Posso et al., 2024)\" isShortName></Paper>, suggesting that certain predictive tasks may be more resilient to quantization than others.\n\nIn facial recognition applications, Caldeira et al. observed extreme performance variance at 4-bit precision: \"a MobileFaceNet trained in the identification task suffered a reduction in accuracy from 99.80% in 6-bit quantization to 5.53% in 4-bit quantization\" <Paper corpusId=\"267035013\" paperTitle=\"(Caldeira et al., 2024)\" isShortName></Paper>. This highlights how task-specific factors can dramatically influence quantization tolerance.\n\n## Information Bottleneck Effects\nFrom an information theory perspective, quantization creates an information bottleneck. Lorenzen et al. observed that \"For the 4-bit networks, as each neuron can take only 2^4 = 16 different values, the total number of possible states per layer decreases significantly... which indicates that the binning used in the estimation of the MI in previous experiments has discarded information vital to the network\" <Paper corpusId=\"235623747\" paperTitle=\"(Lorenzen et al., 2021)\" isShortName></Paper>. This explains why some models experience catastrophic accuracy drops below certain precision thresholds.\n\n## Error Propagation Characteristics\nInterestingly, quantization can have unexpected effects on error resilience in certain hardware implementations. Xiao et al. found that \"the 4-bit model is substantially more resilient to state-independent errors than the 8-bit network\" because \"The large separation between the 16 activation levels effectively cuts off the propagation of accumulated cell errors from one layer to the next\" <Paper corpusId=\"237416732\" paperTitle=\"(Xiao et al., 2021)\" isShortName></Paper>. This demonstrates how quantization can sometimes provide beneficial properties in specific hardware contexts, despite reducing overall precision.\n\nUnderstanding these factors helps explain why \"Naively quantizing a single-precision floating point (FP32) model to 4 bits (INT4) or lower usually incurs a significant accuracy degradation\" <Paper corpusId=\"211146310\" paperTitle=\"(Shkolnik et al., 2020)\" isShortName></Paper>, and provides guidance for developing effective mitigation strategies.",
                "citations": [
                    {
                        "id": "(Charan et al., 2020)",
                        "snippets": [
                            "We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization."
                        ],
                        "paper": {
                            "corpus_id": 218788236,
                            "title": "Accurate Inference With Inaccurate RRAM Devices: A Joint Algorithm-Design Solution",
                            "authors": [
                                {
                                    "authorId": "91781917",
                                    "name": "Gouranga Charan"
                                },
                                {
                                    "authorId": "2942785",
                                    "name": "Abinash Mohanty"
                                },
                                {
                                    "authorId": "3457252",
                                    "name": "Xiaocong Du"
                                },
                                {
                                    "authorId": "144828251",
                                    "name": "Gokul Krishnan"
                                },
                                {
                                    "authorId": "145553390",
                                    "name": "R. Joshi"
                                },
                                {
                                    "authorId": "1965873861",
                                    "name": "Yu Cao"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE Journal on Exploratory Solid-State Computational Devices and Circuits",
                            "n_citations": 30
                        },
                        "score": 0.9853515625
                    },
                    {
                        "id": "(Caldeira et al., 2024)",
                        "snippets": [
                            "The authors prove that model performance suffers a bigger impact when the first layers are quantized due to the forward propagation of the performed approximations, allowing for stronger quantization when progressing toward the last layers of the network. The final MPQ framework comprised 8, 4 and 2-bit quantization for the initial, middle and final layers, respectively, resulting in a network with only 10% of the computational footprint of the original model and a performance drop of 0.68 percentual points on LFW. Thus, MPQ can be extremely useful in establishing a compromise between achieving good performances and having computationally efficient models",
                            ".When b = 8 or b = 6, the performance degradation was small in the identification and verification tasks and, in some cases, the quantized models even slightly surpassed the baseline. For b = 4, the performance suffered an extreme drop. As an example, a MobileFaceNet trained in the identification task suffered a reduction in accuracy from 99.80% in 6-bit quantization to 5.53% in 4-bit quantization. Hence, in this experimental setup, 4-bit quantization is not enough to achieve good performance, showing that this precision level does not allow the model to learn how to encode the relevant features properly."
                        ],
                        "paper": {
                            "corpus_id": 267035013,
                            "title": "Model Compression Techniques in Biometrics Applications: A Survey",
                            "authors": [
                                {
                                    "authorId": "2219025387",
                                    "name": "Eduarda Caldeira"
                                },
                                {
                                    "authorId": "2044499858",
                                    "name": "Pedro C. Neto"
                                },
                                {
                                    "authorId": "1516862195",
                                    "name": "Marco Huber"
                                },
                                {
                                    "authorId": "2265721",
                                    "name": "N. Damer"
                                },
                                {
                                    "authorId": "145279873",
                                    "name": "Ana F. Sequeira"
                                }
                            ],
                            "year": 2024,
                            "venue": "Information Fusion",
                            "n_citations": 10
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Posso et al., 2024)",
                        "snippets": [
                            "Further experiments have revealed that activations are notably more sensitive to low-bit quantization, with quantization errors below 4 bits frequently causing orientation decoding errors and disrupting the neural network's training process",
                            "Our findings also indicate that the binarization of early layers diminishes the generalization capabilities of the neural network",
                            "Therefore, quantization below 8 bits has a more pronounced impact on orientation accuracy than on position accuracy."
                        ],
                        "paper": {
                            "corpus_id": 271050128,
                            "title": "Real-Time Spacecraft Pose Estimation Using Mixed-Precision Quantized Neural Network on COTS Reconfigurable MPSoC",
                            "authors": [
                                {
                                    "authorId": "2164124197",
                                    "name": "Julien Posso"
                                },
                                {
                                    "authorId": "2259651671",
                                    "name": "Guy Bois"
                                },
                                {
                                    "authorId": "2246316919",
                                    "name": "Yvon Savaria"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE International New Circuits and Systems Conference",
                            "n_citations": 0
                        },
                        "score": 0.95703125
                    },
                    {
                        "id": "(Pasricha, 2024)",
                        "snippets": [
                            "Studies have shown that activations are more susceptible to bitwidth reduction than weight parameters. Therefore, an alternative for increasing inference accuracy is to not binarize activations, while keeping the weights binarized. To determine the appropriate activation bitwidth (precision), an analysis was performed in (Sunny et al., 2021), where weights were restricted to binary (1-bit) values, but the bit precision level of the activations was altered from 1-bit to 16-bits, across various CNN models and learning tasks. During CNN model training, weights were binarized during the forward and backward propagations but not during the parameter update step, because keeping high precision weights during the updates is necessary for stochastic gradient descent (SGD) to work efficiently. After training, all weights were in binary format, while the precision of input activations was varied. Figure 9(a) shows the results of varying activation bitwidth across four different CNN models and their datasets. It was observed that the accuracy had notable change initially as activations bits were increased, but this gain in accuracy soon saturated. Based on the results, 1-bit weights were considered along with 4-bit activations, which allowed reducing bitwidths without significantly reducing model accuracy."
                        ],
                        "paper": {
                            "corpus_id": 271571530,
                            "title": "Optical Computing for Deep Neural Network Acceleration: Foundations, Recent Developments, and Emerging Directions",
                            "authors": [
                                {
                                    "authorId": "1681003",
                                    "name": "S. Pasricha"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.953125
                    },
                    {
                        "id": "(Sunny et al., 2021)",
                        "snippets": [
                            "Domain specific neural network accelerators have garnered attention because of their improved energy efficiency and inference performance compared to CPUs and GPUs. Such accelerators are thus well suited for resource-constrained embedded systems. However, mapping sophisticated neural network models on these accelerators still entails significant energy and memory consumption, along with high inference time overhead. Binarized neural networks (BNNs), which utilize single-bit weights, represent an efficient way to implement and deploy neural network models on accelerators. In this paper, we present a novel optical-domain BNN accelerator, named ROBIN, which intelligently integrates heterogeneous microring resonator optical devices with complementary capabilities to efficiently implement the key functionalities in BNNs. We perform detailed fabrication-process variation analyses at the optical device level, explore efficient corrective tuning for these devices, and integrate circuit-level optimization to counter thermal variations. As a result, our proposed ROBIN architecture possesses the desirable traits of being robust, energy-efficient, low latency, and high throughput, when executing BNN models. Our analysis shows that ROBIN can outperform the best-known optical BNN accelerators and many electronic accelerators. Specifically, our energy-efficient ROBIN design exhibits energy-per-bit values that are \u223c4 \u00d7 lower than electronic BNN accelerators and \u223c933 \u00d7 lower than a recently proposed photonic BNN accelerator, while a performance-efficient ROBIN design shows \u223c3 \u00d7 and \u223c25 \u00d7 better performance than electronic and photonic BNN accelerators, respectively."
                        ],
                        "paper": {
                            "corpus_id": 235795063,
                            "title": "ROBIN: A Robust Optical Binary Neural Network Accelerator",
                            "authors": [
                                {
                                    "authorId": "1507532841",
                                    "name": "Febin P. Sunny"
                                },
                                {
                                    "authorId": "1752857527",
                                    "name": "Asif Mirza"
                                },
                                {
                                    "authorId": "2260207",
                                    "name": "Mahdi Nikdast"
                                },
                                {
                                    "authorId": "1681003",
                                    "name": "S. Pasricha"
                                }
                            ],
                            "year": 2021,
                            "venue": "ACM Transactions on Embedded Computing Systems",
                            "n_citations": 35
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lang et al., 2024)",
                        "snippets": [
                            "LLM-QAT demonstrates better accuracy with a configuration of 4-bit weights, 4-bit KV caches, and 8-bit activations, compared to a uniform 4-bit setting across all precision [35]."
                        ],
                        "paper": {
                            "corpus_id": 273821224,
                            "title": "A Comprehensive Study on Quantization Techniques for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2329225877",
                                    "name": "Jiedong Lang"
                                },
                                {
                                    "authorId": "2329318690",
                                    "name": "Zhehao Guo"
                                },
                                {
                                    "authorId": "2329202190",
                                    "name": "Shuyu Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "2024 4th International Conference on Artificial Intelligence, Robotics, and Communication (ICAIRC)",
                            "n_citations": 12
                        },
                        "score": 0.9541015625
                    },
                    {
                        "id": "(Lorenzen et al., 2021)",
                        "snippets": [
                            "For the 4-bit networks, as each neuron can take only 2^4 = 16 different values, the total number of possible states per layer decreases significantly in this setting, and as expected we see lower overall MI measurements",
                            "However, the network accuracy has now degraded compared to the non-quantized networks (see Supplementary Material F), which indicates that the binning used in the estimation of the MI in previous experiments has discarded information vital to the network."
                        ],
                        "paper": {
                            "corpus_id": 235623747,
                            "title": "Information Bottleneck: Exact Analysis of (Quantized) Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2048034204",
                                    "name": "S. Lorenzen"
                                },
                                {
                                    "authorId": "1748824",
                                    "name": "C. Igel"
                                },
                                {
                                    "authorId": "2069766353",
                                    "name": "M. Nielsen"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 18
                        },
                        "score": 0.95458984375
                    },
                    {
                        "id": "(Xiao et al., 2021)",
                        "snippets": [
                            "Fig. 19(a) shows that the 4-bit model is substantially more resilient to state-independent errors than the 8-bit network. This results entirely from activation quantization, and not weight quantization. The large separation between the 16 activation levels effectively cuts off the propagation of accumulated cell errors from one layer to the next. The same cell error results in a smaller dot product error on average. Fig. 19(b) shows that the 4-bit network is also more resilient to state-proportional error, but here the benefit is smaller. This can be explained by the different weight value distributions of the two networks. With only 16 levels, the distribution of the 4-bit weights cannot peak as sharply at zero as the 8-bit weights, which have 256 levels. As a result, the memory cells in the 4-bit network have a significantly higher average conductance (7.52% of G max ) than the cells in the 8-bit network (1.95% of G max ). Thus, deeply quantized weights can actually be harmful for analog systems, as it leads to a higher state-proportional error per cell. This is ultimately outweighed by the benefit of quantized activations, so there is a net improvement in error sensitivity."
                        ],
                        "paper": {
                            "corpus_id": 237416732,
                            "title": "On the Accuracy of Analog Neural Network Inference Accelerators [Feature]",
                            "authors": [
                                {
                                    "authorId": "2061135402",
                                    "name": "T. Xiao"
                                },
                                {
                                    "authorId": "153505332",
                                    "name": "Ben Feinberg"
                                },
                                {
                                    "authorId": "2464307",
                                    "name": "C. Bennett"
                                },
                                {
                                    "authorId": "35839928",
                                    "name": "V. Prabhakar"
                                },
                                {
                                    "authorId": "144956677",
                                    "name": "Prashant Saxena"
                                },
                                {
                                    "authorId": "1578156774",
                                    "name": "V. Agrawal"
                                },
                                {
                                    "authorId": "3491847",
                                    "name": "S. Agarwal"
                                },
                                {
                                    "authorId": "30782065",
                                    "name": "M. Marinella"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Circuits and Systems Magazine",
                            "n_citations": 37
                        },
                        "score": 0.974609375
                    },
                    {
                        "id": "(Shkolnik et al., 2020)",
                        "snippets": [
                            "Naively quantizing a single-precision floating point (FP32) model to 4 bits (INT4) or lower usually incurs a significant accuracy degradation. Many studies have tried to mitigate this accuracy decrease by offering different quantization methods."
                        ],
                        "paper": {
                            "corpus_id": 211146310,
                            "title": "Robust Quantization: One Model to Rule Them All",
                            "authors": [
                                {
                                    "authorId": "2065832429",
                                    "name": "Moran Shkolnik"
                                },
                                {
                                    "authorId": "104387774",
                                    "name": "Brian Chmiel"
                                },
                                {
                                    "authorId": "2607278",
                                    "name": "Ron Banner"
                                },
                                {
                                    "authorId": "19190632",
                                    "name": "Gil Shomron"
                                },
                                {
                                    "authorId": "51498210",
                                    "name": "Yury Nahshan"
                                },
                                {
                                    "authorId": "49791556",
                                    "name": "A. Bronstein"
                                },
                                {
                                    "authorId": "1948330",
                                    "name": "U. Weiser"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 76
                        },
                        "score": 0.9599609375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Mitigation Techniques for 4-bit Quantization",
                "tldr": "Multiple techniques have been developed to mitigate the accuracy degradation in 4-bit quantization, including quantization-aware training, mixed precision approaches, and adapter fine-tuning. These methods can significantly reduce or even eliminate the accuracy gap between 4-bit and higher precision models. (10 sources)",
                "text": "\nNeural network quantization to 4-bit precision can cause significant accuracy degradation when applied naively, but several effective mitigation techniques have been developed to address this challenge. These approaches enable the use of 4-bit precision while maintaining accuracy comparable to higher-precision models.\n\n## Quantization-Aware Training (QAT)\n\nQuantization-aware training has emerged as one of the most effective techniques for mitigating accuracy loss at 4-bit precision. Unlike post-training quantization (PTQ), QAT incorporates quantization effects during the training process, allowing the network to adapt to lower precision. Krishnamoorthi found that most networks can obtain accuracies within 5% of 8-bit quantization through fine-tuning with 4-bit weights while keeping activations at 8-bit precision <Paper corpusId=\"49356451\" paperTitle=\"(Krishnamoorthi, 2018)\" isShortName></Paper>.\n\nThe Factorized Asymmetric Quantization (FAQ) method demonstrated particularly impressive results, with FAQ-trained 4-bit networks outperforming all comparable quantization methods. While these networks required significantly longer fine-tuning (approximately 110 epochs) compared to 8-bit networks, they eventually matched or exceeded the accuracy of pretrained floating-point models <Paper corpusId=\"52197199\" paperTitle=\"(McKinstry et al., 2018)\" isShortName></Paper>.\n\nMore recent approaches like LLM-QAT (Large Language Model Quantization-Aware Training) have been specifically designed for language models, showing that a 4-8-4 configuration (4-bit weights, 8-bit activations, 4-bit KV cache) of a 30B parameter model can outperform an 8-bit 13B model <Paper corpusId=\"258959117\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>.\n\n## Mixed Precision Approaches\n\nMixed precision quantization applies different bit-widths to different parts of the network, strategically preserving higher precision where it matters most:\n\n1. **Layer-specific quantization**: Keeping the first and last layers at higher precision while using 4-bit for intermediate layers has proven effective. Abdolrashidi et al. found that \"models in 4-bit (with first and last layers in 8-bit) offer a consistent advantage over 8-bit\" <Paper corpusId=\"234337594\" paperTitle=\"(Abdolrashidi et al., 2021)\" isShortName></Paper>.\n\n2. **Parameter-specific quantization**: Maintaining higher precision for activations while using lower precision for weights can be beneficial. This asymmetric approach leverages the insight that activations are typically more sensitive to quantization than weights.\n\n## Advanced Optimization Techniques\n\nSeveral specialized optimization techniques have been developed specifically for 4-bit quantization:\n\n1. **Parameter Stateless Gradient Descent (PSGD)**: This method has demonstrated remarkable improvements for 4-bit models, outperforming alternative approaches by approximately 8% on ImageNet <Paper corpusId=\"218862856\" paperTitle=\"(Kim et al., 2020)\" isShortName></Paper>.\n\n2. **Block-wise Quantization**: Dettmers et al. found that splitting parameters into small independently quantized blocks significantly improves the bit-level scaling trade-off for 4-bit models <Paper corpusId=\"254853733\" paperTitle=\"(Dettmers et al., 2022)\" isShortName></Paper>.\n\n3. **Dynamic Bit-width (DyBit)**: This approach dynamically adjusts quantization precision and has achieved 1.97% higher ImageNet inference accuracy compared to previous state-of-the-art methods at 4-bit quantization <Paper corpusId=\"257205844\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>.\n\n## Adapter Fine-Tuning\n\nFor large language models, adapter fine-tuning has emerged as a particularly effective approach. This technique involves adding small trainable modules to an already quantized model and fine-tuning only these adapters.\n\nResearch by Dettmers et al. demonstrated that 4-bit adapter fine-tuning can fully recover the performance lost due to quantization in models ranging from 125M to 3B parameters. Their experiments on GLUE and Super-NaturalInstructions datasets showed that \"16-bit, 8-bit, and 4-bit adapter methods replicate the performance of the fully finetuned 16-bit baseline\" <Paper corpusId=\"258841328\" paperTitle=\"(Dettmers et al., 2023)\" isShortName></Paper>.\n\n## Combining Techniques\n\nThe most successful approaches often combine multiple mitigation strategies. For example, LLM-QAT leverages both quantization-aware training and mixed precision (4-bit weights, 8-bit activations) to achieve optimal results <Paper corpusId=\"258959117\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>.\n\nThese various mitigation techniques have collectively transformed 4-bit quantization from a significant compromise on accuracy to a viable option that \"strikes the best balance between minimal computing/memory complexity and minimal model accuracy degradation\" <Paper corpusId=\"236087695\" paperTitle=\"(Chin et al., 2021)\" isShortName></Paper>. As noted by Shkolnik et al., while \"naively quantizing a single-precision floating point (FP32) model to 4 bits (INT4) or lower usually incurs a significant accuracy degradation,\" the various mitigation techniques discussed above have substantially addressed this challenge <Paper corpusId=\"211146310\" paperTitle=\"(Shkolnik et al., 2020)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Krishnamoorthi, 2018)",
                        "snippets": [
                            "We note that at 8-bits of precision, post training quantization schemes provide close to floating point accuracy. In order to better understand the benefits of quantization aware training, we perform experiments to assess performance at 4 bit quantization for weights and activations",
                            "It is interesting to see that for most networks, one can obtain accuracies within 5% of 8-bit quantization with fine tuning 4 bit weights (column 4 of Table 5). The improvements due to fine tuning are also more apparent at 4 bits. Note that activations are quantized to 8-bits in these experiments."
                        ],
                        "paper": {
                            "corpus_id": 49356451,
                            "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
                            "authors": [
                                {
                                    "authorId": "2065915235",
                                    "name": "Raghuraman Krishnamoorthi"
                                }
                            ],
                            "year": 2018,
                            "venue": "arXiv.org",
                            "n_citations": 1021
                        },
                        "score": 0.98876953125
                    },
                    {
                        "id": "(McKinstry et al., 2018)",
                        "snippets": [
                            "FAQ trained 8-bit networks outperform all comparable quantization methods in all but one instance and exceeded pretrained fp32 network accuracy with only one epoch of training following quantization for all networks explored (Table 1). Immediately following quantization, network accuracy was nearly at the level of the pretrained networks (data not shown) with one exception, Inception-v3, which started at 72.34% top-1. Since the networks started close to a good solution, they did not require extensive fine-tuning to return to and surpass pretrained networks. \n\nFAQ trained 4-bit network accuracy outperforms all comparable quantization methods, exceeding the next closest approach by over 0.5% for ResNet-18 (Jung et al., 2018), and matched or exceeded pretrained fp32 network accuracy. FAQ 4-bit networks required significantly longer fine-tuning -110 epochs -for networks trained, ResNet-18, ResNet-34, and ResNet-50. In constrast to the 8-bit cases, immediately following quantization, network accuracy dropped precipitously, requiring significant fine-tuning to match and surpass the pretrained networks."
                        ],
                        "paper": {
                            "corpus_id": 52197199,
                            "title": "Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference",
                            "authors": [
                                {
                                    "authorId": "46571359",
                                    "name": "J. McKinstry"
                                },
                                {
                                    "authorId": "2357931",
                                    "name": "S. K. Esser"
                                },
                                {
                                    "authorId": "2730753",
                                    "name": "R. Appuswamy"
                                },
                                {
                                    "authorId": "2064431971",
                                    "name": "Deepika Bablani"
                                },
                                {
                                    "authorId": "2248110488",
                                    "name": "John V. Arthur"
                                },
                                {
                                    "authorId": "3121907",
                                    "name": "Izzet B. Yildiz"
                                },
                                {
                                    "authorId": "1944330",
                                    "name": "D. Modha"
                                }
                            ],
                            "year": 2018,
                            "venue": "arXiv.org",
                            "n_citations": 94
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Liu et al._1, 2023)",
                        "snippets": [
                            "First, 8-bit quantization should be preferred over smaller full precision models, and PTQ methods are sufficient for this case. An 8-8-8 30B quantized model outperforms a 13B model of similar size, and should have lower latency and higher throughput in practice. This also holds for an 8-bit 13B model compared with a 16-bit 7B model. Furthermore, 4-bit models quantized using LLM-QAT should be preferred over 8-bit models of similar size. For instance a 4-8-4 LLM-QAT 30B outperforms an 8-bit LLaMA-13B, and a 4-8-8 LLM-QAT 13B is better than an 8-bit LLaMA-7B."
                        ],
                        "paper": {
                            "corpus_id": 258959117,
                            "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2109370860",
                                    "name": "Zechun Liu"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "2112729504",
                                    "name": "Changsheng Zhao"
                                },
                                {
                                    "authorId": "48025720",
                                    "name": "Ernie Chang"
                                },
                                {
                                    "authorId": "37502184",
                                    "name": "Pierre Stock"
                                },
                                {
                                    "authorId": "2121361882",
                                    "name": "Yashar Mehdad"
                                },
                                {
                                    "authorId": "152345059",
                                    "name": "Yangyang Shi"
                                },
                                {
                                    "authorId": "2065915235",
                                    "name": "Raghuraman Krishnamoorthi"
                                },
                                {
                                    "authorId": "144137037",
                                    "name": "Vikas Chandra"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 209
                        },
                        "score": 0.9541015625
                    },
                    {
                        "id": "(Abdolrashidi et al., 2021)",
                        "snippets": [
                            "In this work, we analyzed how quantization at different precisions influences the compute cost-quality Pareto curves on ResNet models",
                            "Additionally, models in 4-bit (with first and last layers in 8-bit) offer a consistent advantage over 8-bit, using both linear and quadratic cost models. This observation suggests that 4-bit may be a preferred numeric format for quantizing neural networks."
                        ],
                        "paper": {
                            "corpus_id": 234337594,
                            "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit",
                            "authors": [
                                {
                                    "authorId": "1938271",
                                    "name": "AmirAli Abdolrashidi"
                                },
                                {
                                    "authorId": "2108552747",
                                    "name": "Lisa Wang"
                                },
                                {
                                    "authorId": "3504647",
                                    "name": "Shivani Agrawal"
                                },
                                {
                                    "authorId": "3274291",
                                    "name": "J. Malmaud"
                                },
                                {
                                    "authorId": "145573927",
                                    "name": "Oleg Rybakov"
                                },
                                {
                                    "authorId": "108381331",
                                    "name": "Chas Leichner"
                                },
                                {
                                    "authorId": "2065594000",
                                    "name": "Lukasz Lew"
                                }
                            ],
                            "year": 2021,
                            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
                            "n_citations": 36
                        },
                        "score": 0.98583984375
                    },
                    {
                        "id": "(Kim et al., 2020)",
                        "snippets": [
                            "On the other hand, ImageNet experiment generally shows reasonable results until 6-bits but the accuracy drastically drops at 4-bits. PSGD targeting 8-bits and 6-bits marginally improves on all bits, yet also experiences drastic accuracy drop at 4-bits. In contrast, Gradient L1 (\u03bb = 0.05) and PSGD @ W4 maintain the performance of the quantized models even at 4-bits. Comparing with the second best method Gradient L1 (\u03bb = 0.05) (Alizadeh et al., 2020), PSGD outperforms it at all bit-widths. At full precision (FP), 8-, 6-and 4-bits, the gap of performance between (Alizadeh et al., 2020) and ours are about 4.2%, 3.8%, 1.8% and 8%, respectively. From Table 2, while the quantization noise may slightly degrade the accuracy in some cases, a general trend that using more bits leads to higher accuracy is demonstrated."
                        ],
                        "paper": {
                            "corpus_id": 218862856,
                            "title": "Position-based Scaled Gradient for Model Quantization and Sparse Training",
                            "authors": [
                                {
                                    "authorId": "49476045",
                                    "name": "Jangho Kim"
                                },
                                {
                                    "authorId": "1713608836",
                                    "name": "Kiyoon Yoo"
                                },
                                {
                                    "authorId": "3160425",
                                    "name": "Nojun Kwak"
                                }
                            ],
                            "year": 2020,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.98046875
                    },
                    {
                        "id": "(Dettmers et al., 2022)",
                        "snippets": [
                            "We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float). Overall, our findings show that {4-bit} precision is almost universally optimal for total model bits and zero-shot accuracy."
                        ],
                        "paper": {
                            "corpus_id": 254853733,
                            "title": "The case for 4-bit precision: k-bit Inference Scaling Laws",
                            "authors": [
                                {
                                    "authorId": "3239480",
                                    "name": "Tim Dettmers"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 234
                        },
                        "score": 0.982421875
                    },
                    {
                        "id": "(Zhou et al., 2023)",
                        "snippets": [
                            "Experimental results demonstrate that the ImageNet inference accuracy via DyBit is 1.97% higher than the state-of-the-art at 4-bit quantization"
                        ],
                        "paper": {
                            "corpus_id": 257205844,
                            "title": "DyBit: Dynamic Bit-Precision Numbers for Efficient Quantized Neural Network Inference",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Jiajun Zhou"
                                },
                                {
                                    "authorId": "2110436131",
                                    "name": "Jiajun Wu"
                                },
                                {
                                    "authorId": "1939358",
                                    "name": "Yizhao Gao"
                                },
                                {
                                    "authorId": "40879066",
                                    "name": "Yuhao Ding"
                                },
                                {
                                    "authorId": "144259094",
                                    "name": "Chaofan Tao"
                                },
                                {
                                    "authorId": "143771569",
                                    "name": "Bo Li"
                                },
                                {
                                    "authorId": "1910352",
                                    "name": "Fengbin Tu"
                                },
                                {
                                    "authorId": "145210800",
                                    "name": "Kwang-Ting Cheng"
                                },
                                {
                                    "authorId": "1710259",
                                    "name": "Hayden Kwok-Hay So"
                                },
                                {
                                    "authorId": "1873081",
                                    "name": "Ngai Wong"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
                            "n_citations": 7
                        },
                        "score": 0.96435546875
                    },
                    {
                        "id": "(Dettmers et al., 2023)",
                        "snippets": [
                            "Recent findings have established that 4-bit quantization for inference is possible, but leads to performance degradation relative to 16-bit [13]18]. This raises the crucial question of whether the lost performance can be recovered by conducting 4-bit adapter finetuning. We test this for two setups. The first focuses on a comparison with full 16-bit finetuning of RoBERTA and T5 models sized 125M to 3B parameters on GLUE and the Super-NaturalInstructions dataset. Results are shown in Table 3. In both datasets, we observe that 16-bit, 8-bit, and 4-bit adapter methods replicate the performance of the fully finetuned 16-bit baseline. This suggests that the performance lost due to the imprecise quantization can be fully recovered through adapter finetuning after quantization."
                        ],
                        "paper": {
                            "corpus_id": 258841328,
                            "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
                            "authors": [
                                {
                                    "authorId": "3239480",
                                    "name": "Tim Dettmers"
                                },
                                {
                                    "authorId": "51152502",
                                    "name": "Artidoro Pagnoni"
                                },
                                {
                                    "authorId": "14487640",
                                    "name": "Ari Holtzman"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2606
                        },
                        "score": 0.955078125
                    },
                    {
                        "id": "(Chin et al., 2021)",
                        "snippets": [
                            "The experimental results from recent research [17] [18] show a comparison of model accuracy under different bit-width quantization in the image classification task, and the results support that the 4-bit quantization models strike the best balance between minimal computing/memory complexity and minimal model accuracy degradation."
                        ],
                        "paper": {
                            "corpus_id": 236087695,
                            "title": "A High-Performance Adaptive Quantization Approach for Edge CNN Applications",
                            "authors": [
                                {
                                    "authorId": "1597397664",
                                    "name": "Hsu-Hsun Chin"
                                },
                                {
                                    "authorId": "1769065",
                                    "name": "R. Tsay"
                                },
                                {
                                    "authorId": "120155431",
                                    "name": "Hsin-I Wu"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.9677734375
                    },
                    {
                        "id": "(Shkolnik et al., 2020)",
                        "snippets": [
                            "Naively quantizing a single-precision floating point (FP32) model to 4 bits (INT4) or lower usually incurs a significant accuracy degradation. Many studies have tried to mitigate this accuracy decrease by offering different quantization methods."
                        ],
                        "paper": {
                            "corpus_id": 211146310,
                            "title": "Robust Quantization: One Model to Rule Them All",
                            "authors": [
                                {
                                    "authorId": "2065832429",
                                    "name": "Moran Shkolnik"
                                },
                                {
                                    "authorId": "104387774",
                                    "name": "Brian Chmiel"
                                },
                                {
                                    "authorId": "2607278",
                                    "name": "Ron Banner"
                                },
                                {
                                    "authorId": "19190632",
                                    "name": "Gil Shomron"
                                },
                                {
                                    "authorId": "51498210",
                                    "name": "Yury Nahshan"
                                },
                                {
                                    "authorId": "49791556",
                                    "name": "A. Bronstein"
                                },
                                {
                                    "authorId": "1948330",
                                    "name": "U. Weiser"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 76
                        },
                        "score": 0.9599609375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Trade-offs and Benefits of 4-bit Quantization",
                "tldr": "4-bit quantization offers substantial model size reduction and computational efficiency gains compared to 8-bit and 16-bit quantization, with models typically 2-13x smaller than full precision counterparts. Despite modest accuracy drops, 4-bit models often provide the optimal balance between efficiency and performance, making them increasingly preferable for deployment in resource-constrained environments. (9 sources)",
                "text": "\nNeural network quantization presents a fundamental trade-off between model precision and computational efficiency. The 4-bit quantization level offers distinct advantages that make it increasingly attractive for many real-world applications, despite the accuracy challenges discussed in previous sections.\n\n## Memory and Storage Benefits\n\nOne of the most significant advantages of 4-bit quantization is the dramatic reduction in model size. Wang et al. demonstrated that 4-bit quantized networks can be approximately 13 times smaller than their full-precision counterparts <Paper corpusId=\"244531228\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>. This substantial size reduction enables the deployment of larger model architectures on memory-constrained devices that would otherwise be unable to accommodate them.\n\n4-bit quantization also introduces increased weight sparsity, with Wang et al. reporting that 4-bit weight quantized networks can achieve more than 38% zero parameters, significantly higher than the 2.65% observed in 8-bit models <Paper corpusId=\"244531228\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>. This sparsity can be leveraged to further optimize storage and computation.\n\n## Computational Efficiency\n\nThe computational advantages of 4-bit quantization extend beyond mere storage benefits. Trusov et al. found that 4-bit quantization delivered a 48% overall inference speedup compared to full-precision models, outperforming the 39% speedup achieved with 8-bit quantization <Paper corpusId=\"221655691\" paperTitle=\"(Trusov et al., 2020)\" isShortName></Paper>. This additional performance gain can be critical for applications where inference latency is a primary concern.\n\nChudakov et al. confirmed that \"the use of uniform low-bit quantization (4- and 6-bit quantization) has significant advantages in speed and resource requirements for inference\" compared to the current 8-bit standard <Paper corpusId=\"245351885\" paperTitle=\"(Chudakov et al., 2021)\" isShortName></Paper>.\n\n## The Efficiency-Accuracy Balance\n\nResearch increasingly suggests that 4-bit quantization represents an optimal trade-off point between efficiency and accuracy for many applications. Chin et al. concluded that \"4-bit quantization models strike the best balance between minimal computing/memory complexity and minimal model accuracy degradation\" <Paper corpusId=\"236087695\" paperTitle=\"(Chin et al., 2021)\" isShortName></Paper>. This observation is particularly significant for deployment scenarios where both computational resources and accuracy requirements must be carefully balanced.\n\nAbdolrashidi et al. conducted a comprehensive analysis of quantization at different precision levels and found that \"models in 4-bit (with first and last layers in 8-bit) offer a consistent advantage over 8-bit, using both linear and quadratic cost models,\" leading them to suggest that \"4-bit may be a preferred numeric format for quantizing neural networks\" <Paper corpusId=\"234337594\" paperTitle=\"(Abdolrashidi et al., 2021)\" isShortName></Paper>.\n\nFor large language models specifically, Liu et al. found that \"4-bit models quantized using LLM-QAT should be preferred over 8-bit models of similar size,\" with specific examples showing that \"a 4-8-4 LLM-QAT 30B outperforms an 8-bit LLaMA-13B, and a 4-8-8 LLM-QAT 13B is better than an 8-bit LLaMA-7B\" <Paper corpusId=\"258959117\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>. This suggests that the benefits of 4-bit quantization extend even to state-of-the-art language models when proper quantization techniques are applied.\n\nDettmers et al. reinforced this finding through extensive research on parameter quantization, concluding that \"4-bit precision is almost universally optimal for total model bits and zero-shot accuracy\" <Paper corpusId=\"254853733\" paperTitle=\"(Dettmers et al., 2022)\" isShortName></Paper>. This provides strong evidence that 4-bit quantization represents a sweet spot in the precision-efficiency trade-off curve.\n\n## Hardware Error Resilience\n\nInterestingly, 4-bit quantization can provide unexpected benefits in certain hardware implementations. Xiao et al. discovered that \"the 4-bit model is substantially more resilient to state-independent errors than the 8-bit network\" in analog computing systems <Paper corpusId=\"237416732\" paperTitle=\"(Xiao et al., 2021)\" isShortName></Paper>. This increased resilience stems from the activation quantization, as \"the large separation between the 16 activation levels effectively cuts off the propagation of accumulated cell errors from one layer to the next\" <Paper corpusId=\"237416732\" paperTitle=\"(Xiao et al., 2021)\" isShortName></Paper>. This additional benefit could be particularly valuable for emerging hardware architectures where error tolerance is a concern.\n\n## Channel-Wise Optimization\n\nRecent research has explored mixed-precision approaches that selectively apply different bit-widths at the channel level. Motetti et al. found that \"4-bit channels represent a significant component, especially for the size and NE16 regularizers, costing less than twice the 2-bit ones while providing significant accuracy advantages\" <Paper corpusId=\"270870579\" paperTitle=\"(Motetti et al., 2024)\" isShortName></Paper>. This suggests that 4-bit precision provides a favorable return on investment in terms of the computational cost versus accuracy benefit, particularly compared to 2-bit precision which offers minimal representational capacity.\n\nIn summary, while 4-bit quantization introduces more significant accuracy challenges than 8-bit or 16-bit alternatives, its substantial benefits in terms of memory reduction, computational efficiency, and overall deployment practicality make it an increasingly attractive option for many applications. With appropriate mitigation techniques, 4-bit quantization can deliver an optimal balance between model efficiency and performance across a wide range of neural network architectures and tasks.",
                "citations": [
                    {
                        "id": "(Wang et al., 2021)",
                        "snippets": [
                            "The results in Table 3 show that the quantized model with 8-bit weights and activations only has a small drop of  2.2% in the mean MF score. The model size of the quantized network is around 4X smaller than the full precision network. In addition, QAT brings weight sparsity, where 2.65% of the parameters are zero after the quantization. The results for the 6-bit and 4-bit weight quantized models are also presented in Table 3. Compared with the 8-bit weight quantized model, the 6-bit weight quantized model shows a drop of 0.2% in the mean MF score but achieves around 4 times weight sparsity. The 4-bit weight quantized network shows a larger drop in the mean MF score but has more than 38% zero parameters. In addition, its model size is 13X smaller than the full precision model."
                        ],
                        "paper": {
                            "corpus_id": 244531228,
                            "title": "LiteEdge: Lightweight Semantic Edge Detection Network",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Hao Wang"
                                },
                                {
                                    "authorId": "118657651",
                                    "name": "Hasan Al-Banna Mohamed"
                                },
                                {
                                    "authorId": "15678675",
                                    "name": "Zuowen Wang"
                                },
                                {
                                    "authorId": "4765464",
                                    "name": "Bodo Rueckauer"
                                },
                                {
                                    "authorId": "1704961",
                                    "name": "Shih-Chii Liu"
                                }
                            ],
                            "year": 2021,
                            "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
                            "n_citations": 3
                        },
                        "score": 0.9599609375
                    },
                    {
                        "id": "(Trusov et al., 2020)",
                        "snippets": [
                            "4-bit quantization gives 95.0% accuracy and 48% overall inference speedup, while an 8-bit quantized network gives 95.4% accuracy and 39% speedup."
                        ],
                        "paper": {
                            "corpus_id": 221655691,
                            "title": "Fast Implementation of 4-bit Convolutional Neural Networks for Mobile Devices",
                            "authors": [
                                {
                                    "authorId": "1441088616",
                                    "name": "A. Trusov"
                                },
                                {
                                    "authorId": "37239110",
                                    "name": "E. Limonova"
                                },
                                {
                                    "authorId": "89361905",
                                    "name": "Dmitry Slugin"
                                },
                                {
                                    "authorId": "35331830",
                                    "name": "D. Nikolaev"
                                },
                                {
                                    "authorId": "35343060",
                                    "name": "V. Arlazarov"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Pattern Recognition",
                            "n_citations": 17
                        },
                        "score": 0.98291015625
                    },
                    {
                        "id": "(Chudakov et al., 2021)",
                        "snippets": [
                            "At the moment, the standard is 8-bit uniform quantization. Nevertheless, the use of uniform low-bit quantization (4- and 6-bit quantization) has significant advantages in speed and resource requirements for inference."
                        ],
                        "paper": {
                            "corpus_id": 245351885,
                            "title": "Quantization noise in low bit quantization and iterative adaptation to quantization noise in quantizable neural networks",
                            "authors": [
                                {
                                    "authorId": "2146460322",
                                    "name": "D. Chudakov"
                                },
                                {
                                    "authorId": "144749173",
                                    "name": "A. Goncharenko"
                                },
                                {
                                    "authorId": "81455739",
                                    "name": "S. Alyamkin"
                                },
                                {
                                    "authorId": "2146460282",
                                    "name": "A. Densidov"
                                }
                            ],
                            "year": 2021,
                            "venue": "Journal of Physics: Conference Series",
                            "n_citations": 0
                        },
                        "score": 0.9658203125
                    },
                    {
                        "id": "(Chin et al., 2021)",
                        "snippets": [
                            "The experimental results from recent research [17] [18] show a comparison of model accuracy under different bit-width quantization in the image classification task, and the results support that the 4-bit quantization models strike the best balance between minimal computing/memory complexity and minimal model accuracy degradation."
                        ],
                        "paper": {
                            "corpus_id": 236087695,
                            "title": "A High-Performance Adaptive Quantization Approach for Edge CNN Applications",
                            "authors": [
                                {
                                    "authorId": "1597397664",
                                    "name": "Hsu-Hsun Chin"
                                },
                                {
                                    "authorId": "1769065",
                                    "name": "R. Tsay"
                                },
                                {
                                    "authorId": "120155431",
                                    "name": "Hsin-I Wu"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.9677734375
                    },
                    {
                        "id": "(Abdolrashidi et al., 2021)",
                        "snippets": [
                            "In this work, we analyzed how quantization at different precisions influences the compute cost-quality Pareto curves on ResNet models",
                            "Additionally, models in 4-bit (with first and last layers in 8-bit) offer a consistent advantage over 8-bit, using both linear and quadratic cost models. This observation suggests that 4-bit may be a preferred numeric format for quantizing neural networks."
                        ],
                        "paper": {
                            "corpus_id": 234337594,
                            "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit",
                            "authors": [
                                {
                                    "authorId": "1938271",
                                    "name": "AmirAli Abdolrashidi"
                                },
                                {
                                    "authorId": "2108552747",
                                    "name": "Lisa Wang"
                                },
                                {
                                    "authorId": "3504647",
                                    "name": "Shivani Agrawal"
                                },
                                {
                                    "authorId": "3274291",
                                    "name": "J. Malmaud"
                                },
                                {
                                    "authorId": "145573927",
                                    "name": "Oleg Rybakov"
                                },
                                {
                                    "authorId": "108381331",
                                    "name": "Chas Leichner"
                                },
                                {
                                    "authorId": "2065594000",
                                    "name": "Lukasz Lew"
                                }
                            ],
                            "year": 2021,
                            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
                            "n_citations": 36
                        },
                        "score": 0.98583984375
                    },
                    {
                        "id": "(Liu et al._1, 2023)",
                        "snippets": [
                            "First, 8-bit quantization should be preferred over smaller full precision models, and PTQ methods are sufficient for this case. An 8-8-8 30B quantized model outperforms a 13B model of similar size, and should have lower latency and higher throughput in practice. This also holds for an 8-bit 13B model compared with a 16-bit 7B model. Furthermore, 4-bit models quantized using LLM-QAT should be preferred over 8-bit models of similar size. For instance a 4-8-4 LLM-QAT 30B outperforms an 8-bit LLaMA-13B, and a 4-8-8 LLM-QAT 13B is better than an 8-bit LLaMA-7B."
                        ],
                        "paper": {
                            "corpus_id": 258959117,
                            "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2109370860",
                                    "name": "Zechun Liu"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "2112729504",
                                    "name": "Changsheng Zhao"
                                },
                                {
                                    "authorId": "48025720",
                                    "name": "Ernie Chang"
                                },
                                {
                                    "authorId": "37502184",
                                    "name": "Pierre Stock"
                                },
                                {
                                    "authorId": "2121361882",
                                    "name": "Yashar Mehdad"
                                },
                                {
                                    "authorId": "152345059",
                                    "name": "Yangyang Shi"
                                },
                                {
                                    "authorId": "2065915235",
                                    "name": "Raghuraman Krishnamoorthi"
                                },
                                {
                                    "authorId": "144137037",
                                    "name": "Vikas Chandra"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 209
                        },
                        "score": 0.9541015625
                    },
                    {
                        "id": "(Dettmers et al., 2022)",
                        "snippets": [
                            "We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float). Overall, our findings show that {4-bit} precision is almost universally optimal for total model bits and zero-shot accuracy."
                        ],
                        "paper": {
                            "corpus_id": 254853733,
                            "title": "The case for 4-bit precision: k-bit Inference Scaling Laws",
                            "authors": [
                                {
                                    "authorId": "3239480",
                                    "name": "Tim Dettmers"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 234
                        },
                        "score": 0.982421875
                    },
                    {
                        "id": "(Xiao et al., 2021)",
                        "snippets": [
                            "Fig. 19(a) shows that the 4-bit model is substantially more resilient to state-independent errors than the 8-bit network. This results entirely from activation quantization, and not weight quantization. The large separation between the 16 activation levels effectively cuts off the propagation of accumulated cell errors from one layer to the next. The same cell error results in a smaller dot product error on average. Fig. 19(b) shows that the 4-bit network is also more resilient to state-proportional error, but here the benefit is smaller. This can be explained by the different weight value distributions of the two networks. With only 16 levels, the distribution of the 4-bit weights cannot peak as sharply at zero as the 8-bit weights, which have 256 levels. As a result, the memory cells in the 4-bit network have a significantly higher average conductance (7.52% of G max ) than the cells in the 8-bit network (1.95% of G max ). Thus, deeply quantized weights can actually be harmful for analog systems, as it leads to a higher state-proportional error per cell. This is ultimately outweighed by the benefit of quantized activations, so there is a net improvement in error sensitivity."
                        ],
                        "paper": {
                            "corpus_id": 237416732,
                            "title": "On the Accuracy of Analog Neural Network Inference Accelerators [Feature]",
                            "authors": [
                                {
                                    "authorId": "2061135402",
                                    "name": "T. Xiao"
                                },
                                {
                                    "authorId": "153505332",
                                    "name": "Ben Feinberg"
                                },
                                {
                                    "authorId": "2464307",
                                    "name": "C. Bennett"
                                },
                                {
                                    "authorId": "35839928",
                                    "name": "V. Prabhakar"
                                },
                                {
                                    "authorId": "144956677",
                                    "name": "Prashant Saxena"
                                },
                                {
                                    "authorId": "1578156774",
                                    "name": "V. Agrawal"
                                },
                                {
                                    "authorId": "3491847",
                                    "name": "S. Agarwal"
                                },
                                {
                                    "authorId": "30782065",
                                    "name": "M. Marinella"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Circuits and Systems Magazine",
                            "n_citations": 37
                        },
                        "score": 0.974609375
                    },
                    {
                        "id": "(Motetti et al., 2024)",
                        "snippets": [
                            "In \"Medium\" models (central plot), very few parameters, if any, are quantized at 2 bits, since it is the least interesting precision due to the low representational capacity, but not negligible cost. In contrast, 4-bit channels represent a significant component, especially for the size and NE16 regularizers, costing less then twice the 2-bit ones while providing significant accuracy advantages."
                        ],
                        "paper": {
                            "corpus_id": 270870579,
                            "title": "Joint Pruning and Channel-Wise Mixed-Precision Quantization for Efficient Deep Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2223967064",
                                    "name": "Beatrice Alessandra Motetti"
                                },
                                {
                                    "authorId": "2114394120",
                                    "name": "Matteo Risso"
                                },
                                {
                                    "authorId": "51091982",
                                    "name": "Alessio Burrello"
                                },
                                {
                                    "authorId": "2231805248",
                                    "name": "Enrico Macii"
                                },
                                {
                                    "authorId": "1695243",
                                    "name": "M. Poncino"
                                },
                                {
                                    "authorId": "36204425",
                                    "name": "D. J. Pagliari"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE transactions on computers",
                            "n_citations": 4
                        },
                        "score": 0.951171875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Specific Application Cases and Performance Results",
                "tldr": "4-bit quantization impacts different neural network models and applications with varying degrees of accuracy loss. While many simple tasks and networks maintain good performance with 4-bit precision, complex models and tasks can experience significant degradation, though the efficiency benefits often justify the tradeoff. (18 sources)",
                "text": "\nHere are key empirical findings from various studies on 4-bit quantization across different application domains:\n\n## Image Classification Networks\n- **ResNet-34**: Achieved 74.52% top-1 accuracy with 4-bit weights and activations, surprisingly outperforming the 32-bit version (73.59%) <Paper corpusId=\"231699188\" paperTitle=\"(Liang et al., 2021)\" isShortName></Paper> <Paper corpusId=\"3455952\" paperTitle=\"(Mishra et al., 2017)\" isShortName></Paper>.\n \n- **ImageNet Models**: DyBit quantization technique achieved 1.97% higher accuracy than other state-of-the-art methods at 4-bit precision <Paper corpusId=\"257205844\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>.\n\n- **General CNN Models**: Moving from 8-bit to 4-bit weight quantization typically results in 2-5% accuracy degradation across various image classification models <Paper corpusId=\"261049574\" paperTitle=\"(Frumkin et al., 2023)\" isShortName></Paper>.\n\n- **Compressed Networks**: 4-bit quantized networks can be approximately 13 times smaller than full-precision counterparts with over 38% zero parameters, significantly higher than the 2.65% observed in 8-bit models <Paper corpusId=\"244531228\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>.\n\n## Specific Model Architectures\n- **SqueezeNet**: When 4-bit quantization was applied to different layers, accuracy impacts varied dramatically:\n - 'fire6-expand3x3' layer: Accuracy dropped to 42.5%\n - 'fire8-expand3x3' layer: Accuracy dropped to 72.5% \n - 'fire9-expand3x3' layer: Accuracy dropped to 85% (recommended layer for quantization)\n - No accuracy changes were observed with 8-bit and 16-bit quantization on any layer <Paper corpusId=\"225583435\" paperTitle=\"(Yee et al., 2020)\" isShortName></Paper>.\n\n- **Spiking Neural Networks (SNNs)**: Demonstrated varying robustness to 4-bit quantization based on dataset complexity:\n - MNIST/CIFAR10: Minimal accuracy loss (0.22-1.75%)\n - ImageNet: Significant accuracy loss (8.8%)\n - Conclusion: \"Weights would better be encoded in 4-bit or over\" for larger networks <Paper corpusId=\"221103917\" paperTitle=\"(Zhou et al., 2019)\" isShortName></Paper>.\n\n- **Simple Classification Models**: On straightforward datasets like MNIST, CIFAR10, and CIFAR100, 4-bit quantization showed no measurable accuracy loss, while on complex datasets like ImageNet, it caused approximately 12% performance loss <Paper corpusId=\"235266137\" paperTitle=\"(Guo et al., 2021)\" isShortName></Paper>.\n\n## Face Recognition Systems\n- **MobileFaceNet**: Demonstrated extreme sensitivity to bit reduction, with accuracy in identification tasks dropping dramatically from 99.80% with 6-bit quantization to just 5.53% with 4-bit quantization <Paper corpusId=\"267035013\" paperTitle=\"(Caldeira et al., 2024)\" isShortName></Paper>.\n\n- **Mixed Precision Quantization (MPQ)**: Using 8-bit for initial layers, 4-bit for middle layers, and 2-bit for final layers reduced computational footprint by 90% with only 0.68 percentage point performance drop on LFW dataset <Paper corpusId=\"267035013\" paperTitle=\"(Caldeira et al., 2024)\" isShortName></Paper>.\n\n## Language Models\n- **Large Language Models (LLMs)**: Emergent abilities (in-context learning, chain-of-thought reasoning, instruction following) were minimally affected by 4-bit quantization:\n - 4-bit and 8-bit results were very similar to 16-bit floating-point performance\n - 2-bit quantization caused results to drop to near-random levels <Paper corpusId=\"259937594\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\n- **Adapter Fine-tuning**: 4-bit adapter fine-tuning fully recovered performance lost due to quantization in models ranging from 125M to 3B parameters on GLUE and Super-NaturalInstructions datasets <Paper corpusId=\"258841328\" paperTitle=\"(Dettmers et al., 2023)\" isShortName></Paper>.\n\n## Hardware Implementation Results\n- **NeuRRAM Hardware**: Successfully implemented models with inputs to all convolutional and fully connected layers quantized to 4-bit or below, noting that accuracy could be improved by using higher precision for sensitive input and output layers <Paper corpusId=\"251644990\" paperTitle=\"(Wan et al., 2022)\" isShortName></Paper> <Paper corpusId=\"67788003\" paperTitle=\"(Esser et al., 2019)\" isShortName></Paper>.\n\n- **Speed and Efficiency Metrics**: 4-bit quantization delivered 48% overall inference speedup compared to full-precision models, outperforming the 39% speedup achieved with 8-bit quantization, while maintaining 95.0% accuracy compared to 95.4% with 8-bit quantization <Paper corpusId=\"221655691\" paperTitle=\"(Trusov et al., 2020)\" isShortName></Paper>.\n\n## Other Network Types\n- **Ternary Shift Networks**: Demonstrated remarkable resilience to quantization, with only 0.35% accuracy loss when moving from 4-bit (98.76% accuracy) to 2-bit precision (98.41% accuracy) <Paper corpusId=\"250607637\" paperTitle=\"(Avila et al., 2022)\" isShortName></Paper>.\n\n- **General CNN Compression**: Compression down to 4-bits resulted in accuracy drops of only 1-2% depending on architecture, with significant compression rates (5x for convolutions with 256 channel inputs) <Paper corpusId=\"57573783\" paperTitle=\"(Gennari et al., 2019)\" isShortName></Paper>. Models using 8-bit precision typically lose only around 0.1% accuracy, while 4-bit models show 1-3% accuracy drops without retraining <Paper corpusId=\"57573783\" paperTitle=\"(Gennari et al., 2019)\" isShortName></Paper> <Paper corpusId=\"51610353\" paperTitle=\"(Gysel et al., 2018)\" isShortName></Paper>.\n\n- **4-bit Quantized Classification Models**: Showed maximum accuracy loss of 4.0% with higher training stability variance compared to higher-precision models, requiring multiple training runs to achieve satisfactory performance <Paper corpusId=\"73479694\" paperTitle=\"(Wei et al., 2019)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Liang et al., 2021)",
                        "snippets": [
                            "ResNet-34 with 4-bit weights and activation obtained 74.52% top-1 accuracy while the 32-bit version is 73.59% (Mishra et al., 2017)"
                        ],
                        "paper": {
                            "corpus_id": 231699188,
                            "title": "Pruning and Quantization for Deep Neural Network Acceleration: A Survey",
                            "authors": [
                                {
                                    "authorId": "123804153",
                                    "name": "Tailin Liang"
                                },
                                {
                                    "authorId": "145073018",
                                    "name": "C. Glossner"
                                },
                                {
                                    "authorId": "66148232",
                                    "name": "Lei Wang"
                                },
                                {
                                    "authorId": "2072685048",
                                    "name": "Shaobo Shi"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neurocomputing",
                            "n_citations": 700
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Mishra et al., 2017)",
                        "snippets": [
                            "For computer vision applications, prior works have shown the efficacy of reducing numeric precision of model parameters (network weights) in deep neural networks. Activation maps, however, occupy a large memory footprint during both the training and inference step when using mini-batches of inputs. One way to reduce this large memory footprint is to reduce the precision of activations. However, past works have shown that reducing the precision of activations hurts model accuracy. We study schemes to train networks from scratch using reduced-precision activations without hurting accuracy. We reduce the precision of activation maps (along with model parameters) and increase the number of filter maps in a layer, and find that this scheme matches or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly improve the execution efficiency (e.g. reduce dynamic memory footprint, memory bandwidth and computational energy) and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN - wide reduced-precision networks. We report results and show that WRPN scheme is better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks."
                        ],
                        "paper": {
                            "corpus_id": 3455952,
                            "title": "WRPN: Wide Reduced-Precision Networks",
                            "authors": [
                                {
                                    "authorId": "35769149",
                                    "name": "Asit K. Mishra"
                                },
                                {
                                    "authorId": "145870092",
                                    "name": "Eriko Nurvitadhi"
                                },
                                {
                                    "authorId": "2115139013",
                                    "name": "Jeffrey J. Cook"
                                },
                                {
                                    "authorId": "33027790",
                                    "name": "Debbie Marr"
                                }
                            ],
                            "year": 2017,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 267
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhou et al., 2023)",
                        "snippets": [
                            "Experimental results demonstrate that the ImageNet inference accuracy via DyBit is 1.97% higher than the state-of-the-art at 4-bit quantization"
                        ],
                        "paper": {
                            "corpus_id": 257205844,
                            "title": "DyBit: Dynamic Bit-Precision Numbers for Efficient Quantized Neural Network Inference",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Jiajun Zhou"
                                },
                                {
                                    "authorId": "2110436131",
                                    "name": "Jiajun Wu"
                                },
                                {
                                    "authorId": "1939358",
                                    "name": "Yizhao Gao"
                                },
                                {
                                    "authorId": "40879066",
                                    "name": "Yuhao Ding"
                                },
                                {
                                    "authorId": "144259094",
                                    "name": "Chaofan Tao"
                                },
                                {
                                    "authorId": "143771569",
                                    "name": "Bo Li"
                                },
                                {
                                    "authorId": "1910352",
                                    "name": "Fengbin Tu"
                                },
                                {
                                    "authorId": "145210800",
                                    "name": "Kwang-Ting Cheng"
                                },
                                {
                                    "authorId": "1710259",
                                    "name": "Hayden Kwok-Hay So"
                                },
                                {
                                    "authorId": "1873081",
                                    "name": "Ngai Wong"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
                            "n_citations": 7
                        },
                        "score": 0.96435546875
                    },
                    {
                        "id": "(Frumkin et al., 2023)",
                        "snippets": [
                            "Moving from 8-bit to 4-bit weight quantization, we see an accuracy degradation of about 2 \u2212 5% across all models."
                        ],
                        "paper": {
                            "corpus_id": 261049574,
                            "title": "Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers",
                            "authors": [
                                {
                                    "authorId": "2094690757",
                                    "name": "N. Frumkin"
                                },
                                {
                                    "authorId": "2193705",
                                    "name": "Dibakar Gope"
                                },
                                {
                                    "authorId": "92419662",
                                    "name": "D. Marculescu"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 17
                        },
                        "score": 0.9658203125
                    },
                    {
                        "id": "(Wang et al., 2021)",
                        "snippets": [
                            "The results in Table 3 show that the quantized model with 8-bit weights and activations only has a small drop of  2.2% in the mean MF score. The model size of the quantized network is around 4X smaller than the full precision network. In addition, QAT brings weight sparsity, where 2.65% of the parameters are zero after the quantization. The results for the 6-bit and 4-bit weight quantized models are also presented in Table 3. Compared with the 8-bit weight quantized model, the 6-bit weight quantized model shows a drop of 0.2% in the mean MF score but achieves around 4 times weight sparsity. The 4-bit weight quantized network shows a larger drop in the mean MF score but has more than 38% zero parameters. In addition, its model size is 13X smaller than the full precision model."
                        ],
                        "paper": {
                            "corpus_id": 244531228,
                            "title": "LiteEdge: Lightweight Semantic Edge Detection Network",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Hao Wang"
                                },
                                {
                                    "authorId": "118657651",
                                    "name": "Hasan Al-Banna Mohamed"
                                },
                                {
                                    "authorId": "15678675",
                                    "name": "Zuowen Wang"
                                },
                                {
                                    "authorId": "4765464",
                                    "name": "Bodo Rueckauer"
                                },
                                {
                                    "authorId": "1704961",
                                    "name": "Shih-Chii Liu"
                                }
                            ],
                            "year": 2021,
                            "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
                            "n_citations": 3
                        },
                        "score": 0.9599609375
                    },
                    {
                        "id": "(Yee et al., 2020)",
                        "snippets": [
                            "For 4-bit fixed-point quantization, the accuracy drops to 72.5% and 85% from the original 90% after quantization on the weights in the layer 'fire8-expand3x3' and 'fire9expand3x3'. The accuracy drops the to the lowest which is 42.5% after 4-bit fixed point quantization is being applied on the weights in the layer 'fire6-expand3x3'. This might due to the large value changes in total weight after quantization. Although the model size is compressed the most after applying weight quantization in the layer 'fire8-expand3x3', but there is 17.5% accuracy drop. Hence, it is recommended to apply 4-bit quantization on weights in layer 'fire-9-expand3x3' when doing 4-bit fixed point quantization on the weights in a single layer of the network as it causes a slight drop in accuracy while reducing the model size. For 8-bit and 16-bit fixed-point quantization, there is no change even when weight quantization is applied on any of the layers."
                        ],
                        "paper": {
                            "corpus_id": 225583435,
                            "title": "Face Recognition and Machine Learning at the Edge",
                            "authors": [
                                {
                                    "authorId": "2004954858",
                                    "name": "Joanne Ling Sin Yee"
                                },
                                {
                                    "authorId": "2412102",
                                    "name": "U. U. Sheikh"
                                },
                                {
                                    "authorId": "1951977",
                                    "name": "M. Mokji"
                                },
                                {
                                    "authorId": "2111988471",
                                    "name": "S. Rahman"
                                }
                            ],
                            "year": 2020,
                            "venue": "IOP Conference Series: Materials Science and Engineering",
                            "n_citations": 2
                        },
                        "score": 0.98193359375
                    },
                    {
                        "id": "(Zhou et al., 2019)",
                        "snippets": [
                            "For weight quantization, the weights were quantized to 32-bit, 8-bit, 4-bit, and 2-bit words. Thanks to our easy training models, we could retrain the deep SNNs simply following the procedure developed for conventional CNN quantization (Li, Zhang, and Liu 2016;(Rastegari et al., 2016). Specifically, the forward inference used quantized weights while the backward gradient propagation used full-precision weights. We first trained with 32-bit quantization. After the training converged, we applied 8-bit quantization and retrain the SNNs. This procedure was repeated until the 2-bit quantization. Table 4 shows the testing accuracy under weight quantization. For the models over MNIST, CIFAR10 and Ima-geNet, weight quantization caused the worst accuracy loss of 0.22%, 1.75%, and 8.8%, respectively. As comparison, we listed two typical CNN weight quantization results (Cheng et al. 2018;(Zhang et al., 2018), which indicated a similar performance degradation pace. The results demonstrated that the SNN models were robust to weight quantization for relatively small datasets and small networks. For larger networks, the weights would better be encoded in 4-bit or over."
                        ],
                        "paper": {
                            "corpus_id": 221103917,
                            "title": "Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance",
                            "authors": [
                                {
                                    "authorId": "81622425",
                                    "name": "Shibo Zhou"
                                },
                                {
                                    "authorId": "4107514",
                                    "name": "Li Xiaohua"
                                },
                                {
                                    "authorId": "2118426969",
                                    "name": "Ying Chen"
                                },
                                {
                                    "authorId": "30908225",
                                    "name": "S. T. Chandrasekaran"
                                },
                                {
                                    "authorId": "9442413",
                                    "name": "A. Sanyal"
                                }
                            ],
                            "year": 2019,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 96
                        },
                        "score": 0.96435546875
                    },
                    {
                        "id": "(Guo et al., 2021)",
                        "snippets": [
                            "We tried 4-bit quantization on some simple datasets(MNIST, CIFAR10, CIFAR100), and there was no quantization loss. We also tried 4-bit quantization on big dataset(ImageNet). The experiment result shows that there is about 12% performance loss, but due to the inference efficiency of 4-bit quantization, this loss may be tolerable in some application scenarios."
                        ],
                        "paper": {
                            "corpus_id": 235266137,
                            "title": "Integer-Only Neural Network Quantization Scheme Based on Shift-Batch-Normalization",
                            "authors": [
                                {
                                    "authorId": "2892749",
                                    "name": "Qingyu Guo"
                                },
                                {
                                    "authorId": "2156166711",
                                    "name": "Yuan Wang"
                                },
                                {
                                    "authorId": "39658679",
                                    "name": "Xiaoxin Cui"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.9814453125
                    },
                    {
                        "id": "(Caldeira et al., 2024)",
                        "snippets": [
                            "The authors prove that model performance suffers a bigger impact when the first layers are quantized due to the forward propagation of the performed approximations, allowing for stronger quantization when progressing toward the last layers of the network. The final MPQ framework comprised 8, 4 and 2-bit quantization for the initial, middle and final layers, respectively, resulting in a network with only 10% of the computational footprint of the original model and a performance drop of 0.68 percentual points on LFW. Thus, MPQ can be extremely useful in establishing a compromise between achieving good performances and having computationally efficient models",
                            ".When b = 8 or b = 6, the performance degradation was small in the identification and verification tasks and, in some cases, the quantized models even slightly surpassed the baseline. For b = 4, the performance suffered an extreme drop. As an example, a MobileFaceNet trained in the identification task suffered a reduction in accuracy from 99.80% in 6-bit quantization to 5.53% in 4-bit quantization. Hence, in this experimental setup, 4-bit quantization is not enough to achieve good performance, showing that this precision level does not allow the model to learn how to encode the relevant features properly."
                        ],
                        "paper": {
                            "corpus_id": 267035013,
                            "title": "Model Compression Techniques in Biometrics Applications: A Survey",
                            "authors": [
                                {
                                    "authorId": "2219025387",
                                    "name": "Eduarda Caldeira"
                                },
                                {
                                    "authorId": "2044499858",
                                    "name": "Pedro C. Neto"
                                },
                                {
                                    "authorId": "1516862195",
                                    "name": "Marco Huber"
                                },
                                {
                                    "authorId": "2265721",
                                    "name": "N. Damer"
                                },
                                {
                                    "authorId": "145279873",
                                    "name": "Ana F. Sequeira"
                                }
                            ],
                            "year": 2024,
                            "venue": "Information Fusion",
                            "n_citations": 10
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "Overall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K."
                        ],
                        "paper": {
                            "corpus_id": 259937594,
                            "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study",
                            "authors": [
                                {
                                    "authorId": "2108129670",
                                    "name": "Peiyu Liu"
                                },
                                {
                                    "authorId": "2119618242",
                                    "name": "Zikang Liu"
                                },
                                {
                                    "authorId": "9136116",
                                    "name": "Ze-Feng Gao"
                                },
                                {
                                    "authorId": "2162036220",
                                    "name": "Dawei Gao"
                                },
                                {
                                    "authorId": "2542603",
                                    "name": "Wayne Xin Zhao"
                                },
                                {
                                    "authorId": "2110479359",
                                    "name": "Yaliang Li"
                                },
                                {
                                    "authorId": "1696332",
                                    "name": "Bolin Ding"
                                },
                                {
                                    "authorId": "153693432",
                                    "name": "Ji-rong Wen"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Language Resources and Evaluation",
                            "n_citations": 35
                        },
                        "score": 0.98291015625
                    },
                    {
                        "id": "(Dettmers et al., 2023)",
                        "snippets": [
                            "Recent findings have established that 4-bit quantization for inference is possible, but leads to performance degradation relative to 16-bit [13]18]. This raises the crucial question of whether the lost performance can be recovered by conducting 4-bit adapter finetuning. We test this for two setups. The first focuses on a comparison with full 16-bit finetuning of RoBERTA and T5 models sized 125M to 3B parameters on GLUE and the Super-NaturalInstructions dataset. Results are shown in Table 3. In both datasets, we observe that 16-bit, 8-bit, and 4-bit adapter methods replicate the performance of the fully finetuned 16-bit baseline. This suggests that the performance lost due to the imprecise quantization can be fully recovered through adapter finetuning after quantization."
                        ],
                        "paper": {
                            "corpus_id": 258841328,
                            "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
                            "authors": [
                                {
                                    "authorId": "3239480",
                                    "name": "Tim Dettmers"
                                },
                                {
                                    "authorId": "51152502",
                                    "name": "Artidoro Pagnoni"
                                },
                                {
                                    "authorId": "14487640",
                                    "name": "Ari Holtzman"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2606
                        },
                        "score": 0.955078125
                    },
                    {
                        "id": "(Wan et al., 2022)",
                        "snippets": [
                            "To efficiently implement the models on NeuRRAM, inputs to all convolutional and fully connected layers are quantized to 4-bit or below. The input bit-precisions of all the models are summarized in Table 1. We perform the quantized training using the parameterized clipping activation technique 46. The accuracies of some of our quantized models are lower than that of the state-of-the-art quantized model because we apply <4-bit quantization to the most sensitive input and output layers of the neural networks, which have been reported to cause large accuracy degradation and are thus often excluded from low-precision quantization 46,54. To obtain better accuracy for quantized models, one can use higher precision for sensitive input and output layers, apply more advanced quantization techniques, and use more optimized data preprocessing, data augmentation and regularization techniques during training."
                        ],
                        "paper": {
                            "corpus_id": 251644990,
                            "title": "A compute-in-memory chip based on resistive random-access memory",
                            "authors": [
                                {
                                    "authorId": "3408296",
                                    "name": "W. Wan"
                                },
                                {
                                    "authorId": "3459360",
                                    "name": "R. Kubendran"
                                },
                                {
                                    "authorId": "1592769267",
                                    "name": "Clemens J. S. Schaefer"
                                },
                                {
                                    "authorId": "32842496",
                                    "name": "S. Eryilmaz"
                                },
                                {
                                    "authorId": "1780073248",
                                    "name": "Wenqiang Zhang"
                                },
                                {
                                    "authorId": "1637988624",
                                    "name": "Dabin Wu"
                                },
                                {
                                    "authorId": "47429806",
                                    "name": "S. Deiss"
                                },
                                {
                                    "authorId": "50501653",
                                    "name": "Priyanka Raina"
                                },
                                {
                                    "authorId": "72601203",
                                    "name": "H. Qian"
                                },
                                {
                                    "authorId": "152632389",
                                    "name": "B. Gao"
                                },
                                {
                                    "authorId": "145940386",
                                    "name": "Siddharth Joshi"
                                },
                                {
                                    "authorId": "13135960",
                                    "name": "Huaqiang Wu"
                                },
                                {
                                    "authorId": "144175897",
                                    "name": "H. P. Wong"
                                },
                                {
                                    "authorId": "2702388",
                                    "name": "G. Cauwenberghs"
                                }
                            ],
                            "year": 2022,
                            "venue": "Nature",
                            "n_citations": 503
                        },
                        "score": 0.9560546875
                    },
                    {
                        "id": "(Esser et al., 2019)",
                        "snippets": [
                            "Deep networks run with low precision operations at inference time offer power and space advantages over high precision alternatives, but need to overcome the challenge of maintaining high accuracy as precision decreases. Here, we present a method for training such networks, Learned Step Size Quantization, that achieves the highest accuracy to date on the ImageNet dataset when using models, from a variety of architectures, with weights and activations quantized to 2-, 3- or 4-bits of precision, and that can train 3-bit models that reach full precision baseline accuracy. Our approach builds upon existing methods for learning weights in quantized networks by improving how the quantizer itself is configured. Specifically, we introduce a novel means to estimate and scale the task loss gradient at each weight and activation layer's quantizer step size, such that it can be learned in conjunction with other network parameters. This approach works using different levels of precision as needed for a given system and requires only a simple modification of existing training code."
                        ],
                        "paper": {
                            "corpus_id": 67788003,
                            "title": "Learned Step Size Quantization",
                            "authors": [
                                {
                                    "authorId": "2357931",
                                    "name": "S. K. Esser"
                                },
                                {
                                    "authorId": "46571359",
                                    "name": "J. McKinstry"
                                },
                                {
                                    "authorId": "2064431971",
                                    "name": "Deepika Bablani"
                                },
                                {
                                    "authorId": "2730753",
                                    "name": "R. Appuswamy"
                                },
                                {
                                    "authorId": "1944330",
                                    "name": "D. Modha"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 810
                        },
                        "score": 0
                    },
                    {
                        "id": "(Trusov et al., 2020)",
                        "snippets": [
                            "4-bit quantization gives 95.0% accuracy and 48% overall inference speedup, while an 8-bit quantized network gives 95.4% accuracy and 39% speedup."
                        ],
                        "paper": {
                            "corpus_id": 221655691,
                            "title": "Fast Implementation of 4-bit Convolutional Neural Networks for Mobile Devices",
                            "authors": [
                                {
                                    "authorId": "1441088616",
                                    "name": "A. Trusov"
                                },
                                {
                                    "authorId": "37239110",
                                    "name": "E. Limonova"
                                },
                                {
                                    "authorId": "89361905",
                                    "name": "Dmitry Slugin"
                                },
                                {
                                    "authorId": "35331830",
                                    "name": "D. Nikolaev"
                                },
                                {
                                    "authorId": "35343060",
                                    "name": "V. Arlazarov"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Pattern Recognition",
                            "n_citations": 17
                        },
                        "score": 0.98291015625
                    },
                    {
                        "id": "(Avila et al., 2022)",
                        "snippets": [
                            "Quantization had a mild impact on the overall performance. For integer quantization, for example, quantizing with 16 bits showed to be slightly benefitial compared to quantizatizing with 2 bits with latter dropping accuracy only 0.06 % on the test set. Similar trend was found for the ternary shift network. While quantizing with 4 bits gives 98.76 % accuracy, quantizing with 2 bits provides 98.41 % accuracy, representing a decay of only 0.35 %."
                        ],
                        "paper": {
                            "corpus_id": 250607637,
                            "title": "Low-bit Shift Network for End-to-End Spoken Language Understanding",
                            "authors": [
                                {
                                    "authorId": "36941802",
                                    "name": "Anderson R. Avila"
                                },
                                {
                                    "authorId": "2337172863",
                                    "name": "Khalil Bibi"
                                },
                                {
                                    "authorId": "2115428434",
                                    "name": "Ruizhi Yang"
                                },
                                {
                                    "authorId": "2108184905",
                                    "name": "Xinlin Li"
                                },
                                {
                                    "authorId": "2064677272",
                                    "name": "Chao Xing"
                                },
                                {
                                    "authorId": "2135917026",
                                    "name": "Xiao Chen"
                                }
                            ],
                            "year": 2022,
                            "venue": "Interspeech",
                            "n_citations": 4
                        },
                        "score": 0.97998046875
                    },
                    {
                        "id": "(Gennari et al., 2019)",
                        "snippets": [
                            "The results also shows that compression down to 4-bits (which in convolutions with channel size input of 256 would yield a 5x compression rate) results in an accuracy drop of only 1% to 2% depending on the architecture",
                            "Also as known in previous research papers [26](Gysel et al., 2018), models of 8/8 bits lose only around 0.1% accuracy. For models of 5/5 and 4/4, we get a drop of 1% to 3% in accuracy. To our knowledge, this is the smallest bit-width for fast inference that has been reported when models are neither retrained nor adapted."
                        ],
                        "paper": {
                            "corpus_id": 57573783,
                            "title": "DSConv: Efficient Convolution Operator",
                            "authors": [
                                {
                                    "authorId": "2065676996",
                                    "name": "Marcelo Gennari"
                                },
                                {
                                    "authorId": "144924636",
                                    "name": "Roger Fawcett"
                                },
                                {
                                    "authorId": "2824784",
                                    "name": "V. Prisacariu"
                                }
                            ],
                            "year": 2019,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 67
                        },
                        "score": 0.9765625
                    },
                    {
                        "id": "(Gysel et al., 2018)",
                        "snippets": [
                            "Convolutional neural networks (CNNs) have led to remarkable progress in a number of key pattern recognition tasks, such as visual scene understanding and speech recognition, that potentially enable numerous applications. Consequently, there is a significant need to deploy trained CNNs to resource-constrained embedded systems. Inference using pretrained modern deep CNNs, however, requires significant system resources, including computation, energy, and memory space. To enable efficient implementation of trained CNNs, a viable approach is to approximate the network with an implementation-friendly model with only negligible degradation in classification accuracy. We present Ristretto, a CNN approximation framework that enables empirical investigation of the tradeoff between various number representation and word width choices and the classification accuracy of the model. Specifically, Ristretto analyzes a given CNN with respect to numerical range required to represent weights, activations, and intermediate results of convolutional and fully connected layers, and subsequently, it simulates the impact of reduced word width or lower precision arithmetic operators on the model accuracy. Moreover, Ristretto can fine-tune a quantized network to further improve its classification accuracy under a given number representation and word width configuration. Given a maximum classification accuracy degradation tolerance of 1%, we use Ristretto to demonstrate that three ImageNet networks can be condensed to use 8-bit dynamic fixed point for network weights and activations. Ristretto is available as a popular open-source software project1 and has already been viewed over 1 000 times on Github as of the submission of this brief.1https://github.com/pmgysel/caffe"
                        ],
                        "paper": {
                            "corpus_id": 51610353,
                            "title": "Ristretto: A Framework for Empirical Study of Resource-Efficient Inference in Convolutional Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2656137",
                                    "name": "Philipp Gysel"
                                },
                                {
                                    "authorId": "34585064",
                                    "name": "J. Pimentel"
                                },
                                {
                                    "authorId": "50789183",
                                    "name": "Mohammad Motamedi"
                                },
                                {
                                    "authorId": "2034444",
                                    "name": "S. Ghiasi"
                                }
                            ],
                            "year": 2018,
                            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
                            "n_citations": 229
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wei et al., 2019)",
                        "snippets": [
                            "The classification accuracy of the 4-bit quantized model has the maximum loss of 4.0%. The other quantized models obtain satisfying performance and the average results are all around 97.2%, with a 1.2% accuracy degradation, compared to floating-point types. It can be seen that the classification accuracy degradation is reduced as the bit-width increases, which may be due to the increased quantization resolution resulting in a computing error reduction. However, the performance can hardly improve after reaching the maximum as the inherent redundancy within the network is limited. The standards deviations of 4-bit and 6-bit quantized models are higher than the others. This means that the stability of training is not good when adopting a low bit-width quantization scheme and multiple trainings are required to obtain a satisfying performance."
                        ],
                        "paper": {
                            "corpus_id": 73479694,
                            "title": "FPGA-Based Hybrid-Type Implementation of Quantized Neural Networks for Remote Sensing Applications",
                            "authors": [
                                {
                                    "authorId": "1382778048",
                                    "name": "Xin Wei"
                                },
                                {
                                    "authorId": "2109188137",
                                    "name": "Wenchao Liu"
                                },
                                {
                                    "authorId": "2146072742",
                                    "name": "Lei Chen"
                                },
                                {
                                    "authorId": "145044878",
                                    "name": "Long Ma"
                                },
                                {
                                    "authorId": "92779203",
                                    "name": "He Chen"
                                },
                                {
                                    "authorId": "2199301922",
                                    "name": "Zhuang Yin"
                                }
                            ],
                            "year": 2019,
                            "venue": "Italian National Conference on Sensors",
                            "n_citations": 28
                        },
                        "score": 0.97509765625
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.22791299999999998
    }
}