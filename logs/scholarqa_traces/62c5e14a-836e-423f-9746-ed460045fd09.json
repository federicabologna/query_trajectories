{
    "query": "What are the most advanced multimodal language models capable of generating both text and images as output, and what architectures enable this capability?",
    "user_id": "lib_user",
    "task_id": "62c5e14a-836e-423f-9746-ed460045fd09",
    "timestamp": "2025-06-23T22:51:01.356356",
    "n_retrieval": 256,
    "n_retrieved": 272,
    "n_candidates": 40,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.5981759999999999,
    "decomposed_query": {
        "rewritten_query": "Most advanced multimodal language models capable of generating both text and images as output, and what architectures enable this capability.",
        "keyword_query": "multimodal language models text image generation architectures",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.00984,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Retrieval-Augmented Multimodal Language Modeling",
            "venue": "International Conference on Machine Learning",
            "year": 2023,
            "reference_count": 62,
            "citation_count": 108,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2211.12561",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.12561, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "19168196",
                    "name": "Michihiro Yasunaga"
                },
                {
                    "authorId": "2201435",
                    "name": "Armen Aghajanyan"
                },
                {
                    "authorId": "3040379",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2191899140",
                    "name": "Rich James"
                },
                {
                    "authorId": "1702139",
                    "name": "J. Leskovec"
                },
                {
                    "authorId": "145419642",
                    "name": "Percy Liang"
                },
                {
                    "authorId": "35084211",
                    "name": "M. Lewis"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "2072801764",
                    "name": "Wen-tau Yih"
                }
            ],
            "abstract": "Recent multimodal models such as DALL-E and\r\nCM3 have achieved remarkable progress in textto-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the\r\nmodel parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrievalaugmented multimodal model, which enables a base multimodal model (generator) to refer to relevant knowledge fetched by a retriever from external memory (e.g., multimodal documents on the web). Specifically, we implement a retriever using the pretrained CLIP model and a generator using the CM3 Transformer architecture, and\r\ntrain this model using the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate mixtures of text and images.\r\nWe show that RA-CM3 significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption generation tasks (12 FID and 17 CIDEr improvements on MSCOCO), while requiring much less compute for training (<30% of DALL-E). Moreover, we show that RA-CM3 exhibits novel capabilities such as knowledge-intensive image generation and multimodal in-context learning",
            "corpus_id": 253802096,
            "sentences": [
                {
                    "corpus_id": "253802096",
                    "title": "Retrieval-Augmented Multimodal Language Modeling",
                    "text": "We present a retrieval-augmented multimodal model that can retrieve and generate both text and images. As illustrated in Figure 1, given an input multimodal document (i.e., arbitrary sequence of text/images), we use a retriever that retrieves relevant multimodal documents from an external memory, and uses a generator to refer to the retrieved documents and make predictions for the input document (i.e., generate the continuation). We design the multimodal retriever as a dense retriever with a mixed-modal encoder that can encode combinations of text and images (e.g., pretrained CLIP; \u00a73.2). We build the retrieval-augmented generator using the CM3 Transformer architecture, and we prepend the retrieved documents to the main input document that we feed into the generator ( \u00a73.3). We describe how we train this model and use it for text-to-image or image-to-text generation in \u00a73.4. \n\nNotably, our resulting model, Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate combinations of text and images, which is the most general capability among existing multimodal models (Table 1). Moreover, while we build on existing techniques such as CLIP and CM3, we are the first to establish a method to unify them into a performant retrieval-augmented model through extensive analyses of design choices ( \u00a7C.3).",
                    "score": 0.5060725423797264,
                    "section_title": "Approach",
                    "char_start_offset": 6721,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 102
                        },
                        {
                            "start": 103,
                            "end": 433
                        },
                        {
                            "start": 434,
                            "end": 595
                        },
                        {
                            "start": 596,
                            "end": 785
                        },
                        {
                            "start": 786,
                            "end": 887
                        },
                        {
                            "start": 890,
                            "end": 1128
                        },
                        {
                            "start": 1129,
                            "end": 1349
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9912109375
                }
            ],
            "relevance_judgement": 0.9912109375,
            "relevance_judgment_input_expanded": "# Title: Retrieval-Augmented Multimodal Language Modeling\n# Venue: International Conference on Machine Learning\n# Authors: Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, J. Leskovec, Percy Liang, M. Lewis, Luke Zettlemoyer, Wen-tau Yih\n## Abstract\nRecent multimodal models such as DALL-E and\r\nCM3 have achieved remarkable progress in textto-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the\r\nmodel parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrievalaugmented multimodal model, which enables a base multimodal model (generator) to refer to relevant knowledge fetched by a retriever from external memory (e.g., multimodal documents on the web). Specifically, we implement a retriever using the pretrained CLIP model and a generator using the CM3 Transformer architecture, and\r\ntrain this model using the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate mixtures of text and images.\r\nWe show that RA-CM3 significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption generation tasks (12 FID and 17 CIDEr improvements on MSCOCO), while requiring much less compute for training (<30% of DALL-E). Moreover, we show that RA-CM3 exhibits novel capabilities such as knowledge-intensive image generation and multimodal in-context learning\n## Approach\nWe present a retrieval-augmented multimodal model that can retrieve and generate both text and images. As illustrated in Figure 1, given an input multimodal document (i.e., arbitrary sequence of text/images), we use a retriever that retrieves relevant multimodal documents from an external memory, and uses a generator to refer to the retrieved documents and make predictions for the input document (i.e., generate the continuation). We design the multimodal retriever as a dense retriever with a mixed-modal encoder that can encode combinations of text and images (e.g., pretrained CLIP; \u00a73.2). We build the retrieval-augmented generator using the CM3 Transformer architecture, and we prepend the retrieved documents to the main input document that we feed into the generator ( \u00a73.3). We describe how we train this model and use it for text-to-image or image-to-text generation in \u00a73.4. \n\nNotably, our resulting model, Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate combinations of text and images, which is the most general capability among existing multimodal models (Table 1). Moreover, while we build on existing techniques such as CLIP and CM3, we are the first to establish a method to unify them into a performant retrieval-augmented model through extensive analyses of design choices ( \u00a7C.3).",
            "reference_string": "[253802096 | Yasunaga et al. | 2023 | Citations: 108]"
        },
        {
            "title": "Generating Images with Multimodal Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 73,
            "citation_count": 259,
            "influential_citation_count": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2305.17216",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.17216, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "23978705",
                    "name": "Jing Yu Koh"
                },
                {
                    "authorId": "47070750",
                    "name": "Daniel Fried"
                },
                {
                    "authorId": "145124475",
                    "name": "R. Salakhutdinov"
                }
            ],
            "abstract": "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.",
            "corpus_id": 258959284,
            "sentences": [
                {
                    "corpus_id": "258959284",
                    "title": "Generating Images with Multimodal Language Models",
                    "text": "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.",
                    "score": 0.5937684388250244,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98779296875
                },
                {
                    "corpus_id": "258959284",
                    "title": "Generating Images with Multimodal Language Models",
                    "text": "GILL is the first multimodal language model capable of conditioning on image-and-text inputs to generate meaningful images interleaved with text. Hence, our experiments primarily focus on evaluating its ability to produce novel images (Sec. 4.1). Our results show that GILL improves over Stable Diffusion [49] on tasks that require processing long-form text such as dialogue and discourse. We also benchmark the performance of models in deciding whether to retrieve or generate (see appendix). GILL is capable of generating text, retrieving images, and generating images. Despite being more general than prior work [56,4,31], we find that GILL performs comparably to or better than existing multimodal LMs on contextual image retrieval and text generation tasks (see Sec. 5).",
                    "score": 0.493552565347128,
                    "section_title": "Experiments",
                    "char_start_offset": 16923,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 240
                        },
                        {
                            "start": 241,
                            "end": 246
                        },
                        {
                            "start": 247,
                            "end": 389
                        },
                        {
                            "start": 390,
                            "end": 493
                        },
                        {
                            "start": 494,
                            "end": 571
                        },
                        {
                            "start": 572,
                            "end": 775
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 305,
                            "end": 309,
                            "matchedPaperCorpusId": "245335280"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.984375
                },
                {
                    "corpus_id": "258959284",
                    "title": "Generating Images with Multimodal Language Models",
                    "text": "Autoregressive language models (LMs) and large language models (LLMs) trained on text corpora have shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the effectiveness of grounding text-only LMs to images for vision-and-language tasks [56,4,29,33,31,35], to embodied settings for robotics [3,18], offline reinforcement learning [48], and more. These methods typically keep most of the LLM weights frozen. This allows them to leverage the capabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn from in-context examples [9], more effectively process longer context, and condition on inputs more strongly. \n\nIn this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49]) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs [52], in contrast to other methods which require interleaved image-text data [4,2]. Our approach is computationally efficient and does not require running the image generation model at training time. To achieve strong image generation performance, we propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special FROMAGe Retrieval only multimodal language model. Image outputs limited to the candidate retrieval set.",
                    "score": 0.5264562935828627,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 382
                        },
                        {
                            "start": 383,
                            "end": 443
                        },
                        {
                            "start": 444,
                            "end": 690
                        },
                        {
                            "start": 693,
                            "end": 791
                        },
                        {
                            "start": 792,
                            "end": 1000
                        },
                        {
                            "start": 1001,
                            "end": 1245
                        },
                        {
                            "start": 1246,
                            "end": 1411
                        },
                        {
                            "start": 1412,
                            "end": 1527
                        },
                        {
                            "start": 1528,
                            "end": 1697
                        },
                        {
                            "start": 1698,
                            "end": 1815
                        },
                        {
                            "start": 1816,
                            "end": 1869
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1180,
                            "end": 1184,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 1328,
                            "end": 1332,
                            "matchedPaperCorpusId": "51876975"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97119140625
                }
            ],
            "relevance_judgement": 0.98779296875,
            "relevance_judgment_input_expanded": "# Title: Generating Images with Multimodal Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Jing Yu Koh, Daniel Fried, R. Salakhutdinov\n## Abstract\nWe propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.\n## Introduction\nAutoregressive language models (LMs) and large language models (LLMs) trained on text corpora have shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the effectiveness of grounding text-only LMs to images for vision-and-language tasks [56,4,29,33,31,35], to embodied settings for robotics [3,18], offline reinforcement learning [48], and more. These methods typically keep most of the LLM weights frozen. This allows them to leverage the capabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn from in-context examples [9], more effectively process longer context, and condition on inputs more strongly. \n\nIn this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49]) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs [52], in contrast to other methods which require interleaved image-text data [4,2]. Our approach is computationally efficient and does not require running the image generation model at training time. To achieve strong image generation performance, we propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special FROMAGe Retrieval only multimodal language model. Image outputs limited to the candidate retrieval set.\n\n## Experiments\nGILL is the first multimodal language model capable of conditioning on image-and-text inputs to generate meaningful images interleaved with text. Hence, our experiments primarily focus on evaluating its ability to produce novel images (Sec. 4.1). Our results show that GILL improves over Stable Diffusion [49] on tasks that require processing long-form text such as dialogue and discourse. We also benchmark the performance of models in deciding whether to retrieve or generate (see appendix). GILL is capable of generating text, retrieving images, and generating images. Despite being more general than prior work [56,4,31], we find that GILL performs comparably to or better than existing multimodal LMs on contextual image retrieval and text generation tasks (see Sec. 5).",
            "reference_string": "[258959284 | Koh et al. | 2023 | Citations: 259]"
        },
        {
            "title": "Jointly Training Large Autoregressive Multimodal Models",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 58,
            "citation_count": 31,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.15564",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.15564, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2185505465",
                    "name": "Emanuele Aiello"
                },
                {
                    "authorId": "49297123",
                    "name": "L. Yu"
                },
                {
                    "authorId": "2247227174",
                    "name": "Yixin Nie"
                },
                {
                    "authorId": "2201435",
                    "name": "Armen Aghajanyan"
                },
                {
                    "authorId": "9185192",
                    "name": "Barlas O\u011fuz"
                }
            ],
            "abstract": "In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.",
            "corpus_id": 262898422,
            "sentences": [
                {
                    "corpus_id": "262898422",
                    "title": "Jointly Training Large Autoregressive Multimodal Models",
                    "text": "In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.",
                    "score": 0.5078988503273124,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.986328125
                },
                {
                    "corpus_id": "262898422",
                    "title": "Jointly Training Large Autoregressive Multimodal Models",
                    "text": "Autoregressive text-to-image models, as exemplified by works such as Yu et al. (2023;2022), have made remarkable strides in generating highly detailed images, paralleling the achievements of Diffusion Models Nichol et al. (2022); Ramesh et al. (2022); Rombach et al. (2022). These models bear architectural resemblance to Large Language Models (LLMs), yet their training regimen is tailored for paired image-text data. LLMs on the other hand (Brown et al., 2020;Zhang et al., 2022;Touvron et al., 2023) are limited to text-based output, thus lacking multimodal generative capabilities despite their proficiency in textual tasks. The subfield of Multimodal Large Models has emerged in recent years Tsimpoukelli et al. (2021); Alayrac et al. (2022); Li et al. (2022a) in the quest to bring together the disparate strengths of vision and language models. Despite important advances in this direction, these models still predominantly generate one modality, thereby constraining their expressiveness. This study aspires to break this limitation by developing a multimodal model capable of generating integrated text and image outputs. \n\nTo achieve this objective, we conduct a comprehensive empirical investigation into the fusion of two specialized autoregressive, decoder-only, large transformer models, each designed for unique tasks (one for text-to-image and a text only model). We introduce a set of methods under the umbrella of the Joint Autoregressive Mixture (JAM) framework. In building this framework, we take advantage of the inherent architectural compatibility of autoregressive text-to-image models with LLMs, allowing us to do deep model fusion and joint training in ways which would otherwise not be possible. Our modular and data-efficient solution allows for deep, rapid and effective integration of continually evolving large models, using less than 1% of the original pretraining data for both parent models. \n\nOur contributions to this study are twofold.",
                    "score": 0.5179802691243277,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 274
                        },
                        {
                            "start": 275,
                            "end": 418
                        },
                        {
                            "start": 419,
                            "end": 628
                        },
                        {
                            "start": 629,
                            "end": 851
                        },
                        {
                            "start": 852,
                            "end": 996
                        },
                        {
                            "start": 997,
                            "end": 1130
                        },
                        {
                            "start": 1133,
                            "end": 1379
                        },
                        {
                            "start": 1380,
                            "end": 1481
                        },
                        {
                            "start": 1482,
                            "end": 1723
                        },
                        {
                            "start": 1724,
                            "end": 1926
                        },
                        {
                            "start": 1929,
                            "end": 1973
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 252,
                            "end": 273,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 442,
                            "end": 462,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 697,
                            "end": 723,
                            "matchedPaperCorpusId": "235658331"
                        },
                        {
                            "start": 725,
                            "end": 746,
                            "matchedPaperCorpusId": "252199400"
                        },
                        {
                            "start": 748,
                            "end": 765,
                            "matchedPaperCorpusId": "246411402"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96826171875
                }
            ],
            "relevance_judgement": 0.986328125,
            "relevance_judgment_input_expanded": "# Title: Jointly Training Large Autoregressive Multimodal Models\n# Venue: International Conference on Learning Representations\n# Authors: Emanuele Aiello, L. Yu, Yixin Nie, Armen Aghajanyan, Barlas O\u011fuz\n## Abstract\nIn recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.\n## INTRODUCTION\nAutoregressive text-to-image models, as exemplified by works such as Yu et al. (2023;2022), have made remarkable strides in generating highly detailed images, paralleling the achievements of Diffusion Models Nichol et al. (2022); Ramesh et al. (2022); Rombach et al. (2022). These models bear architectural resemblance to Large Language Models (LLMs), yet their training regimen is tailored for paired image-text data. LLMs on the other hand (Brown et al., 2020;Zhang et al., 2022;Touvron et al., 2023) are limited to text-based output, thus lacking multimodal generative capabilities despite their proficiency in textual tasks. The subfield of Multimodal Large Models has emerged in recent years Tsimpoukelli et al. (2021); Alayrac et al. (2022); Li et al. (2022a) in the quest to bring together the disparate strengths of vision and language models. Despite important advances in this direction, these models still predominantly generate one modality, thereby constraining their expressiveness. This study aspires to break this limitation by developing a multimodal model capable of generating integrated text and image outputs. \n\nTo achieve this objective, we conduct a comprehensive empirical investigation into the fusion of two specialized autoregressive, decoder-only, large transformer models, each designed for unique tasks (one for text-to-image and a text only model). We introduce a set of methods under the umbrella of the Joint Autoregressive Mixture (JAM) framework. In building this framework, we take advantage of the inherent architectural compatibility of autoregressive text-to-image models with LLMs, allowing us to do deep model fusion and joint training in ways which would otherwise not be possible. Our modular and data-efficient solution allows for deep, rapid and effective integration of continually evolving large models, using less than 1% of the original pretraining data for both parent models. \n\nOur contributions to this study are twofold.",
            "reference_string": "[262898422 | Aiello et al. | 2023 | Citations: 31]"
        },
        {
            "title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 61,
            "citation_count": 339,
            "influential_citation_count": 50,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.09818, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301457346",
                    "name": "Chameleon Team"
                },
                {
                    "authorId": "39960571",
                    "name": "Jacob Kahn"
                }
            ],
            "abstract": "We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",
            "corpus_id": 269791516,
            "sentences": [
                {
                    "corpus_id": "269791516",
                    "title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models",
                    "text": "We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",
                    "score": 0.4892621430495129,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98291015625
                },
                {
                    "corpus_id": "269791516",
                    "title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models",
                    "text": "Recent multimodal foundation models are very widely adopted but still model different modalities separately, often using modality specific encoders or decoders. This can limit their ability to integrate information across modalities and generate multimodal documents that can contain arbitrary sequences of images and text. In this paper, we present Chameleon, a family of mixed-modal foundation models capable of generating and reasoning with mixed sequences of arbitrarily interleaved textual and image content (Figures 2-4). This allows for full multimodal document modeling, which is a direct generalization of standard multimodal tasks such as image generation, understanding and reasoning over images, and text-only LLMs. Chameleon is instead designed to be mixed-modal from inception and uses a uniform architecture trained from scratch in an end-to-end fashion on an interleaved mixture of all modalities, i.e., images, text, and code. \n\nOur unified approach uses fully token-based representations for both image and textual modalities (Figure 1). By quantizing images into discrete tokens, analogous to words in text, we can apply the same transformer architecture to sequences of both image and text tokens, without the need for separate image/text encoders (Alayrac et al., 2022;Liu et al., 2023b;Lauren\u00e7on et al., 2023) or domain-specific decoders (Ramesh et al., 2022;Jin et al., 2023;Betker et al., 2023). This early-fusion approach, where all modalities are projected into a shared representational space from the start, allows for seamless reasoning and generation across modalities. However, it also presents significant technical challenges, particularly in terms of optimization stability and scaling. \n\nWe address these challenges through a combination of architectural innovations and training techniques. We introduce novel modifications to the transformer architecture, such as query-key normalization and revised placement of layer norms, which we find to be crucial for stable training in the mixed-modal setting (Section 2.3).",
                    "score": 0.5063740497990757,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 160
                        },
                        {
                            "start": 161,
                            "end": 323
                        },
                        {
                            "start": 324,
                            "end": 527
                        },
                        {
                            "start": 528,
                            "end": 727
                        },
                        {
                            "start": 728,
                            "end": 943
                        },
                        {
                            "start": 946,
                            "end": 1055
                        },
                        {
                            "start": 1056,
                            "end": 1419
                        },
                        {
                            "start": 1420,
                            "end": 1599
                        },
                        {
                            "start": 1600,
                            "end": 1720
                        },
                        {
                            "start": 1723,
                            "end": 1826
                        },
                        {
                            "start": 1827,
                            "end": 2052
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1268,
                            "end": 1290,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 1398,
                            "end": 1418,
                            "matchedPaperCorpusId": "264403242"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.966796875
                }
            ],
            "relevance_judgement": 0.98291015625,
            "relevance_judgment_input_expanded": "# Title: Chameleon: Mixed-Modal Early-Fusion Foundation Models\n# Venue: arXiv.org\n# Authors: Chameleon Team, Jacob Kahn\n## Abstract\nWe present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.\n## Introduction\nRecent multimodal foundation models are very widely adopted but still model different modalities separately, often using modality specific encoders or decoders. This can limit their ability to integrate information across modalities and generate multimodal documents that can contain arbitrary sequences of images and text. In this paper, we present Chameleon, a family of mixed-modal foundation models capable of generating and reasoning with mixed sequences of arbitrarily interleaved textual and image content (Figures 2-4). This allows for full multimodal document modeling, which is a direct generalization of standard multimodal tasks such as image generation, understanding and reasoning over images, and text-only LLMs. Chameleon is instead designed to be mixed-modal from inception and uses a uniform architecture trained from scratch in an end-to-end fashion on an interleaved mixture of all modalities, i.e., images, text, and code. \n\nOur unified approach uses fully token-based representations for both image and textual modalities (Figure 1). By quantizing images into discrete tokens, analogous to words in text, we can apply the same transformer architecture to sequences of both image and text tokens, without the need for separate image/text encoders (Alayrac et al., 2022;Liu et al., 2023b;Lauren\u00e7on et al., 2023) or domain-specific decoders (Ramesh et al., 2022;Jin et al., 2023;Betker et al., 2023). This early-fusion approach, where all modalities are projected into a shared representational space from the start, allows for seamless reasoning and generation across modalities. However, it also presents significant technical challenges, particularly in terms of optimization stability and scaling. \n\nWe address these challenges through a combination of architectural innovations and training techniques. We introduce novel modifications to the transformer architecture, such as query-key normalization and revised placement of layer norms, which we find to be crucial for stable training in the mixed-modal setting (Section 2.3).",
            "reference_string": "[269791516 | Team et al. | 2024 | Citations: 339]"
        },
        {
            "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 70,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.20996, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306250779",
                    "name": "Sicheng Mo"
                },
                {
                    "authorId": "2279870002",
                    "name": "Thao Nguyen"
                },
                {
                    "authorId": "2358313855",
                    "name": "Xun Huang"
                },
                {
                    "authorId": "2317302544",
                    "name": "Siddharth Srinivasan Iyer"
                },
                {
                    "authorId": "2273697246",
                    "name": "Yijun Li"
                },
                {
                    "authorId": "2358093278",
                    "name": "Yuchen Liu"
                },
                {
                    "authorId": "2358039089",
                    "name": "Abhishek Tandon"
                },
                {
                    "authorId": "2257003591",
                    "name": "Eli Shechtman"
                },
                {
                    "authorId": "2265586470",
                    "name": "Krishna Kumar Singh"
                },
                {
                    "authorId": "2272231489",
                    "name": "Yong Jae Lee"
                },
                {
                    "authorId": "2274216889",
                    "name": "Bolei Zhou"
                },
                {
                    "authorId": "2351822426",
                    "name": "Yuheng Li"
                }
            ],
            "abstract": "We propose X-Fusion, a framework that extends pretrained Large Language Models (LLMs) for multimodal tasks while preserving their language capabilities. X-Fusion employs a dual-tower design with modality-specific weights, keeping the LLM's parameters frozen while integrating vision-specific information for both understanding and generation. Our experiments demonstrate that X-Fusion consistently outperforms alternative architectures on both image-to-text and text-to-image tasks. We find that incorporating understanding-focused data improves generation quality, reducing image data noise enhances overall performance, and feature alignment accelerates convergence for smaller models but has minimal impact on larger ones. Our findings provide valuable insights into building efficient unified multimodal models.",
            "corpus_id": 278171331,
            "sentences": [],
            "relevance_judgement": 0.98291015625,
            "relevance_judgment_input_expanded": "# Title: X-Fusion: Introducing New Modality to Frozen Large Language Models\n# Venue: arXiv.org\n# Authors: Sicheng Mo, Thao Nguyen, Xun Huang, Siddharth Srinivasan Iyer, Yijun Li, Yuchen Liu, Abhishek Tandon, Eli Shechtman, Krishna Kumar Singh, Yong Jae Lee, Bolei Zhou, Yuheng Li\n## Abstract\nWe propose X-Fusion, a framework that extends pretrained Large Language Models (LLMs) for multimodal tasks while preserving their language capabilities. X-Fusion employs a dual-tower design with modality-specific weights, keeping the LLM's parameters frozen while integrating vision-specific information for both understanding and generation. Our experiments demonstrate that X-Fusion consistently outperforms alternative architectures on both image-to-text and text-to-image tasks. We find that incorporating understanding-focused data improves generation quality, reducing image data noise enhances overall performance, and feature alignment accelerates convergence for smaller models but has minimal impact on larger ones. Our findings provide valuable insights into building efficient unified multimodal models.\n",
            "reference_string": "[278171331 | Mo et al. | 2025 | Citations: 2]"
        },
        {
            "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 104,
            "citation_count": 39,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.07975, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326805499",
                    "name": "Yiyang Ma"
                },
                {
                    "authorId": "2326998908",
                    "name": "Xingchao Liu"
                },
                {
                    "authorId": "2326444644",
                    "name": "Xi-aokang Chen"
                },
                {
                    "authorId": "2326440587",
                    "name": "Wen Liu"
                },
                {
                    "authorId": "2330475459",
                    "name": "Chengyue Wu"
                },
                {
                    "authorId": "2326514370",
                    "name": "Zhiyu Wu"
                },
                {
                    "authorId": "2327000291",
                    "name": "Zizheng Pan"
                },
                {
                    "authorId": "2279107352",
                    "name": "Zhenda Xie"
                },
                {
                    "authorId": "2315951020",
                    "name": "Haowei Zhang"
                },
                {
                    "authorId": "2279159169",
                    "name": "Xingkai Yu"
                },
                {
                    "authorId": "2330538675",
                    "name": "Liang Zhao"
                },
                {
                    "authorId": "2330376936",
                    "name": "Yisong Wang"
                },
                {
                    "authorId": "2330291409",
                    "name": "Jiaying Liu"
                },
                {
                    "authorId": "2278217940",
                    "name": "C. Ruan"
                }
            ],
            "abstract": "We present JanusFlow, a powerful framework that unifies image understanding and generation in a single model. JanusFlow introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. To further improve the performance of our unified model, we adopt two key strategies: (i) decoupling the understanding and generation encoders, and (ii) aligning their representations during unified training. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models.",
            "corpus_id": 273969525,
            "sentences": [
                {
                    "corpus_id": "273969525",
                    "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
                    "text": "Large language models (LLMs) have demonstrated remarkable capabilities in learning diverse knowledge and generalizing to new scenarios [1,7,8,69,91]. Leveraging these capabilities, researchers have developed sophisticated models specialized in image comprehension [2,15,47,49,56,58] and text-to-image generation [23,73,76,79]. \n\nThe field has recently shifted toward creating unified systems capable of handling both tasks simultaneously. One prominent direction involves utilizing pre-trained text-to-image models for high-quality generation while training LLMs to generate conditions for these models [19,[25][26][27]87]. However, this approach introduces architectural complexity and potentially constrains the model's capabilities through maintaining separate LLM and generative components. Alternative approaches [88,97,99,100,108] propose training a single LLM for both tasks, typically incorporating either diffusion models [32,83] or vector-quantized autoregressive models [22,86]. \n\nOur approach builds upon recent breakthroughs in rectified flow models [3,23,55,61,62], which provide a simple framework for generative modeling while delivering exceptional empir-  ical performance [23,36,45]. Building on these advances, we propose JanusFlow, a powerful unified multimodal model that seamlessly integrates rectified flow with LLM architecture. Following a minimalist design principle, our architecture requires only a lightweight encoder and decoder to adapt the LLM for rectified flow operations. To optimize JanusFlow's performance, we implement two key strategies: First, we maintain separate vision encoders for understanding and generation tasks, preventing task interference and thus enhancing comprehension capabilities. Second, we align the intermediate representations between generation and understanding modules during training, strengthening semantic coherence in the generation process. \n\nJanusFlow shows state-of-the-art performances in both multimodal comprehension and text-to-image generation compared to existing unified approaches, and even outperforms several specialized methods.",
                    "score": 0.4900186929088897,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 149
                        },
                        {
                            "start": 150,
                            "end": 326
                        },
                        {
                            "start": 329,
                            "end": 438
                        },
                        {
                            "start": 439,
                            "end": 623
                        },
                        {
                            "start": 624,
                            "end": 794
                        },
                        {
                            "start": 795,
                            "end": 989
                        },
                        {
                            "start": 992,
                            "end": 1202
                        },
                        {
                            "start": 1203,
                            "end": 1353
                        },
                        {
                            "start": 1354,
                            "end": 1507
                        },
                        {
                            "start": 1508,
                            "end": 1737
                        },
                        {
                            "start": 1738,
                            "end": 1909
                        },
                        {
                            "start": 1912,
                            "end": 2110
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 264,
                            "end": 267,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 267,
                            "end": 270,
                            "matchedPaperCorpusId": "258615266"
                        },
                        {
                            "start": 273,
                            "end": 276,
                            "matchedPaperCorpusId": "256390509"
                        },
                        {
                            "start": 276,
                            "end": 279,
                            "matchedPaperCorpusId": "263672058"
                        },
                        {
                            "start": 279,
                            "end": 281,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 312,
                            "end": 316,
                            "matchedPaperCorpusId": "268247980"
                        },
                        {
                            "start": 316,
                            "end": 319,
                            "matchedPaperCorpusId": "259341735"
                        },
                        {
                            "start": 322,
                            "end": 325,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 603,
                            "end": 607,
                            "matchedPaperCorpusId": "261975252"
                        },
                        {
                            "start": 619,
                            "end": 622,
                            "matchedPaperCorpusId": "259765944"
                        },
                        {
                            "start": 931,
                            "end": 935,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 981,
                            "end": 985,
                            "matchedPaperCorpusId": "229297973"
                        },
                        {
                            "start": 1063,
                            "end": 1066,
                            "matchedPaperCorpusId": "252668615"
                        },
                        {
                            "start": 1066,
                            "end": 1069,
                            "matchedPaperCorpusId": "268247980"
                        },
                        {
                            "start": 1069,
                            "end": 1072,
                            "matchedPaperCorpusId": "252734897"
                        },
                        {
                            "start": 1072,
                            "end": 1075,
                            "matchedPaperCorpusId": "252111177"
                        },
                        {
                            "start": 1075,
                            "end": 1078,
                            "matchedPaperCorpusId": "261697392"
                        },
                        {
                            "start": 1191,
                            "end": 1195,
                            "matchedPaperCorpusId": "268247980"
                        },
                        {
                            "start": 1198,
                            "end": 1201,
                            "matchedPaperCorpusId": "259275061"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9814453125
                },
                {
                    "corpus_id": "273969525",
                    "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
                    "text": "Visual Generation with Flow-based Generative Models. Recent years have witnessed remarkable progress in visual generation through diffusion models [32,83], leading to impressive models like [67,73,[76][77][78][79]. Building on these advances, flow-based generative models [3,55,61] emerged as a simplified alternative framework. These approaches have recently enabled advanced visual generation models [23,36] that achieve superior empirical performance with faster sampling. Our work demonstrates that rectified flow [60][61][62] can be effectively integrated into LLMs, creating unified models that excel in both understanding and generation tasks. \n\nUnified Models For Understanding and Generation. The development of multimodal large language models (MLLMs) has enabled effective integration of text and visual information. Building upon powerful LLMs [7,91,92], recent MLLMs [2,15,49,56,58,64] have demonstrated exceptional multimodal understanding capabilities. Current research increasingly focuses on architectures that can simultaneously handle visual understanding and generation tasks. One approach extends MLLMs with pre-trained diffusion models [19, 25-27, 87, 101]. However, these systems essentially utilize diffusion models as external tools, where the MLLM generates conditions for image generation without possessing direct generative capabilities. This separation often results in suboptimal performance compared to standalone diffusion models [25,87]. Another line of work [88,97,99,100,108] aim to train a single LLM for both tasks. Many of these methods employ vector-quantization [22,86] to convert images into discrete tokens, enabling unified autoregressive processing [88,97]. While straightforward to implement, these approaches are inherently limited by their image tokenization quality. \n\nOur work focuses on developing unified models that combine autoregressive capabilities with flow/diffusion models, leveraging their proven effectiveness in visual generation.",
                    "score": 0.45156124054951113,
                    "section_title": "Related Work",
                    "char_start_offset": 2700,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 52
                        },
                        {
                            "start": 53,
                            "end": 328
                        },
                        {
                            "start": 329,
                            "end": 475
                        },
                        {
                            "start": 476,
                            "end": 650
                        },
                        {
                            "start": 653,
                            "end": 701
                        },
                        {
                            "start": 702,
                            "end": 827
                        },
                        {
                            "start": 828,
                            "end": 967
                        },
                        {
                            "start": 968,
                            "end": 1096
                        },
                        {
                            "start": 1097,
                            "end": 1179
                        },
                        {
                            "start": 1180,
                            "end": 1366
                        },
                        {
                            "start": 1367,
                            "end": 1471
                        },
                        {
                            "start": 1472,
                            "end": 1553
                        },
                        {
                            "start": 1554,
                            "end": 1702
                        },
                        {
                            "start": 1703,
                            "end": 1815
                        },
                        {
                            "start": 1818,
                            "end": 1992
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 147,
                            "end": 151,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 194,
                            "end": 197,
                            "matchedPaperCorpusId": "259341735"
                        },
                        {
                            "start": 201,
                            "end": 205,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 205,
                            "end": 209,
                            "matchedPaperCorpusId": "254854449"
                        },
                        {
                            "start": 209,
                            "end": 213,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 272,
                            "end": 275,
                            "matchedPaperCorpusId": "252668615"
                        },
                        {
                            "start": 275,
                            "end": 278,
                            "matchedPaperCorpusId": "252734897"
                        },
                        {
                            "start": 278,
                            "end": 281,
                            "matchedPaperCorpusId": "252111177"
                        },
                        {
                            "start": 402,
                            "end": 406,
                            "matchedPaperCorpusId": "268247980"
                        },
                        {
                            "start": 522,
                            "end": 526,
                            "matchedPaperCorpusId": "252111177"
                        },
                        {
                            "start": 526,
                            "end": 530,
                            "matchedPaperCorpusId": "261697392"
                        },
                        {
                            "start": 880,
                            "end": 883,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 883,
                            "end": 886,
                            "matchedPaperCorpusId": "258615266"
                        },
                        {
                            "start": 886,
                            "end": 889,
                            "matchedPaperCorpusId": "256390509"
                        },
                        {
                            "start": 889,
                            "end": 892,
                            "matchedPaperCorpusId": "263672058"
                        },
                        {
                            "start": 892,
                            "end": 895,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 1467,
                            "end": 1470,
                            "matchedPaperCorpusId": "259765944"
                        },
                        {
                            "start": 1603,
                            "end": 1607,
                            "matchedPaperCorpusId": "229297973"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9619140625
                }
            ],
            "relevance_judgement": 0.9814453125,
            "relevance_judgment_input_expanded": "# Title: JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation\n# Venue: arXiv.org\n# Authors: Yiyang Ma, Xingchao Liu, Xi-aokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, Liang Zhao, Yisong Wang, Jiaying Liu, C. Ruan\n## Abstract\nWe present JanusFlow, a powerful framework that unifies image understanding and generation in a single model. JanusFlow introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. To further improve the performance of our unified model, we adopt two key strategies: (i) decoupling the understanding and generation encoders, and (ii) aligning their representations during unified training. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models.\n## Introduction\nLarge language models (LLMs) have demonstrated remarkable capabilities in learning diverse knowledge and generalizing to new scenarios [1,7,8,69,91]. Leveraging these capabilities, researchers have developed sophisticated models specialized in image comprehension [2,15,47,49,56,58] and text-to-image generation [23,73,76,79]. \n\nThe field has recently shifted toward creating unified systems capable of handling both tasks simultaneously. One prominent direction involves utilizing pre-trained text-to-image models for high-quality generation while training LLMs to generate conditions for these models [19,[25][26][27]87]. However, this approach introduces architectural complexity and potentially constrains the model's capabilities through maintaining separate LLM and generative components. Alternative approaches [88,97,99,100,108] propose training a single LLM for both tasks, typically incorporating either diffusion models [32,83] or vector-quantized autoregressive models [22,86]. \n\nOur approach builds upon recent breakthroughs in rectified flow models [3,23,55,61,62], which provide a simple framework for generative modeling while delivering exceptional empir-  ical performance [23,36,45]. Building on these advances, we propose JanusFlow, a powerful unified multimodal model that seamlessly integrates rectified flow with LLM architecture. Following a minimalist design principle, our architecture requires only a lightweight encoder and decoder to adapt the LLM for rectified flow operations. To optimize JanusFlow's performance, we implement two key strategies: First, we maintain separate vision encoders for understanding and generation tasks, preventing task interference and thus enhancing comprehension capabilities. Second, we align the intermediate representations between generation and understanding modules during training, strengthening semantic coherence in the generation process. \n\nJanusFlow shows state-of-the-art performances in both multimodal comprehension and text-to-image generation compared to existing unified approaches, and even outperforms several specialized methods.\n\n## Related Work\nVisual Generation with Flow-based Generative Models. Recent years have witnessed remarkable progress in visual generation through diffusion models [32,83], leading to impressive models like [67,73,[76][77][78][79]. Building on these advances, flow-based generative models [3,55,61] emerged as a simplified alternative framework. These approaches have recently enabled advanced visual generation models [23,36] that achieve superior empirical performance with faster sampling. Our work demonstrates that rectified flow [60][61][62] can be effectively integrated into LLMs, creating unified models that excel in both understanding and generation tasks. \n\nUnified Models For Understanding and Generation. The development of multimodal large language models (MLLMs) has enabled effective integration of text and visual information. Building upon powerful LLMs [7,91,92], recent MLLMs [2,15,49,56,58,64] have demonstrated exceptional multimodal understanding capabilities. Current research increasingly focuses on architectures that can simultaneously handle visual understanding and generation tasks. One approach extends MLLMs with pre-trained diffusion models [19, 25-27, 87, 101]. However, these systems essentially utilize diffusion models as external tools, where the MLLM generates conditions for image generation without possessing direct generative capabilities. This separation often results in suboptimal performance compared to standalone diffusion models [25,87]. Another line of work [88,97,99,100,108] aim to train a single LLM for both tasks. Many of these methods employ vector-quantization [22,86] to convert images into discrete tokens, enabling unified autoregressive processing [88,97]. While straightforward to implement, these approaches are inherently limited by their image tokenization quality. \n\nOur work focuses on developing unified models that combine autoregressive capabilities with flow/diffusion models, leveraging their proven effectiveness in visual generation.",
            "reference_string": "[273969525 | Ma et al. | 2024 | Citations: 39]"
        },
        {
            "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 317,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.02567, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2359211182",
                    "name": "Xinjie Zhang"
                },
                {
                    "authorId": "2359201363",
                    "name": "Jintao Guo"
                },
                {
                    "authorId": "2337775059",
                    "name": "Shanshan Zhao"
                },
                {
                    "authorId": "2345947101",
                    "name": "Minghao Fu"
                },
                {
                    "authorId": "2275153684",
                    "name": "Lunhao Duan"
                },
                {
                    "authorId": "2339237238",
                    "name": "Guo-Hua Wang"
                },
                {
                    "authorId": "2304395071",
                    "name": "Qing-Guo Chen"
                },
                {
                    "authorId": "2304361003",
                    "name": "Zhao Xu"
                },
                {
                    "authorId": "2305289815",
                    "name": "Weihua Luo"
                },
                {
                    "authorId": "2304530663",
                    "name": "Kaifu Zhang"
                }
            ],
            "abstract": "Recent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey are available on GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).",
            "corpus_id": 278327487,
            "sentences": [
                {
                    "corpus_id": "278327487",
                    "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
                    "text": "In recent years, the rapid advancement of large language models (LLMs), such as LLaMa [1], [2], PanGu [3], [4], Qwen [5], [6], and GPT [7], has revolutionized artificial intelligence. These models have scaled up in both size and capability, enabling breakthroughs across diverse applications. Alongside this progress, LLMs have been extended into multimodal domains, giving rise to powerful multimodal understanding models like LLaVa [8], Qwen-VL [9], [10], InternVL [11], Ovis [12], and GPT4 [13]. These models have expanded their capabilities beyond simple image captioning to performing complex reasoning tasks based on user instructions. On the other hand, image generation technology has also experienced rapid development, with models like SD series [14], [15] and FLUX [16] now capable of producing high-quality images that adhere closely to user prompts. \n\nThe predominant architectural paradigm for LLMs and multimodal understanding models is autoregressive gener- ation [17], which relies on decoder-only structures and nexttoken prediction for sequential text generation. In contrast, the field of text-to-image generation has evolved along a different trajectory. Initially dominated by Generative Adversarial Networks (GANs) [18], image generation has since transitioned to diffusion-based models [19], which leverage architectures like UNet [14] and DiT [20], [21] alongside advanced text encoders such as CLIP [22] and T5 [23]. Despite some explorations into using LLM-inspired architectures for image generation [24], [25], [26], diffusionbased approaches remain the state-of-the-art in terms of performance currently. \n\nWhile autoregressive models lag behind diffusion-based methods in image generation quality, their structural consistency with LLMs makes them particularly appealing for developing unified multimodal systems. A unified model capable of both understanding and generating multimodal content holds immense potential: it could generate images based on complex instructions, reason about visual data, and visualize multimodal analyses through generated outputs.",
                    "score": 0.5726672793110983,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 183
                        },
                        {
                            "start": 184,
                            "end": 292
                        },
                        {
                            "start": 293,
                            "end": 498
                        },
                        {
                            "start": 499,
                            "end": 641
                        },
                        {
                            "start": 642,
                            "end": 862
                        },
                        {
                            "start": 865,
                            "end": 1082
                        },
                        {
                            "start": 1083,
                            "end": 1175
                        },
                        {
                            "start": 1176,
                            "end": 1442
                        },
                        {
                            "start": 1443,
                            "end": 1634
                        },
                        {
                            "start": 1637,
                            "end": 1844
                        },
                        {
                            "start": 1845,
                            "end": 2092
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 135,
                            "end": 138,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 434,
                            "end": 437,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 467,
                            "end": 471,
                            "matchedPaperCorpusId": "266521410"
                        },
                        {
                            "start": 756,
                            "end": 760,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 762,
                            "end": 766,
                            "matchedPaperCorpusId": "268247980"
                        },
                        {
                            "start": 1238,
                            "end": 1242,
                            "matchedPaperCorpusId": "1033682"
                        },
                        {
                            "start": 1310,
                            "end": 1314,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 1355,
                            "end": 1359,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 1368,
                            "end": 1372,
                            "matchedPaperCorpusId": "254854389"
                        },
                        {
                            "start": 1425,
                            "end": 1429,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 1437,
                            "end": 1441,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 1534,
                            "end": 1538,
                            "matchedPaperCorpusId": "270560593"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97998046875
                },
                {
                    "corpus_id": "278327487",
                    "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
                    "text": "Unified multimodal models aim to build a single architecture capable of both understanding and generating data across multiple modalities. These models are designed to process diverse forms of input (e.g., text, image, video, audio) and produce outputs in one or more modalities in a unified manner. A typical unified multimodal framework can be abstracted into three core components: modalityspecific encoders that project different input modalities into a representation space; a modality-fusion backbone that integrates information from multiple modalities and enables cross-modal reasoning; and modality-specific decoders that generate output in the desired modality (e.g., text generation or image synthesis). \n\nIn this section, we primarily focus on unified multimodal models that support vision-language understanding and generation, i.e., models that take both image and text as input and produce either text or image as output. As shown in Fig. 5, existing unified models can be broadly categorized into three main types: diffusion models, autoregressive models, and fused AR + diffusion models. For autoregressive models, we further classify them based on their modality encoding methods into four subcategories: pixel-based encoding, semantic-based encoding, learnable query-based encoding, and hybrid encoding. Each of these encoding strategies represents different ways of handling visual and textual data, leading to varying levels of integration and flexibility in the multimodal representations. Fused AR + diffusion models are divided into two subcategories based on modality encoding: pixel-based encoding and hybrid encoding. These models combine aspects of both autoregressive and diffusion techniques, offering a promising approach to more unified and efficient multimodal generation. \n\nIn the following sections, we will delve deeper into each category: Section 3.1 explores diffusion-based models, discussing their unique advantages in terms of generating high-quality images and text from noisy representations. Section 3.2 focuses on autoregressive-based models, detailing how different encoding methods impact their performance in vision-language tasks. Section 3.3 covers fused AR + diffusion models, examining how the combination of these two paradigms can enhance multimodal generation capabilities.",
                    "score": 0.6012701686887088,
                    "section_title": "UNIFIED MULTIMODAL MODELS FOR UNDER-STANDING AND GENERATION",
                    "char_start_offset": 20720,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 138
                        },
                        {
                            "start": 139,
                            "end": 299
                        },
                        {
                            "start": 300,
                            "end": 714
                        },
                        {
                            "start": 717,
                            "end": 936
                        },
                        {
                            "start": 937,
                            "end": 1104
                        },
                        {
                            "start": 1105,
                            "end": 1322
                        },
                        {
                            "start": 1323,
                            "end": 1511
                        },
                        {
                            "start": 1512,
                            "end": 1644
                        },
                        {
                            "start": 1645,
                            "end": 1805
                        },
                        {
                            "start": 1808,
                            "end": 2035
                        },
                        {
                            "start": 2036,
                            "end": 2179
                        },
                        {
                            "start": 2180,
                            "end": 2328
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97607421875
                },
                {
                    "corpus_id": "278327487",
                    "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
                    "text": "A unified model capable of both understanding and generating multimodal content holds immense potential: it could generate images based on complex instructions, reason about visual data, and visualize multimodal analyses through generated outputs. The unveiling of GPT-4o's enhanced capabilities [27] in March 2025 has further highlighted this potential, sparking widespread interest in unification. \n\nHowever, designing such a unified framework presents significant challenges. It requires integrating the strengths of autoregressive models for reasoning and text generation with the robustness of diffusion-based models for highquality image synthesis. Key questions remain unresolved, state of unified multimodal models (as illustrated in Fig. 1), thereby benefiting future research endeavors, we present this survey. We begin by introducing the foundational concepts and recent advancements in both multimodal understanding and image generation, covering both autoregressive and diffusion-based paradigms. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. Within the autoregressive and hybrid categories, we further classify models based on their image tokenization strategies, reflecting the diversity of approaches in this area. \n\nBeyond architecture, we assemble datasets and benchmarks tailored for training and evaluating unified multimodal models. These resources span multimodal understanding, text-to-image generation, image editing, and other relevant tasks, providing a foundation for future exploration. Finally, we discuss the key challenges facing this nascent field, including efficient tokenization strategy, data construction, model evaluation, etc. Tackling these challenges will be crucial for advancing the capabilities and scalability of unified multimodal models. \n\nIn the community, there exist excellent surveys on large language models [40], [41], multimodal understanding [42], [43], [44], and image generation [45], [46], while our work focuses specifically on the integration of understanding and generation tasks. Readers are encouraged to consult these complementary surveys for a broader perspective on related topics. We aim to inspire further research in this rapidly evolving field and provide a valuable reference for the community.",
                    "score": 0.6084644227441394,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 1860,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 247
                        },
                        {
                            "start": 248,
                            "end": 399
                        },
                        {
                            "start": 402,
                            "end": 478
                        },
                        {
                            "start": 479,
                            "end": 654
                        },
                        {
                            "start": 655,
                            "end": 820
                        },
                        {
                            "start": 821,
                            "end": 1009
                        },
                        {
                            "start": 1010,
                            "end": 1221
                        },
                        {
                            "start": 1222,
                            "end": 1396
                        },
                        {
                            "start": 1399,
                            "end": 1519
                        },
                        {
                            "start": 1520,
                            "end": 1680
                        },
                        {
                            "start": 1681,
                            "end": 1950
                        },
                        {
                            "start": 1953,
                            "end": 2207
                        },
                        {
                            "start": 2208,
                            "end": 2314
                        },
                        {
                            "start": 2315,
                            "end": 2432
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 2032,
                            "end": 2036,
                            "matchedPaperCorpusId": "259360395"
                        },
                        {
                            "start": 2063,
                            "end": 2067,
                            "matchedPaperCorpusId": "271750640"
                        },
                        {
                            "start": 2069,
                            "end": 2073,
                            "matchedPaperCorpusId": "265308931"
                        },
                        {
                            "start": 2075,
                            "end": 2079,
                            "matchedPaperCorpusId": "259243718"
                        },
                        {
                            "start": 2102,
                            "end": 2106,
                            "matchedPaperCorpusId": "252199918"
                        },
                        {
                            "start": 2108,
                            "end": 2112,
                            "matchedPaperCorpusId": "265821110"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96240234375
                }
            ],
            "relevance_judgement": 0.97998046875,
            "relevance_judgment_input_expanded": "# Title: Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities\n# Venue: arXiv.org\n# Authors: Xinjie Zhang, Jintao Guo, Shanshan Zhao, Minghao Fu, Lunhao Duan, Guo-Hua Wang, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang\n## Abstract\nRecent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey are available on GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).\n## INTRODUCTION\nIn recent years, the rapid advancement of large language models (LLMs), such as LLaMa [1], [2], PanGu [3], [4], Qwen [5], [6], and GPT [7], has revolutionized artificial intelligence. These models have scaled up in both size and capability, enabling breakthroughs across diverse applications. Alongside this progress, LLMs have been extended into multimodal domains, giving rise to powerful multimodal understanding models like LLaVa [8], Qwen-VL [9], [10], InternVL [11], Ovis [12], and GPT4 [13]. These models have expanded their capabilities beyond simple image captioning to performing complex reasoning tasks based on user instructions. On the other hand, image generation technology has also experienced rapid development, with models like SD series [14], [15] and FLUX [16] now capable of producing high-quality images that adhere closely to user prompts. \n\nThe predominant architectural paradigm for LLMs and multimodal understanding models is autoregressive gener- ation [17], which relies on decoder-only structures and nexttoken prediction for sequential text generation. In contrast, the field of text-to-image generation has evolved along a different trajectory. Initially dominated by Generative Adversarial Networks (GANs) [18], image generation has since transitioned to diffusion-based models [19], which leverage architectures like UNet [14] and DiT [20], [21] alongside advanced text encoders such as CLIP [22] and T5 [23]. Despite some explorations into using LLM-inspired architectures for image generation [24], [25], [26], diffusionbased approaches remain the state-of-the-art in terms of performance currently. \n\nWhile autoregressive models lag behind diffusion-based methods in image generation quality, their structural consistency with LLMs makes them particularly appealing for developing unified multimodal systems. A unified model capable of both understanding and generating multimodal content holds immense potential: it could generate images based on complex instructions, reason about visual data, and visualize multimodal analyses through generated outputs.\n...\nA unified model capable of both understanding and generating multimodal content holds immense potential: it could generate images based on complex instructions, reason about visual data, and visualize multimodal analyses through generated outputs. The unveiling of GPT-4o's enhanced capabilities [27] in March 2025 has further highlighted this potential, sparking widespread interest in unification. \n\nHowever, designing such a unified framework presents significant challenges. It requires integrating the strengths of autoregressive models for reasoning and text generation with the robustness of diffusion-based models for highquality image synthesis. Key questions remain unresolved, state of unified multimodal models (as illustrated in Fig. 1), thereby benefiting future research endeavors, we present this survey. We begin by introducing the foundational concepts and recent advancements in both multimodal understanding and image generation, covering both autoregressive and diffusion-based paradigms. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. Within the autoregressive and hybrid categories, we further classify models based on their image tokenization strategies, reflecting the diversity of approaches in this area. \n\nBeyond architecture, we assemble datasets and benchmarks tailored for training and evaluating unified multimodal models. These resources span multimodal understanding, text-to-image generation, image editing, and other relevant tasks, providing a foundation for future exploration. Finally, we discuss the key challenges facing this nascent field, including efficient tokenization strategy, data construction, model evaluation, etc. Tackling these challenges will be crucial for advancing the capabilities and scalability of unified multimodal models. \n\nIn the community, there exist excellent surveys on large language models [40], [41], multimodal understanding [42], [43], [44], and image generation [45], [46], while our work focuses specifically on the integration of understanding and generation tasks. Readers are encouraged to consult these complementary surveys for a broader perspective on related topics. We aim to inspire further research in this rapidly evolving field and provide a valuable reference for the community.\n\n## UNIFIED MULTIMODAL MODELS FOR UNDER-STANDING AND GENERATION\nUnified multimodal models aim to build a single architecture capable of both understanding and generating data across multiple modalities. These models are designed to process diverse forms of input (e.g., text, image, video, audio) and produce outputs in one or more modalities in a unified manner. A typical unified multimodal framework can be abstracted into three core components: modalityspecific encoders that project different input modalities into a representation space; a modality-fusion backbone that integrates information from multiple modalities and enables cross-modal reasoning; and modality-specific decoders that generate output in the desired modality (e.g., text generation or image synthesis). \n\nIn this section, we primarily focus on unified multimodal models that support vision-language understanding and generation, i.e., models that take both image and text as input and produce either text or image as output. As shown in Fig. 5, existing unified models can be broadly categorized into three main types: diffusion models, autoregressive models, and fused AR + diffusion models. For autoregressive models, we further classify them based on their modality encoding methods into four subcategories: pixel-based encoding, semantic-based encoding, learnable query-based encoding, and hybrid encoding. Each of these encoding strategies represents different ways of handling visual and textual data, leading to varying levels of integration and flexibility in the multimodal representations. Fused AR + diffusion models are divided into two subcategories based on modality encoding: pixel-based encoding and hybrid encoding. These models combine aspects of both autoregressive and diffusion techniques, offering a promising approach to more unified and efficient multimodal generation. \n\nIn the following sections, we will delve deeper into each category: Section 3.1 explores diffusion-based models, discussing their unique advantages in terms of generating high-quality images and text from noisy representations. Section 3.2 focuses on autoregressive-based models, detailing how different encoding methods impact their performance in vision-language tasks. Section 3.3 covers fused AR + diffusion models, examining how the combination of these two paradigms can enhance multimodal generation capabilities.",
            "reference_string": "[278327487 | Zhang et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Unified Autoregressive Visual Generation and Understanding with Continuous Tokens",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 61,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.13436, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2347484699",
                    "name": "Lijie Fan"
                },
                {
                    "authorId": "34689393",
                    "name": "Luming Tang"
                },
                {
                    "authorId": "2333872078",
                    "name": "Siyang Qin"
                },
                {
                    "authorId": "2307269819",
                    "name": "Tianhong Li"
                },
                {
                    "authorId": "2350843695",
                    "name": "Xuan Yang"
                },
                {
                    "authorId": "2266238535",
                    "name": "Siyuan Qiao"
                },
                {
                    "authorId": "2350755056",
                    "name": "Andreas Steiner"
                },
                {
                    "authorId": "2333512682",
                    "name": "Chen Sun"
                },
                {
                    "authorId": "2281034398",
                    "name": "Yuanzhen Li"
                },
                {
                    "authorId": "2351408307",
                    "name": "Tao Zhu"
                },
                {
                    "authorId": "2325950285",
                    "name": "Michael Rubinstein"
                },
                {
                    "authorId": "3063676",
                    "name": "Michalis Raptis"
                },
                {
                    "authorId": "2262516617",
                    "name": "Deqing Sun"
                },
                {
                    "authorId": "1737285",
                    "name": "Radu Soricut"
                }
            ],
            "abstract": "We present UniFluid, a unified autoregressive framework for joint visual generation and understanding leveraging continuous visual tokens. Our unified autoregressive architecture processes multimodal image and text inputs, generating discrete tokens for text and continuous tokens for image. We find though there is an inherent trade-off between the image generation and understanding task, a carefully tuned training recipe enables them to improve each other. By selecting an appropriate loss balance weight, the unified model achieves results comparable to or exceeding those of single-task baselines on both tasks. Furthermore, we demonstrate that employing stronger pre-trained LLMs and random-order generation during training is important to achieve high-fidelity image generation within this unified framework. Built upon the Gemma model series, UniFluid exhibits competitive performance across both image generation and understanding, demonstrating strong transferability to various downstream tasks, including image editing for generation, as well as visual captioning and question answering for understanding.",
            "corpus_id": 277103675,
            "sentences": [
                {
                    "corpus_id": "277103675",
                    "title": "Unified Autoregressive Visual Generation and Understanding with Continuous Tokens",
                    "text": "Large Language Models (LLMs) have recently advanced from text-centric architectures, such as BERT [11] and GPT [35], toward multimodal systems capable of understanding and generating content across different modalities. GPT-3 [5] and PaLM [9] show that scaling language models leads to emergent capabilities, while Flamingo [2] further demonstrates that incorporating visual inputs facilitates unified multimodal reasoning. This trend toward unified vision-language model-using a single model for diverse tasks of visual understanding and generation-has promising potential for leveraging knowledge and reasoning abilities that transfer across different vision and language tasks, ultimately facilitating more robust and generalizable multimodal representation and modeling capabilities. \n\nMotivated by the advantages and strong scaling properties of autoregressive models, coupled with their simplicity, we investigate a pure autoregressive framework for unified visual generation and understanding, without the limitations introduced by vector quantization (VQ). In this paper, we introduce UniFluid, a unified framework that leverages continuous visual tokens within an autoregressive architecture to jointly handle vision-language generation and understanding tasks. Building upon pre-trained Gemma [48] on large-scale text corpus, UniFluid unlocks powerful visual generation and understanding capabilities through training with paired image-text data, and further allows these two tasks to mutually benefit each other within a single architecture. Specifically, UniFluid adopts a unified autoregressive framework where both text and continuous visual inputs are embedded as tokens in the same space, enabling seamless joint training of image generation and understanding tasks. UniFluid integrates a continuous tokenizer [12,21] for image generation and a pre-trained SigLIP [59] image encoder for understanding tasks, while textual inputs are processed using a standard SentencePiece tokenizer [20]. The resulting multimodal sequences are modeled autoregressively using Gemma [48] as the underlying transformer backbone. Task-specific prediction heads-a diffusion-based head for image generation and a cross-entropy head for text generation-ensure effective modality-specific training and inference, enabling UniFluid to efficiently learn shared representations that mutually enhance its generation and understanding capabilities.",
                    "score": 0.4603593860514052,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 219
                        },
                        {
                            "start": 220,
                            "end": 423
                        },
                        {
                            "start": 424,
                            "end": 787
                        },
                        {
                            "start": 790,
                            "end": 1064
                        },
                        {
                            "start": 1065,
                            "end": 1270
                        },
                        {
                            "start": 1271,
                            "end": 1552
                        },
                        {
                            "start": 1553,
                            "end": 1782
                        },
                        {
                            "start": 1783,
                            "end": 2005
                        },
                        {
                            "start": 2006,
                            "end": 2126
                        },
                        {
                            "start": 2127,
                            "end": 2436
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 98,
                            "end": 102,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 226,
                            "end": 229,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 239,
                            "end": 242,
                            "matchedPaperCorpusId": "247951931"
                        },
                        {
                            "start": 324,
                            "end": 327,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 1830,
                            "end": 1833,
                            "matchedPaperCorpusId": "270560593"
                        },
                        {
                            "start": 1880,
                            "end": 1884,
                            "matchedPaperCorpusId": "257767223"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9794921875
                }
            ],
            "relevance_judgement": 0.9794921875,
            "relevance_judgment_input_expanded": "# Title: Unified Autoregressive Visual Generation and Understanding with Continuous Tokens\n# Venue: arXiv.org\n# Authors: Lijie Fan, Luming Tang, Siyang Qin, Tianhong Li, Xuan Yang, Siyuan Qiao, Andreas Steiner, Chen Sun, Yuanzhen Li, Tao Zhu, Michael Rubinstein, Michalis Raptis, Deqing Sun, Radu Soricut\n## Abstract\nWe present UniFluid, a unified autoregressive framework for joint visual generation and understanding leveraging continuous visual tokens. Our unified autoregressive architecture processes multimodal image and text inputs, generating discrete tokens for text and continuous tokens for image. We find though there is an inherent trade-off between the image generation and understanding task, a carefully tuned training recipe enables them to improve each other. By selecting an appropriate loss balance weight, the unified model achieves results comparable to or exceeding those of single-task baselines on both tasks. Furthermore, we demonstrate that employing stronger pre-trained LLMs and random-order generation during training is important to achieve high-fidelity image generation within this unified framework. Built upon the Gemma model series, UniFluid exhibits competitive performance across both image generation and understanding, demonstrating strong transferability to various downstream tasks, including image editing for generation, as well as visual captioning and question answering for understanding.\n## Introduction\nLarge Language Models (LLMs) have recently advanced from text-centric architectures, such as BERT [11] and GPT [35], toward multimodal systems capable of understanding and generating content across different modalities. GPT-3 [5] and PaLM [9] show that scaling language models leads to emergent capabilities, while Flamingo [2] further demonstrates that incorporating visual inputs facilitates unified multimodal reasoning. This trend toward unified vision-language model-using a single model for diverse tasks of visual understanding and generation-has promising potential for leveraging knowledge and reasoning abilities that transfer across different vision and language tasks, ultimately facilitating more robust and generalizable multimodal representation and modeling capabilities. \n\nMotivated by the advantages and strong scaling properties of autoregressive models, coupled with their simplicity, we investigate a pure autoregressive framework for unified visual generation and understanding, without the limitations introduced by vector quantization (VQ). In this paper, we introduce UniFluid, a unified framework that leverages continuous visual tokens within an autoregressive architecture to jointly handle vision-language generation and understanding tasks. Building upon pre-trained Gemma [48] on large-scale text corpus, UniFluid unlocks powerful visual generation and understanding capabilities through training with paired image-text data, and further allows these two tasks to mutually benefit each other within a single architecture. Specifically, UniFluid adopts a unified autoregressive framework where both text and continuous visual inputs are embedded as tokens in the same space, enabling seamless joint training of image generation and understanding tasks. UniFluid integrates a continuous tokenizer [12,21] for image generation and a pre-trained SigLIP [59] image encoder for understanding tasks, while textual inputs are processed using a standard SentencePiece tokenizer [20]. The resulting multimodal sequences are modeled autoregressively using Gemma [48] as the underlying transformer backbone. Task-specific prediction heads-a diffusion-based head for image generation and a cross-entropy head for text generation-ensure effective modality-specific training and inference, enabling UniFluid to efficiently learn shared representations that mutually enhance its generation and understanding capabilities.",
            "reference_string": "[277103675 | Fan et al. | 2025 | Citations: 8]"
        },
        {
            "title": "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 37,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.11925, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306162276",
                    "name": "Yi Fang"
                },
                {
                    "authorId": "2057050247",
                    "name": "Bowen Jin"
                },
                {
                    "authorId": "2346106510",
                    "name": "Jiacheng Shen"
                },
                {
                    "authorId": "2307734628",
                    "name": "Sirui Ding"
                },
                {
                    "authorId": "2282981308",
                    "name": "Qiaoyu Tan"
                },
                {
                    "authorId": "2257136881",
                    "name": "Jiawei Han"
                }
            ],
            "abstract": "The rapid development of Multimodal Large Language Models (MLLMs) has enabled the integration of multiple modalities, including texts and images, within the large language model (LLM) framework. However, texts and images are usually interconnected, forming a multimodal attributed graph (MMAG). It is underexplored how MLLMs can incorporate the relational information (\\textit{i.e.}, graph structure) and semantic information (\\textit{i.e.,} texts and images) on such graphs for multimodal comprehension and generation. In this paper, we propose GraphGPT-o, which supports omni-multimodal understanding and creation on MMAGs. We first comprehensively study linearization variants to transform semantic and structural information as input for MLLMs. Then, we propose a hierarchical aligner that enables deep graph encoding, bridging the gap between MMAGs and MLLMs. Finally, we explore the inference choices, adapting MLLM to interleaved text and image generation in graph scenarios. Extensive experiments on three datasets from different domains demonstrate the effectiveness of our proposed method. Datasets and codes will be open-sourced upon acceptance.",
            "corpus_id": 276422421,
            "sentences": [
                {
                    "corpus_id": "276422421",
                    "title": "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs",
                    "text": "Multimodal Large Language Models (MLLMs) have advanced the field by enabling unified multimodal understanding and generation within a single autoregressive framework [37,38]. In terms of multimodal comprehension, models like Flamingo [1] process visual data interleaved with text, utilizing a gated cross-attention layer to encode    inputs and produce free-form textual output. BLIP-2 [20] introduces the Q-Former architecture, which maps images into a hidden space aligned with text tokens in LLMs, while LLaVA [21] simplifies this framework further with a projector and explores instruction tuning within the multimodal domain. Despite these advancements, current MLLMs primarily emphasize text generation and lack the capability to synthesize multimodal outputs (e.g., images). To address this, DreamLLM [6] integrates an LLM backbone with a   diffusion model to enable image generation as a multimodal output. Emu2 [31] scales this architecture to 37B parame-ters, demonstrating strong multimodal in-context learning and the ability to handle complex tasks requiring real-time reasoning, such as visual prompting and object-grounded generation. Chameleon [33] proposes a stable training strategy from the ground up, featuring an alignment process and architectural parameterization tailored to early-fusion, token-based, mixed-modal settings. Nevertheless, most existing approaches overlook the relational dynamics between text and images, limiting their applicability to multimodal content generation tasks on multimodal attributed graphs (MMAGs).",
                    "score": 0.5604713211678088,
                    "section_title": "Multimodal Large Language Models",
                    "char_start_offset": 21994,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 175,
                            "end": 378
                        },
                        {
                            "start": 379,
                            "end": 630
                        },
                        {
                            "start": 631,
                            "end": 781
                        },
                        {
                            "start": 782,
                            "end": 914
                        },
                        {
                            "start": 915,
                            "end": 1149
                        },
                        {
                            "start": 1150,
                            "end": 1347
                        },
                        {
                            "start": 1348,
                            "end": 1553
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 234,
                            "end": 237,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 386,
                            "end": 390,
                            "matchedPaperCorpusId": "256390509"
                        },
                        {
                            "start": 513,
                            "end": 517,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 920,
                            "end": 924,
                            "matchedPaperCorpusId": "266374640"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97900390625
                }
            ],
            "relevance_judgement": 0.97900390625,
            "relevance_judgment_input_expanded": "# Title: GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs\n# Venue: arXiv.org\n# Authors: Yi Fang, Bowen Jin, Jiacheng Shen, Sirui Ding, Qiaoyu Tan, Jiawei Han\n## Abstract\nThe rapid development of Multimodal Large Language Models (MLLMs) has enabled the integration of multiple modalities, including texts and images, within the large language model (LLM) framework. However, texts and images are usually interconnected, forming a multimodal attributed graph (MMAG). It is underexplored how MLLMs can incorporate the relational information (\\textit{i.e.}, graph structure) and semantic information (\\textit{i.e.,} texts and images) on such graphs for multimodal comprehension and generation. In this paper, we propose GraphGPT-o, which supports omni-multimodal understanding and creation on MMAGs. We first comprehensively study linearization variants to transform semantic and structural information as input for MLLMs. Then, we propose a hierarchical aligner that enables deep graph encoding, bridging the gap between MMAGs and MLLMs. Finally, we explore the inference choices, adapting MLLM to interleaved text and image generation in graph scenarios. Extensive experiments on three datasets from different domains demonstrate the effectiveness of our proposed method. Datasets and codes will be open-sourced upon acceptance.\n## Multimodal Large Language Models\nMultimodal Large Language Models (MLLMs) have advanced the field by enabling unified multimodal understanding and generation within a single autoregressive framework [37,38]. In terms of multimodal comprehension, models like Flamingo [1] process visual data interleaved with text, utilizing a gated cross-attention layer to encode    inputs and produce free-form textual output. BLIP-2 [20] introduces the Q-Former architecture, which maps images into a hidden space aligned with text tokens in LLMs, while LLaVA [21] simplifies this framework further with a projector and explores instruction tuning within the multimodal domain. Despite these advancements, current MLLMs primarily emphasize text generation and lack the capability to synthesize multimodal outputs (e.g., images). To address this, DreamLLM [6] integrates an LLM backbone with a   diffusion model to enable image generation as a multimodal output. Emu2 [31] scales this architecture to 37B parame-ters, demonstrating strong multimodal in-context learning and the ability to handle complex tasks requiring real-time reasoning, such as visual prompting and object-grounded generation. Chameleon [33] proposes a stable training strategy from the ground up, featuring an alignment process and architectural parameterization tailored to early-fusion, token-based, mixed-modal settings. Nevertheless, most existing approaches overlook the relational dynamics between text and images, limiting their applicability to multimodal content generation tasks on multimodal attributed graphs (MMAGs).",
            "reference_string": "[276422421 | Fang et al. | 2025 | Citations: 2]"
        },
        {
            "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 55,
            "citation_count": 37,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.09251, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2274151094",
                    "name": "Jinguo Zhu"
                },
                {
                    "authorId": "2258784456",
                    "name": "Xiaohan Ding"
                },
                {
                    "authorId": "152988335",
                    "name": "Yixiao Ge"
                },
                {
                    "authorId": "51123495",
                    "name": "Yuying Ge"
                },
                {
                    "authorId": "2254048096",
                    "name": "Sijie Zhao"
                },
                {
                    "authorId": "2310758544",
                    "name": "Hengshuang Zhao"
                },
                {
                    "authorId": "2274490604",
                    "name": "Xiaohua Wang"
                },
                {
                    "authorId": "2265579883",
                    "name": "Ying Shan"
                }
            ],
            "abstract": "In this work, we introduce Vision-Language Generative Pre-trained Transformer (VL-GPT), a transformer model proficient at concurrently perceiving and generating visual and linguistic data. VL-GPT achieves a unified pre-training approach for both image and text modalities by employing a straightforward auto-regressive objective, thereby enabling the model to process image and text as seamlessly as a language model processes text. To accomplish this, we initially propose a novel image tokenizer-detokenizer framework for visual data, specifically designed to transform raw images into a sequence of continuous embeddings and reconstruct them accordingly. In combination with the existing text tokenizer and detokenizer, this framework allows for the encoding of interleaved image-text data into a multimodal sequence, which can subsequently be fed into the transformer model. Consequently, VL-GPT can perform large-scale pre-training on multimodal corpora utilizing a unified auto-regressive objective (i.e., next-token prediction). Upon completion of pre-training, VL-GPT exhibits remarkable zero-shot and few-shot performance across a diverse range of vision and language understanding and generation tasks, including image captioning, visual question answering, text-to-image generation, and more. Additionally, the pre-trained model retrains in-context learning capabilities when provided with multimodal prompts. We further conduct instruction tuning on our VL-GPT, highlighting its exceptional potential for multimodal assistance. The source code and model weights shall be released.",
            "corpus_id": 266210376,
            "sentences": [
                {
                    "corpus_id": "266210376",
                    "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
                    "text": "VL-GPT aims to process the vision and language understanding and generation within a single transformer model in a unified way, similar to GPT handles language tasks. It is capable of perceiving the interleaved multi-modal data and generating content across various modalities. By employing unified modeling, our VL-GPT can conduct autoregressive pre-training on web-scale multimodal corpora, thereby holding the potential to serve as a powerful foundation model in the multimodal research community. \n\nArchitecture As depicted at the bottom of Fig. 1, our VL-GPT comprises five components: a large vision-language transformer model M, an image tokenizer E v , a text tokenizer E t , an image detokenizer D v and a text detokenizer D t . In comparison to a language model, VL-GPT incorporates additional image tokenizer and image detokenizer elements. \n\nGiven any interleaved image-text data, the image tokenizer and the text tokenizer initially encode them into a multimodal sequence. More specifically, the image tokenizer E v converts each image into N continuous visual embeddings x v . Additionally, two special tokens [IMG] and [/IMG] are appended at the beginning and end of the visual embeddings, respectively. The visual embeddings are then combined with the discrete text tokens encoded by the text tokenizer E t to form a interleaved multimodal sequence v = (v 1 , v 2 , . . . , v n ), where v i can be either a discrete text token or a continuous visual embedding. The multimodal sequence v is then fed into the large VL model M for unified auto-regressive modeling. \n\nThe output embedding M(v i ) can be flexibly transformed into a text embedding through a language modeling head for the predefined vocabulary or into a visual embedding with a separate regression head. During training, the selection of the transformed head depends on whether the target for the current embedding is a text token or a visual embedding.",
                    "score": 0.5045350455543993,
                    "section_title": "VL-GPT",
                    "char_start_offset": 13294,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 166
                        },
                        {
                            "start": 167,
                            "end": 277
                        },
                        {
                            "start": 278,
                            "end": 500
                        },
                        {
                            "start": 503,
                            "end": 737
                        },
                        {
                            "start": 738,
                            "end": 851
                        },
                        {
                            "start": 854,
                            "end": 985
                        },
                        {
                            "start": 986,
                            "end": 1090
                        },
                        {
                            "start": 1091,
                            "end": 1218
                        },
                        {
                            "start": 1219,
                            "end": 1387
                        },
                        {
                            "start": 1388,
                            "end": 1476
                        },
                        {
                            "start": 1477,
                            "end": 1578
                        },
                        {
                            "start": 1581,
                            "end": 1782
                        },
                        {
                            "start": 1783,
                            "end": 1932
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97607421875
                },
                {
                    "corpus_id": "266210376",
                    "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
                    "text": "Driven by the remarkable success of large language models (LLMs) in the field of natural language processing (NLP) [40,41,54], there has been a surge of interest within multimodal community to develop large vision-language (VL) models. One of the promising approaches, exemplified by Flamingo [1], BLIP2 [24], LLAVA [25], have explored how to build large VL models based on powerful pre-trained LLMs. These studies typically adopted a similar architecture: a pre-trained image encoder and an LLM are connected via a trainable connection module, which aligns the image feature and text embeddings, thereby enabling language models to accept images and text as inputs and generate a text sequence. \n\nTo expand the capabilities of generating image in a multimodal context, certain efforts, e.g., Visual ChatGPT [47], attempt to connect LLMs with image generation tools in a cascaded pipeline by transferring text messages, which inevitably introduce instability and noise. Alternatively, another line of research achieves it by optimizing models in an end-to-end manner [9,18,23,30,48]. By aligning the output space with the image diffusion models, VL models can not only perceive but also generate images and text. \n\nA crucial characteristic of large language models is autoregressive modeling [31], i.e., predicting next token, which facilitates language understanding and generation in a unified manner. However, in the aforementioned studies, the inconsistency of image embeddings between LLM's input and output sides compels the model to treat input images and generated images differently, resulting in separate modeling for image understanding and generation. Meanwhile, this discrepancy also obstructs the implementation of autoregressive training loss on image embeddings. \n\nIn this study, we introduce VL-GPT, a large visionlanguage generative pre-trained transformer that enables the unified training of both visual and linguistic data using an auto-regressive objective, as depicted in Fig. 1.",
                    "score": 0.4723180431407133,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 235
                        },
                        {
                            "start": 236,
                            "end": 400
                        },
                        {
                            "start": 401,
                            "end": 695
                        },
                        {
                            "start": 698,
                            "end": 969
                        },
                        {
                            "start": 970,
                            "end": 1083
                        },
                        {
                            "start": 1084,
                            "end": 1212
                        },
                        {
                            "start": 1215,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1663
                        },
                        {
                            "start": 1664,
                            "end": 1778
                        },
                        {
                            "start": 1781,
                            "end": 2002
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.962890625
                }
            ],
            "relevance_judgement": 0.97607421875,
            "relevance_judgment_input_expanded": "# Title: VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation\n# Venue: arXiv.org\n# Authors: Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, Ying Shan\n## Abstract\nIn this work, we introduce Vision-Language Generative Pre-trained Transformer (VL-GPT), a transformer model proficient at concurrently perceiving and generating visual and linguistic data. VL-GPT achieves a unified pre-training approach for both image and text modalities by employing a straightforward auto-regressive objective, thereby enabling the model to process image and text as seamlessly as a language model processes text. To accomplish this, we initially propose a novel image tokenizer-detokenizer framework for visual data, specifically designed to transform raw images into a sequence of continuous embeddings and reconstruct them accordingly. In combination with the existing text tokenizer and detokenizer, this framework allows for the encoding of interleaved image-text data into a multimodal sequence, which can subsequently be fed into the transformer model. Consequently, VL-GPT can perform large-scale pre-training on multimodal corpora utilizing a unified auto-regressive objective (i.e., next-token prediction). Upon completion of pre-training, VL-GPT exhibits remarkable zero-shot and few-shot performance across a diverse range of vision and language understanding and generation tasks, including image captioning, visual question answering, text-to-image generation, and more. Additionally, the pre-trained model retrains in-context learning capabilities when provided with multimodal prompts. We further conduct instruction tuning on our VL-GPT, highlighting its exceptional potential for multimodal assistance. The source code and model weights shall be released.\n## Introduction\nDriven by the remarkable success of large language models (LLMs) in the field of natural language processing (NLP) [40,41,54], there has been a surge of interest within multimodal community to develop large vision-language (VL) models. One of the promising approaches, exemplified by Flamingo [1], BLIP2 [24], LLAVA [25], have explored how to build large VL models based on powerful pre-trained LLMs. These studies typically adopted a similar architecture: a pre-trained image encoder and an LLM are connected via a trainable connection module, which aligns the image feature and text embeddings, thereby enabling language models to accept images and text as inputs and generate a text sequence. \n\nTo expand the capabilities of generating image in a multimodal context, certain efforts, e.g., Visual ChatGPT [47], attempt to connect LLMs with image generation tools in a cascaded pipeline by transferring text messages, which inevitably introduce instability and noise. Alternatively, another line of research achieves it by optimizing models in an end-to-end manner [9,18,23,30,48]. By aligning the output space with the image diffusion models, VL models can not only perceive but also generate images and text. \n\nA crucial characteristic of large language models is autoregressive modeling [31], i.e., predicting next token, which facilitates language understanding and generation in a unified manner. However, in the aforementioned studies, the inconsistency of image embeddings between LLM's input and output sides compels the model to treat input images and generated images differently, resulting in separate modeling for image understanding and generation. Meanwhile, this discrepancy also obstructs the implementation of autoregressive training loss on image embeddings. \n\nIn this study, we introduce VL-GPT, a large visionlanguage generative pre-trained transformer that enables the unified training of both visual and linguistic data using an auto-regressive objective, as depicted in Fig. 1.\n\n## VL-GPT\nVL-GPT aims to process the vision and language understanding and generation within a single transformer model in a unified way, similar to GPT handles language tasks. It is capable of perceiving the interleaved multi-modal data and generating content across various modalities. By employing unified modeling, our VL-GPT can conduct autoregressive pre-training on web-scale multimodal corpora, thereby holding the potential to serve as a powerful foundation model in the multimodal research community. \n\nArchitecture As depicted at the bottom of Fig. 1, our VL-GPT comprises five components: a large vision-language transformer model M, an image tokenizer E v , a text tokenizer E t , an image detokenizer D v and a text detokenizer D t . In comparison to a language model, VL-GPT incorporates additional image tokenizer and image detokenizer elements. \n\nGiven any interleaved image-text data, the image tokenizer and the text tokenizer initially encode them into a multimodal sequence. More specifically, the image tokenizer E v converts each image into N continuous visual embeddings x v . Additionally, two special tokens [IMG] and [/IMG] are appended at the beginning and end of the visual embeddings, respectively. The visual embeddings are then combined with the discrete text tokens encoded by the text tokenizer E t to form a interleaved multimodal sequence v = (v 1 , v 2 , . . . , v n ), where v i can be either a discrete text token or a continuous visual embedding. The multimodal sequence v is then fed into the large VL model M for unified auto-regressive modeling. \n\nThe output embedding M(v i ) can be flexibly transformed into a text embedding through a language modeling head for the predefined vocabulary or into a visual embedding with a separate regression head. During training, the selection of the transformed head depends on whether the target for the current embedding is a text token or a visual embedding.",
            "reference_string": "[266210376 | Zhu et al. | 2023 | Citations: 37]"
        },
        {
            "title": "PAR: Prompt-Aware Token Reduction Method for Efficient Large Multimodal Models",
            "venue": "",
            "year": 2024,
            "reference_count": 30,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.07278, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325197953",
                    "name": "Yingen Liu"
                },
                {
                    "authorId": "2257429787",
                    "name": "Fan Wu"
                },
                {
                    "authorId": "2262008831",
                    "name": "Ruihui Li"
                },
                {
                    "authorId": "2242362590",
                    "name": "Zhuo Tang"
                },
                {
                    "authorId": "2278395518",
                    "name": "Kenli Li"
                }
            ],
            "abstract": "Multimodal large language models (MLLMs) demonstrate strong performance across visual tasks, but their efficiency is hindered by significant computational and memory demands from processing long contexts in multimodal inputs. To address this, we introduce PAR (Prompt-Aware Token Reduction), a novel and plug-and-play approach that reduces visual tokens efficiently without compromising model performance. Unlike previous methods that rely heavily on attention mechanisms and overlooking cross-modal interactions , we uses a prompt-aware strategy to adpative identify and cluster essential visual tokens. PAR categorizes visual context redundancy into two types: external and internal. External redundancy is minimized through semantic retrieval, while internal redundancy is addressed using a token routing mechanism. This method substantially reduces computational load without requiring additional training or complex architectural modifications. \\textbf{Experimental results demonstrate that across various visual question answering tasks, PAR reduces FLOPs by 83\\% with a compression ratio of 89\\%, while retaining 97\\% of baseline accuracy.} The adaptive design of PAR achieves a 2x token reduction ratio compared to prior approaches, enabling a better balance between performance and efficiency.",
            "corpus_id": 273233771,
            "sentences": [
                {
                    "corpus_id": "273233771",
                    "title": "PAR: Prompt-Aware Token Reduction Method for Efficient Large Multimodal Models",
                    "text": "Multimodal large language models (MLLMs) represent a significant advancement in artificial intelligence by integrating visual encoders with pre-trained large language models. This integration allows these models to jointly process and understand diverse modalities, such as images and text, enabling more comprehensive and contextually aware analyses. \n\nFor a given image I, the visual encoder f visual transforms it into a series of token representations X I : \n\nHere, X I denotes a set of visual tokens that capture essential visual features and semantic information from the image. This process allows image content to be encoded in a format compatible with the language model, facilitating the effective integration of visual and textual data. \n\nTo ensure that the visual tokens are compatible with the language model's processing framework, a visual projection layer is employed. This layer converts visual tokens X I into language embedding tokens y 1 , y 2 , . . . , y n . The transformation is performed such that these language embedding tokens share the same dimensional space d as the word embeddings used by the language model: \n\nwhere f projection denotes the projection function that aligns the dimensionality of the visual tokens with that of the language embeddings. This alignment ensures that the visual information encoded in y 1 , y 2 , . . . , y n can be processed seamlessly alongside the textual information by the language model. Consequently, the MLMM can effectively integrate and leverage both visual and textual representations, improving its ability to generate accurate and contextually relevant outputs.",
                    "score": 0.4617909254246055,
                    "section_title": "Preliminaries",
                    "char_start_offset": 10284,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 175,
                            "end": 351
                        },
                        {
                            "start": 354,
                            "end": 461
                        },
                        {
                            "start": 464,
                            "end": 584
                        },
                        {
                            "start": 585,
                            "end": 747
                        },
                        {
                            "start": 750,
                            "end": 884
                        },
                        {
                            "start": 885,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 979
                        },
                        {
                            "start": 980,
                            "end": 1139
                        },
                        {
                            "start": 1142,
                            "end": 1282
                        },
                        {
                            "start": 1283,
                            "end": 1362
                        },
                        {
                            "start": 1363,
                            "end": 1453
                        },
                        {
                            "start": 1454,
                            "end": 1634
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97607421875
                }
            ],
            "relevance_judgement": 0.97607421875,
            "relevance_judgment_input_expanded": "# Title: PAR: Prompt-Aware Token Reduction Method for Efficient Large Multimodal Models\n# Venue: \n# Authors: Yingen Liu, Fan Wu, Ruihui Li, Zhuo Tang, Kenli Li\n## Abstract\nMultimodal large language models (MLLMs) demonstrate strong performance across visual tasks, but their efficiency is hindered by significant computational and memory demands from processing long contexts in multimodal inputs. To address this, we introduce PAR (Prompt-Aware Token Reduction), a novel and plug-and-play approach that reduces visual tokens efficiently without compromising model performance. Unlike previous methods that rely heavily on attention mechanisms and overlooking cross-modal interactions , we uses a prompt-aware strategy to adpative identify and cluster essential visual tokens. PAR categorizes visual context redundancy into two types: external and internal. External redundancy is minimized through semantic retrieval, while internal redundancy is addressed using a token routing mechanism. This method substantially reduces computational load without requiring additional training or complex architectural modifications. \\textbf{Experimental results demonstrate that across various visual question answering tasks, PAR reduces FLOPs by 83\\% with a compression ratio of 89\\%, while retaining 97\\% of baseline accuracy.} The adaptive design of PAR achieves a 2x token reduction ratio compared to prior approaches, enabling a better balance between performance and efficiency.\n## Preliminaries\nMultimodal large language models (MLLMs) represent a significant advancement in artificial intelligence by integrating visual encoders with pre-trained large language models. This integration allows these models to jointly process and understand diverse modalities, such as images and text, enabling more comprehensive and contextually aware analyses. \n\nFor a given image I, the visual encoder f visual transforms it into a series of token representations X I : \n\nHere, X I denotes a set of visual tokens that capture essential visual features and semantic information from the image. This process allows image content to be encoded in a format compatible with the language model, facilitating the effective integration of visual and textual data. \n\nTo ensure that the visual tokens are compatible with the language model's processing framework, a visual projection layer is employed. This layer converts visual tokens X I into language embedding tokens y 1 , y 2 , . . . , y n . The transformation is performed such that these language embedding tokens share the same dimensional space d as the word embeddings used by the language model: \n\nwhere f projection denotes the projection function that aligns the dimensionality of the visual tokens with that of the language embeddings. This alignment ensures that the visual information encoded in y 1 , y 2 , . . . , y n can be processed seamlessly alongside the textual information by the language model. Consequently, the MLMM can effectively integrate and leverage both visual and textual representations, improving its ability to generate accurate and contextually relevant outputs.",
            "reference_string": "[273233771 | Liu et al. | 2024 | Citations: 0]"
        },
        {
            "title": "The Narrow Gate: Localized Image-Text Communication in Vision-Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 62,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.06646, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319817038",
                    "name": "Alessandro Serra"
                },
                {
                    "authorId": "2284691756",
                    "name": "Francesco Ortu"
                },
                {
                    "authorId": "2334479581",
                    "name": "Emanuele Panizon"
                },
                {
                    "authorId": "108193025",
                    "name": "L. Valeriani"
                },
                {
                    "authorId": "2308039508",
                    "name": "Lorenzo Basile"
                },
                {
                    "authorId": "5366502",
                    "name": "A. Ansuini"
                },
                {
                    "authorId": "104350259",
                    "name": "Diego Doimo"
                },
                {
                    "authorId": "2269294135",
                    "name": "Alberto Cazzaniga"
                }
            ],
            "abstract": "Recent advances in multimodal training have significantly improved the integration of image understanding and generation within a unified model. This study investigates how vision-language models (VLMs) handle image-understanding tasks, specifically focusing on how visual information is processed and transferred to the textual domain. We compare VLMs that generate both images and text with those that output only text, highlighting key differences in information flow. We find that in models with multimodal outputs, image and text embeddings are more separated within the residual stream. Additionally, models vary in how information is exchanged from visual to textual tokens. VLMs that only output text exhibit a distributed communication pattern, where information is exchanged through multiple image tokens. In contrast, models trained for image and text generation tend to rely on a single token that acts as a narrow gate for visual information. We demonstrate that ablating this single token significantly deteriorates performance on image understanding tasks. Furthermore, modifying this token enables effective steering of the image semantics, showing that targeted, local interventions can reliably control the model's global behavior.",
            "corpus_id": 274597781,
            "sentences": [
                {
                    "corpus_id": "274597781",
                    "title": "The Narrow Gate: Localized Image-Text Communication in Vision-Language Models",
                    "text": "The rise of foundation models [1] trained on vast amounts of text has transformed natural language processing (NLP), \n\nshowing that a single large language model (LLM) [2] can handle many different linguistic tasks [3][4][5]. The rich set of features encoded in LLM embeddings has been then used as an effective prior knowledge both for text-conditional image generation [6][7][8] and image understanding [9][10][11][12][13]. Recently, the availability of large open datasets [14,15] and improved techniques to align text and image embeddings [16] have also enabled the creation of multimodal models that can both understand and generate visual content within a single architecture [17][18][19][20]. This unification allows a richer understanding of the visual world, as generative tasks often require insight into the fundamental concepts and relationships within the data [21]. For example, a model that generates images from text descriptions must grasp the semantic content of those images to ensure they faithfully reflect the details and intent of the text [22,23]. As a result, research has rapidly progressed to integrate multiple modalities into a unified framework with increasingly deeper multimodal fusion. \n\nEarly approaches used cross-attention modules between modality-specific encoders [17]. Further advancements highlighted the importance of using a pre-trained LLM backbone combined with lightweight projection layers, often fine-tuning the LLM to refine the multimodal representations [19,24,25]. More recently, research has shown success in training multimodal-output vision-language models (VLMs) from scratch [20,26], achieving performance close to VLMs that only output text on visual understanding tasks.",
                    "score": 0.6016878827604403,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 119,
                            "end": 225
                        },
                        {
                            "start": 226,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 699
                        },
                        {
                            "start": 700,
                            "end": 879
                        },
                        {
                            "start": 880,
                            "end": 1071
                        },
                        {
                            "start": 1072,
                            "end": 1218
                        },
                        {
                            "start": 1221,
                            "end": 1307
                        },
                        {
                            "start": 1308,
                            "end": 1515
                        },
                        {
                            "start": 1516,
                            "end": 1728
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 30,
                            "end": 33,
                            "matchedPaperCorpusId": "237091588"
                        },
                        {
                            "start": 215,
                            "end": 218,
                            "matchedPaperCorpusId": "6401679"
                        },
                        {
                            "start": 374,
                            "end": 377,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 405,
                            "end": 408,
                            "matchedPaperCorpusId": "244954250"
                        },
                        {
                            "start": 408,
                            "end": 412,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 412,
                            "end": 416,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 543,
                            "end": 547,
                            "matchedPaperCorpusId": "263605855"
                        },
                        {
                            "start": 682,
                            "end": 686,
                            "matchedPaperCorpusId": "258822817"
                        },
                        {
                            "start": 690,
                            "end": 694,
                            "matchedPaperCorpusId": "267750101"
                        },
                        {
                            "start": 874,
                            "end": 878,
                            "matchedPaperCorpusId": "261975252"
                        },
                        {
                            "start": 1302,
                            "end": 1306,
                            "matchedPaperCorpusId": "258822817"
                        },
                        {
                            "start": 1504,
                            "end": 1508,
                            "matchedPaperCorpusId": "267750101"
                        },
                        {
                            "start": 1511,
                            "end": 1514,
                            "matchedPaperCorpusId": "261696650"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97509765625
                }
            ],
            "relevance_judgement": 0.97509765625,
            "relevance_judgment_input_expanded": "# Title: The Narrow Gate: Localized Image-Text Communication in Vision-Language Models\n# Venue: arXiv.org\n# Authors: Alessandro Serra, Francesco Ortu, Emanuele Panizon, L. Valeriani, Lorenzo Basile, A. Ansuini, Diego Doimo, Alberto Cazzaniga\n## Abstract\nRecent advances in multimodal training have significantly improved the integration of image understanding and generation within a unified model. This study investigates how vision-language models (VLMs) handle image-understanding tasks, specifically focusing on how visual information is processed and transferred to the textual domain. We compare VLMs that generate both images and text with those that output only text, highlighting key differences in information flow. We find that in models with multimodal outputs, image and text embeddings are more separated within the residual stream. Additionally, models vary in how information is exchanged from visual to textual tokens. VLMs that only output text exhibit a distributed communication pattern, where information is exchanged through multiple image tokens. In contrast, models trained for image and text generation tend to rely on a single token that acts as a narrow gate for visual information. We demonstrate that ablating this single token significantly deteriorates performance on image understanding tasks. Furthermore, modifying this token enables effective steering of the image semantics, showing that targeted, local interventions can reliably control the model's global behavior.\n## Introduction\nThe rise of foundation models [1] trained on vast amounts of text has transformed natural language processing (NLP), \n\nshowing that a single large language model (LLM) [2] can handle many different linguistic tasks [3][4][5]. The rich set of features encoded in LLM embeddings has been then used as an effective prior knowledge both for text-conditional image generation [6][7][8] and image understanding [9][10][11][12][13]. Recently, the availability of large open datasets [14,15] and improved techniques to align text and image embeddings [16] have also enabled the creation of multimodal models that can both understand and generate visual content within a single architecture [17][18][19][20]. This unification allows a richer understanding of the visual world, as generative tasks often require insight into the fundamental concepts and relationships within the data [21]. For example, a model that generates images from text descriptions must grasp the semantic content of those images to ensure they faithfully reflect the details and intent of the text [22,23]. As a result, research has rapidly progressed to integrate multiple modalities into a unified framework with increasingly deeper multimodal fusion. \n\nEarly approaches used cross-attention modules between modality-specific encoders [17]. Further advancements highlighted the importance of using a pre-trained LLM backbone combined with lightweight projection layers, often fine-tuning the LLM to refine the multimodal representations [19,24,25]. More recently, research has shown success in training multimodal-output vision-language models (VLMs) from scratch [20,26], achieving performance close to VLMs that only output text on visual understanding tasks.",
            "reference_string": "[274597781 | Serra et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Vision-Driven Prompt Optimization for Large Language Models in Multimodal Generative Tasks",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 31,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.02527, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2338834916",
                    "name": "Leo Franklin"
                },
                {
                    "authorId": "2324583118",
                    "name": "Apiradee Boonmee"
                },
                {
                    "authorId": "2324586634",
                    "name": "Kritsada Wongsuwan"
                }
            ],
            "abstract": "Vision generation remains a challenging frontier in artificial intelligence, requiring seamless integration of visual understanding and generative capabilities. In this paper, we propose a novel framework, Vision-Driven Prompt Optimization (VDPO), that leverages Large Language Models (LLMs) to dynamically generate textual prompts from visual inputs, guiding high-fidelity image synthesis. VDPO combines a visual embedding prompt tuner, a textual instruction generator, and a vision generation module to achieve state-of-the-art performance in diverse vision generation tasks. Extensive experiments on benchmarks such as COCO and Sketchy demonstrate that VDPO consistently outperforms existing methods, achieving significant improvements in FID, LPIPS, and BLEU/CIDEr scores. Additional analyses reveal the scalability, robustness, and generalization capabilities of VDPO, making it a versatile solution for in-domain and out-of-domain tasks. Human evaluations further validate the practical superiority of VDPO in generating visually appealing and semantically coherent outputs.",
            "corpus_id": 275337159,
            "sentences": [
                {
                    "corpus_id": "275337159",
                    "title": "Vision-Driven Prompt Optimization for Large Language Models in Multimodal Generative Tasks",
                    "text": "Large Vision-Language Models (LVLMs) represent a significant advancement in multimodal AI by integrating visual model and Language Models [18], [19] into a unified framework. These models are capable of handling a wide range of tasks, including visual understanding, text-image alignment, image captioning, and even multimodal generation [20]. Recent works have expanded their applications and enhanced their architectures to achieve better performance, efficiency, and scalability [21]- [23]. \n\nA primary focus in the development of LVLMs has been the design of architectures that effectively unify language and vision modalities. Recent models have proposed end-to-end frameworks that leverage shared embeddings for both text and images, enabling them to excel at tasks requiring fine-grained multimodal reasoning [24]. Additionally, techniques such as mixture of experts and relational reasoning mechanisms have been introduced to improve scalability and enhance the relational reasoning capabilities of LVLMs [25], [26]. \n\nAnother significant research direction involves improving the handling of long-contextual inputs and outputs, allowing LVLMs to perform better on complex tasks such as document understanding and scene analysis [27]. These advancements enable models to process large amounts of information while maintaining efficiency and coherence [28]. Furthermore, models have been tailored for specialized tasks, including bilingual optical character recognition and text-based grounding, achieving state-of-the-art performance in domain-specific applications [29], [30]. \n\nDespite these advancements, challenges remain in evaluating LVLMs effectively. Current evaluation methodologies often fail to capture the full spectrum of capabilities offered by these models. Recent works have emphasized the importance of developing more comprehensive benchmarks and metrics to evaluate multimodal reasoning, contextual comprehension, and generative quality [31], [32]. \n\nIn summary, the field of LVLMs is rapidly evolving, with continuous innovations driving their applicability to increasingly complex tasks. From architectural advancements to application-specific adaptations, LVLMs are poised to play a central role in the future of AI research.",
                    "score": 0.47111388728350256,
                    "section_title": "B. Large Vision-Language Models",
                    "char_start_offset": 6180,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 175,
                            "end": 343
                        },
                        {
                            "start": 344,
                            "end": 493
                        },
                        {
                            "start": 496,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 821
                        },
                        {
                            "start": 822,
                            "end": 1024
                        },
                        {
                            "start": 1027,
                            "end": 1242
                        },
                        {
                            "start": 1243,
                            "end": 1364
                        },
                        {
                            "start": 1365,
                            "end": 1585
                        },
                        {
                            "start": 1588,
                            "end": 1666
                        },
                        {
                            "start": 1667,
                            "end": 1780
                        },
                        {
                            "start": 1781,
                            "end": 1975
                        },
                        {
                            "start": 1978,
                            "end": 2116
                        },
                        {
                            "start": 2117,
                            "end": 2255
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 144,
                            "end": 148,
                            "matchedPaperCorpusId": "247244544"
                        },
                        {
                            "start": 338,
                            "end": 342,
                            "matchedPaperCorpusId": "256358384"
                        },
                        {
                            "start": 488,
                            "end": 492,
                            "matchedPaperCorpusId": "249712198"
                        },
                        {
                            "start": 816,
                            "end": 820,
                            "matchedPaperCorpusId": "270923961"
                        },
                        {
                            "start": 1013,
                            "end": 1017,
                            "matchedPaperCorpusId": "267311517"
                        },
                        {
                            "start": 1359,
                            "end": 1363,
                            "matchedPaperCorpusId": "270391793"
                        },
                        {
                            "start": 1574,
                            "end": 1578,
                            "matchedPaperCorpusId": "273186005"
                        },
                        {
                            "start": 1964,
                            "end": 1968,
                            "matchedPaperCorpusId": "268793433"
                        },
                        {
                            "start": 1970,
                            "end": 1974,
                            "matchedPaperCorpusId": "273963138"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97509765625
                }
            ],
            "relevance_judgement": 0.97509765625,
            "relevance_judgment_input_expanded": "# Title: Vision-Driven Prompt Optimization for Large Language Models in Multimodal Generative Tasks\n# Venue: arXiv.org\n# Authors: Leo Franklin, Apiradee Boonmee, Kritsada Wongsuwan\n## Abstract\nVision generation remains a challenging frontier in artificial intelligence, requiring seamless integration of visual understanding and generative capabilities. In this paper, we propose a novel framework, Vision-Driven Prompt Optimization (VDPO), that leverages Large Language Models (LLMs) to dynamically generate textual prompts from visual inputs, guiding high-fidelity image synthesis. VDPO combines a visual embedding prompt tuner, a textual instruction generator, and a vision generation module to achieve state-of-the-art performance in diverse vision generation tasks. Extensive experiments on benchmarks such as COCO and Sketchy demonstrate that VDPO consistently outperforms existing methods, achieving significant improvements in FID, LPIPS, and BLEU/CIDEr scores. Additional analyses reveal the scalability, robustness, and generalization capabilities of VDPO, making it a versatile solution for in-domain and out-of-domain tasks. Human evaluations further validate the practical superiority of VDPO in generating visually appealing and semantically coherent outputs.\n## B. Large Vision-Language Models\nLarge Vision-Language Models (LVLMs) represent a significant advancement in multimodal AI by integrating visual model and Language Models [18], [19] into a unified framework. These models are capable of handling a wide range of tasks, including visual understanding, text-image alignment, image captioning, and even multimodal generation [20]. Recent works have expanded their applications and enhanced their architectures to achieve better performance, efficiency, and scalability [21]- [23]. \n\nA primary focus in the development of LVLMs has been the design of architectures that effectively unify language and vision modalities. Recent models have proposed end-to-end frameworks that leverage shared embeddings for both text and images, enabling them to excel at tasks requiring fine-grained multimodal reasoning [24]. Additionally, techniques such as mixture of experts and relational reasoning mechanisms have been introduced to improve scalability and enhance the relational reasoning capabilities of LVLMs [25], [26]. \n\nAnother significant research direction involves improving the handling of long-contextual inputs and outputs, allowing LVLMs to perform better on complex tasks such as document understanding and scene analysis [27]. These advancements enable models to process large amounts of information while maintaining efficiency and coherence [28]. Furthermore, models have been tailored for specialized tasks, including bilingual optical character recognition and text-based grounding, achieving state-of-the-art performance in domain-specific applications [29], [30]. \n\nDespite these advancements, challenges remain in evaluating LVLMs effectively. Current evaluation methodologies often fail to capture the full spectrum of capabilities offered by these models. Recent works have emphasized the importance of developing more comprehensive benchmarks and metrics to evaluate multimodal reasoning, contextual comprehension, and generative quality [31], [32]. \n\nIn summary, the field of LVLMs is rapidly evolving, with continuous innovations driving their applicability to increasingly complex tasks. From architectural advancements to application-specific adaptations, LVLMs are poised to play a central role in the future of AI research.",
            "reference_string": "[275337159 | Franklin et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Recognition-Synergistic Scene Text Editing",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.08387, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313679002",
                    "name": "Zhengyao Fang"
                },
                {
                    "authorId": "10344582",
                    "name": "Pengyuan Lyu"
                },
                {
                    "authorId": "2313920076",
                    "name": "Jingjing Wu"
                },
                {
                    "authorId": "2248958848",
                    "name": "Chengquan Zhang"
                },
                {
                    "authorId": "2349736964",
                    "name": "Jun Yu"
                },
                {
                    "authorId": "2261251059",
                    "name": "Guan-Lun Lu"
                },
                {
                    "authorId": "1678473",
                    "name": "Wenjie Pei"
                }
            ],
            "abstract": "Scene text editing aims to modify text content within scene images while maintaining style consistency. Traditional methods achieve this by explicitly disentangling style and content from the source image and then fusing the style with the target content, while ensuring content consistency using a pre-trained recognition model. Despite notable progress, these methods suffer from complex pipelines, leading to suboptimal performance in complex scenarios. In this work, we introduce Recognition-Synergistic Scene Text Editing (RS-STE), a novel approach that fully exploits the intrinsic synergy of text recognition for editing. Our model seamlessly integrates text recognition with text editing within a unified framework, and leverages the recognition model's ability to implicitly disentangle style and content while ensuring content consistency. Specifically, our approach employs a multi-modal parallel decoder based on transformer architecture, which predicts both text content and stylized images in parallel. Additionally, our cyclic self-supervised fine-tuning strategy enables effective training on unpaired real-world data without ground truth, enhancing style and content consistency through a twice-cyclic generation process. Built on a relatively simple architecture, RS-STE achieves state-of-the-art performance on both synthetic and real-world benchmarks, and further demonstrates the effectiveness of leveraging the generated hard cases to boost the performance of downstream recognition tasks. Code is available at https://github.com/ZhengyaoFang/RS-STE.",
            "corpus_id": 276929408,
            "sentences": [
                {
                    "corpus_id": "276929408",
                    "title": "Recognition-Synergistic Scene Text Editing",
                    "text": "In response to the notable progress of large language models in natural language processing [1,2,6], the field of multimodal large language models (MLLM) has made significant strides in recent years. MLLMs leverage both natural language and visual inputs, allowing these models to understand and manipulate visual data guided by textual descriptions. This dual-modality capability builds upon foundational image generation models, such as GANs [12] and diffusion models [15], but advances them by incorporating language as a critical component in model design. Recent works [1,11,26,38,40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs [3,13,29] offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation. Inspired by these methods, our approach integrates the multi-modal language model RS-STE, which is specialized in scene text editing.",
                    "score": 0.5333905798872862,
                    "section_title": "MLLM for Image Generation and Editing",
                    "char_start_offset": 6474,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 199
                        },
                        {
                            "start": 200,
                            "end": 350
                        },
                        {
                            "start": 351,
                            "end": 560
                        },
                        {
                            "start": 561,
                            "end": 757
                        },
                        {
                            "start": 758,
                            "end": 941
                        },
                        {
                            "start": 942,
                            "end": 1202
                        },
                        {
                            "start": 1203,
                            "end": 1336
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 97,
                            "end": 99,
                            "matchedPaperCorpusId": "247951931"
                        },
                        {
                            "start": 470,
                            "end": 474,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 583,
                            "end": 586,
                            "matchedPaperCorpusId": "266374640"
                        },
                        {
                            "start": 966,
                            "end": 969,
                            "matchedPaperCorpusId": "253581213"
                        },
                        {
                            "start": 972,
                            "end": 975,
                            "matchedPaperCorpusId": "253581838"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9736328125
                }
            ],
            "relevance_judgement": 0.9736328125,
            "relevance_judgment_input_expanded": "# Title: Recognition-Synergistic Scene Text Editing\n# Venue: arXiv.org\n# Authors: Zhengyao Fang, Pengyuan Lyu, Jingjing Wu, Chengquan Zhang, Jun Yu, Guan-Lun Lu, Wenjie Pei\n## Abstract\nScene text editing aims to modify text content within scene images while maintaining style consistency. Traditional methods achieve this by explicitly disentangling style and content from the source image and then fusing the style with the target content, while ensuring content consistency using a pre-trained recognition model. Despite notable progress, these methods suffer from complex pipelines, leading to suboptimal performance in complex scenarios. In this work, we introduce Recognition-Synergistic Scene Text Editing (RS-STE), a novel approach that fully exploits the intrinsic synergy of text recognition for editing. Our model seamlessly integrates text recognition with text editing within a unified framework, and leverages the recognition model's ability to implicitly disentangle style and content while ensuring content consistency. Specifically, our approach employs a multi-modal parallel decoder based on transformer architecture, which predicts both text content and stylized images in parallel. Additionally, our cyclic self-supervised fine-tuning strategy enables effective training on unpaired real-world data without ground truth, enhancing style and content consistency through a twice-cyclic generation process. Built on a relatively simple architecture, RS-STE achieves state-of-the-art performance on both synthetic and real-world benchmarks, and further demonstrates the effectiveness of leveraging the generated hard cases to boost the performance of downstream recognition tasks. Code is available at https://github.com/ZhengyaoFang/RS-STE.\n## MLLM for Image Generation and Editing\nIn response to the notable progress of large language models in natural language processing [1,2,6], the field of multimodal large language models (MLLM) has made significant strides in recent years. MLLMs leverage both natural language and visual inputs, allowing these models to understand and manipulate visual data guided by textual descriptions. This dual-modality capability builds upon foundational image generation models, such as GANs [12] and diffusion models [15], but advances them by incorporating language as a critical component in model design. Recent works [1,11,26,38,40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs [3,13,29] offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation. Inspired by these methods, our approach integrates the multi-modal language model RS-STE, which is specialized in scene text editing.",
            "reference_string": "[276929408 | Fang et al. | 2025 | Citations: 1]"
        },
        {
            "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 28,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.15188, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2254168373",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2290545053",
                    "name": "Xiaochuang Han"
                },
                {
                    "authorId": "2333891614",
                    "name": "Chunting Zhou"
                },
                {
                    "authorId": "151253861",
                    "name": "Weixin Liang"
                },
                {
                    "authorId": "2255374957",
                    "name": "Xi Victoria Lin"
                },
                {
                    "authorId": "2256736753",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "2296724476",
                    "name": "Lili Yu"
                }
            ],
            "abstract": "We present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.",
            "corpus_id": 274860024,
            "sentences": [
                {
                    "corpus_id": "274860024",
                    "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
                    "text": "We present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.",
                    "score": 0.47621811913564116,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9736328125
                },
                {
                    "corpus_id": "274860024",
                    "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
                    "text": "Over the past few years, we have seen significant progress in multimodal generative models capable of understanding and generating interleaved text and images in arbitrary sequences (Dong et al., 2023;Koh et al., 2024;Lin et al., 2024b). Models like Transfusion (Zhou et al., 2024), Chameleon (Team, 2024b), and Unified-IO (Lu et al., 2022(Lu et al., , 2024) ) demonstrate the potential of unified architectures that seamlessly handle both image and text modalities. However, these models typically train from scratch, demanding significant computational resources to achieve proficiency across all modalities. The computational cost of mastering even a single modality is substantial-training a state-of-the-art text-only large language models (LLMs) like Llama-3 (Dubey et al., 2024) requires training over 15 trillion tokens. \n\nGiven these computational demands, we investigate an alternative paradigm that reuses and adapts existing pretrained LLMs (Ge et al., 2023;Sun et al., 2023;Wu et al., 2024b). We address a fundamental research question: How to preserve the text-only performance of pretrained LLMs while equipping them with visual understanding and generation abilities? Our experiments show that naive finetuning of pretrained text-only LLMs on multimodal data leads to significant degradation of their language processing capabilities. \n\nTo address this challenge, we introduce LMFusion, a framework that enhances a pretrained text-only LLM, Llama-3 (Dubey et al., 2024) with multimodal capabilities by building upon the recipe of Transfusion (Zhou et al., 2024). Drawing from recent and parallel work on modality separation (Shen et al., 2023;Chen et al., 2023;Liang et al., 2024;Liu et al., 2024a), LMFusion integrates the original Llama modules pretrained for language processing while introducing additional dedicated transformer modules for visual understanding and generation tasks.",
                    "score": 0.5102285217352757,
                    "section_title": "Introduction",
                    "char_start_offset": 2915,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 466
                        },
                        {
                            "start": 467,
                            "end": 610
                        },
                        {
                            "start": 611,
                            "end": 828
                        },
                        {
                            "start": 831,
                            "end": 1005
                        },
                        {
                            "start": 1006,
                            "end": 1183
                        },
                        {
                            "start": 1184,
                            "end": 1350
                        },
                        {
                            "start": 1353,
                            "end": 1578
                        },
                        {
                            "start": 1579,
                            "end": 1903
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 201,
                            "end": 218,
                            "matchedPaperCorpusId": "258959284"
                        },
                        {
                            "start": 323,
                            "end": 339,
                            "matchedPaperCorpusId": "249848272"
                        },
                        {
                            "start": 339,
                            "end": 360,
                            "matchedPaperCorpusId": "266573555"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95947265625
                }
            ],
            "relevance_judgement": 0.9736328125,
            "relevance_judgment_input_expanded": "# Title: LMFusion: Adapting Pretrained Language Models for Multimodal Generation\n# Venue: arXiv.org\n# Authors: Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke S. Zettlemoyer, Lili Yu\n## Abstract\nWe present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.\n## Introduction\nOver the past few years, we have seen significant progress in multimodal generative models capable of understanding and generating interleaved text and images in arbitrary sequences (Dong et al., 2023;Koh et al., 2024;Lin et al., 2024b). Models like Transfusion (Zhou et al., 2024), Chameleon (Team, 2024b), and Unified-IO (Lu et al., 2022(Lu et al., , 2024) ) demonstrate the potential of unified architectures that seamlessly handle both image and text modalities. However, these models typically train from scratch, demanding significant computational resources to achieve proficiency across all modalities. The computational cost of mastering even a single modality is substantial-training a state-of-the-art text-only large language models (LLMs) like Llama-3 (Dubey et al., 2024) requires training over 15 trillion tokens. \n\nGiven these computational demands, we investigate an alternative paradigm that reuses and adapts existing pretrained LLMs (Ge et al., 2023;Sun et al., 2023;Wu et al., 2024b). We address a fundamental research question: How to preserve the text-only performance of pretrained LLMs while equipping them with visual understanding and generation abilities? Our experiments show that naive finetuning of pretrained text-only LLMs on multimodal data leads to significant degradation of their language processing capabilities. \n\nTo address this challenge, we introduce LMFusion, a framework that enhances a pretrained text-only LLM, Llama-3 (Dubey et al., 2024) with multimodal capabilities by building upon the recipe of Transfusion (Zhou et al., 2024). Drawing from recent and parallel work on modality separation (Shen et al., 2023;Chen et al., 2023;Liang et al., 2024;Liu et al., 2024a), LMFusion integrates the original Llama modules pretrained for language processing while introducing additional dedicated transformer modules for visual understanding and generation tasks.",
            "reference_string": "[274860024 | Shi et al. | 2024 | Citations: 28]"
        },
        {
            "title": "Unified Modeling Language Code Generation from Diagram Images Using Multimodal Large Language Models",
            "venue": "Machine Learning with Applications",
            "year": 2025,
            "reference_count": 35,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.12293, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2309151434",
                    "name": "Averi Bates"
                },
                {
                    "authorId": "2363651648",
                    "name": "Ryan Vavricka"
                },
                {
                    "authorId": "2350512740",
                    "name": "Shane Carleton"
                },
                {
                    "authorId": "2350505892",
                    "name": "Ruosi Shao"
                },
                {
                    "authorId": "2084643963",
                    "name": "Chongle Pan"
                }
            ],
            "abstract": null,
            "corpus_id": 277065986,
            "sentences": [
                {
                    "corpus_id": "277065986",
                    "title": "Unified Modeling Language Code Generation from Diagram Images Using Multimodal Large Language Models",
                    "text": "Large Language Models (LLMs) have demonstrated impressive capabilities in understanding and generating human-like text. Advanced transformer-based architectures such as BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and T5 (Raffel et al., 2020) capture contextual relationships within text, enabling tasks like summarization, question answering, and code generation. However, LLMs process only textual data, limiting their utility in domains requiring multimodal understanding, such as image or video analysis. Researchers developed MM-LLMs that incorporate visual, auditory, and textual data into unified frameworks to address these limitations. Models like CLIP (Radford et al., 2021) align visual embeddings with textual embeddings, enabling cross-modal tasks such as visual question answering (VQA) and zero-shot image classification. For example, CLIP (Contrastive Language-Image Pretraining) learns shared visual and textual semantics from large-scale datasets of image-text pairs, effectively bridging linguistic context and visual perception. MM-LLMs leverage these multimodal capabilities to tackle tasks that traditional LLMs cannot address, such as interpreting UML diagrams and generating corresponding executable code. MM-LLMs align diagram structures with text-based outputs by combining visual encoders with language models, enhancing accuracy and flexibility for applications like UML-to-code automation.",
                    "score": 0.4945990740507372,
                    "section_title": "Addressing the Problem",
                    "char_start_offset": 4189,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 375
                        },
                        {
                            "start": 376,
                            "end": 519
                        },
                        {
                            "start": 520,
                            "end": 655
                        },
                        {
                            "start": 656,
                            "end": 847
                        },
                        {
                            "start": 848,
                            "end": 1059
                        },
                        {
                            "start": 1060,
                            "end": 1240
                        },
                        {
                            "start": 1241,
                            "end": 1429
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 232,
                            "end": 253,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97314453125
                }
            ],
            "relevance_judgement": 0.97314453125,
            "relevance_judgment_input_expanded": "# Title: Unified Modeling Language Code Generation from Diagram Images Using Multimodal Large Language Models\n# Venue: Machine Learning with Applications\n# Authors: Averi Bates, Ryan Vavricka, Shane Carleton, Ruosi Shao, Chongle Pan\n## Abstract\nNone\n## Addressing the Problem\nLarge Language Models (LLMs) have demonstrated impressive capabilities in understanding and generating human-like text. Advanced transformer-based architectures such as BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and T5 (Raffel et al., 2020) capture contextual relationships within text, enabling tasks like summarization, question answering, and code generation. However, LLMs process only textual data, limiting their utility in domains requiring multimodal understanding, such as image or video analysis. Researchers developed MM-LLMs that incorporate visual, auditory, and textual data into unified frameworks to address these limitations. Models like CLIP (Radford et al., 2021) align visual embeddings with textual embeddings, enabling cross-modal tasks such as visual question answering (VQA) and zero-shot image classification. For example, CLIP (Contrastive Language-Image Pretraining) learns shared visual and textual semantics from large-scale datasets of image-text pairs, effectively bridging linguistic context and visual perception. MM-LLMs leverage these multimodal capabilities to tackle tasks that traditional LLMs cannot address, such as interpreting UML diagrams and generating corresponding executable code. MM-LLMs align diagram structures with text-based outputs by combining visual encoders with language models, enhancing accuracy and flexibility for applications like UML-to-code automation.",
            "reference_string": "[277065986 | Bates et al. | 2025 | Citations: 0]"
        },
        {
            "title": "ARMOR: Empowering Multimodal Understanding Model with Interleaved Multimodal Generation Capability",
            "venue": "",
            "year": 2025,
            "reference_count": 44,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.06542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2349938581",
                    "name": "Jianwen Sun"
                },
                {
                    "authorId": "2349426352",
                    "name": "Yukang Feng"
                },
                {
                    "authorId": "2294251099",
                    "name": "Chuanhao Li"
                },
                {
                    "authorId": "2350533645",
                    "name": "Fanrui Zhang"
                },
                {
                    "authorId": "2349380936",
                    "name": "Zizhen Li"
                },
                {
                    "authorId": "2349374151",
                    "name": "Jiaxin Ai"
                },
                {
                    "authorId": "2349391740",
                    "name": "Sizhuo Zhou"
                },
                {
                    "authorId": "2349402231",
                    "name": "Yu Dai"
                },
                {
                    "authorId": "2350252584",
                    "name": "Shenglin Zhang"
                },
                {
                    "authorId": "2349384231",
                    "name": "Kaipeng Zhang"
                }
            ],
            "abstract": "Unified multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, and often struggle to generate interleaved text-image. We present ARMOR, a resource-efficient and pure autoregressive framework that achieves both understanding and generation by fine-tuning existing multimodal large language models (MLLMs). Specifically, ARMOR extends existing MLLMs from three perspectives: (1) For model architecture, an asymmetric encoder-decoder architecture with a forward-switching mechanism is introduced to unify embedding space integrating textual and visual modalities for enabling natural text-image interleaved generation with minimal computational overhead. (2) For training data, a meticulously curated, high-quality interleaved dataset is collected for fine-tuning MLLMs. (3) For the training algorithm, we propose a ``what or how to generate'' algorithm to empower existing MLLMs with multimodal generation capabilities while preserving their multimodal understanding capabilities, through three progressive training stages based on the collected dataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs with promising image generation capabilities, using limited training resources. Our code will be released soon at https://github.com/finyorko/armor.",
            "corpus_id": 276902413,
            "sentences": [
                {
                    "corpus_id": "276902413",
                    "title": "ARMOR: Empowering Multimodal Understanding Model with Interleaved Multimodal Generation Capability",
                    "text": "Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, and often struggle to generate interleaved text-image. We present ARMOR, a resource-efficient and pure autoregressive framework that achieves both understanding and generation by fine-tuning existing multimodal large language models (MLLMs). Specifically, ARMOR extends existing MLLMs from three perspectives: (1) For model architecture, an asymmetric encoder-decoder architecture with a forward-switching mechanism is introduced to unify embedding space integrating textual and visual modalities for enabling natural text-image interleaved generation with minimal computational overhead. (2) For training data, a meticulously curated, high-quality interleaved dataset is collected for fine-tuning MLLMs. (3) For the training algorithm, we propose a ``what or how to generate\"algorithm to empower existing MLLMs with multimodal generation capabilities while preserving their multimodal understanding capabilities, through three progressive training stages based on the collected dataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs with promising image generation capabilities, using limited training resources. Our code will be released soon at https://armor.github.io.",
                    "score": 0.556345558912799,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97216796875
                },
                {
                    "corpus_id": "276902413",
                    "title": "ARMOR: Empowering Multimodal Understanding Model with Interleaved Multimodal Generation Capability",
                    "text": "Unified understanding and generation is a crucial direction in the development of vision-and-language models, requiring a model to simultaneously handle understanding tasks (e.g., visual question answering) and generation tasks (e.g., text-to-image generation). Existing Unified Models (UniMs) for understanding and generation, such as Showo [41] and Janus-pro [7], are designed to simultaneously learn both multimodal understanding and generation capabilities. Despite their impressive performance, the training of such models demands substantial computational resources, which significantly hinders their scalability and ability to accommodate personalized modifications. Furthermore, most UniMs struggle to generate interleaved textimage. To this end, we present a resource-efficient autoregressive framework named ARMOR, which fine-tunes existing multimodal large language model (MLLMs) to upgrade them to UniMs from three perspectives, including model architecture, training data and training algorithm. Sample generation from ARMOR is shown in Figure 1. \n\nWe first introduce an asymmetric encoder-decoder architecture into existing MLLMs to enable them output natural interleaved text-image. Specifically, ARMOR fully retains the encoder and decoder in MLLMs while incorporating an asymmetric image decoder, as shown in Figure 2. In doing so, the strong semantic perception and understanding capabilities of the MLLMs are almost preserved, while the asymmetric image decoder enables image generation with minimal computational overhead. To alleviate the long-tail distribution problem across different modalities in the answer space, we propose a forward-switching mechanism to dynamically control which modality's answer space is used for prediction based on the model input. \n\nThen we collect a meticulously curated, high-quality interleaved dataset, and propose a \"What or How to Generate\"  To validate the effectiveness of the proposed ARMOR framework, we incorporate InternVL2.5 [8] into ARMOR (named \"Ours\" here), and conduct extensive experiments on 9 benchmarks to evaluate multimodal understanding and generation capabilities.",
                    "score": 0.4724836193161517,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 261
                        },
                        {
                            "start": 262,
                            "end": 461
                        },
                        {
                            "start": 462,
                            "end": 673
                        },
                        {
                            "start": 674,
                            "end": 741
                        },
                        {
                            "start": 742,
                            "end": 1008
                        },
                        {
                            "start": 1009,
                            "end": 1059
                        },
                        {
                            "start": 1062,
                            "end": 1197
                        },
                        {
                            "start": 1198,
                            "end": 1542
                        },
                        {
                            "start": 1543,
                            "end": 1782
                        },
                        {
                            "start": 1785,
                            "end": 2141
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96337890625
                }
            ],
            "relevance_judgement": 0.97216796875,
            "relevance_judgment_input_expanded": "# Title: ARMOR: Empowering Multimodal Understanding Model with Interleaved Multimodal Generation Capability\n# Venue: \n# Authors: Jianwen Sun, Yukang Feng, Chuanhao Li, Fanrui Zhang, Zizhen Li, Jiaxin Ai, Sizhuo Zhou, Yu Dai, Shenglin Zhang, Kaipeng Zhang\n## Abstract\nUnified multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, and often struggle to generate interleaved text-image. We present ARMOR, a resource-efficient and pure autoregressive framework that achieves both understanding and generation by fine-tuning existing multimodal large language models (MLLMs). Specifically, ARMOR extends existing MLLMs from three perspectives: (1) For model architecture, an asymmetric encoder-decoder architecture with a forward-switching mechanism is introduced to unify embedding space integrating textual and visual modalities for enabling natural text-image interleaved generation with minimal computational overhead. (2) For training data, a meticulously curated, high-quality interleaved dataset is collected for fine-tuning MLLMs. (3) For the training algorithm, we propose a ``what or how to generate'' algorithm to empower existing MLLMs with multimodal generation capabilities while preserving their multimodal understanding capabilities, through three progressive training stages based on the collected dataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs with promising image generation capabilities, using limited training resources. Our code will be released soon at https://github.com/finyorko/armor.\n## Introduction\nUnified understanding and generation is a crucial direction in the development of vision-and-language models, requiring a model to simultaneously handle understanding tasks (e.g., visual question answering) and generation tasks (e.g., text-to-image generation). Existing Unified Models (UniMs) for understanding and generation, such as Showo [41] and Janus-pro [7], are designed to simultaneously learn both multimodal understanding and generation capabilities. Despite their impressive performance, the training of such models demands substantial computational resources, which significantly hinders their scalability and ability to accommodate personalized modifications. Furthermore, most UniMs struggle to generate interleaved textimage. To this end, we present a resource-efficient autoregressive framework named ARMOR, which fine-tunes existing multimodal large language model (MLLMs) to upgrade them to UniMs from three perspectives, including model architecture, training data and training algorithm. Sample generation from ARMOR is shown in Figure 1. \n\nWe first introduce an asymmetric encoder-decoder architecture into existing MLLMs to enable them output natural interleaved text-image. Specifically, ARMOR fully retains the encoder and decoder in MLLMs while incorporating an asymmetric image decoder, as shown in Figure 2. In doing so, the strong semantic perception and understanding capabilities of the MLLMs are almost preserved, while the asymmetric image decoder enables image generation with minimal computational overhead. To alleviate the long-tail distribution problem across different modalities in the answer space, we propose a forward-switching mechanism to dynamically control which modality's answer space is used for prediction based on the model input. \n\nThen we collect a meticulously curated, high-quality interleaved dataset, and propose a \"What or How to Generate\"  To validate the effectiveness of the proposed ARMOR framework, we incorporate InternVL2.5 [8] into ARMOR (named \"Ours\" here), and conduct extensive experiments on 9 benchmarks to evaluate multimodal understanding and generation capabilities.",
            "reference_string": "[276902413 | Sun et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Holistic Evaluation for Interleaved Text-and-Image Generation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 54,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.14643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2123130842",
                    "name": "Minqian Liu"
                },
                {
                    "authorId": "2136442661",
                    "name": "Zhiyang Xu"
                },
                {
                    "authorId": "2254334449",
                    "name": "Zihao Lin"
                },
                {
                    "authorId": "2243992517",
                    "name": "Trevor Ashby"
                },
                {
                    "authorId": "2218905547",
                    "name": "Joy Rimchala"
                },
                {
                    "authorId": "2307975528",
                    "name": "Jiaxin Zhang"
                },
                {
                    "authorId": "2238885968",
                    "name": "Lifu Huang"
                }
            ],
            "abstract": "Interleaved text-and-image generation has been an intriguing research direction, where the models are required to generate both images and text pieces in an arbitrary order. Despite the emerging advancements in interleaved generation, the progress in its evaluation still significantly lags behind. Existing evaluation benchmarks do not support arbitrarily interleaved images and text for both inputs and outputs, and they only cover a limited number of domains and use cases. Also, current works predominantly use similarity-based metrics which fall short in assessing the quality in open-ended scenarios. To this end, we introduce InterleavedBench, the first benchmark carefully curated for the evaluation of interleaved text-and-image generation. InterleavedBench features a rich array of tasks to cover diverse real-world use cases. In addition, we present InterleavedEval, a strong reference-free metric powered by GPT-4o to deliver accurate and explainable evaluation. We carefully define five essential evaluation aspects for InterleavedEval, including text quality, perceptual quality, image coherence, text-image coherence, and helpfulness, to ensure a comprehensive and fine-grained assessment. Through extensive experiments and rigorous human evaluation, we show that our benchmark and metric can effectively evaluate the existing models with a strong correlation with human judgments surpassing previous reference-based metrics. We also provide substantial findings and insights to foster future research in interleaved generation and its evaluation.",
            "corpus_id": 270688372,
            "sentences": [
                {
                    "corpus_id": "270688372",
                    "title": "Holistic Evaluation for Interleaved Text-and-Image Generation",
                    "text": "Large Multimodal Models for Interleaved Generation The advent of large multimodal models (LMMs) (Koh et al., 2023;Sun et al., 2023a) has significantly advanced the field of interleaved textand-image generation. Previous models such as DALL-E (Ramesh et al., 2021) and Stable Diffusion (Podell et al., 2023) have demonstrated impressive capabilities in generating high-quality images conditioned on textual descriptions. However, previous focus has predominantly been on unidirectional generation tasks, either from text to image or image to text, without considering the interleaved generation scenarios where text and images are seamlessly integrated within the same output. Recent works have begun to address this gap, with the LMMs extended with diffusion models, exploring the generation of mixed text and image outputs (Koh et al., 2023;Sun et al., 2023b;Dong et al., 2024;Tian et al., 2024;Zhan et al., 2024;Chen et al., 2023). These models leverage advanced architectures and training techniques to enhance their ability to produce coherent and contextually relevant interleaved content. Despite these advancements, the evaluation of such models remains an underexplored area, with most evaluations still relying on separate assessments of text and image quality or simplistic reference-based metrics. Our proposed INTERLEAVEDBENCH benchmark aims to bridge this gap by providing a holistic evaluation framework tailored specifically for interleaved text-and-image generation.",
                    "score": 0.4842476342124168,
                    "section_title": "Related Work",
                    "char_start_offset": 5358,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 210
                        },
                        {
                            "start": 211,
                            "end": 419
                        },
                        {
                            "start": 420,
                            "end": 675
                        },
                        {
                            "start": 676,
                            "end": 933
                        },
                        {
                            "start": 934,
                            "end": 1094
                        },
                        {
                            "start": 1095,
                            "end": 1308
                        },
                        {
                            "start": 1309,
                            "end": 1482
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 96,
                            "end": 114,
                            "matchedPaperCorpusId": "258959284"
                        },
                        {
                            "start": 242,
                            "end": 263,
                            "matchedPaperCorpusId": "232035663"
                        },
                        {
                            "start": 824,
                            "end": 842,
                            "matchedPaperCorpusId": "258959284"
                        },
                        {
                            "start": 860,
                            "end": 878,
                            "matchedPaperCorpusId": "261975252"
                        },
                        {
                            "start": 896,
                            "end": 914,
                            "matchedPaperCorpusId": "267750101"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97021484375
                }
            ],
            "relevance_judgement": 0.97021484375,
            "relevance_judgment_input_expanded": "# Title: Holistic Evaluation for Interleaved Text-and-Image Generation\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Minqian Liu, Zhiyang Xu, Zihao Lin, Trevor Ashby, Joy Rimchala, Jiaxin Zhang, Lifu Huang\n## Abstract\nInterleaved text-and-image generation has been an intriguing research direction, where the models are required to generate both images and text pieces in an arbitrary order. Despite the emerging advancements in interleaved generation, the progress in its evaluation still significantly lags behind. Existing evaluation benchmarks do not support arbitrarily interleaved images and text for both inputs and outputs, and they only cover a limited number of domains and use cases. Also, current works predominantly use similarity-based metrics which fall short in assessing the quality in open-ended scenarios. To this end, we introduce InterleavedBench, the first benchmark carefully curated for the evaluation of interleaved text-and-image generation. InterleavedBench features a rich array of tasks to cover diverse real-world use cases. In addition, we present InterleavedEval, a strong reference-free metric powered by GPT-4o to deliver accurate and explainable evaluation. We carefully define five essential evaluation aspects for InterleavedEval, including text quality, perceptual quality, image coherence, text-image coherence, and helpfulness, to ensure a comprehensive and fine-grained assessment. Through extensive experiments and rigorous human evaluation, we show that our benchmark and metric can effectively evaluate the existing models with a strong correlation with human judgments surpassing previous reference-based metrics. We also provide substantial findings and insights to foster future research in interleaved generation and its evaluation.\n## Related Work\nLarge Multimodal Models for Interleaved Generation The advent of large multimodal models (LMMs) (Koh et al., 2023;Sun et al., 2023a) has significantly advanced the field of interleaved textand-image generation. Previous models such as DALL-E (Ramesh et al., 2021) and Stable Diffusion (Podell et al., 2023) have demonstrated impressive capabilities in generating high-quality images conditioned on textual descriptions. However, previous focus has predominantly been on unidirectional generation tasks, either from text to image or image to text, without considering the interleaved generation scenarios where text and images are seamlessly integrated within the same output. Recent works have begun to address this gap, with the LMMs extended with diffusion models, exploring the generation of mixed text and image outputs (Koh et al., 2023;Sun et al., 2023b;Dong et al., 2024;Tian et al., 2024;Zhan et al., 2024;Chen et al., 2023). These models leverage advanced architectures and training techniques to enhance their ability to produce coherent and contextually relevant interleaved content. Despite these advancements, the evaluation of such models remains an underexplored area, with most evaluations still relying on separate assessments of text and image quality or simplistic reference-based metrics. Our proposed INTERLEAVEDBENCH benchmark aims to bridge this gap by providing a holistic evaluation framework tailored specifically for interleaved text-and-image generation.",
            "reference_string": "[270688372 | Liu et al. | 2024 | Citations: 11]"
        },
        {
            "title": "Exploring Social Media Image Categorization Using Large Models with Different Adaptation Methods: A Case Study on Cultural Nature's Contributions to People",
            "venue": "",
            "year": 2024,
            "reference_count": 34,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.00275, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "35729419",
                    "name": "Rohaifa Khaldi"
                },
                {
                    "authorId": "2273276043",
                    "name": "Domingo Alcaraz-Segura"
                },
                {
                    "authorId": "2323737781",
                    "name": "Ignacio S'anchez-Herrera"
                },
                {
                    "authorId": "2271941340",
                    "name": "Javier Mart\u00ednez-L\u00f3pez"
                },
                {
                    "authorId": "2323747675",
                    "name": "Carlos Javier Navarro"
                },
                {
                    "authorId": "3030006",
                    "name": "S. Tabik"
                }
            ],
            "abstract": "Social media images provide valuable insights for modeling, mapping, and understanding human interactions with natural and cultural heritage. However, categorizing these images into semantically meaningful groups remains highly complex due to the vast diversity and heterogeneity of their visual content as they contain an open-world human and nature elements. This challenge becomes greater when categories involve abstract concepts and lack consistent visual patterns. Related studies involve human supervision in the categorization process and the lack of public benchmark datasets make comparisons between these works unfeasible. On the other hand, the continuous advances in large models, including Large Language Models (LLMs), Large Visual Models (LVMs), and Large Visual Language Models (LVLMs), provide a large space of unexplored solutions. In this work 1) we introduce FLIPS a dataset of Flickr images that capture the interaction between human and nature, and 2) evaluate various solutions based on different types and combinations of large models using various adaptation methods. We assess and report their performance in terms of cost, productivity, scalability, and result quality to address the challenges of social media image categorization.",
            "corpus_id": 273022981,
            "sentences": [
                {
                    "corpus_id": "273022981",
                    "title": "Exploring Social Media Image Categorization Using Large Models with Different Adaptation Methods: A Case Study on Cultural Nature's Contributions to People",
                    "text": "LVLMs have emerged as a natural evolution of integrating LLMs with LVMs, enabling seamless multimodal interaction. By utilizing machine-generated instruction-following data during training, these models facilitate more dynamic and interactive communication between humans and machines. LVLMs are capable of processing both visual and textual inputs, generating coherent and contextually relevant responses in natural language. \n\nThese models serve as general-purpose problem solvers, demonstrating good performance across various tasks-particularly when prompt engineering is used to guide the model, and when the task aligns with the domain on which the model was trained.Notable LVLMs include BLIP (Li et al. 2022), one of the earliest unified vision-language encoder-decoder architectures; LLaVA (Large Language and Vision Assistant) (Liu et al. 2023), an end-to-end multimodal model that connects a vision encoder to a language model for versatile visual-language understanding; and GPT-4 (Achiam et al. 2023), a proprietary foundation model capable of multimodal reasoning, accessible exclusively via APIs such as ChatGPT.",
                    "score": 0.46867682491285334,
                    "section_title": "Large Vision Language Models:",
                    "char_start_offset": 14563,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 114
                        },
                        {
                            "start": 115,
                            "end": 285
                        },
                        {
                            "start": 286,
                            "end": 426
                        },
                        {
                            "start": 429,
                            "end": 1127
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 700,
                            "end": 716,
                            "matchedPaperCorpusId": "246411402"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9697265625
                }
            ],
            "relevance_judgement": 0.9697265625,
            "relevance_judgment_input_expanded": "# Title: Exploring Social Media Image Categorization Using Large Models with Different Adaptation Methods: A Case Study on Cultural Nature's Contributions to People\n# Venue: \n# Authors: Rohaifa Khaldi, Domingo Alcaraz-Segura, Ignacio S'anchez-Herrera, Javier Mart\u00ednez-L\u00f3pez, Carlos Javier Navarro, S. Tabik\n## Abstract\nSocial media images provide valuable insights for modeling, mapping, and understanding human interactions with natural and cultural heritage. However, categorizing these images into semantically meaningful groups remains highly complex due to the vast diversity and heterogeneity of their visual content as they contain an open-world human and nature elements. This challenge becomes greater when categories involve abstract concepts and lack consistent visual patterns. Related studies involve human supervision in the categorization process and the lack of public benchmark datasets make comparisons between these works unfeasible. On the other hand, the continuous advances in large models, including Large Language Models (LLMs), Large Visual Models (LVMs), and Large Visual Language Models (LVLMs), provide a large space of unexplored solutions. In this work 1) we introduce FLIPS a dataset of Flickr images that capture the interaction between human and nature, and 2) evaluate various solutions based on different types and combinations of large models using various adaptation methods. We assess and report their performance in terms of cost, productivity, scalability, and result quality to address the challenges of social media image categorization.\n## Large Vision Language Models:\nLVLMs have emerged as a natural evolution of integrating LLMs with LVMs, enabling seamless multimodal interaction. By utilizing machine-generated instruction-following data during training, these models facilitate more dynamic and interactive communication between humans and machines. LVLMs are capable of processing both visual and textual inputs, generating coherent and contextually relevant responses in natural language. \n\nThese models serve as general-purpose problem solvers, demonstrating good performance across various tasks-particularly when prompt engineering is used to guide the model, and when the task aligns with the domain on which the model was trained.Notable LVLMs include BLIP (Li et al. 2022), one of the earliest unified vision-language encoder-decoder architectures; LLaVA (Large Language and Vision Assistant) (Liu et al. 2023), an end-to-end multimodal model that connects a vision encoder to a language model for versatile visual-language understanding; and GPT-4 (Achiam et al. 2023), a proprietary foundation model capable of multimodal reasoning, accessible exclusively via APIs such as ChatGPT.",
            "reference_string": "[273022981 | Khaldi et al. | 2024 | Citations: 1]"
        },
        {
            "title": "VLM-HOI: Vision Language Models for Interpretable Human-Object Interaction Analysis",
            "venue": "ECCV Workshops",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.18038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1490936425",
                    "name": "Donggoo Kang"
                },
                {
                    "authorId": "1783599864",
                    "name": "Dasol Jeong"
                },
                {
                    "authorId": "2321673431",
                    "name": "Hyunmin Lee"
                },
                {
                    "authorId": "2205412863",
                    "name": "Sangwoo Park"
                },
                {
                    "authorId": "31237187",
                    "name": "Hasil Park"
                },
                {
                    "authorId": "2211171908",
                    "name": "Sunkyu Kwon"
                },
                {
                    "authorId": "2268097132",
                    "name": "Yeongjoon Kim"
                },
                {
                    "authorId": "2261728538",
                    "name": "Joonki Paik"
                }
            ],
            "abstract": "The Large Vision Language Model (VLM) has recently addressed remarkable progress in bridging two fundamental modalities. VLM, trained by a sufficiently large dataset, exhibits a comprehensive understanding of both visual and linguistic to perform diverse tasks. To distill this knowledge accurately, in this paper, we introduce a novel approach that explicitly utilizes VLM as an objective function form for the Human-Object Interaction (HOI) detection task (\\textbf{VLM-HOI}). Specifically, we propose a method that quantifies the similarity of the predicted HOI triplet using the Image-Text matching technique. We represent HOI triplets linguistically to fully utilize the language comprehension of VLMs, which are more suitable than CLIP models due to their localization and object-centric nature. This matching score is used as an objective for contrastive optimization. To our knowledge, this is the first utilization of VLM language abilities for HOI detection. Experiments demonstrate the effectiveness of our method, achieving state-of-the-art HOI detection accuracy on benchmarks. We believe integrating VLMs into HOI detection represents important progress towards more advanced and interpretable analysis of human-object interactions.",
            "corpus_id": 274305669,
            "sentences": [
                {
                    "corpus_id": "274305669",
                    "title": "VLM-HOI: Vision Language Models for Interpretable Human-Object Interaction Analysis",
                    "text": "Integrating vision and language has been a long-standing goal in artificial intelligence. Early work focused on image captioning [39], generating textual descriptions of image contents. Recent years have seen rapid progress in visual question answering [1,2], enabled by large-scale datasets [7] and deep neural encoder-decoder models [43]. \n\nMore advanced vision-language tasks require a tighter integration between the visual and linguistic modalities. This has led to a surge of interest in unified multimodal representation models that can process both images and text within a single framework [3]. \n\nOne line of work explores joint embedding models to learn aligned vector representations for image regions and language fragments [15,19]. However, these approaches do not explicitly model interactions between modalities. \n\nRecently, pretrained language models, such as BERT [11] and GPT [4], have significantly influenced the vision-language domain. Researchers have extended these models to handle multimodal inputs, leveraging their contextual understanding of text. More recent methods like VisualBERT [23] and LXMERT [38] utilize pretrained BERT weights for joint image-text comprehension. \n\nUnified architectures like ViLBERT [30], VL-BERT [36], UNITER [8], BLIP [21] integrate masked language modeling objectives alongside paired image-text prediction tasks within a single Transformer model. These models set new stateof-the-art results on downstream tasks like visual question answering (VQA), visual reasoning, and image retrieval. \n\nSpecifically, we utilize BLIP as our vision-language model backbone. BLIP uses a flexible multimodal encoder-decoder model that can handle both understanding and generation tasks. It also improves training data quality through generating new image captions and filtering noise. These advantages lead to strong performance. \n\nInstruction-based VLMs like GPT-4V [33], LLaVA [28], and InstructBLIP [10] are a new type of large vision-language model that are trained to follow natural language instructions and prompts to perform various tasks. A key advantage these models have over traditional fine-tuning is they can adapt to new tasks without needing gradient updates or lots of specific training data. The instruction format allows rapid adaptation.",
                    "score": 0.4963214839671234,
                    "section_title": "Vision-Lauguage Model",
                    "char_start_offset": 7308,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 89
                        },
                        {
                            "start": 90,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 340
                        },
                        {
                            "start": 343,
                            "end": 454
                        },
                        {
                            "start": 455,
                            "end": 603
                        },
                        {
                            "start": 606,
                            "end": 744
                        },
                        {
                            "start": 745,
                            "end": 827
                        },
                        {
                            "start": 830,
                            "end": 956
                        },
                        {
                            "start": 957,
                            "end": 1075
                        },
                        {
                            "start": 1076,
                            "end": 1200
                        },
                        {
                            "start": 1203,
                            "end": 1405
                        },
                        {
                            "start": 1406,
                            "end": 1547
                        },
                        {
                            "start": 1550,
                            "end": 1618
                        },
                        {
                            "start": 1619,
                            "end": 1729
                        },
                        {
                            "start": 1730,
                            "end": 1827
                        },
                        {
                            "start": 1828,
                            "end": 1872
                        },
                        {
                            "start": 1875,
                            "end": 2090
                        },
                        {
                            "start": 2091,
                            "end": 2252
                        },
                        {
                            "start": 2253,
                            "end": 2300
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 129,
                            "end": 133,
                            "matchedPaperCorpusId": "1169492"
                        },
                        {
                            "start": 253,
                            "end": 256,
                            "matchedPaperCorpusId": "3753452"
                        },
                        {
                            "start": 256,
                            "end": 258,
                            "matchedPaperCorpusId": "3180429"
                        },
                        {
                            "start": 335,
                            "end": 339,
                            "matchedPaperCorpusId": "1055111"
                        },
                        {
                            "start": 599,
                            "end": 602,
                            "matchedPaperCorpusId": "10137425"
                        },
                        {
                            "start": 736,
                            "end": 740,
                            "matchedPaperCorpusId": "8517067"
                        },
                        {
                            "start": 894,
                            "end": 897,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1238,
                            "end": 1242,
                            "matchedPaperCorpusId": "208637516"
                        },
                        {
                            "start": 1265,
                            "end": 1268,
                            "matchedPaperCorpusId": "216080982"
                        },
                        {
                            "start": 1275,
                            "end": 1279,
                            "matchedPaperCorpusId": "246411402"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96923828125
                }
            ],
            "relevance_judgement": 0.96923828125,
            "relevance_judgment_input_expanded": "# Title: VLM-HOI: Vision Language Models for Interpretable Human-Object Interaction Analysis\n# Venue: ECCV Workshops\n# Authors: Donggoo Kang, Dasol Jeong, Hyunmin Lee, Sangwoo Park, Hasil Park, Sunkyu Kwon, Yeongjoon Kim, Joonki Paik\n## Abstract\nThe Large Vision Language Model (VLM) has recently addressed remarkable progress in bridging two fundamental modalities. VLM, trained by a sufficiently large dataset, exhibits a comprehensive understanding of both visual and linguistic to perform diverse tasks. To distill this knowledge accurately, in this paper, we introduce a novel approach that explicitly utilizes VLM as an objective function form for the Human-Object Interaction (HOI) detection task (\\textbf{VLM-HOI}). Specifically, we propose a method that quantifies the similarity of the predicted HOI triplet using the Image-Text matching technique. We represent HOI triplets linguistically to fully utilize the language comprehension of VLMs, which are more suitable than CLIP models due to their localization and object-centric nature. This matching score is used as an objective for contrastive optimization. To our knowledge, this is the first utilization of VLM language abilities for HOI detection. Experiments demonstrate the effectiveness of our method, achieving state-of-the-art HOI detection accuracy on benchmarks. We believe integrating VLMs into HOI detection represents important progress towards more advanced and interpretable analysis of human-object interactions.\n## Vision-Lauguage Model\nIntegrating vision and language has been a long-standing goal in artificial intelligence. Early work focused on image captioning [39], generating textual descriptions of image contents. Recent years have seen rapid progress in visual question answering [1,2], enabled by large-scale datasets [7] and deep neural encoder-decoder models [43]. \n\nMore advanced vision-language tasks require a tighter integration between the visual and linguistic modalities. This has led to a surge of interest in unified multimodal representation models that can process both images and text within a single framework [3]. \n\nOne line of work explores joint embedding models to learn aligned vector representations for image regions and language fragments [15,19]. However, these approaches do not explicitly model interactions between modalities. \n\nRecently, pretrained language models, such as BERT [11] and GPT [4], have significantly influenced the vision-language domain. Researchers have extended these models to handle multimodal inputs, leveraging their contextual understanding of text. More recent methods like VisualBERT [23] and LXMERT [38] utilize pretrained BERT weights for joint image-text comprehension. \n\nUnified architectures like ViLBERT [30], VL-BERT [36], UNITER [8], BLIP [21] integrate masked language modeling objectives alongside paired image-text prediction tasks within a single Transformer model. These models set new stateof-the-art results on downstream tasks like visual question answering (VQA), visual reasoning, and image retrieval. \n\nSpecifically, we utilize BLIP as our vision-language model backbone. BLIP uses a flexible multimodal encoder-decoder model that can handle both understanding and generation tasks. It also improves training data quality through generating new image captions and filtering noise. These advantages lead to strong performance. \n\nInstruction-based VLMs like GPT-4V [33], LLaVA [28], and InstructBLIP [10] are a new type of large vision-language model that are trained to follow natural language instructions and prompts to perform various tasks. A key advantage these models have over traditional fine-tuning is they can adapt to new tasks without needing gradient updates or lots of specific training data. The instruction format allows rapid adaptation.",
            "reference_string": "[274305669 | Kang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "SEED-Bench-2: Benchmarking Multimodal Large Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 66,
            "citation_count": 69,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.17092, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1491798621",
                    "name": "Bohao Li"
                },
                {
                    "authorId": "51123495",
                    "name": "Yuying Ge"
                },
                {
                    "authorId": "152988335",
                    "name": "Yixiao Ge"
                },
                {
                    "authorId": "2243452799",
                    "name": "Guangzhi Wang"
                },
                {
                    "authorId": "2151036422",
                    "name": "Rui Wang"
                },
                {
                    "authorId": "2268726753",
                    "name": "Ruimao Zhang"
                },
                {
                    "authorId": "2265579883",
                    "name": "Ying Shan"
                }
            ],
            "abstract": "Multimodal large language models (MLLMs), building upon the foundation of powerful large language models (LLMs), have recently demonstrated exceptional capabilities in generating not only texts but also images given interleaved multimodal inputs (acting like a combination of GPT-4V and DALL-E 3). However, existing MLLM benchmarks remain limited to assessing only models' comprehension ability of single image-text inputs, failing to keep up with the strides made in MLLMs. A comprehensive benchmark is imperative for investigating the progress and uncovering the limitations of current MLLMs. In this work, we categorize the capabilities of MLLMs into hierarchical levels from $L_0$ to $L_4$ based on the modalities they can accept and generate, and propose SEED-Bench-2, a comprehensive benchmark that evaluates the \\textbf{hierarchical} capabilities of MLLMs. Specifically, SEED-Bench-2 comprises 24K multiple-choice questions with accurate human annotations, which spans 27 dimensions, including the evaluation of both text and image generation. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 23 prominent open-source MLLMs and summarize valuable observations. By revealing the limitations of existing MLLMs through extensive evaluations, we aim for SEED-Bench-2 to provide insights that will motivate future research towards the goal of General Artificial Intelligence. Dataset and evaluation code are available at \\href{https://github.com/AILab-CVC/SEED-Bench}",
            "corpus_id": 265498814,
            "sentences": [
                {
                    "corpus_id": "265498814",
                    "title": "SEED-Bench-2: Benchmarking Multimodal Large Language Models",
                    "text": "In recent years, Large Language Models (LLMs) [8,13,46,47,55] have exhibited remarkable capabilities to understand, reason, and generate texts across a variety of openended tasks. Leveraging the strong generality of LLMs, Multimodal Large Language Models (MLLMs) [3,9,19,26,29,31,32,36,37,42,43,48,48,53,62,68,70] have demonstrated exceptional capabilities in comprehending multimodal data through predicting open-form texts. \n\nRecent work [11,17,18,34,54,60] further empower LLMs with the ability of generating images beyond texts (acting like a combination of GPT-4V [1] and DALL-E 3 [5]), since they contend that the premise for the emergence of multimodal capabilities is that text and image can be represented and processed interchangeably in a unified autoregressive Transformer. However, despite the extensive capabilities of MLLMs, existing MLLM benchmarks [4,15,38,61,63] primarily focus on evaluating single image-text comprehension, thus failing to fully demonstrate the progress and limitations of current MLLMs. The lag of benchmarks behind the rapid development of MLLMs hinders the exploration and evolution of models. \n\nIn this work, we categorize the capabilities of MLLMs into hierarchical levels ranging from L 0 to L 4 based on the modalities they can accept and generate, as depicted in Fig. 1. Building upon LLMs, the lowest-tier capability L 0 involves generating texts given text inputs, while the highest-tier capability L 4 entails producing open-form interleaved image and text output given arbitrary interleaved image-text inputs. Reaching the capability L 4 is a crucial milestone on the path towards General Artificial Intel-ligence (AGI) since a human-level AI should be able to effortlessly digest and create multimodal content. In the capability pyramid, higher levels inherently include the capabilities of lower tiers. This hierarchical categorization not only clearly illustrates the current progress of MLLMs, but also provides a well-defined roadmap for future research.",
                    "score": 0.5360400206334819,
                    "section_title": "Introduction",
                    "char_start_offset": 1364,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 425
                        },
                        {
                            "start": 428,
                            "end": 785
                        },
                        {
                            "start": 786,
                            "end": 1024
                        },
                        {
                            "start": 1025,
                            "end": 1133
                        },
                        {
                            "start": 1136,
                            "end": 1315
                        },
                        {
                            "start": 1316,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1760
                        },
                        {
                            "start": 1761,
                            "end": 1853
                        },
                        {
                            "start": 1854,
                            "end": 2008
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 277,
                            "end": 280,
                            "matchedPaperCorpusId": "256390509"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96826171875
                }
            ],
            "relevance_judgement": 0.96826171875,
            "relevance_judgment_input_expanded": "# Title: SEED-Bench-2: Benchmarking Multimodal Large Language Models\n# Venue: arXiv.org\n# Authors: Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, Ying Shan\n## Abstract\nMultimodal large language models (MLLMs), building upon the foundation of powerful large language models (LLMs), have recently demonstrated exceptional capabilities in generating not only texts but also images given interleaved multimodal inputs (acting like a combination of GPT-4V and DALL-E 3). However, existing MLLM benchmarks remain limited to assessing only models' comprehension ability of single image-text inputs, failing to keep up with the strides made in MLLMs. A comprehensive benchmark is imperative for investigating the progress and uncovering the limitations of current MLLMs. In this work, we categorize the capabilities of MLLMs into hierarchical levels from $L_0$ to $L_4$ based on the modalities they can accept and generate, and propose SEED-Bench-2, a comprehensive benchmark that evaluates the \\textbf{hierarchical} capabilities of MLLMs. Specifically, SEED-Bench-2 comprises 24K multiple-choice questions with accurate human annotations, which spans 27 dimensions, including the evaluation of both text and image generation. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 23 prominent open-source MLLMs and summarize valuable observations. By revealing the limitations of existing MLLMs through extensive evaluations, we aim for SEED-Bench-2 to provide insights that will motivate future research towards the goal of General Artificial Intelligence. Dataset and evaluation code are available at \\href{https://github.com/AILab-CVC/SEED-Bench}\n## Introduction\nIn recent years, Large Language Models (LLMs) [8,13,46,47,55] have exhibited remarkable capabilities to understand, reason, and generate texts across a variety of openended tasks. Leveraging the strong generality of LLMs, Multimodal Large Language Models (MLLMs) [3,9,19,26,29,31,32,36,37,42,43,48,48,53,62,68,70] have demonstrated exceptional capabilities in comprehending multimodal data through predicting open-form texts. \n\nRecent work [11,17,18,34,54,60] further empower LLMs with the ability of generating images beyond texts (acting like a combination of GPT-4V [1] and DALL-E 3 [5]), since they contend that the premise for the emergence of multimodal capabilities is that text and image can be represented and processed interchangeably in a unified autoregressive Transformer. However, despite the extensive capabilities of MLLMs, existing MLLM benchmarks [4,15,38,61,63] primarily focus on evaluating single image-text comprehension, thus failing to fully demonstrate the progress and limitations of current MLLMs. The lag of benchmarks behind the rapid development of MLLMs hinders the exploration and evolution of models. \n\nIn this work, we categorize the capabilities of MLLMs into hierarchical levels ranging from L 0 to L 4 based on the modalities they can accept and generate, as depicted in Fig. 1. Building upon LLMs, the lowest-tier capability L 0 involves generating texts given text inputs, while the highest-tier capability L 4 entails producing open-form interleaved image and text output given arbitrary interleaved image-text inputs. Reaching the capability L 4 is a crucial milestone on the path towards General Artificial Intel-ligence (AGI) since a human-level AI should be able to effortlessly digest and create multimodal content. In the capability pyramid, higher levels inherently include the capabilities of lower tiers. This hierarchical categorization not only clearly illustrates the current progress of MLLMs, but also provides a well-defined roadmap for future research.",
            "reference_string": "[265498814 | Li et al. | 2023 | Citations: 69]"
        },
        {
            "title": "A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 457,
            "citation_count": 47,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10466766.pdf",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.15490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2260382962",
                    "name": "Abolfazl Younesi"
                },
                {
                    "authorId": "153631156",
                    "name": "Mohsen Ansari"
                },
                {
                    "authorId": "2260384993",
                    "name": "Mohammadamin Fazli"
                },
                {
                    "authorId": "2490667",
                    "name": "A. Ejlali"
                },
                {
                    "authorId": "2238832044",
                    "name": "Muhammad Shafique"
                },
                {
                    "authorId": "2247781719",
                    "name": "Joerg Henkel"
                }
            ],
            "abstract": "In today\u2019s digital age, Convolutional Neural Networks (CNNs), a subset of Deep Learning (DL), are widely used for various computer vision tasks such as image classification, object detection, and image segmentation. There are numerous types of CNNs designed to meet specific needs and requirements, including 1D, 2D, and 3D CNNs, as well as dilated, grouped, attention, depthwise convolutions, and NAS, among others. Each type of CNN has its unique structure and characteristics, making it suitable for specific tasks. It\u2019s crucial to gain a thorough understanding and perform a comparative analysis of these different CNN types to understand their strengths and weaknesses. Furthermore, studying the performance, limitations, and practical applications of each type of CNN can aid in the development of new and improved architectures in the future. We also dive into the platforms and frameworks that researchers utilize for their research or development from various perspectives. Additionally, we explore the main research fields of CNN like 6D vision, generative models, and meta-learning. This survey paper provides a comprehensive examination and comparison of various CNN architectures, highlighting their architectural differences and emphasizing their respective advantages, disadvantages, applications, challenges, and future trends.",
            "corpus_id": 267897956,
            "sentences": [
                {
                    "corpus_id": "267897956",
                    "title": "A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends",
                    "text": "In recent epochs, the domain of large language models (LLMs) for natural language processing has witnessed a precipitous progression. Prototypes such as BERT, GPT-3, and PaLM have demonstrated exceptional aptitude in language apprehension and generation, courtesy of self-supervised pretraining on voluminous text corpora [85]. As LLMs expand in magnitude and range, incorporating additional modalities beyond text is a burgeoning field of study. Multimodal LLMs strive to amalgamate language, vision, and other sensory inputs within a singular model architecture. They hold the potential to attain a more holistic understanding of the world by concurrently learning representations across diverse data types [96]. A significant hurdle is the effective fusion of the strengths of CNNs for computer vision and transformer architectures for language modeling. \n\nOne strategy involves employing a dual-stream architecture with distinct CNN and transformer encoders interacting via coattentional transformer layers [97]. The CNN extracts visual features from images, providing contextual information that can guide language generation and comprehension. The transformer architecture models the semantics and syntax of text. Their interaction enables the generation of captions based on image content or the retrieval of pertinent images for textual queries. Alternative methods directly incorporate CNNs within the transformer architecture as visual token encoders that operate with text token encoders [98]. The CNN projections of image patches are appended to text token embeddings as inputs to the transformer layers. This unified architecture allows for end-to-end optimization of parameters for both vision and language tasks. Self-supervised pretraining continues to be vital for multimodal LLMs to learn effective joint representations before downstream task tuning. Contrastive learning objectives that predict associations between modalities have proven highly effective [99]. Models pre-trained on large datasets of image-text pairs have demonstrated robust zeroshot transfer performance on multimodal tasks. \n\nAs multimodal LLMs increase in scale, the efficient combination of diverse convolution types and attention mechanisms will be crucial. Compact CNN architectures could help to reduce the cost of computing. Sparse attention and memory compression techniques can assist with scalability.",
                    "score": 0.5014845515179497,
                    "section_title": "H. Evolution of Language Models and Multimodal LLMs",
                    "char_start_offset": 60107,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 446
                        },
                        {
                            "start": 447,
                            "end": 564
                        },
                        {
                            "start": 565,
                            "end": 714
                        },
                        {
                            "start": 715,
                            "end": 857
                        },
                        {
                            "start": 860,
                            "end": 1016
                        },
                        {
                            "start": 1017,
                            "end": 1149
                        },
                        {
                            "start": 1150,
                            "end": 1219
                        },
                        {
                            "start": 1220,
                            "end": 1353
                        },
                        {
                            "start": 1354,
                            "end": 1504
                        },
                        {
                            "start": 1505,
                            "end": 1616
                        },
                        {
                            "start": 1617,
                            "end": 1727
                        },
                        {
                            "start": 1728,
                            "end": 1869
                        },
                        {
                            "start": 1870,
                            "end": 1981
                        },
                        {
                            "start": 1982,
                            "end": 2114
                        },
                        {
                            "start": 2117,
                            "end": 2251
                        },
                        {
                            "start": 2252,
                            "end": 2321
                        },
                        {
                            "start": 2322,
                            "end": 2401
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96630859375
                }
            ],
            "relevance_judgement": 0.96630859375,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends\n# Venue: IEEE Access\n# Authors: Abolfazl Younesi, Mohsen Ansari, Mohammadamin Fazli, A. Ejlali, Muhammad Shafique, Joerg Henkel\n## Abstract\nIn today\u2019s digital age, Convolutional Neural Networks (CNNs), a subset of Deep Learning (DL), are widely used for various computer vision tasks such as image classification, object detection, and image segmentation. There are numerous types of CNNs designed to meet specific needs and requirements, including 1D, 2D, and 3D CNNs, as well as dilated, grouped, attention, depthwise convolutions, and NAS, among others. Each type of CNN has its unique structure and characteristics, making it suitable for specific tasks. It\u2019s crucial to gain a thorough understanding and perform a comparative analysis of these different CNN types to understand their strengths and weaknesses. Furthermore, studying the performance, limitations, and practical applications of each type of CNN can aid in the development of new and improved architectures in the future. We also dive into the platforms and frameworks that researchers utilize for their research or development from various perspectives. Additionally, we explore the main research fields of CNN like 6D vision, generative models, and meta-learning. This survey paper provides a comprehensive examination and comparison of various CNN architectures, highlighting their architectural differences and emphasizing their respective advantages, disadvantages, applications, challenges, and future trends.\n## H. Evolution of Language Models and Multimodal LLMs\nIn recent epochs, the domain of large language models (LLMs) for natural language processing has witnessed a precipitous progression. Prototypes such as BERT, GPT-3, and PaLM have demonstrated exceptional aptitude in language apprehension and generation, courtesy of self-supervised pretraining on voluminous text corpora [85]. As LLMs expand in magnitude and range, incorporating additional modalities beyond text is a burgeoning field of study. Multimodal LLMs strive to amalgamate language, vision, and other sensory inputs within a singular model architecture. They hold the potential to attain a more holistic understanding of the world by concurrently learning representations across diverse data types [96]. A significant hurdle is the effective fusion of the strengths of CNNs for computer vision and transformer architectures for language modeling. \n\nOne strategy involves employing a dual-stream architecture with distinct CNN and transformer encoders interacting via coattentional transformer layers [97]. The CNN extracts visual features from images, providing contextual information that can guide language generation and comprehension. The transformer architecture models the semantics and syntax of text. Their interaction enables the generation of captions based on image content or the retrieval of pertinent images for textual queries. Alternative methods directly incorporate CNNs within the transformer architecture as visual token encoders that operate with text token encoders [98]. The CNN projections of image patches are appended to text token embeddings as inputs to the transformer layers. This unified architecture allows for end-to-end optimization of parameters for both vision and language tasks. Self-supervised pretraining continues to be vital for multimodal LLMs to learn effective joint representations before downstream task tuning. Contrastive learning objectives that predict associations between modalities have proven highly effective [99]. Models pre-trained on large datasets of image-text pairs have demonstrated robust zeroshot transfer performance on multimodal tasks. \n\nAs multimodal LLMs increase in scale, the efficient combination of diverse convolution types and attention mechanisms will be crucial. Compact CNN architectures could help to reduce the cost of computing. Sparse attention and memory compression techniques can assist with scalability.",
            "reference_string": "[267897956 | Younesi et al. | 2024 | Citations: 47]"
        },
        {
            "title": "WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 58,
            "citation_count": 26,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.07265, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2350309448",
                    "name": "Yuwei Niu"
                },
                {
                    "authorId": "2253434320",
                    "name": "Munan Ning"
                },
                {
                    "authorId": "2349744842",
                    "name": "Mengren Zheng"
                },
                {
                    "authorId": "2333420566",
                    "name": "Bin Lin"
                },
                {
                    "authorId": "2328406001",
                    "name": "Peng Jin"
                },
                {
                    "authorId": "2315613899",
                    "name": "Jiaqi Liao"
                },
                {
                    "authorId": "2253467098",
                    "name": "Kun-Peng Ning"
                },
                {
                    "authorId": "2256419313",
                    "name": "Bin Zhu"
                },
                {
                    "authorId": "2280992738",
                    "name": "Li Yuan"
                }
            ],
            "abstract": "Text-to-Image (T2I) models are capable of generating high-quality artistic creations and visual content. However, existing research and evaluation standards predominantly focus on image realism and shallow text-image alignment, lacking a comprehensive assessment of complex semantic understanding and world knowledge integration in text to image generation. To address this challenge, we propose $\\textbf{WISE}$, the first benchmark specifically designed for $\\textbf{W}$orld Knowledge-$\\textbf{I}$nformed $\\textbf{S}$emantic $\\textbf{E}$valuation. WISE moves beyond simple word-pixel mapping by challenging models with 1000 meticulously crafted prompts across 25 sub-domains in cultural common sense, spatio-temporal reasoning, and natural science. To overcome the limitations of traditional CLIP metric, we introduce $\\textbf{WiScore}$, a novel quantitative metric for assessing knowledge-image alignment. Through comprehensive testing of 20 models (10 dedicated T2I models and 10 unified multimodal models) using 1,000 structured prompts spanning 25 subdomains, our findings reveal significant limitations in their ability to effectively integrate and apply world knowledge during image generation, highlighting critical pathways for enhancing knowledge incorporation and application in next-generation T2I models. Code and data are available at https://github.com/PKU-YuanGroup/WISE.",
            "corpus_id": 276929205,
            "sentences": [
                {
                    "corpus_id": "276929205",
                    "title": "WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation",
                    "text": "Unified multimodal models aim to construct general-purpose models capable of processing both textual and visual inputs, and performing cross-modal generation and understanding. These models [4,8,16,17,21,25,35,38,39,41,[43][44][45]48] are typically built upon powerful large language models (LLMs) [54] and extend next-token prediction [2] to image generation: the LLM generates visual tokens, and a VQ-VAE [42] or Diffusion model serves as a detokenizer. Moreover, Transfusion [55] and Show-O [49] demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework. D-DiT [22] achieves both Text-to-Image (T2I) and Imageto-Text (I2T) tasks using an end-to-end diffusion model. A crucial question concerning unified multimodal models is whether their understanding and generation capabilities can mutually enhance each other. Some studies [41,45] have provided evidence supporting this phenomenon. However, in contrast to the rich and comprehensive benchmarks for multimodal understanding, T2I benchmarks are often relatively",
                    "score": 0.4894732914888311,
                    "section_title": "Unified Multimodal Models",
                    "char_start_offset": 6123,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 176
                        },
                        {
                            "start": 177,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 624
                        },
                        {
                            "start": 625,
                            "end": 735
                        },
                        {
                            "start": 736,
                            "end": 883
                        },
                        {
                            "start": 884,
                            "end": 955
                        },
                        {
                            "start": 956,
                            "end": 1083
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96630859375
                }
            ],
            "relevance_judgement": 0.96630859375,
            "relevance_judgment_input_expanded": "# Title: WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation\n# Venue: arXiv.org\n# Authors: Yuwei Niu, Munan Ning, Mengren Zheng, Bin Lin, Peng Jin, Jiaqi Liao, Kun-Peng Ning, Bin Zhu, Li Yuan\n## Abstract\nText-to-Image (T2I) models are capable of generating high-quality artistic creations and visual content. However, existing research and evaluation standards predominantly focus on image realism and shallow text-image alignment, lacking a comprehensive assessment of complex semantic understanding and world knowledge integration in text to image generation. To address this challenge, we propose $\\textbf{WISE}$, the first benchmark specifically designed for $\\textbf{W}$orld Knowledge-$\\textbf{I}$nformed $\\textbf{S}$emantic $\\textbf{E}$valuation. WISE moves beyond simple word-pixel mapping by challenging models with 1000 meticulously crafted prompts across 25 sub-domains in cultural common sense, spatio-temporal reasoning, and natural science. To overcome the limitations of traditional CLIP metric, we introduce $\\textbf{WiScore}$, a novel quantitative metric for assessing knowledge-image alignment. Through comprehensive testing of 20 models (10 dedicated T2I models and 10 unified multimodal models) using 1,000 structured prompts spanning 25 subdomains, our findings reveal significant limitations in their ability to effectively integrate and apply world knowledge during image generation, highlighting critical pathways for enhancing knowledge incorporation and application in next-generation T2I models. Code and data are available at https://github.com/PKU-YuanGroup/WISE.\n## Unified Multimodal Models\nUnified multimodal models aim to construct general-purpose models capable of processing both textual and visual inputs, and performing cross-modal generation and understanding. These models [4,8,16,17,21,25,35,38,39,41,[43][44][45]48] are typically built upon powerful large language models (LLMs) [54] and extend next-token prediction [2] to image generation: the LLM generates visual tokens, and a VQ-VAE [42] or Diffusion model serves as a detokenizer. Moreover, Transfusion [55] and Show-O [49] demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework. D-DiT [22] achieves both Text-to-Image (T2I) and Imageto-Text (I2T) tasks using an end-to-end diffusion model. A crucial question concerning unified multimodal models is whether their understanding and generation capabilities can mutually enhance each other. Some studies [41,45] have provided evidence supporting this phenomenon. However, in contrast to the rich and comprehensive benchmarks for multimodal understanding, T2I benchmarks are often relatively",
            "reference_string": "[276929205 | Niu et al. | 2025 | Citations: 26]"
        },
        {
            "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 82,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.03651, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2248946985",
                    "name": "Rui Zhao"
                },
                {
                    "authorId": "2258958963",
                    "name": "Weijia Mao"
                },
                {
                    "authorId": "2344762475",
                    "name": "Mike Zheng Shou"
                }
            ],
            "abstract": "Adapting generative models to specific domains presents an effective solution for satisfying specialized requirements. However, adapting to some complex domains remains challenging, especially when these domains require substantial paired data to capture the targeted distributions. Since unpaired data from a single modality, such as vision or language, is more readily available, we utilize the bidirectional mappings between vision and language learned by the unified generative model to enable training on unpaired data for domain adaptation. Specifically, we propose DoraCycle, which integrates two multimodal cycles: text-to-image-to-text and image-to-text-to-image. The model is optimized through cross-entropy loss computed at the cycle endpoints, where both endpoints share the same modality. This facilitates self-evolution of the model without reliance on annotated text-image pairs. Experimental results demonstrate that for tasks independent of paired knowledge, such as stylization, DoraCycle can effectively adapt the unified model using only unpaired data. For tasks involving new paired knowledge, such as specific identities, a combination of a small set of paired image-text examples and larger-scale unpaired data is sufficient for effective domain-oriented adaptation. The code will be released at https://github.com/showlab/DoraCycle.",
            "corpus_id": 276782523,
            "sentences": [
                {
                    "corpus_id": "276782523",
                    "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
                    "text": "Unified multimodal generative models aim to bridge the gap between understanding and generation tasks, and integrate vision and language into a single framework, enabling the model to learn shared representations across modalities [1,2,15,19,59,62,71,72,74,76,77,79,85]. SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder [45] are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM [15] extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities. Our work leverages the advancements made by these foundational models and explores how to adapt the foundational model to specific domains.",
                    "score": 0.48554158297839234,
                    "section_title": "Unified Multimodal Generative Models",
                    "char_start_offset": 6395,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 270
                        },
                        {
                            "start": 271,
                            "end": 507
                        },
                        {
                            "start": 508,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 831
                        },
                        {
                            "start": 832,
                            "end": 1109
                        },
                        {
                            "start": 1110,
                            "end": 1273
                        },
                        {
                            "start": 1274,
                            "end": 1413
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 234,
                            "end": 236,
                            "matchedPaperCorpusId": "262898422"
                        },
                        {
                            "start": 236,
                            "end": 239,
                            "matchedPaperCorpusId": "261975252"
                        },
                        {
                            "start": 242,
                            "end": 245,
                            "matchedPaperCorpusId": "266374640"
                        },
                        {
                            "start": 373,
                            "end": 377,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 517,
                            "end": 521,
                            "matchedPaperCorpusId": "261975252"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9658203125
                }
            ],
            "relevance_judgement": 0.9658203125,
            "relevance_judgment_input_expanded": "# Title: DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles\n# Venue: arXiv.org\n# Authors: Rui Zhao, Weijia Mao, Mike Zheng Shou\n## Abstract\nAdapting generative models to specific domains presents an effective solution for satisfying specialized requirements. However, adapting to some complex domains remains challenging, especially when these domains require substantial paired data to capture the targeted distributions. Since unpaired data from a single modality, such as vision or language, is more readily available, we utilize the bidirectional mappings between vision and language learned by the unified generative model to enable training on unpaired data for domain adaptation. Specifically, we propose DoraCycle, which integrates two multimodal cycles: text-to-image-to-text and image-to-text-to-image. The model is optimized through cross-entropy loss computed at the cycle endpoints, where both endpoints share the same modality. This facilitates self-evolution of the model without reliance on annotated text-image pairs. Experimental results demonstrate that for tasks independent of paired knowledge, such as stylization, DoraCycle can effectively adapt the unified model using only unpaired data. For tasks involving new paired knowledge, such as specific identities, a combination of a small set of paired image-text examples and larger-scale unpaired data is sufficient for effective domain-oriented adaptation. The code will be released at https://github.com/showlab/DoraCycle.\n## Unified Multimodal Generative Models\nUnified multimodal generative models aim to bridge the gap between understanding and generation tasks, and integrate vision and language into a single framework, enabling the model to learn shared representations across modalities [1,2,15,19,59,62,71,72,74,76,77,79,85]. SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder [45] are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM [15] extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities. Our work leverages the advancements made by these foundational models and explores how to adapt the foundational model to specific domains.",
            "reference_string": "[276782523 | Zhao et al. | 2025 | Citations: 1]"
        },
        {
            "title": "ICCV23 Visual-Dialog Emotion Explanation Challenge: SEU_309 Team Technical Report",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 13,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.09760, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2311429185",
                    "name": "Yixiao Yuan"
                },
                {
                    "authorId": "2275279906",
                    "name": "Yingzhe Peng"
                }
            ],
            "abstract": "The Visual-Dialog Based Emotion Explanation Generation Challenge focuses on generating emotion explanations through visual-dialog interactions in art discussions. Our approach combines state-of-the-art multi-modal models, including Language Model (LM) and Large Vision Language Model (LVLM), to achieve superior performance. By leveraging these models, we outperform existing benchmarks, securing the top rank in the ICCV23 Visual-Dialog Based Emotion Explanation Generation Challenge, which is part of the 5th Workshop On Closing The Loop Between Vision And Language (CLCV) with significant scores in F1 and BLEU metrics. Our method demonstrates exceptional ability in generating accurate emotion explanations, advancing our understanding of emotional impacts in art.",
            "corpus_id": 271213320,
            "sentences": [
                {
                    "corpus_id": "271213320",
                    "title": "ICCV23 Visual-Dialog Emotion Explanation Challenge: SEU_309 Team Technical Report",
                    "text": "To bridge this gap, the research and development of Large Vision Language Models (LVLMs) [9,2,10,15,14] have gained momentum. LVLMs are designed to perceive and understand both textual and visual information, thereby broadening the scope of applications for AI models. This integration allows for a more holistic understanding of content, enabling models to perform tasks that involve both text and images, such as image captioning, visual question answering, and visual grounding. \n\nLVLMs are built upon the foundation of LLMs, incorporating additional components that allow them to process and understand visual data. These components typically include visual receptors for image processing, input-output interfaces that enable the model to handle multimodal data, and training pipelines that are tailored to accommodate the complexities of learning from both text and images. Furthermore, these models are often trained on multilingual multimodal cleaned corpora, enhancing their ability to function across different languages and modalities. \n\nThe development of LVLMs represents a significant step forward in the field of AI, offering new possibilities for applications that require an understanding of the world that extends beyond text. By combining the capabilities of LLMs with the ability to process visual information, LVLMs are paving the way for more advanced and versatile AI systems. \n\nThese models have set new benchmarks in various visualcentric tasks, demonstrating their potential in enhancing the performance and applicability of AI across a broad range of domains.",
                    "score": 0.4558425490719533,
                    "section_title": "Related Work",
                    "char_start_offset": 4643,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 125
                        },
                        {
                            "start": 126,
                            "end": 268
                        },
                        {
                            "start": 269,
                            "end": 481
                        },
                        {
                            "start": 484,
                            "end": 619
                        },
                        {
                            "start": 620,
                            "end": 878
                        },
                        {
                            "start": 879,
                            "end": 1045
                        },
                        {
                            "start": 1048,
                            "end": 1243
                        },
                        {
                            "start": 1244,
                            "end": 1398
                        },
                        {
                            "start": 1401,
                            "end": 1585
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9658203125
                }
            ],
            "relevance_judgement": 0.9658203125,
            "relevance_judgment_input_expanded": "# Title: ICCV23 Visual-Dialog Emotion Explanation Challenge: SEU_309 Team Technical Report\n# Venue: arXiv.org\n# Authors: Yixiao Yuan, Yingzhe Peng\n## Abstract\nThe Visual-Dialog Based Emotion Explanation Generation Challenge focuses on generating emotion explanations through visual-dialog interactions in art discussions. Our approach combines state-of-the-art multi-modal models, including Language Model (LM) and Large Vision Language Model (LVLM), to achieve superior performance. By leveraging these models, we outperform existing benchmarks, securing the top rank in the ICCV23 Visual-Dialog Based Emotion Explanation Generation Challenge, which is part of the 5th Workshop On Closing The Loop Between Vision And Language (CLCV) with significant scores in F1 and BLEU metrics. Our method demonstrates exceptional ability in generating accurate emotion explanations, advancing our understanding of emotional impacts in art.\n## Related Work\nTo bridge this gap, the research and development of Large Vision Language Models (LVLMs) [9,2,10,15,14] have gained momentum. LVLMs are designed to perceive and understand both textual and visual information, thereby broadening the scope of applications for AI models. This integration allows for a more holistic understanding of content, enabling models to perform tasks that involve both text and images, such as image captioning, visual question answering, and visual grounding. \n\nLVLMs are built upon the foundation of LLMs, incorporating additional components that allow them to process and understand visual data. These components typically include visual receptors for image processing, input-output interfaces that enable the model to handle multimodal data, and training pipelines that are tailored to accommodate the complexities of learning from both text and images. Furthermore, these models are often trained on multilingual multimodal cleaned corpora, enhancing their ability to function across different languages and modalities. \n\nThe development of LVLMs represents a significant step forward in the field of AI, offering new possibilities for applications that require an understanding of the world that extends beyond text. By combining the capabilities of LLMs with the ability to process visual information, LVLMs are paving the way for more advanced and versatile AI systems. \n\nThese models have set new benchmarks in various visualcentric tasks, demonstrating their potential in enhancing the performance and applicability of AI across a broad range of domains.",
            "reference_string": "[271213320 | Yuan et al. | 2024 | Citations: 0]"
        },
        {
            "title": "OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 80,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.08686, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261949071",
                    "name": "Jialv Zou"
                },
                {
                    "authorId": "2060439659",
                    "name": "Bencheng Liao"
                },
                {
                    "authorId": "2261816376",
                    "name": "Qian Zhang"
                },
                {
                    "authorId": "2257432695",
                    "name": "Wenyu Liu"
                },
                {
                    "authorId": "2266175736",
                    "name": "Xinggang Wang"
                }
            ],
            "abstract": "Recent advancements in unified multimodal understanding and visual generation (or multimodal generation) models have been hindered by their quadratic computational complexity and dependence on large-scale training data. We present OmniMamba, the first linear-architecture-based multimodal generation model that generates both text and images through a unified next-token prediction paradigm. The model fully leverages Mamba-2's high computational and memory efficiency, extending its capabilities from text generation to multimodal generation. To address the data inefficiency of existing unified models, we propose two key innovations: (1) decoupled vocabularies to guide modality-specific generation, and (2) task-specific LoRA for parameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage training strategy to mitigate data imbalance between two tasks. Equipped with these techniques, OmniMamba achieves competitive performance with JanusFlow while surpassing Show-o across benchmarks, despite being trained on merely 2M image-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba stands out with outstanding inference efficiency, achieving up to a 119.2 times speedup and 63% GPU memory reduction for long-sequence generation compared to Transformer-based counterparts. Code and models are released at https://github.com/hustvl/OmniMamba",
            "corpus_id": 276928288,
            "sentences": [
                {
                    "corpus_id": "276928288",
                    "title": "OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models",
                    "text": "In recent years, Large Language Models (LLMs) [2,5,15,59,60] have achieved remarkable advancements, igniting significant research interest in extending their fundamental capabilities to the visual domain. Consequently, researchers have developed a series of Multimodal Large Language Models (MLLMs) for tasks such as multimodal understanding [42,43,75,77] and visual generation [31,55]. \n\nRecent studies have emerged that seek to integrate multimodal understanding with visual generation, aiming to develop unified systems capable of handling both tasks simultaneously. Such designs hold the potential to foster mutual enhancement between generation and understanding, offering a promising pathway toward truly unifying all modalities. Numerous studies have sought to preserve the text generation paradigm of LLMs while exploring the impact [46,64,66,67] of integrating diverse visual generation paradigms, such as diffusion models [24], flow-based generative models [16,40], and vector-quantized autoregressive models [56]. \n\nUnfortunately, the significant domain gap between image and text presents a critical challenge for unified multimodal generative models: preserving generation capabilities without degrading understanding performance requires an extensive volume of image-text pairs for training, as illustrated in Fig. 1. This not only leads to poor training efficiency but also creates a substantial barrier to the broader development of such models, as only a small fraction of researchers possess the resources to undertake such computationally demanding studies. Moreover, most existing unified multimodal generative models rely on Transformerbased LLMs [61]. However, their quadratic computational complexity results in slow inference speeds, rendering them less practical for real-time applications. \n\nThe challenges faced by existing unified multimodal generative models naturally lead us to ponder: can a model be developed that achieves both training efficiency and inference efficiency? \n\nTo address this, we introduce OmniMamba, a novel unified multimodal generative model that requires only 2M image-text pairs for training. Built on the Mamba-2-1.3B [10] model as the foundational LLM with a unified next token prediction paradigm to generate all modali-ties, OmniMamba leverages the linear computational complexity of state space models (SSMs) to achieve significantly faster inference speeds.",
                    "score": 0.47421368774414,
                    "section_title": "Introduction",
                    "char_start_offset": 624,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 204
                        },
                        {
                            "start": 205,
                            "end": 386
                        },
                        {
                            "start": 389,
                            "end": 569
                        },
                        {
                            "start": 570,
                            "end": 735
                        },
                        {
                            "start": 736,
                            "end": 1024
                        },
                        {
                            "start": 1027,
                            "end": 1331
                        },
                        {
                            "start": 1332,
                            "end": 1576
                        },
                        {
                            "start": 1577,
                            "end": 1673
                        },
                        {
                            "start": 1674,
                            "end": 1815
                        },
                        {
                            "start": 1818,
                            "end": 2006
                        },
                        {
                            "start": 2009,
                            "end": 2146
                        },
                        {
                            "start": 2147,
                            "end": 2172
                        },
                        {
                            "start": 2173,
                            "end": 2417
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 49,
                            "end": 51,
                            "matchedPaperCorpusId": "247951931"
                        },
                        {
                            "start": 342,
                            "end": 346,
                            "matchedPaperCorpusId": "263672058"
                        },
                        {
                            "start": 346,
                            "end": 349,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 378,
                            "end": 382,
                            "matchedPaperCorpusId": "258959284"
                        },
                        {
                            "start": 382,
                            "end": 385,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 967,
                            "end": 971,
                            "matchedPaperCorpusId": "268247980"
                        },
                        {
                            "start": 1668,
                            "end": 1672,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96435546875
                }
            ],
            "relevance_judgement": 0.96435546875,
            "relevance_judgment_input_expanded": "# Title: OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models\n# Venue: arXiv.org\n# Authors: Jialv Zou, Bencheng Liao, Qian Zhang, Wenyu Liu, Xinggang Wang\n## Abstract\nRecent advancements in unified multimodal understanding and visual generation (or multimodal generation) models have been hindered by their quadratic computational complexity and dependence on large-scale training data. We present OmniMamba, the first linear-architecture-based multimodal generation model that generates both text and images through a unified next-token prediction paradigm. The model fully leverages Mamba-2's high computational and memory efficiency, extending its capabilities from text generation to multimodal generation. To address the data inefficiency of existing unified models, we propose two key innovations: (1) decoupled vocabularies to guide modality-specific generation, and (2) task-specific LoRA for parameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage training strategy to mitigate data imbalance between two tasks. Equipped with these techniques, OmniMamba achieves competitive performance with JanusFlow while surpassing Show-o across benchmarks, despite being trained on merely 2M image-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba stands out with outstanding inference efficiency, achieving up to a 119.2 times speedup and 63% GPU memory reduction for long-sequence generation compared to Transformer-based counterparts. Code and models are released at https://github.com/hustvl/OmniMamba\n## Introduction\nIn recent years, Large Language Models (LLMs) [2,5,15,59,60] have achieved remarkable advancements, igniting significant research interest in extending their fundamental capabilities to the visual domain. Consequently, researchers have developed a series of Multimodal Large Language Models (MLLMs) for tasks such as multimodal understanding [42,43,75,77] and visual generation [31,55]. \n\nRecent studies have emerged that seek to integrate multimodal understanding with visual generation, aiming to develop unified systems capable of handling both tasks simultaneously. Such designs hold the potential to foster mutual enhancement between generation and understanding, offering a promising pathway toward truly unifying all modalities. Numerous studies have sought to preserve the text generation paradigm of LLMs while exploring the impact [46,64,66,67] of integrating diverse visual generation paradigms, such as diffusion models [24], flow-based generative models [16,40], and vector-quantized autoregressive models [56]. \n\nUnfortunately, the significant domain gap between image and text presents a critical challenge for unified multimodal generative models: preserving generation capabilities without degrading understanding performance requires an extensive volume of image-text pairs for training, as illustrated in Fig. 1. This not only leads to poor training efficiency but also creates a substantial barrier to the broader development of such models, as only a small fraction of researchers possess the resources to undertake such computationally demanding studies. Moreover, most existing unified multimodal generative models rely on Transformerbased LLMs [61]. However, their quadratic computational complexity results in slow inference speeds, rendering them less practical for real-time applications. \n\nThe challenges faced by existing unified multimodal generative models naturally lead us to ponder: can a model be developed that achieves both training efficiency and inference efficiency? \n\nTo address this, we introduce OmniMamba, a novel unified multimodal generative model that requires only 2M image-text pairs for training. Built on the Mamba-2-1.3B [10] model as the foundational LLM with a unified next token prediction paradigm to generate all modali-ties, OmniMamba leverages the linear computational complexity of state space models (SSMs) to achieve significantly faster inference speeds.",
            "reference_string": "[276928288 | Zou et al. | 2025 | Citations: 1]"
        },
        {
            "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.13388, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2289660121",
                    "name": "Wei Li"
                },
                {
                    "authorId": "2118734325",
                    "name": "Xue Xu"
                },
                {
                    "authorId": "2288740225",
                    "name": "Jiachen Liu"
                },
                {
                    "authorId": "2288393805",
                    "name": "Xinyan Xiao"
                }
            ],
            "abstract": "Existing text-to-image diffusion models primarily generate images from text prompts. However, the inherent conciseness of textual descriptions poses challenges in faithfully synthesizing images with intricate details, such as specific entities or scenes. This paper presents UNIMO-G, a simple multimodal conditional diffusion framework that operates on multimodal prompts with interleaved textual and visual inputs, which demonstrates a unified ability for both text-driven and subject-driven image generation. UNIMO-G comprises two core components: a Multimodal Large Language Model (MLLM) for encoding multimodal prompts, and a conditional denoising diffusion network for generating images based on the encoded multimodal input. We leverage a two-stage training strategy to effectively train the framework: firstly pre-training on large-scale text-image pairs to develop conditional image generation capabilities, and then instruction tuning with multimodal prompts to achieve unified image generation proficiency. A well-designed data processing pipeline involving language grounding and image segmentation is employed to construct multi-modal prompts. UNIMO-G excels in both text-to-image generation and zero-shot subject-driven synthesis, and is notably effective in generating high-fidelity images from complex multimodal prompts involving multiple image entities.",
            "corpus_id": 267200040,
            "sentences": [
                {
                    "corpus_id": "267200040",
                    "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion",
                    "text": "Multimodal Large Language Models (MLLMs) have significantly broadened the capabilities of language models to process various modalities (Liu et al., 2023a;Li et al., 2021Li et al., , 2022a;;Wang et al., 2023;Driess et al., 2023). These models inherently facilitate interleaved vision-language input, effectively handling multiple images. Models such as GILL (Koh et al., 2023), Emu (Sun et al., 2023), and DreamLLM (Dong et al., 2023) specialize in interleaved vision-language generation by aligning the output space of MLLMs with the diffusion image decoder. However, these methods primarily align at a semantic level and may struggle with detailed, subject-driven image generation. BLIP-Diffusion (Li et al., 2023a) synthesizes images by composing subjects with random backgrounds, endowing it with zero-shot, subject-driven text-toimage generation capabilities. However, its specific input template and training process limit scalability for multiple entities. KOSMOS-G (Pan et al., 2023), a model closely related to our work, leverages a MLLM to encode interleaved text-visual inputs, and the U-Net of Stable Diffusion (SD) v1.5 as the image decoder. The key component of KOSMOS-G is an AlignerNet, which is trained solely on textual data to align the output embedding space of the frozen SDv1.5 U-Net with the MLLM. In contrast, our approach centers on training the U-Net model end-to-end specifically for multimodal diffusion, significantly enhancing both the faithfulness and relevance of generated images in multimodal contexts. Differing from alignmentbased approaches, our two-stage training strategy markedly improves the model's proficiency in following multimodal instructions, particularly in complex multi-entity scenarios.",
                    "score": 0.4567096511985207,
                    "section_title": "Generating with Multi-modal Language Models",
                    "char_start_offset": 14730,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 337
                        },
                        {
                            "start": 338,
                            "end": 559
                        },
                        {
                            "start": 560,
                            "end": 683
                        },
                        {
                            "start": 684,
                            "end": 864
                        },
                        {
                            "start": 865,
                            "end": 963
                        },
                        {
                            "start": 964,
                            "end": 1154
                        },
                        {
                            "start": 1155,
                            "end": 1320
                        },
                        {
                            "start": 1321,
                            "end": 1536
                        },
                        {
                            "start": 1537,
                            "end": 1738
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 155,
                            "end": 170,
                            "matchedPaperCorpusId": "229924402"
                        },
                        {
                            "start": 170,
                            "end": 190,
                            "matchedPaperCorpusId": "247519008"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96435546875
                }
            ],
            "relevance_judgement": 0.96435546875,
            "relevance_judgment_input_expanded": "# Title: UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Wei Li, Xue Xu, Jiachen Liu, Xinyan Xiao\n## Abstract\nExisting text-to-image diffusion models primarily generate images from text prompts. However, the inherent conciseness of textual descriptions poses challenges in faithfully synthesizing images with intricate details, such as specific entities or scenes. This paper presents UNIMO-G, a simple multimodal conditional diffusion framework that operates on multimodal prompts with interleaved textual and visual inputs, which demonstrates a unified ability for both text-driven and subject-driven image generation. UNIMO-G comprises two core components: a Multimodal Large Language Model (MLLM) for encoding multimodal prompts, and a conditional denoising diffusion network for generating images based on the encoded multimodal input. We leverage a two-stage training strategy to effectively train the framework: firstly pre-training on large-scale text-image pairs to develop conditional image generation capabilities, and then instruction tuning with multimodal prompts to achieve unified image generation proficiency. A well-designed data processing pipeline involving language grounding and image segmentation is employed to construct multi-modal prompts. UNIMO-G excels in both text-to-image generation and zero-shot subject-driven synthesis, and is notably effective in generating high-fidelity images from complex multimodal prompts involving multiple image entities.\n## Generating with Multi-modal Language Models\nMultimodal Large Language Models (MLLMs) have significantly broadened the capabilities of language models to process various modalities (Liu et al., 2023a;Li et al., 2021Li et al., , 2022a;;Wang et al., 2023;Driess et al., 2023). These models inherently facilitate interleaved vision-language input, effectively handling multiple images. Models such as GILL (Koh et al., 2023), Emu (Sun et al., 2023), and DreamLLM (Dong et al., 2023) specialize in interleaved vision-language generation by aligning the output space of MLLMs with the diffusion image decoder. However, these methods primarily align at a semantic level and may struggle with detailed, subject-driven image generation. BLIP-Diffusion (Li et al., 2023a) synthesizes images by composing subjects with random backgrounds, endowing it with zero-shot, subject-driven text-toimage generation capabilities. However, its specific input template and training process limit scalability for multiple entities. KOSMOS-G (Pan et al., 2023), a model closely related to our work, leverages a MLLM to encode interleaved text-visual inputs, and the U-Net of Stable Diffusion (SD) v1.5 as the image decoder. The key component of KOSMOS-G is an AlignerNet, which is trained solely on textual data to align the output embedding space of the frozen SDv1.5 U-Net with the MLLM. In contrast, our approach centers on training the U-Net model end-to-end specifically for multimodal diffusion, significantly enhancing both the faithfulness and relevance of generated images in multimodal contexts. Differing from alignmentbased approaches, our two-stage training strategy markedly improves the model's proficiency in following multimodal instructions, particularly in complex multi-entity scenarios.",
            "reference_string": "[267200040 | Li et al. | 2024 | Citations: 8]"
        },
        {
            "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 57,
            "citation_count": 15,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.13861, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144484147",
                    "name": "Rongyao Fang"
                },
                {
                    "authorId": "2326302007",
                    "name": "Chengqi Duan"
                },
                {
                    "authorId": "2267252383",
                    "name": "Kun Wang"
                },
                {
                    "authorId": "2274232642",
                    "name": "Hao Li"
                },
                {
                    "authorId": "2274118690",
                    "name": "Hao Tian"
                },
                {
                    "authorId": "2257433221",
                    "name": "Xingyu Zeng"
                },
                {
                    "authorId": "2319817758",
                    "name": "Rui Zhao"
                },
                {
                    "authorId": "2292283383",
                    "name": "Jifeng Dai"
                },
                {
                    "authorId": "2268799275",
                    "name": "Hongsheng Li"
                },
                {
                    "authorId": "2286520347",
                    "name": "Xihui Liu"
                }
            ],
            "abstract": "Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm - from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA.",
            "corpus_id": 273403986,
            "sentences": [
                {
                    "corpus_id": "273403986",
                    "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
                    "text": "Unifying multimodal understanding and generation capabilities within a single model is a critical milestone toward artificial general intelligence (AGI). Towards this goal, recent advancements (Liu et al., 2024b;Zhu et al., 2023a) in multimodal large language models (MLLMs) have made significant progress in integrating visual reasoning and understanding with natural language interfaces. However, developing a unified framework that excels at both comprehending and generating multimodal content remains a significant challenge in the field of artificial intelligence. \n\nRecent studies (Sun et al., 2023;Ge et al., 2024b) have explored MLLM's potential for visual generation, beyond the previously well-explored visual understanding and reasoning with MLLMs. These approaches enable MLLMs to process image-text inputs and produce either textual outputs or semantic-level visual tokens. In the case of image generation, these visual tokens are subsequently transformed into pixel-space images using diffusion-based decoders. Such unified frameworks empower MLLMs to perform a wide spectrum of tasks within a single framework, ranging from detailed visual analysis to creative image synthesis. \n\nHowever, existing MLLM-based methods (Sun et al., 2023;2024b) face a common challenge in the trade-off between diversity for text-to-image generation and high controllability for tasks such as image editing. Previous methods mostly rely on single-granular features extracted from a visual encoder and neglect the varying granularity requirements of different tasks. On the one hand, generating diverse images reflecting the real world from text descriptions requires features that encode coarse semantic concepts. Such features are fed as conditions into the diffusion-based image decoder, allowing the diffusion model to generate diverse images that semantically align with the text prompt. On the other hand, tasks demanding precise control over output images, such as image editing and inpainting, require the LLMs to predict fine-grained features that encode rich, detailed visual information for the image decoder. This dichotomy presents a significant challenge for current MLLM-based methods, which typically generate single-granular feature representations for all tasks.",
                    "score": 0.5622713842952402,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 1200,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 153
                        },
                        {
                            "start": 154,
                            "end": 389
                        },
                        {
                            "start": 390,
                            "end": 570
                        },
                        {
                            "start": 573,
                            "end": 760
                        },
                        {
                            "start": 761,
                            "end": 887
                        },
                        {
                            "start": 888,
                            "end": 1025
                        },
                        {
                            "start": 1026,
                            "end": 1193
                        },
                        {
                            "start": 1196,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1561
                        },
                        {
                            "start": 1562,
                            "end": 1709
                        },
                        {
                            "start": 1710,
                            "end": 1887
                        },
                        {
                            "start": 1888,
                            "end": 2115
                        },
                        {
                            "start": 2116,
                            "end": 2275
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 193,
                            "end": 212,
                            "matchedPaperCorpusId": "258179774"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9638671875
                }
            ],
            "relevance_judgement": 0.9638671875,
            "relevance_judgment_input_expanded": "# Title: PUMA: Empowering Unified MLLM with Multi-granular Visual Generation\n# Venue: arXiv.org\n# Authors: Rongyao Fang, Chengqi Duan, Kun Wang, Hao Li, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Hongsheng Li, Xihui Liu\n## Abstract\nRecent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm - from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA.\n## INTRODUCTION\nUnifying multimodal understanding and generation capabilities within a single model is a critical milestone toward artificial general intelligence (AGI). Towards this goal, recent advancements (Liu et al., 2024b;Zhu et al., 2023a) in multimodal large language models (MLLMs) have made significant progress in integrating visual reasoning and understanding with natural language interfaces. However, developing a unified framework that excels at both comprehending and generating multimodal content remains a significant challenge in the field of artificial intelligence. \n\nRecent studies (Sun et al., 2023;Ge et al., 2024b) have explored MLLM's potential for visual generation, beyond the previously well-explored visual understanding and reasoning with MLLMs. These approaches enable MLLMs to process image-text inputs and produce either textual outputs or semantic-level visual tokens. In the case of image generation, these visual tokens are subsequently transformed into pixel-space images using diffusion-based decoders. Such unified frameworks empower MLLMs to perform a wide spectrum of tasks within a single framework, ranging from detailed visual analysis to creative image synthesis. \n\nHowever, existing MLLM-based methods (Sun et al., 2023;2024b) face a common challenge in the trade-off between diversity for text-to-image generation and high controllability for tasks such as image editing. Previous methods mostly rely on single-granular features extracted from a visual encoder and neglect the varying granularity requirements of different tasks. On the one hand, generating diverse images reflecting the real world from text descriptions requires features that encode coarse semantic concepts. Such features are fed as conditions into the diffusion-based image decoder, allowing the diffusion model to generate diverse images that semantically align with the text prompt. On the other hand, tasks demanding precise control over output images, such as image editing and inpainting, require the LLMs to predict fine-grained features that encode rich, detailed visual information for the image decoder. This dichotomy presents a significant challenge for current MLLM-based methods, which typically generate single-granular feature representations for all tasks.",
            "reference_string": "[273403986 | Fang et al. | 2024 | Citations: 15]"
        },
        {
            "title": "Survey of Different Large Language Model Architectures: Trends, Benchmarks, and Challenges",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 21,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3482107",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.03220, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284694862",
                    "name": "Minghao Shao"
                },
                {
                    "authorId": "2289835770",
                    "name": "Abdul Basit"
                },
                {
                    "authorId": "2285311151",
                    "name": "Ramesh Karri"
                },
                {
                    "authorId": "2238832044",
                    "name": "Muhammad Shafique"
                }
            ],
            "abstract": "Large Language Models (LLMs) represent a class of deep learning models adept at understanding natural language and generating coherent responses to various prompts or queries. These models far exceed the complexity of conventional neural networks, often encompassing dozens of neural network layers and containing billions to trillions of parameters. They are typically trained on vast datasets, utilizing architectures based on transformer blocks. Present-day LLMs are multi-functional, capable of performing a range of tasks from text generation and language translation to question answering, as well as code generation and analysis. An advanced subset of these models, known as Multimodal Large Language Models (MLLMs), extends LLM capabilities to process and interpret multiple data modalities, including images, audio, and video. This enhancement empowers MLLMs with capabilities like video editing, image comprehension, and captioning for visual content. This survey provides a comprehensive overview of the recent advancements in LLMs. We begin by tracing the evolution of LLMs and subsequently delve into the advent and nuances of MLLMs. We analyze emerging state-of-the-art MLLMs, exploring their technical features, strengths, and limitations. Additionally, we present a comparative analysis of these models and discuss their challenges, potential limitations, and prospects for future development.",
            "corpus_id": 273406244,
            "sentences": [],
            "relevance_judgement": 0.9638671875,
            "relevance_judgment_input_expanded": "# Title: Survey of Different Large Language Model Architectures: Trends, Benchmarks, and Challenges\n# Venue: IEEE Access\n# Authors: Minghao Shao, Abdul Basit, Ramesh Karri, Muhammad Shafique\n## Abstract\nLarge Language Models (LLMs) represent a class of deep learning models adept at understanding natural language and generating coherent responses to various prompts or queries. These models far exceed the complexity of conventional neural networks, often encompassing dozens of neural network layers and containing billions to trillions of parameters. They are typically trained on vast datasets, utilizing architectures based on transformer blocks. Present-day LLMs are multi-functional, capable of performing a range of tasks from text generation and language translation to question answering, as well as code generation and analysis. An advanced subset of these models, known as Multimodal Large Language Models (MLLMs), extends LLM capabilities to process and interpret multiple data modalities, including images, audio, and video. This enhancement empowers MLLMs with capabilities like video editing, image comprehension, and captioning for visual content. This survey provides a comprehensive overview of the recent advancements in LLMs. We begin by tracing the evolution of LLMs and subsequently delve into the advent and nuances of MLLMs. We analyze emerging state-of-the-art MLLMs, exploring their technical features, strengths, and limitations. Additionally, we present a comparative analysis of these models and discuss their challenges, potential limitations, and prospects for future development.\n",
            "reference_string": "[273406244 | Shao et al. | 2024 | Citations: 21]"
        },
        {
            "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 81,
            "citation_count": 229,
            "influential_citation_count": 32,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.12528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306603347",
                    "name": "Jinheng Xie"
                },
                {
                    "authorId": "2258958963",
                    "name": "Weijia Mao"
                },
                {
                    "authorId": "2237427303",
                    "name": "Zechen Bai"
                },
                {
                    "authorId": "2248388634",
                    "name": "David Junhao Zhang"
                },
                {
                    "authorId": "2316668014",
                    "name": "Weihao Wang"
                },
                {
                    "authorId": "2298569956",
                    "name": "Kevin Qinghong Lin"
                },
                {
                    "authorId": "2248678348",
                    "name": "Yuchao Gu"
                },
                {
                    "authorId": "2316662510",
                    "name": "Zhijie Chen"
                },
                {
                    "authorId": "2309246065",
                    "name": "Zhenheng Yang"
                },
                {
                    "authorId": "2244623976",
                    "name": "Mike Zheng Shou"
                }
            ],
            "abstract": "We present a unified transformer, i.e., Show-o, that unifies multimodal understanding and generation. Unlike fully autoregressive models, Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities. The unified model flexibly supports a wide range of vision-language tasks including visual question-answering, text-to-image generation, text-guided inpainting/extrapolation, and mixed-modality generation. Across various benchmarks, it demonstrates comparable or superior performance to existing individual models with an equivalent or larger number of parameters tailored for understanding or generation. This significantly highlights its potential as a next-generation foundation model. Code and models are released at https://github.com/showlab/Show-o.",
            "corpus_id": 271924334,
            "sentences": [
                {
                    "corpus_id": "271924334",
                    "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation",
                    "text": "In recent years, an increasing number of studies (Ge et al., 2024;Wu et al., 2023b;Tang et al., 2024;Ye et al., 2024a;Dong et al., 2024;Aiello et al., 2024) have focused on unified multimodal language models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;Sun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive modeling to generate images. SEED-X (Ge et al., 2024) proposes a unified and versatile foundation system capable of handling both multimodal understanding and generation tasks. In this approach, continuous image representations from CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model (LLM) to perform next-word prediction and image representation regression. DreamLLM (Dong et al., 2024) also explores the potential of enabling multimodal comprehension and creation based on LLMs. Chameleon (Team, 2024) introduces a family of token-based mixed-modal models capable of both comprehending and generating images. This approach represents all modalities as discrete tokens and utilizes a unified transformer-based architecture and trains the model from scratch in an end-to-end manner. Compared to this work, we also adopt discrete tokens to represent all modalities. In contrast, we utilize a discrete diffusion process instead of autoregressive modeling for visual generation.",
                    "score": 0.5245112408502469,
                    "section_title": "UNIFIED VISION-LANGUAGE FOUNDATION MODEL",
                    "char_start_offset": 11116,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 253
                        },
                        {
                            "start": 254,
                            "end": 414
                        },
                        {
                            "start": 415,
                            "end": 562
                        },
                        {
                            "start": 563,
                            "end": 799
                        },
                        {
                            "start": 800,
                            "end": 921
                        },
                        {
                            "start": 922,
                            "end": 1051
                        },
                        {
                            "start": 1052,
                            "end": 1223
                        },
                        {
                            "start": 1224,
                            "end": 1305
                        },
                        {
                            "start": 1306,
                            "end": 1416
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 83,
                            "end": 101,
                            "matchedPaperCorpusId": "258822817"
                        },
                        {
                            "start": 118,
                            "end": 136,
                            "matchedPaperCorpusId": "261975252"
                        },
                        {
                            "start": 636,
                            "end": 658,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 809,
                            "end": 827,
                            "matchedPaperCorpusId": "261975252"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9619140625
                }
            ],
            "relevance_judgement": 0.9619140625,
            "relevance_judgment_input_expanded": "# Title: Show-o: One Single Transformer to Unify Multimodal Understanding and Generation\n# Venue: International Conference on Learning Representations\n# Authors: Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, Mike Zheng Shou\n## Abstract\nWe present a unified transformer, i.e., Show-o, that unifies multimodal understanding and generation. Unlike fully autoregressive models, Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities. The unified model flexibly supports a wide range of vision-language tasks including visual question-answering, text-to-image generation, text-guided inpainting/extrapolation, and mixed-modality generation. Across various benchmarks, it demonstrates comparable or superior performance to existing individual models with an equivalent or larger number of parameters tailored for understanding or generation. This significantly highlights its potential as a next-generation foundation model. Code and models are released at https://github.com/showlab/Show-o.\n## UNIFIED VISION-LANGUAGE FOUNDATION MODEL\nIn recent years, an increasing number of studies (Ge et al., 2024;Wu et al., 2023b;Tang et al., 2024;Ye et al., 2024a;Dong et al., 2024;Aiello et al., 2024) have focused on unified multimodal language models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;Sun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive modeling to generate images. SEED-X (Ge et al., 2024) proposes a unified and versatile foundation system capable of handling both multimodal understanding and generation tasks. In this approach, continuous image representations from CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model (LLM) to perform next-word prediction and image representation regression. DreamLLM (Dong et al., 2024) also explores the potential of enabling multimodal comprehension and creation based on LLMs. Chameleon (Team, 2024) introduces a family of token-based mixed-modal models capable of both comprehending and generating images. This approach represents all modalities as discrete tokens and utilizes a unified transformer-based architecture and trains the model from scratch in an end-to-end manner. Compared to this work, we also adopt discrete tokens to represent all modalities. In contrast, we utilize a discrete diffusion process instead of autoregressive modeling for visual generation.",
            "reference_string": "[271924334 | Xie et al. | 2024 | Citations: 229]"
        },
        {
            "title": "A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 113,
            "citation_count": 36,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.01319, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2136025369",
                    "name": "Jiaqi Wang"
                },
                {
                    "authorId": "2273631049",
                    "name": "Hanqi Jiang"
                },
                {
                    "authorId": "2116426849",
                    "name": "Yi-Hsueh Liu"
                },
                {
                    "authorId": "120688117",
                    "name": "Chong-Yi Ma"
                },
                {
                    "authorId": "2273584640",
                    "name": "Xu Zhang"
                },
                {
                    "authorId": "2221032216",
                    "name": "Yi Pan"
                },
                {
                    "authorId": "2210636248",
                    "name": "Mengyuan Liu"
                },
                {
                    "authorId": "2314692435",
                    "name": "Peiran Gu"
                },
                {
                    "authorId": "2314692233",
                    "name": "Sichen Xia"
                },
                {
                    "authorId": "2284031962",
                    "name": "Wenjun Li"
                },
                {
                    "authorId": "2257095790",
                    "name": "Yutong Zhang"
                },
                {
                    "authorId": "2238905102",
                    "name": "Zihao Wu"
                },
                {
                    "authorId": "2145977326",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "2215167446",
                    "name": "Tianyang Zhong"
                },
                {
                    "authorId": "2257302793",
                    "name": "Bao Ge"
                },
                {
                    "authorId": "2269508672",
                    "name": "Tuo Zhang"
                },
                {
                    "authorId": "2251076040",
                    "name": "Ning Qiang"
                },
                {
                    "authorId": "1742535",
                    "name": "Xintao Hu"
                },
                {
                    "authorId": "2300922460",
                    "name": "Xi Jiang"
                },
                {
                    "authorId": "2290203999",
                    "name": "Xin Zhang"
                },
                {
                    "authorId": "2157449203",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "2272672300",
                    "name": "Dinggang Shen"
                },
                {
                    "authorId": "2301126629",
                    "name": "Tianming Liu"
                },
                {
                    "authorId": "2277750447",
                    "name": "Shu Zhang"
                }
            ],
            "abstract": "In an era defined by the explosive growth of data and rapid technological advancements, Multimodal Large Language Models (MLLMs) stand at the forefront of artificial intelligence (AI) systems. Designed to seamlessly integrate diverse data types-including text, images, videos, audio, and physiological sequences-MLLMs address the complexities of real-world applications far beyond the capabilities of single-modality systems. In this paper, we systematically sort out the applications of MLLM in multimodal tasks such as natural language, vision, and audio. We also provide a comparative analysis of the focus of different MLLMs in the tasks, and provide insights into the shortcomings of current MLLMs, and suggest potential directions for future research. Through these discussions, this paper hopes to provide valuable insights for the further development and application of MLLM.",
            "corpus_id": 271693411,
            "sentences": [
                {
                    "corpus_id": "271693411",
                    "title": "A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks",
                    "text": "These fusion processes often employ pre-trained LLMs, which, while initially designed for textual data, are adapted to handle and synthesize multimodal inputs through advanced feature projection and serialization techniques. \n\nMultimodal Output Decoder: Lastly, the multimodal output decoder reconverts the fused, integrated multimodal information back into a usable form tailored to specific tasks, such as Image captioning, the decoder might generate descriptive text based on visual inputs. Video understanding tasks, it could produce annotations or summaries combining both visual and auditory data. Each decoder is meticulously designed to optimize accuracy and quality, ensuring that the output precisely reflects the combined insights gained from the integrated modalities. \n\nTo summarize, the sophisticated architecture of multimodal large language models empowers them to tackle complex tasks by harnessing and synthesizing data across text, images, and audio. This capability not only enhances the performance of AI applications but also opens up new avenues for innovation in how we understand and interact with technology.",
                    "score": 0.5088131217178818,
                    "section_title": "B. Main Components of Multimodal Large Language Models",
                    "char_start_offset": 11790,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 224
                        },
                        {
                            "start": 227,
                            "end": 493
                        },
                        {
                            "start": 494,
                            "end": 603
                        },
                        {
                            "start": 604,
                            "end": 780
                        },
                        {
                            "start": 783,
                            "end": 969
                        },
                        {
                            "start": 970,
                            "end": 1134
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9619140625
                }
            ],
            "relevance_judgement": 0.9619140625,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks\n# Venue: arXiv.org\n# Authors: Jiaqi Wang, Hanqi Jiang, Yi-Hsueh Liu, Chong-Yi Ma, Xu Zhang, Yi Pan, Mengyuan Liu, Peiran Gu, Sichen Xia, Wenjun Li, Yutong Zhang, Zihao Wu, Zheng Liu, Tianyang Zhong, Bao Ge, Tuo Zhang, Ning Qiang, Xintao Hu, Xi Jiang, Xin Zhang, Wei Zhang, Dinggang Shen, Tianming Liu, Shu Zhang\n## Abstract\nIn an era defined by the explosive growth of data and rapid technological advancements, Multimodal Large Language Models (MLLMs) stand at the forefront of artificial intelligence (AI) systems. Designed to seamlessly integrate diverse data types-including text, images, videos, audio, and physiological sequences-MLLMs address the complexities of real-world applications far beyond the capabilities of single-modality systems. In this paper, we systematically sort out the applications of MLLM in multimodal tasks such as natural language, vision, and audio. We also provide a comparative analysis of the focus of different MLLMs in the tasks, and provide insights into the shortcomings of current MLLMs, and suggest potential directions for future research. Through these discussions, this paper hopes to provide valuable insights for the further development and application of MLLM.\n## B. Main Components of Multimodal Large Language Models\nThese fusion processes often employ pre-trained LLMs, which, while initially designed for textual data, are adapted to handle and synthesize multimodal inputs through advanced feature projection and serialization techniques. \n\nMultimodal Output Decoder: Lastly, the multimodal output decoder reconverts the fused, integrated multimodal information back into a usable form tailored to specific tasks, such as Image captioning, the decoder might generate descriptive text based on visual inputs. Video understanding tasks, it could produce annotations or summaries combining both visual and auditory data. Each decoder is meticulously designed to optimize accuracy and quality, ensuring that the output precisely reflects the combined insights gained from the integrated modalities. \n\nTo summarize, the sophisticated architecture of multimodal large language models empowers them to tackle complex tasks by harnessing and synthesizing data across text, images, and audio. This capability not only enhances the performance of AI applications but also opens up new avenues for innovation in how we understand and interact with technology.",
            "reference_string": "[271693411 | Wang et al. | 2024 | Citations: 36]"
        },
        {
            "title": "A Comprehensive Survey of Large AI Models for Future Communications: Foundations, Applications and Challenges",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 198,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.03556, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "40540111",
                    "name": "Feibo Jiang"
                },
                {
                    "authorId": "2290889674",
                    "name": "Cunhua Pan"
                },
                {
                    "authorId": "2152288497",
                    "name": "Li Dong"
                },
                {
                    "authorId": "2244014700",
                    "name": "Kezhi Wang"
                },
                {
                    "authorId": "145118318",
                    "name": "M. Debbah"
                },
                {
                    "authorId": "2340230621",
                    "name": "Dusit Niyato"
                },
                {
                    "authorId": "2267902891",
                    "name": "Zhu Han"
                }
            ],
            "abstract": "The 6G wireless communications aim to establish an intelligent world of ubiquitous connectivity, providing an unprecedented communication experience. Large artificial intelligence models (LAMs) are characterized by significantly larger scales (e.g., billions or trillions of parameters) compared to typical artificial intelligence (AI) models. LAMs exhibit outstanding cognitive abilities, including strong generalization capabilities for fine-tuning to downstream tasks, and emergent capabilities to handle tasks unseen during training. Therefore, LAMs efficiently provide AI services for diverse communication applications, making them crucial tools for addressing complex challenges in future wireless communication systems. This study provides a comprehensive review of the foundations, applications, and challenges of LAMs in communication. First, we introduce the current state of AI-based communication systems, emphasizing the motivation behind integrating LAMs into communications and summarizing the key contributions. We then present an overview of the essential concepts of LAMs in communication. This includes an introduction to the main architectures of LAMs, such as transformer, diffusion models, and mamba. We also explore the classification of LAMs, including large language models (LLMs), large vision models (LVMs), large multimodal models (LMMs), and world models, and examine their potential applications in communication. Additionally, we cover the training methods and evaluation techniques for LAMs in communication systems. Lastly, we introduce optimization strategies such as chain of thought (CoT), retrieval augmented generation (RAG), and agentic systems. Following this, we discuss the research advancements of LAMs across various communication scenarios. Finally, we analyze the challenges in the current research and provide insights into potential future research directions.",
            "corpus_id": 278339632,
            "sentences": [
                {
                    "corpus_id": "278339632",
                    "title": "A Comprehensive Survey of Large AI Models for Future Communications: Foundations, Applications and Challenges",
                    "text": "3) Large multimodal model: LMMs are capable of simultaneously processing and understanding data from different modalities, such as vision, language, haptic and auditory. These models achieve comprehensive processing and reasoning of multimodal information by integrating the features of various modalities in a unified high-dimensional space. They utilize advanced neural network architectures, such as transformers and diffusion models, to extract features from each modality and optimize their representations through techniques such as contrastive learning and self-supervised learning. By training across multiple modalities, these models are able to understand and relate the semantic relationships between different modalities, thereby demonstrating superior performance in handling complex multimodal data and providing intelligent, efficient solutions. Unlike visual language models (VLMs), LMMs support modalities beyond vision and text [90]. \n\nLMMs show strong capabilities in processing multimodal information. Their core technical features are cross-modal fusion and multimodal representation learning: \n\n\u2022 Multimodal representation learning: Multimodal representation learning is an important technology of LMMs. \n\nIt integrates feature representations of different modalities such as images, speech and text into a unified highdimensional space. First, the LMM uses ViT to extract image features and uses a transformer to extract text and speech features. Then, these high-dimensional vectors are fused through methods such as splicing and weighted summation to form a unified feature representation. This fusion enables the LMM to better understand and associate information from different modalities, improving the performance of multimodal tasks. \u2022 Cross-modal fusion: LMMs integrate multiple data types such as text, images, audio and video through cross-modal fusion technology to achieve deeper understanding and analysis. These LMMs can process data from different modalities at the same time and learn the relationship between them. For example, the LMM can combine images with related text to generate richer descriptions; in video analysis, it can understand the visual content as well as the voice and text information in the video. In addition, these LMMs can also perform cross-modal reasoning and prediction, such as generating images or audio from text. Such capabilities make LMMs widely used in NLP, computer vision, speech recognition and other fields.",
                    "score": 0.46866593378768817,
                    "section_title": "Multimodal data",
                    "char_start_offset": 37040,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 169
                        },
                        {
                            "start": 170,
                            "end": 342
                        },
                        {
                            "start": 343,
                            "end": 589
                        },
                        {
                            "start": 590,
                            "end": 860
                        },
                        {
                            "start": 861,
                            "end": 951
                        },
                        {
                            "start": 954,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1114
                        },
                        {
                            "start": 1117,
                            "end": 1225
                        },
                        {
                            "start": 1228,
                            "end": 1359
                        },
                        {
                            "start": 1360,
                            "end": 1469
                        },
                        {
                            "start": 1470,
                            "end": 1614
                        },
                        {
                            "start": 1615,
                            "end": 1763
                        },
                        {
                            "start": 1764,
                            "end": 1942
                        },
                        {
                            "start": 1943,
                            "end": 2054
                        },
                        {
                            "start": 2055,
                            "end": 2257
                        },
                        {
                            "start": 2258,
                            "end": 2382
                        },
                        {
                            "start": 2383,
                            "end": 2484
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 946,
                            "end": 950,
                            "matchedPaperCorpusId": "265351653"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9619140625
                }
            ],
            "relevance_judgement": 0.9619140625,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Survey of Large AI Models for Future Communications: Foundations, Applications and Challenges\n# Venue: arXiv.org\n# Authors: Feibo Jiang, Cunhua Pan, Li Dong, Kezhi Wang, M. Debbah, Dusit Niyato, Zhu Han\n## Abstract\nThe 6G wireless communications aim to establish an intelligent world of ubiquitous connectivity, providing an unprecedented communication experience. Large artificial intelligence models (LAMs) are characterized by significantly larger scales (e.g., billions or trillions of parameters) compared to typical artificial intelligence (AI) models. LAMs exhibit outstanding cognitive abilities, including strong generalization capabilities for fine-tuning to downstream tasks, and emergent capabilities to handle tasks unseen during training. Therefore, LAMs efficiently provide AI services for diverse communication applications, making them crucial tools for addressing complex challenges in future wireless communication systems. This study provides a comprehensive review of the foundations, applications, and challenges of LAMs in communication. First, we introduce the current state of AI-based communication systems, emphasizing the motivation behind integrating LAMs into communications and summarizing the key contributions. We then present an overview of the essential concepts of LAMs in communication. This includes an introduction to the main architectures of LAMs, such as transformer, diffusion models, and mamba. We also explore the classification of LAMs, including large language models (LLMs), large vision models (LVMs), large multimodal models (LMMs), and world models, and examine their potential applications in communication. Additionally, we cover the training methods and evaluation techniques for LAMs in communication systems. Lastly, we introduce optimization strategies such as chain of thought (CoT), retrieval augmented generation (RAG), and agentic systems. Following this, we discuss the research advancements of LAMs across various communication scenarios. Finally, we analyze the challenges in the current research and provide insights into potential future research directions.\n## Multimodal data\n3) Large multimodal model: LMMs are capable of simultaneously processing and understanding data from different modalities, such as vision, language, haptic and auditory. These models achieve comprehensive processing and reasoning of multimodal information by integrating the features of various modalities in a unified high-dimensional space. They utilize advanced neural network architectures, such as transformers and diffusion models, to extract features from each modality and optimize their representations through techniques such as contrastive learning and self-supervised learning. By training across multiple modalities, these models are able to understand and relate the semantic relationships between different modalities, thereby demonstrating superior performance in handling complex multimodal data and providing intelligent, efficient solutions. Unlike visual language models (VLMs), LMMs support modalities beyond vision and text [90]. \n\nLMMs show strong capabilities in processing multimodal information. Their core technical features are cross-modal fusion and multimodal representation learning: \n\n\u2022 Multimodal representation learning: Multimodal representation learning is an important technology of LMMs. \n\nIt integrates feature representations of different modalities such as images, speech and text into a unified highdimensional space. First, the LMM uses ViT to extract image features and uses a transformer to extract text and speech features. Then, these high-dimensional vectors are fused through methods such as splicing and weighted summation to form a unified feature representation. This fusion enables the LMM to better understand and associate information from different modalities, improving the performance of multimodal tasks. \u2022 Cross-modal fusion: LMMs integrate multiple data types such as text, images, audio and video through cross-modal fusion technology to achieve deeper understanding and analysis. These LMMs can process data from different modalities at the same time and learn the relationship between them. For example, the LMM can combine images with related text to generate richer descriptions; in video analysis, it can understand the visual content as well as the voice and text information in the video. In addition, these LMMs can also perform cross-modal reasoning and prediction, such as generating images or audio from text. Such capabilities make LMMs widely used in NLP, computer vision, speech recognition and other fields.",
            "reference_string": "[278339632 | Jiang et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 145,
            "citation_count": 23,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.16602, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115941903",
                    "name": "Jiaxing Huang"
                },
                {
                    "authorId": "2276743977",
                    "name": "Jingyi Zhang"
                },
                {
                    "authorId": "2276609830",
                    "name": "Kai Jiang"
                },
                {
                    "authorId": "49660254",
                    "name": "Han Qiu"
                },
                {
                    "authorId": "2237947102",
                    "name": "Shijian Lu"
                }
            ],
            "abstract": "Traditional computer vision generally solves each single task independently by a dedicated model with the task instruction implicitly designed in the model architecture, arising two limitations: (1) it leads to task-specific models, which require multiple models for different tasks and restrict the potential synergies from diverse tasks; (2) it leads to a pre-defined and fixed model interface that has limited interactivity and adaptability in following user' task instructions. To address them, Visual Instruction Tuning (VIT) has been intensively studied recently, which finetunes a large vision model with language as task instructions, aiming to learn from a wide range of vision tasks described by language instructions a general-purpose multimodal model that can follow arbitrary instructions and thus solve arbitrary tasks specified by the user. This work aims to provide a systematic review of visual instruction tuning, covering (1) the background that presents computer vision task paradigms and the development of VIT; (2) the foundations of VIT that introduce commonly used network architectures, visual instruction tuning frameworks and objectives, and evaluation setups and tasks; (3) the commonly used datasets in visual instruction tuning and evaluation; (4) the review of existing VIT methods that categorizes them with a taxonomy according to both the studied vision task and the method design and highlights the major contributions, strengths, and shortcomings of them; (5) the comparison and discussion of VIT methods over various instruction-following benchmarks; (6) several challenges, open directions and possible future works in visual instruction tuning research.",
            "corpus_id": 266573642,
            "sentences": [
                {
                    "corpus_id": "266573642",
                    "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
                    "text": "GPT4Tools [63] enables open-source language models to effectively use multimodal tools. It constructs a tool-related instructional dataset from advanced language models and utilizes Low-Rank Adaptation (LoRA) optimization to enhance the language models' tool-usage capabilities. Additionally, it proposes a benchmark to evaluate the accuracy of language models in using tools, demonstrating significant improvements in tool usage across various visual tasks. As shown in Figure 17, the GPT4Tools framework involves constructing a tool-related instruction dataset by prompting an advanced language model with various multimodal contexts. This dataset is then used to fine-tune open-source language models using Low-Rank Adaptation (LoRA) optimization, enabling them to effectively use tools for visual tasks such as comprehension and image generation. Additionally, the framework includes a benchmark to evaluate the language models' ability to use tools, showcasing significant improvements in tool usage accuracy. TextBind [72] enhances large language models with multi-turn interleaved multimodal instruction-following capabilities. It significantly reduces the need for high-quality exemplar data, making it more accessible and scalable for real-world tasks. The proposed model, MIM, trained on TextBind, outperforms recent baselines in open-world multimodal conversations, demonstrating remarkable performance in textual response generation, image generation, and overall multimodal instruction-following. As shown in Figure 18, MIM seamlessly integrates image encoder and decoder models to accommodate interleaved image-text inputs and outputs. It supplements large language models with visual input and output modules, enabling the model to process multi-turn interleaved multimodal instructions and generate coherent responses. The architecture is trained in two stages, focusing on aligning the feature spaces of vision and language models and further improving instructionfollowing capabilities.",
                    "score": 0.4601428247175749,
                    "section_title": "Image Generation",
                    "char_start_offset": 47568,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 87
                        },
                        {
                            "start": 88,
                            "end": 278
                        },
                        {
                            "start": 279,
                            "end": 458
                        },
                        {
                            "start": 459,
                            "end": 636
                        },
                        {
                            "start": 637,
                            "end": 850
                        },
                        {
                            "start": 851,
                            "end": 1014
                        },
                        {
                            "start": 1015,
                            "end": 1134
                        },
                        {
                            "start": 1135,
                            "end": 1261
                        },
                        {
                            "start": 1262,
                            "end": 1509
                        },
                        {
                            "start": 1510,
                            "end": 1649
                        },
                        {
                            "start": 1650,
                            "end": 1834
                        },
                        {
                            "start": 1835,
                            "end": 2004
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9609375
                }
            ],
            "relevance_judgement": 0.9609375,
            "relevance_judgment_input_expanded": "# Title: Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey\n# Venue: arXiv.org\n# Authors: Jiaxing Huang, Jingyi Zhang, Kai Jiang, Han Qiu, Shijian Lu\n## Abstract\nTraditional computer vision generally solves each single task independently by a dedicated model with the task instruction implicitly designed in the model architecture, arising two limitations: (1) it leads to task-specific models, which require multiple models for different tasks and restrict the potential synergies from diverse tasks; (2) it leads to a pre-defined and fixed model interface that has limited interactivity and adaptability in following user' task instructions. To address them, Visual Instruction Tuning (VIT) has been intensively studied recently, which finetunes a large vision model with language as task instructions, aiming to learn from a wide range of vision tasks described by language instructions a general-purpose multimodal model that can follow arbitrary instructions and thus solve arbitrary tasks specified by the user. This work aims to provide a systematic review of visual instruction tuning, covering (1) the background that presents computer vision task paradigms and the development of VIT; (2) the foundations of VIT that introduce commonly used network architectures, visual instruction tuning frameworks and objectives, and evaluation setups and tasks; (3) the commonly used datasets in visual instruction tuning and evaluation; (4) the review of existing VIT methods that categorizes them with a taxonomy according to both the studied vision task and the method design and highlights the major contributions, strengths, and shortcomings of them; (5) the comparison and discussion of VIT methods over various instruction-following benchmarks; (6) several challenges, open directions and possible future works in visual instruction tuning research.\n## Image Generation\nGPT4Tools [63] enables open-source language models to effectively use multimodal tools. It constructs a tool-related instructional dataset from advanced language models and utilizes Low-Rank Adaptation (LoRA) optimization to enhance the language models' tool-usage capabilities. Additionally, it proposes a benchmark to evaluate the accuracy of language models in using tools, demonstrating significant improvements in tool usage across various visual tasks. As shown in Figure 17, the GPT4Tools framework involves constructing a tool-related instruction dataset by prompting an advanced language model with various multimodal contexts. This dataset is then used to fine-tune open-source language models using Low-Rank Adaptation (LoRA) optimization, enabling them to effectively use tools for visual tasks such as comprehension and image generation. Additionally, the framework includes a benchmark to evaluate the language models' ability to use tools, showcasing significant improvements in tool usage accuracy. TextBind [72] enhances large language models with multi-turn interleaved multimodal instruction-following capabilities. It significantly reduces the need for high-quality exemplar data, making it more accessible and scalable for real-world tasks. The proposed model, MIM, trained on TextBind, outperforms recent baselines in open-world multimodal conversations, demonstrating remarkable performance in textual response generation, image generation, and overall multimodal instruction-following. As shown in Figure 18, MIM seamlessly integrates image encoder and decoder models to accommodate interleaved image-text inputs and outputs. It supplements large language models with visual input and output modules, enabling the model to process multi-turn interleaved multimodal instructions and generate coherent responses. The architecture is trained in two stages, focusing on aligning the feature spaces of vision and language models and further improving instructionfollowing capabilities.",
            "reference_string": "[266573642 | Huang et al. | 2023 | Citations: 23]"
        },
        {
            "title": "DynRsl-VLM: Enhancing Autonomous Driving Perception with Dynamic Resolution Vision-Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 33,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.11265, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2350428313",
                    "name": "Xirui Zhou"
                },
                {
                    "authorId": "2350351496",
                    "name": "Lianlei Shan"
                },
                {
                    "authorId": "2350347657",
                    "name": "Xiaolin Gui"
                }
            ],
            "abstract": "Visual Question Answering (VQA) models, which fall under the category of vision-language models, conventionally execute multiple downsampling processes on image inputs to strike a balance between computational efficiency and model performance. Although this approach aids in concentrating on salient features and diminishing computational burden, it incurs the loss of vital detailed information, a drawback that is particularly damaging in end-to-end autonomous driving scenarios. Downsampling can lead to an inadequate capture of distant or small objects such as pedestrians, road signs, or obstacles, all of which are crucial for safe navigation. This loss of features negatively impacts an autonomous driving system's capacity to accurately perceive the environment, potentially escalating the risk of accidents. To tackle this problem, we put forward the Dynamic Resolution Vision Language Model (DynRsl-VLM). DynRsl-VLM incorporates a dynamic resolution image input processing approach that captures all entity feature information within an image while ensuring that the image input remains computationally tractable for the Vision Transformer (ViT). Moreover, we devise a novel image-text alignment module to replace the Q-Former, enabling simple and efficient alignment with text when dealing with dynamic resolution image inputs. Our method enhances the environmental perception capabilities of autonomous driving systems without overstepping computational constraints.",
            "corpus_id": 277043666,
            "sentences": [
                {
                    "corpus_id": "277043666",
                    "title": "DynRsl-VLM: Enhancing Autonomous Driving Perception with Dynamic Resolution Vision-Language Models",
                    "text": "Since the introduction of the Transformer architecture in 2017, vision-language models have advanced rapidly. The Transformer's powerful sequence modeling and global attention mechanisms revolutionized natural language processing and laid the groundwork for multimodal integration. Models like ViLBERT [20] and LXMERT [26] applied the Transformer to dual-stream processing of vision and language, separately encoding visual and textual information before fusing them, establishing the basis for multimodal learning. Subsequently, VisualBERT [17] and VL-BERT [25] further advanced the field by integrating visual and textual data within a unified Transformer architecture, enabling more effective multimodal representations. Later, models like CLIP [24] and ALIGN [13] employed large-scale cross-modal contrastive learning to map images and texts into a shared embedding space, significantly improving performance in zero-shot and few-shot learning tasks. In multimodal dialogue and question answering, Visual Dialog [6] enabled models to understand and generate image descriptions in multi-turn conversations. More recent work, such as Microsoft's VLMo [4], introduced a unified vision-language model capable of handling multiple tasks like visual question answering and image captioning. \n\nRecent research emphasizes multimodal feature alignment, crucial for enhancing model performance. For instance, BLIP-2 [15] introduces the Q-Former module-a lightweight query Transformer that extracts languagerelated information from visual features, achieving more efficient and fine-grained alignment. These advancements in pre-training and feature alignment have not only improved vision-language models across various tasks but also revitalized the development of multimodal artificial intelligence.",
                    "score": 0.48883357377971415,
                    "section_title": "Vision-Language Models",
                    "char_start_offset": 3631,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 109
                        },
                        {
                            "start": 110,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 515
                        },
                        {
                            "start": 516,
                            "end": 723
                        },
                        {
                            "start": 724,
                            "end": 954
                        },
                        {
                            "start": 955,
                            "end": 1109
                        },
                        {
                            "start": 1110,
                            "end": 1288
                        },
                        {
                            "start": 1291,
                            "end": 1388
                        },
                        {
                            "start": 1389,
                            "end": 1594
                        },
                        {
                            "start": 1595,
                            "end": 1794
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 318,
                            "end": 322,
                            "matchedPaperCorpusId": "201103729"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96044921875
                }
            ],
            "relevance_judgement": 0.96044921875,
            "relevance_judgment_input_expanded": "# Title: DynRsl-VLM: Enhancing Autonomous Driving Perception with Dynamic Resolution Vision-Language Models\n# Venue: arXiv.org\n# Authors: Xirui Zhou, Lianlei Shan, Xiaolin Gui\n## Abstract\nVisual Question Answering (VQA) models, which fall under the category of vision-language models, conventionally execute multiple downsampling processes on image inputs to strike a balance between computational efficiency and model performance. Although this approach aids in concentrating on salient features and diminishing computational burden, it incurs the loss of vital detailed information, a drawback that is particularly damaging in end-to-end autonomous driving scenarios. Downsampling can lead to an inadequate capture of distant or small objects such as pedestrians, road signs, or obstacles, all of which are crucial for safe navigation. This loss of features negatively impacts an autonomous driving system's capacity to accurately perceive the environment, potentially escalating the risk of accidents. To tackle this problem, we put forward the Dynamic Resolution Vision Language Model (DynRsl-VLM). DynRsl-VLM incorporates a dynamic resolution image input processing approach that captures all entity feature information within an image while ensuring that the image input remains computationally tractable for the Vision Transformer (ViT). Moreover, we devise a novel image-text alignment module to replace the Q-Former, enabling simple and efficient alignment with text when dealing with dynamic resolution image inputs. Our method enhances the environmental perception capabilities of autonomous driving systems without overstepping computational constraints.\n## Vision-Language Models\nSince the introduction of the Transformer architecture in 2017, vision-language models have advanced rapidly. The Transformer's powerful sequence modeling and global attention mechanisms revolutionized natural language processing and laid the groundwork for multimodal integration. Models like ViLBERT [20] and LXMERT [26] applied the Transformer to dual-stream processing of vision and language, separately encoding visual and textual information before fusing them, establishing the basis for multimodal learning. Subsequently, VisualBERT [17] and VL-BERT [25] further advanced the field by integrating visual and textual data within a unified Transformer architecture, enabling more effective multimodal representations. Later, models like CLIP [24] and ALIGN [13] employed large-scale cross-modal contrastive learning to map images and texts into a shared embedding space, significantly improving performance in zero-shot and few-shot learning tasks. In multimodal dialogue and question answering, Visual Dialog [6] enabled models to understand and generate image descriptions in multi-turn conversations. More recent work, such as Microsoft's VLMo [4], introduced a unified vision-language model capable of handling multiple tasks like visual question answering and image captioning. \n\nRecent research emphasizes multimodal feature alignment, crucial for enhancing model performance. For instance, BLIP-2 [15] introduces the Q-Former module-a lightweight query Transformer that extracts languagerelated information from visual features, achieving more efficient and fine-grained alignment. These advancements in pre-training and feature alignment have not only improved vision-language models across various tasks but also revitalized the development of multimodal artificial intelligence.",
            "reference_string": "[277043666 | Zhou et al. | 2025 | Citations: 0]"
        },
        {
            "title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 0,
            "citation_count": 26,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.07910, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2543684",
                    "name": "Kaijie Zhu"
                },
                {
                    "authorId": "2261935625",
                    "name": "Qinlin Zhao"
                },
                {
                    "authorId": "2261741520",
                    "name": "Hao Chen"
                },
                {
                    "authorId": "2145270616",
                    "name": "Jindong Wang"
                },
                {
                    "authorId": "2249681654",
                    "name": "Xing Xie"
                }
            ],
            "abstract": "The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported.",
            "corpus_id": 266191741,
            "sentences": [
                {
                    "corpus_id": "266191741",
                    "title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
                    "text": "Trained on a 3T multilingual corpus, they excel in language understanding, commonsense reasoning, and reading comprehension. \n\n\u2022 BLIP2 (Li et al., 2023b): This visual-language model is proposed by Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. BLIP-2 leverages frozen pre-trained image encoders and large language models (LLMs) by training a lightweight, 12-layer Transformer encoder in between them, achieving excellent performance in various vision-language tasks \n\n\u2022 LLaVA (Liu et al., 2024): LlaVA (Language-Image LLaMA) is a multimodal model combining language and image data. It extends the LLaMA architecture to handle both modalities, enabling tasks like image captioning, visual question answering, and imagebased text generation. \n\n\u2022 Qwen-VL series (Bai et al., 2023): Qwen-VL (Qwen Large Vision Language Model) is the multimodal version of the large model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-VL accepts image, text, and bounding box as inputs, outputs text, and bounding box. \n\n\u2022 InternLM-XComposer2-VL (Dong et al., 2024): InternLM-XComposer2 is a cuttingedge vision-language model excelling in free-form text-image composition and comprehension, crafting content from diverse inputs like outlines, detailed specs, and reference images. Using a Partial LoRA (PLoRA) approach, it balances vision understanding and text composition. \n\nProprietary LLMs: \n\n\u2022 ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b): OpenAI's ChatGPT and GPT-4 are advanced iterations of the GPT series. ChatGPT is tailored for interactive tasks, while GPT-4 is the most proficient in the series and supports image input. \n\n\u2022 PaLM 2 (Anil et al., 2023): PaLM 2 is an advanced language model that excels in multilingual and reasoning capabilities, offering greater computational efficiency than its predecessor, PaLM.",
                    "score": 0.48221741341542845,
                    "section_title": "Conclusion and Discussion",
                    "char_start_offset": 11812,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 124
                        },
                        {
                            "start": 127,
                            "end": 469
                        },
                        {
                            "start": 472,
                            "end": 585
                        },
                        {
                            "start": 586,
                            "end": 743
                        },
                        {
                            "start": 746,
                            "end": 890
                        },
                        {
                            "start": 891,
                            "end": 934
                        },
                        {
                            "start": 935,
                            "end": 1023
                        },
                        {
                            "start": 1026,
                            "end": 1285
                        },
                        {
                            "start": 1286,
                            "end": 1379
                        },
                        {
                            "start": 1382,
                            "end": 1399
                        },
                        {
                            "start": 1402,
                            "end": 1524
                        },
                        {
                            "start": 1525,
                            "end": 1642
                        },
                        {
                            "start": 1645,
                            "end": 1837
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 135,
                            "end": 153,
                            "matchedPaperCorpusId": "256390509"
                        },
                        {
                            "start": 480,
                            "end": 498,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 763,
                            "end": 781,
                            "matchedPaperCorpusId": "263625818"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96044921875
                }
            ],
            "relevance_judgement": 0.96044921875,
            "relevance_judgment_input_expanded": "# Title: PromptBench: A Unified Library for Evaluation of Large Language Models\n# Venue: arXiv.org\n# Authors: Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, Xing Xie\n## Abstract\nThe evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported.\n## Conclusion and Discussion\nTrained on a 3T multilingual corpus, they excel in language understanding, commonsense reasoning, and reading comprehension. \n\n\u2022 BLIP2 (Li et al., 2023b): This visual-language model is proposed by Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. BLIP-2 leverages frozen pre-trained image encoders and large language models (LLMs) by training a lightweight, 12-layer Transformer encoder in between them, achieving excellent performance in various vision-language tasks \n\n\u2022 LLaVA (Liu et al., 2024): LlaVA (Language-Image LLaMA) is a multimodal model combining language and image data. It extends the LLaMA architecture to handle both modalities, enabling tasks like image captioning, visual question answering, and imagebased text generation. \n\n\u2022 Qwen-VL series (Bai et al., 2023): Qwen-VL (Qwen Large Vision Language Model) is the multimodal version of the large model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-VL accepts image, text, and bounding box as inputs, outputs text, and bounding box. \n\n\u2022 InternLM-XComposer2-VL (Dong et al., 2024): InternLM-XComposer2 is a cuttingedge vision-language model excelling in free-form text-image composition and comprehension, crafting content from diverse inputs like outlines, detailed specs, and reference images. Using a Partial LoRA (PLoRA) approach, it balances vision understanding and text composition. \n\nProprietary LLMs: \n\n\u2022 ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b): OpenAI's ChatGPT and GPT-4 are advanced iterations of the GPT series. ChatGPT is tailored for interactive tasks, while GPT-4 is the most proficient in the series and supports image input. \n\n\u2022 PaLM 2 (Anil et al., 2023): PaLM 2 is an advanced language model that excels in multilingual and reasoning capabilities, offering greater computational efficiency than its predecessor, PaLM.",
            "reference_string": "[266191741 | Zhu et al. | 2023 | Citations: 26]"
        },
        {
            "title": "Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 98,
            "citation_count": 59,
            "influential_citation_count": 12,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.02657, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308683426",
                    "name": "Dongyang Liu"
                },
                {
                    "authorId": "2283419424",
                    "name": "Shitian Zhao"
                },
                {
                    "authorId": "2300371391",
                    "name": "Le Zhuo"
                },
                {
                    "authorId": "2284068796",
                    "name": "Weifeng Lin"
                },
                {
                    "authorId": "2312346585",
                    "name": "Yu Qiao"
                },
                {
                    "authorId": "2266421952",
                    "name": "Hongsheng Li"
                },
                {
                    "authorId": "2310909023",
                    "name": "Peng Gao"
                }
            ],
            "abstract": "We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. By initializing from multimodal Generative PreTraining (mGPT), we demonstrate that decoder-only Autoregressive (AR) model can achieve image generation performance comparable to modern diffusion models with high efficiency through Flexible Progressive Supervised Fine-tuning (FP-SFT). Equipped with our proposed Unambiguous image Representation (UniRep), Lumina-mGPT can flexibly generate high-quality images of varying aspect ratios. Building on the strong image generation capabilities, we further explore Ominiponent Supervised Fine-tuning (Omni-SFT), an initial attempt to elevate Lumina-mGPT into a unified multi-modal generalist. The resulting model demonstrates versatile multimodal capabilities, including visual generation tasks like text-to-image/multiview generation and controllable generation, visual recognition tasks like segmentation and depth estimation, and vision-language tasks like multi-turn visual question answering, showing the rosy potential of the technical direction. Codes and checkpoints are available at https://github.com/Alpha-VLLM/Lumina-mGPT.",
            "corpus_id": 271709823,
            "sentences": [
                {
                    "corpus_id": "271709823",
                    "title": "Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining",
                    "text": "Multimodal Large Language Models Recent large language models (LLMs) (Chowdhery et al., 2022;Touvron et al., 2023;Brown et al., 2020;Achiam et al., 2023;Team et al., 2023) demonstrate strong instruction-following and reasoning capabilities, coupled with extensive world knowledge. To extend LLMs' expertise from the text domain to multimodal domains such as images and videos, prior works (Liu et al., 2023;Lin et al., 2023a;Maaz et al., 2023;Lin et al., 2023b) have aligned pretrained encoders from various modalities with LLMs by curating multimodal instruction tuning datasets. Although these multimodal large language models (MLLMs) have exhibited powerful visual understanding capabilities, their multimodal functionality is primarily limited to perceiving the visual world, exemplified by tasks such as visual question answering and image captioning, rather than generating visual outputs. Another line of research (Koh et al., 2024;Dong et al., 2024;Sun et al., 2023;Wu et al.) has proposed augmenting MLLMs with the ability to generate images, videos, and audio from text instructions. These approaches introduce additional visual tokens for generation and align these generative tokens as conditional information with a pretrained generator, such as Stable Diffusion (Esser et al., 2024;Podell et al., 2023) for text-to-image generation. Consequently, the generation capabilities heavily rely on the external expert generator rather than MLLMs themselves, resulting in inconsistent and inferior generation results. To combine the strength of both approaches, our model aims to learn both understanding and generation of images using an MLLM with native multimodal capabilities, drawing inspiration from Chameleon (Team, 2024a), a mixed-modal early-fusion foundation model.",
                    "score": 0.4813120636292394,
                    "section_title": "B. Related Work",
                    "char_start_offset": 27737,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 280
                        },
                        {
                            "start": 281,
                            "end": 580
                        },
                        {
                            "start": 581,
                            "end": 895
                        },
                        {
                            "start": 896,
                            "end": 1093
                        },
                        {
                            "start": 1094,
                            "end": 1346
                        },
                        {
                            "start": 1347,
                            "end": 1523
                        },
                        {
                            "start": 1524,
                            "end": 1781
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 921,
                            "end": 939,
                            "matchedPaperCorpusId": "258959284"
                        },
                        {
                            "start": 939,
                            "end": 957,
                            "matchedPaperCorpusId": "261975252"
                        },
                        {
                            "start": 974,
                            "end": 984,
                            "matchedPaperCorpusId": "261696650"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96044921875
                }
            ],
            "relevance_judgement": 0.96044921875,
            "relevance_judgment_input_expanded": "# Title: Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining\n# Venue: arXiv.org\n# Authors: Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, Peng Gao\n## Abstract\nWe present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. By initializing from multimodal Generative PreTraining (mGPT), we demonstrate that decoder-only Autoregressive (AR) model can achieve image generation performance comparable to modern diffusion models with high efficiency through Flexible Progressive Supervised Fine-tuning (FP-SFT). Equipped with our proposed Unambiguous image Representation (UniRep), Lumina-mGPT can flexibly generate high-quality images of varying aspect ratios. Building on the strong image generation capabilities, we further explore Ominiponent Supervised Fine-tuning (Omni-SFT), an initial attempt to elevate Lumina-mGPT into a unified multi-modal generalist. The resulting model demonstrates versatile multimodal capabilities, including visual generation tasks like text-to-image/multiview generation and controllable generation, visual recognition tasks like segmentation and depth estimation, and vision-language tasks like multi-turn visual question answering, showing the rosy potential of the technical direction. Codes and checkpoints are available at https://github.com/Alpha-VLLM/Lumina-mGPT.\n## B. Related Work\nMultimodal Large Language Models Recent large language models (LLMs) (Chowdhery et al., 2022;Touvron et al., 2023;Brown et al., 2020;Achiam et al., 2023;Team et al., 2023) demonstrate strong instruction-following and reasoning capabilities, coupled with extensive world knowledge. To extend LLMs' expertise from the text domain to multimodal domains such as images and videos, prior works (Liu et al., 2023;Lin et al., 2023a;Maaz et al., 2023;Lin et al., 2023b) have aligned pretrained encoders from various modalities with LLMs by curating multimodal instruction tuning datasets. Although these multimodal large language models (MLLMs) have exhibited powerful visual understanding capabilities, their multimodal functionality is primarily limited to perceiving the visual world, exemplified by tasks such as visual question answering and image captioning, rather than generating visual outputs. Another line of research (Koh et al., 2024;Dong et al., 2024;Sun et al., 2023;Wu et al.) has proposed augmenting MLLMs with the ability to generate images, videos, and audio from text instructions. These approaches introduce additional visual tokens for generation and align these generative tokens as conditional information with a pretrained generator, such as Stable Diffusion (Esser et al., 2024;Podell et al., 2023) for text-to-image generation. Consequently, the generation capabilities heavily rely on the external expert generator rather than MLLMs themselves, resulting in inconsistent and inferior generation results. To combine the strength of both approaches, our model aims to learn both understanding and generation of images using an MLLM with native multimodal capabilities, drawing inspiration from Chameleon (Team, 2024a), a mixed-modal early-fusion foundation model.",
            "reference_string": "[271709823 | Liu et al. | 2024 | Citations: 59]"
        },
        {
            "title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 86,
            "citation_count": 8,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.11829, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2312369775",
                    "name": "Yue Cao"
                },
                {
                    "authorId": "2312345209",
                    "name": "Yangzhou Liu"
                },
                {
                    "authorId": "2305731793",
                    "name": "Zhe Chen"
                },
                {
                    "authorId": "2146870036",
                    "name": "Guangchen Shi"
                },
                {
                    "authorId": "2257133501",
                    "name": "Wenhai Wang"
                },
                {
                    "authorId": "2326454048",
                    "name": "Danhuai Zhao"
                },
                {
                    "authorId": "2276323159",
                    "name": "Tong Lu"
                }
            ],
            "abstract": "Despite significant advancements in Multimodal Large Language Models (MLLMs) for understanding complex human intentions through cross-modal interactions, capturing intricate image details remains challenging. Previous methods integrating multiple vision encoders to enhance visual detail introduce redundancy and computational overhead. We observe that most MLLMs utilize only the last-layer feature map of the vision encoder for visual representation, neglecting the rich fine-grained information in shallow feature maps. To address this issue, we propose \\modelname, a simple yet effective multi-layer feature fuser that efficiently integrates deep and shallow features from Vision Transformers (ViTs). Specifically, it leverages semantically aligned deep features as queries to dynamically extract missing details from shallow features, thus preserving semantic alignment while enriching the representation with fine-grained information. Applied to the LLaVA-1.5 model, \\modelname~achieves significant improvements in visual representation and benchmark performance, providing a more flexible and lightweight solution compared to multi-encoder ensemble methods. The code and model have been released at https://github.com/yuecao0119/MMFuser.",
            "corpus_id": 273350685,
            "sentences": [
                {
                    "corpus_id": "273350685",
                    "title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding",
                    "text": "Multimodal Large Language Models (MLLMs) integrate visual representations from images with linguistic embeddings from text, thereby enhancing the models' capabilities in comprehending and generating language descriptions of visual content. Most open-source MLLMs employ architectures that include a pre-trained vision encoder, an LLM, and a crossmodal connector. Early models, such as the BLIP series [1], [36], utilized the Q-Former module to align text and images, thus improving multimodal capabilities. Flamingo [37] employed a gated cross-attention mechanism to integrate images and text. LLaVA-1.5 [3] adopted an MLP projector to connect the pre-trained vision encoder with the LLM. InternVL [4], [6] employed a dynamic resolution strategy, segmenting images into tiles and encoding both the tiles and the thumbnail view together. It then uses a pixel shuffle operation to reduce the number of visual tokens before integrating these features with the LLM through an MLP projector. \n\nAdditionally, private MLLMs such as the Gemini series [38], [39], GPT-4V [7], and Claude-3V series [40], along with open-source MLLMs like MiniGPT-4 [23], Qwen-VL [5], CogVLM [41], the VisionLLM series [42], [43], and the All-Seeing series [44], [45], among others [46], have demonstrated robust multimodal capabilities. These models exemplify the powerful ability of MLLMs to understand, generalize, and reason with multimodal information, consistently setting new benchmarks in multimodal tasks.",
                    "score": 0.48624043838698516,
                    "section_title": "II. RELATED WORK A. Multimodal Large Language Model",
                    "char_start_offset": 6868,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 362
                        },
                        {
                            "start": 363,
                            "end": 506
                        },
                        {
                            "start": 507,
                            "end": 593
                        },
                        {
                            "start": 594,
                            "end": 688
                        },
                        {
                            "start": 689,
                            "end": 836
                        },
                        {
                            "start": 837,
                            "end": 986
                        },
                        {
                            "start": 989,
                            "end": 1309
                        },
                        {
                            "start": 1310,
                            "end": 1486
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 401,
                            "end": 404,
                            "matchedPaperCorpusId": "258615266"
                        },
                        {
                            "start": 406,
                            "end": 410,
                            "matchedPaperCorpusId": "256390509"
                        },
                        {
                            "start": 516,
                            "end": 520,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 604,
                            "end": 607,
                            "matchedPaperCorpusId": "263672058"
                        },
                        {
                            "start": 698,
                            "end": 701,
                            "matchedPaperCorpusId": "266521410"
                        },
                        {
                            "start": 1191,
                            "end": 1195,
                            "matchedPaperCorpusId": "258762579"
                        },
                        {
                            "start": 1229,
                            "end": 1233,
                            "matchedPaperCorpusId": "260438589"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9599609375
                }
            ],
            "relevance_judgement": 0.9599609375,
            "relevance_judgment_input_expanded": "# Title: MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding\n# Venue: arXiv.org\n# Authors: Yue Cao, Yangzhou Liu, Zhe Chen, Guangchen Shi, Wenhai Wang, Danhuai Zhao, Tong Lu\n## Abstract\nDespite significant advancements in Multimodal Large Language Models (MLLMs) for understanding complex human intentions through cross-modal interactions, capturing intricate image details remains challenging. Previous methods integrating multiple vision encoders to enhance visual detail introduce redundancy and computational overhead. We observe that most MLLMs utilize only the last-layer feature map of the vision encoder for visual representation, neglecting the rich fine-grained information in shallow feature maps. To address this issue, we propose \\modelname, a simple yet effective multi-layer feature fuser that efficiently integrates deep and shallow features from Vision Transformers (ViTs). Specifically, it leverages semantically aligned deep features as queries to dynamically extract missing details from shallow features, thus preserving semantic alignment while enriching the representation with fine-grained information. Applied to the LLaVA-1.5 model, \\modelname~achieves significant improvements in visual representation and benchmark performance, providing a more flexible and lightweight solution compared to multi-encoder ensemble methods. The code and model have been released at https://github.com/yuecao0119/MMFuser.\n## II. RELATED WORK A. Multimodal Large Language Model\nMultimodal Large Language Models (MLLMs) integrate visual representations from images with linguistic embeddings from text, thereby enhancing the models' capabilities in comprehending and generating language descriptions of visual content. Most open-source MLLMs employ architectures that include a pre-trained vision encoder, an LLM, and a crossmodal connector. Early models, such as the BLIP series [1], [36], utilized the Q-Former module to align text and images, thus improving multimodal capabilities. Flamingo [37] employed a gated cross-attention mechanism to integrate images and text. LLaVA-1.5 [3] adopted an MLP projector to connect the pre-trained vision encoder with the LLM. InternVL [4], [6] employed a dynamic resolution strategy, segmenting images into tiles and encoding both the tiles and the thumbnail view together. It then uses a pixel shuffle operation to reduce the number of visual tokens before integrating these features with the LLM through an MLP projector. \n\nAdditionally, private MLLMs such as the Gemini series [38], [39], GPT-4V [7], and Claude-3V series [40], along with open-source MLLMs like MiniGPT-4 [23], Qwen-VL [5], CogVLM [41], the VisionLLM series [42], [43], and the All-Seeing series [44], [45], among others [46], have demonstrated robust multimodal capabilities. These models exemplify the powerful ability of MLLMs to understand, generalize, and reason with multimodal information, consistently setting new benchmarks in multimodal tasks.",
            "reference_string": "[273350685 | Cao et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 185,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.08068, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261895888",
                    "name": "Hao Yang"
                },
                {
                    "authorId": "49339265",
                    "name": "Yanyan Zhao"
                },
                {
                    "authorId": "143792101",
                    "name": "Yang Wu"
                },
                {
                    "authorId": "2214828656",
                    "name": "Shilong Wang"
                },
                {
                    "authorId": "2261492452",
                    "name": "Tian Zheng"
                },
                {
                    "authorId": "2261686105",
                    "name": "Hongbo Zhang"
                },
                {
                    "authorId": "2279751409",
                    "name": "Wanxiang Che"
                },
                {
                    "authorId": "2203961541",
                    "name": "Bing Qin"
                }
            ],
            "abstract": "Compared to traditional sentiment analysis, which only considers text, multimodal sentiment analysis needs to consider emotional signals from multimodal sources simultaneously and is therefore more consistent with the way how humans process sentiment in real-world scenarios. It involves processing emotional information from various sources such as natural language, images, videos, audio, physiological signals, etc. However, although other modalities also contain diverse emotional cues, natural language usually contains richer contextual information and therefore always occupies a crucial position in multimodal sentiment analysis. The emergence of ChatGPT has opened up immense potential for applying large language models (LLMs) to text-centric multimodal tasks. However, it is still unclear how existing LLMs can adapt better to text-centric multimodal sentiment analysis tasks. This survey aims to (1) present a comprehensive review of recent research in text-centric multimodal sentiment analysis tasks, (2) examine the potential of LLMs for text-centric multimodal sentiment analysis, outlining their approaches, advantages, and limitations, (3) summarize the application scenarios of LLM-based multimodal sentiment analysis technology, and (4) explore the challenges and potential research directions for multimodal sentiment analysis in the future.",
            "corpus_id": 270391394,
            "sentences": [
                {
                    "corpus_id": "270391394",
                    "title": "Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey",
                    "text": "Large multimodal models (LMMs) are created to handle and integrate various data types, such as text, images, audio, and video.LMMs extend the capabilities of LLMs by incorporating additional modalities, allowing for a more comprehensive understanding and generation of diverse content.The development of LMMs is driven by the need to more accurately reflect the multimodal nature of human communication and perception.While traditional LLMs like GPT-4 are primarily text-based, LMMs are capable of processing and generating outputs across various data types.For instance, they can interpret visual inputs, generate textual descriptions from images, and even handle audio data, thus bridging the gap between different forms of information.One of the critical advancements in LMMs is the ability to create a unified multimodal embedding space.This involves using separate encoders for each modality to generate data-specific representations, which are then aligned into a cohesive multimodal space.This unified approach allows the models to integrate and correlate information from different sources seamlessly.\n\nNotable examples include Gemini [111], GPT-4V, and ImageBind [110].These models showcase the ability to process text, images, audio, and video, enhancing functionalities such as translation, image recognition, and more.In addition to these well-known models, other emerging models are also making significant strides: BLIP-2 [112] introduces a novel approach to integrate a frozen pre-trained visual encoder with a frozen large language model using a Q-former module.This module employs learnable input queries that interact with image features and the LLM, allowing for effective cross-modal learning.This setup helps maintain the versatility of the LLM while incorporating visual information effectively.LLava [113] is a represent large multimodal model integrating a pre-trained CLIP [116] visual encoder (ViT-L/14), the Vicuna [115] language model, and a simple linear projection layer.",
                    "score": 0.4733562840727251,
                    "section_title": "Large Multimodal Models",
                    "char_start_offset": 7553,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 126
                        },
                        {
                            "start": 126,
                            "end": 285
                        },
                        {
                            "start": 285,
                            "end": 418
                        },
                        {
                            "start": 418,
                            "end": 558
                        },
                        {
                            "start": 558,
                            "end": 738
                        },
                        {
                            "start": 738,
                            "end": 841
                        },
                        {
                            "start": 841,
                            "end": 996
                        },
                        {
                            "start": 996,
                            "end": 1109
                        },
                        {
                            "start": 1111,
                            "end": 1178
                        },
                        {
                            "start": 1178,
                            "end": 1330
                        },
                        {
                            "start": 1330,
                            "end": 1578
                        },
                        {
                            "start": 1578,
                            "end": 1713
                        },
                        {
                            "start": 1713,
                            "end": 1817
                        },
                        {
                            "start": 1817,
                            "end": 2001
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1172,
                            "end": 1177,
                            "matchedPaperCorpusId": "258564264"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9580078125
                }
            ],
            "relevance_judgement": 0.9580078125,
            "relevance_judgment_input_expanded": "# Title: Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey\n# Venue: arXiv.org\n# Authors: Hao Yang, Yanyan Zhao, Yang Wu, Shilong Wang, Tian Zheng, Hongbo Zhang, Wanxiang Che, Bing Qin\n## Abstract\nCompared to traditional sentiment analysis, which only considers text, multimodal sentiment analysis needs to consider emotional signals from multimodal sources simultaneously and is therefore more consistent with the way how humans process sentiment in real-world scenarios. It involves processing emotional information from various sources such as natural language, images, videos, audio, physiological signals, etc. However, although other modalities also contain diverse emotional cues, natural language usually contains richer contextual information and therefore always occupies a crucial position in multimodal sentiment analysis. The emergence of ChatGPT has opened up immense potential for applying large language models (LLMs) to text-centric multimodal tasks. However, it is still unclear how existing LLMs can adapt better to text-centric multimodal sentiment analysis tasks. This survey aims to (1) present a comprehensive review of recent research in text-centric multimodal sentiment analysis tasks, (2) examine the potential of LLMs for text-centric multimodal sentiment analysis, outlining their approaches, advantages, and limitations, (3) summarize the application scenarios of LLM-based multimodal sentiment analysis technology, and (4) explore the challenges and potential research directions for multimodal sentiment analysis in the future.\n## Large Multimodal Models\nLarge multimodal models (LMMs) are created to handle and integrate various data types, such as text, images, audio, and video.LMMs extend the capabilities of LLMs by incorporating additional modalities, allowing for a more comprehensive understanding and generation of diverse content.The development of LMMs is driven by the need to more accurately reflect the multimodal nature of human communication and perception.While traditional LLMs like GPT-4 are primarily text-based, LMMs are capable of processing and generating outputs across various data types.For instance, they can interpret visual inputs, generate textual descriptions from images, and even handle audio data, thus bridging the gap between different forms of information.One of the critical advancements in LMMs is the ability to create a unified multimodal embedding space.This involves using separate encoders for each modality to generate data-specific representations, which are then aligned into a cohesive multimodal space.This unified approach allows the models to integrate and correlate information from different sources seamlessly.\n\nNotable examples include Gemini [111], GPT-4V, and ImageBind [110].These models showcase the ability to process text, images, audio, and video, enhancing functionalities such as translation, image recognition, and more.In addition to these well-known models, other emerging models are also making significant strides: BLIP-2 [112] introduces a novel approach to integrate a frozen pre-trained visual encoder with a frozen large language model using a Q-former module.This module employs learnable input queries that interact with image features and the LLM, allowing for effective cross-modal learning.This setup helps maintain the versatility of the LLM while incorporating visual information effectively.LLava [113] is a represent large multimodal model integrating a pre-trained CLIP [116] visual encoder (ViT-L/14), the Vicuna [115] language model, and a simple linear projection layer.",
            "reference_string": "[270391394 | Yang et al. | 2024 | Citations: 13]"
        },
        {
            "title": "A Light and Smart Wearable Platform with Multimodal Foundation Model for Enhanced Spatial Reasoning in People with Blindness and Low Vision",
            "venue": "ECCV Workshops",
            "year": 2025,
            "reference_count": 39,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.10875, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2337112489",
                    "name": "Alexey Magay"
                },
                {
                    "authorId": "2268422900",
                    "name": "Dhurba Tripathi"
                },
                {
                    "authorId": "2264589226",
                    "name": "Yu Hao"
                },
                {
                    "authorId": "2362289799",
                    "name": "Yi Fang"
                }
            ],
            "abstract": "People with blindness and low vision (pBLV) face significant challenges, struggling to navigate environments and locate objects due to limited visual cues. Spatial reasoning is crucial for these individuals, as it enables them to understand and interpret the spatial relationships in their surroundings, enhancing their ability to navigate and interact more safely and independently. Current multi-modal large language (MLLM) models for low vision people lack the spatial reasoning capabilities needed to effectively assist in these tasks. Moreover, there is a notable absence of lightweight, easy-to-use systems that allow pBLV to effectively perceive and interact with their surrounding environment. In this paper, we propose a novel spatial enhanced multi-modal large language model based approach for visually impaired individuals. By fine-tuning the MLLM to incorporate spatial reasoning capabilities, our method significantly improves the understanding of environmental context, which is critical for navigation and object recognition. The innovation extends to a hardware component, designed as an attachment for glasses, ensuring increased accessibility and ease of use. This integration leverages advanced VLMs to interpret visual data and provide real-time, spatially aware feedback to the user. Our approach aims to bridge the gap between advanced machine learning models and practical, user-friendly assistive devices, offering a robust solution for visually impaired users to navigate their surroundings more effectively and independently. The paper includes an in-depth evaluation using the VizWiz dataset, demonstrating substantial improvements in accuracy and user experience. Additionally, we design a comprehensive dataset to evaluate our method's effectiveness in realworld situations, demonstrating substantial improvements in accuracy and user experience.",
            "corpus_id": 278714599,
            "sentences": [
                {
                    "corpus_id": "278714599",
                    "title": "A Light and Smart Wearable Platform with Multimodal Foundation Model for Enhanced Spatial Reasoning in People with Blindness and Low Vision",
                    "text": "Foundation models, encompassing large-scale pretrained models such as Large Language Models (LLMs) and Multi-modal Large Language Models (MLLMs), have revolutionized the field of artificial intelligence by demonstrating exceptional capabilities across a wide range of tasks. These models, trained on vast amounts of data, are capable of understanding and generating human-like text, as well as interpreting visual inputs in a meaningful way. The GPT-3 model, for instance, has shown remarkable proficiency in natural language understanding and generation, enabling applications ranging from automated customer service to complex problem-solving [6]. Similarly, MLLMs like CLIP [29] and VL-BERT [33] have illustrated the potential of integrating visual and linguistic information, allowing for tasks such as image captioning and text-based image generation. \n\nRecent advancements have introduced multimodal models that integrate capabilities across multiple modalities, further enhancing their utility. GPT-4 [24], for instance, extends the capabilities of its predecessors by incorporating more sophisticated multimodal inputs, allowing it to process and generate text, images, and other forms of data simultaneously. This capability significantly enhances its application in fields requiring complex reasoning and contextual understanding across different types of information. LLaVA (Large Language and Vision Assistant) [21] is another recent model that exemplifies the integration of visual and linguistic processing. LLaVA is designed to handle tasks that require understanding and generating coherent responses based on both visual and textual inputs, such as detailed scene descriptions and interactive question-answering involving visual context. \n\nDespite their impressive performance, foundation models face limitations, particularly in specialized domains requiring fine-grained understanding and reasoning. The black-box nature of these models often poses challenges in interpretability and reliability, especially in critical applications like assistive technology for visually impaired individuals. Furthermore, their generalist design may not adequately address specific needs such as spatial reasoning and real-time responsiveness, which are crucial for effective assistive technologies.",
                    "score": 0.47259253440194904,
                    "section_title": "Foundation Models",
                    "char_start_offset": 5730,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 274
                        },
                        {
                            "start": 275,
                            "end": 441
                        },
                        {
                            "start": 442,
                            "end": 649
                        },
                        {
                            "start": 650,
                            "end": 856
                        },
                        {
                            "start": 859,
                            "end": 1001
                        },
                        {
                            "start": 1002,
                            "end": 1217
                        },
                        {
                            "start": 1218,
                            "end": 1378
                        },
                        {
                            "start": 1379,
                            "end": 1521
                        },
                        {
                            "start": 1522,
                            "end": 1754
                        },
                        {
                            "start": 1757,
                            "end": 1918
                        },
                        {
                            "start": 1919,
                            "end": 2112
                        },
                        {
                            "start": 2113,
                            "end": 2303
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 645,
                            "end": 648,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 677,
                            "end": 681,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 1423,
                            "end": 1427,
                            "matchedPaperCorpusId": "258179774"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95751953125
                }
            ],
            "relevance_judgement": 0.95751953125,
            "relevance_judgment_input_expanded": "# Title: A Light and Smart Wearable Platform with Multimodal Foundation Model for Enhanced Spatial Reasoning in People with Blindness and Low Vision\n# Venue: ECCV Workshops\n# Authors: Alexey Magay, Dhurba Tripathi, Yu Hao, Yi Fang\n## Abstract\nPeople with blindness and low vision (pBLV) face significant challenges, struggling to navigate environments and locate objects due to limited visual cues. Spatial reasoning is crucial for these individuals, as it enables them to understand and interpret the spatial relationships in their surroundings, enhancing their ability to navigate and interact more safely and independently. Current multi-modal large language (MLLM) models for low vision people lack the spatial reasoning capabilities needed to effectively assist in these tasks. Moreover, there is a notable absence of lightweight, easy-to-use systems that allow pBLV to effectively perceive and interact with their surrounding environment. In this paper, we propose a novel spatial enhanced multi-modal large language model based approach for visually impaired individuals. By fine-tuning the MLLM to incorporate spatial reasoning capabilities, our method significantly improves the understanding of environmental context, which is critical for navigation and object recognition. The innovation extends to a hardware component, designed as an attachment for glasses, ensuring increased accessibility and ease of use. This integration leverages advanced VLMs to interpret visual data and provide real-time, spatially aware feedback to the user. Our approach aims to bridge the gap between advanced machine learning models and practical, user-friendly assistive devices, offering a robust solution for visually impaired users to navigate their surroundings more effectively and independently. The paper includes an in-depth evaluation using the VizWiz dataset, demonstrating substantial improvements in accuracy and user experience. Additionally, we design a comprehensive dataset to evaluate our method's effectiveness in realworld situations, demonstrating substantial improvements in accuracy and user experience.\n## Foundation Models\nFoundation models, encompassing large-scale pretrained models such as Large Language Models (LLMs) and Multi-modal Large Language Models (MLLMs), have revolutionized the field of artificial intelligence by demonstrating exceptional capabilities across a wide range of tasks. These models, trained on vast amounts of data, are capable of understanding and generating human-like text, as well as interpreting visual inputs in a meaningful way. The GPT-3 model, for instance, has shown remarkable proficiency in natural language understanding and generation, enabling applications ranging from automated customer service to complex problem-solving [6]. Similarly, MLLMs like CLIP [29] and VL-BERT [33] have illustrated the potential of integrating visual and linguistic information, allowing for tasks such as image captioning and text-based image generation. \n\nRecent advancements have introduced multimodal models that integrate capabilities across multiple modalities, further enhancing their utility. GPT-4 [24], for instance, extends the capabilities of its predecessors by incorporating more sophisticated multimodal inputs, allowing it to process and generate text, images, and other forms of data simultaneously. This capability significantly enhances its application in fields requiring complex reasoning and contextual understanding across different types of information. LLaVA (Large Language and Vision Assistant) [21] is another recent model that exemplifies the integration of visual and linguistic processing. LLaVA is designed to handle tasks that require understanding and generating coherent responses based on both visual and textual inputs, such as detailed scene descriptions and interactive question-answering involving visual context. \n\nDespite their impressive performance, foundation models face limitations, particularly in specialized domains requiring fine-grained understanding and reasoning. The black-box nature of these models often poses challenges in interpretability and reliability, especially in critical applications like assistive technology for visually impaired individuals. Furthermore, their generalist design may not adequately address specific needs such as spatial reasoning and real-time responsiveness, which are crucial for effective assistive technologies.",
            "reference_string": "[278714599 | Magay et al. | 2025 | Citations: 0]"
        },
        {
            "title": "VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop Question Answering",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 45,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.08269, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2248365762",
                    "name": "Qi Zhi Lim"
                },
                {
                    "authorId": "84161127",
                    "name": "C. Lee"
                },
                {
                    "authorId": "30039300",
                    "name": "K. Lim"
                },
                {
                    "authorId": "2446021",
                    "name": "K. Anbananthen"
                }
            ],
            "abstract": "The increasing availability of multimodal data across text, tables, and images presents new challenges for developing models capable of complex cross-modal reasoning. Existing methods for Multimodal Multi-hop Question Answering (MMQA) often suffer from limited reasoning capabilities, reliance on modality conversion, and inadequate alignment between visual and textual representations. To address these limitations, this paper introduces Vision-Language Multimodal Transformer (VLMT), a unified architecture that integrates a transformer-based vision encoder with a sequence-to-sequence language model. VLMT employs a direct token-level injection mechanism to fuse visual and textual inputs within a shared embedding space, eliminating the need for intermediate projection layers. To enhance cross-modal alignment and reasoning, a three-stage pretraining strategy is proposed to progressively align vision-language representations and improve the model's capacity for multimodal understanding. Based on the pretrained backbone, two task-specific modules are instantiated to form a two-stage MMQA framework: a multimodal reranker that predicts document relevance scores and utilizes a relative threshold with top-k strategy for context retrieval, and a multimodal question answering model that generates contextually grounded answers based on the retrieved evidence. Comprehensive experiments on two benchmark datasets demonstrate the effectiveness of the proposed approach. On MultimodalQA validation set, VLMT-Large achieves 76.5% Exact Match and 80.1% F1, outperforming the previous state-of-the-art by +9.1% in Exact Match and +8.8% in F1. On WebQA, it attains a QA score of 47.6, surpassing prior models such as PERQA by +3.2. These results highlight VLMT's strong capabilities in multimodal reasoning and its potential to advance real-world information retrieval and question answering systems.",
            "corpus_id": 277741397,
            "sentences": [
                {
                    "corpus_id": "277741397",
                    "title": "VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop Question Answering",
                    "text": "Large language models (LLMs) have demonstrated exceptional capabilities in understanding and generating human language, significantly advancing the field of natural language processing across diverse applications. Building on these developments, vision-language models have been introduced to extend the utility of LLMs by incorporating visual inputs. These models enable multimodal understanding and support a wide range of tasks such as image captioning, visual reasoning, and optical character recognition. \n\nRecent progress has led to the emergence of multimodal large language models (MLLMs) [37], [34], which have become a prominent paradigm for addressing multimodal tasks. MLLMs typically integrate a pre-trained language model with a vision encoder within a unified architecture. Instruction-following capability is considered essential for handling various downstream tasks, and therefore instructiontuned LLMs [36], [38] are frequently adopted as the language backbone in these models. In parallel, visual encoders [39], [40] are utilized to extract semantically rich representations from visual data. To enable seamless interaction between visual and textual modalities, a projector module is commonly used to map visual features into the language embedding space. This configuration allows the language model to jointly process and reason over multimodal inputs. \n\nWhile MLLMs exhibit strong generalization capabilities and have achieved state-of-the-art performance on several vision-language benchmarks, their application to domainspecific tasks such as multimodal multi-hop question answering (MMQA) [1], [2] remains limited. MMQA introduces distinct challenges, including open-domain retrieval from heterogeneous modalities and multi-step reasoning over diverse information sources. Existing MLLM architectures are often not optimized for fine-grained modality alignment or scalable context retrieval, which are critical for accurate and contextually grounded answer generation in MMQA. \n\nDespite the progress in MLLMs, there remains a lack of unified frameworks specifically tailored for MMQA tasks. Current models either rely heavily on pre-processing pipelines or depend on modality-specific transformations, which may lead to information loss and suboptimal cross-modal reasoning. Moreover, most MLLMs are not pretrained with objectives explicitly designed for visual reasoning in multi-hop settings.",
                    "score": 0.4807148714569963,
                    "section_title": "B. Multimodal Large Language Models",
                    "char_start_offset": 12205,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 213
                        },
                        {
                            "start": 214,
                            "end": 351
                        },
                        {
                            "start": 352,
                            "end": 509
                        },
                        {
                            "start": 512,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 788
                        },
                        {
                            "start": 789,
                            "end": 996
                        },
                        {
                            "start": 997,
                            "end": 1112
                        },
                        {
                            "start": 1113,
                            "end": 1276
                        },
                        {
                            "start": 1277,
                            "end": 1375
                        },
                        {
                            "start": 1378,
                            "end": 1641
                        },
                        {
                            "start": 1642,
                            "end": 1799
                        },
                        {
                            "start": 1800,
                            "end": 2003
                        },
                        {
                            "start": 2006,
                            "end": 2117
                        },
                        {
                            "start": 2118,
                            "end": 2301
                        },
                        {
                            "start": 2302,
                            "end": 2421
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 597,
                            "end": 601,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 603,
                            "end": 607,
                            "matchedPaperCorpusId": "263672058"
                        },
                        {
                            "start": 921,
                            "end": 925,
                            "matchedPaperCorpusId": "253018554"
                        },
                        {
                            "start": 927,
                            "end": 931,
                            "matchedPaperCorpusId": "259129398"
                        },
                        {
                            "start": 1026,
                            "end": 1030,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 1032,
                            "end": 1036,
                            "matchedPaperCorpusId": "257767223"
                        },
                        {
                            "start": 1621,
                            "end": 1624,
                            "matchedPaperCorpusId": "237385768"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95703125
                }
            ],
            "relevance_judgement": 0.95703125,
            "relevance_judgment_input_expanded": "# Title: VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop Question Answering\n# Venue: arXiv.org\n# Authors: Qi Zhi Lim, C. Lee, K. Lim, K. Anbananthen\n## Abstract\nThe increasing availability of multimodal data across text, tables, and images presents new challenges for developing models capable of complex cross-modal reasoning. Existing methods for Multimodal Multi-hop Question Answering (MMQA) often suffer from limited reasoning capabilities, reliance on modality conversion, and inadequate alignment between visual and textual representations. To address these limitations, this paper introduces Vision-Language Multimodal Transformer (VLMT), a unified architecture that integrates a transformer-based vision encoder with a sequence-to-sequence language model. VLMT employs a direct token-level injection mechanism to fuse visual and textual inputs within a shared embedding space, eliminating the need for intermediate projection layers. To enhance cross-modal alignment and reasoning, a three-stage pretraining strategy is proposed to progressively align vision-language representations and improve the model's capacity for multimodal understanding. Based on the pretrained backbone, two task-specific modules are instantiated to form a two-stage MMQA framework: a multimodal reranker that predicts document relevance scores and utilizes a relative threshold with top-k strategy for context retrieval, and a multimodal question answering model that generates contextually grounded answers based on the retrieved evidence. Comprehensive experiments on two benchmark datasets demonstrate the effectiveness of the proposed approach. On MultimodalQA validation set, VLMT-Large achieves 76.5% Exact Match and 80.1% F1, outperforming the previous state-of-the-art by +9.1% in Exact Match and +8.8% in F1. On WebQA, it attains a QA score of 47.6, surpassing prior models such as PERQA by +3.2. These results highlight VLMT's strong capabilities in multimodal reasoning and its potential to advance real-world information retrieval and question answering systems.\n## B. Multimodal Large Language Models\nLarge language models (LLMs) have demonstrated exceptional capabilities in understanding and generating human language, significantly advancing the field of natural language processing across diverse applications. Building on these developments, vision-language models have been introduced to extend the utility of LLMs by incorporating visual inputs. These models enable multimodal understanding and support a wide range of tasks such as image captioning, visual reasoning, and optical character recognition. \n\nRecent progress has led to the emergence of multimodal large language models (MLLMs) [37], [34], which have become a prominent paradigm for addressing multimodal tasks. MLLMs typically integrate a pre-trained language model with a vision encoder within a unified architecture. Instruction-following capability is considered essential for handling various downstream tasks, and therefore instructiontuned LLMs [36], [38] are frequently adopted as the language backbone in these models. In parallel, visual encoders [39], [40] are utilized to extract semantically rich representations from visual data. To enable seamless interaction between visual and textual modalities, a projector module is commonly used to map visual features into the language embedding space. This configuration allows the language model to jointly process and reason over multimodal inputs. \n\nWhile MLLMs exhibit strong generalization capabilities and have achieved state-of-the-art performance on several vision-language benchmarks, their application to domainspecific tasks such as multimodal multi-hop question answering (MMQA) [1], [2] remains limited. MMQA introduces distinct challenges, including open-domain retrieval from heterogeneous modalities and multi-step reasoning over diverse information sources. Existing MLLM architectures are often not optimized for fine-grained modality alignment or scalable context retrieval, which are critical for accurate and contextually grounded answer generation in MMQA. \n\nDespite the progress in MLLMs, there remains a lack of unified frameworks specifically tailored for MMQA tasks. Current models either rely heavily on pre-processing pipelines or depend on modality-specific transformations, which may lead to information loss and suboptimal cross-modal reasoning. Moreover, most MLLMs are not pretrained with objectives explicitly designed for visual reasoning in multi-hop settings.",
            "reference_string": "[277741397 | Lim et al. | 2025 | Citations: 0]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "274023126",
            "title": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
            "text": "Current multimodal generative models can be broadly classified into three distinct categories. The first category includes Any-to-Text (Any Modality to Text) Models, which integrate inputs from multiple modalities, encode them, and project into the word embedding space of the LLM for generating textual output [3]- [5], [27]- [30]. The second category encompasses Any-to-Vision (Any Modality to Vision) Models, which encode inputs across different modalities as conditional information and leverage diffusion models to generate visual outputs [10], [31]- [38]. Thirdly, Any-to-Any (Any Modality to Any Modality) Models perceive inputs and generate outputs in arbitrary combinations of text, image, video, and audio [2], [39]- [41]. We summarize the combinations of modalities regarding both inputs and outputs of all categories in TABLE I. Additionally, we provide a comprehensive analysis of their underlying architectures as follows: \n\n\u2022 Any-to-Text Models. Typical models in this category consist of three primary components: an encoder, a pre-trained LLM, and a modality interface that connects them. The modality encoder functions akin to human sensory organs, transforming raw visual or audio data into compact representations. A common approach is to use pre-trained encoders that are already aligned with language data, as seen in CLIP models [43], which facilitate alignment with LLMs. The LLM, often chosen from established pre-trained models like LLaMA [44] and Vicuna [44], serves as the central reasoning unit. These models benefit from extensive pre-training on web corpora, allowing for rich knowledge representation and reasoning capabilities. To bridge the gap between modalities and language, a modality interface is introduced. This interface can either be a learnable projector that directly aligns the encoded modality features with the LLM's input requirements or an expert model that translates non-textual data into language. Overall, Any-to-Text Models utilize a multi-module architecture to effectively integrate multimodal inputs and generate coherent textual outputs. \n\n\u2022 Any-to-Vision Models.",
            "score": 0.6207922329762711,
            "section_title": "A. Multimodal Generative Models",
            "char_start_offset": 8699,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2096
                },
                {
                    "start": 2099,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 314,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 316,
                    "end": 319,
                    "matchedPaperCorpusId": "267412182"
                },
                {
                    "start": 321,
                    "end": 325,
                    "matchedPaperCorpusId": "258291930"
                },
                {
                    "start": 544,
                    "end": 548,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 721,
                    "end": 725,
                    "matchedPaperCorpusId": "261696650"
                },
                {
                    "start": 1352,
                    "end": 1356,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94140625
        },
        {
            "corpus_id": "278327487",
            "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
            "text": "A unified model capable of both understanding and generating multimodal content holds immense potential: it could generate images based on complex instructions, reason about visual data, and visualize multimodal analyses through generated outputs. The unveiling of GPT-4o's enhanced capabilities [27] in March 2025 has further highlighted this potential, sparking widespread interest in unification. \n\nHowever, designing such a unified framework presents significant challenges. It requires integrating the strengths of autoregressive models for reasoning and text generation with the robustness of diffusion-based models for highquality image synthesis. Key questions remain unresolved, state of unified multimodal models (as illustrated in Fig. 1), thereby benefiting future research endeavors, we present this survey. We begin by introducing the foundational concepts and recent advancements in both multimodal understanding and image generation, covering both autoregressive and diffusion-based paradigms. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. Within the autoregressive and hybrid categories, we further classify models based on their image tokenization strategies, reflecting the diversity of approaches in this area. \n\nBeyond architecture, we assemble datasets and benchmarks tailored for training and evaluating unified multimodal models. These resources span multimodal understanding, text-to-image generation, image editing, and other relevant tasks, providing a foundation for future exploration. Finally, we discuss the key challenges facing this nascent field, including efficient tokenization strategy, data construction, model evaluation, etc. Tackling these challenges will be crucial for advancing the capabilities and scalability of unified multimodal models. \n\nIn the community, there exist excellent surveys on large language models [40], [41], multimodal understanding [42], [43], [44], and image generation [45], [46], while our work focuses specifically on the integration of understanding and generation tasks. Readers are encouraged to consult these complementary surveys for a broader perspective on related topics. We aim to inspire further research in this rapidly evolving field and provide a valuable reference for the community.",
            "score": 0.6084644227441394,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1860,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1950
                },
                {
                    "start": 1953,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2314
                },
                {
                    "start": 2315,
                    "end": 2432
                }
            ],
            "ref_mentions": [
                {
                    "start": 2032,
                    "end": 2036,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 2063,
                    "end": 2067,
                    "matchedPaperCorpusId": "271750640"
                },
                {
                    "start": 2069,
                    "end": 2073,
                    "matchedPaperCorpusId": "265308931"
                },
                {
                    "start": 2075,
                    "end": 2079,
                    "matchedPaperCorpusId": "259243718"
                },
                {
                    "start": 2102,
                    "end": 2106,
                    "matchedPaperCorpusId": "252199918"
                },
                {
                    "start": 2108,
                    "end": 2112,
                    "matchedPaperCorpusId": "265821110"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96240234375
        },
        {
            "corpus_id": "277501830",
            "title": "ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement",
            "text": "\"What I cannot create, I do not understand.\" --Richard Feynman Recent advancements in Large Language Models (LLMs) have significantly enhanced their capability to handle multimodal tasks, particularly by integrating visual inputs into language models. Efforts such as the LLaVA series and the QwenVL series [38,37,1,64] have demonstrated remarkable visual comprehension performance. Meanwhile, the development of text-to-image generation models, such as diffusion-based approaches [53,50,51,54] and more recent autoregressive approaches [52,16,74], has made substantial strides in generating high-fidelity images. These developments have driven the push towards creating unified Multimodal Large Language Models (MLLMs) that seamlessly integrate both visual understanding and generation capabilities. A unified model holds promise not only for advancing task coordination and generalization but also for contributing to the exploration of artificial general intelligence (AGI). By merging understanding and generation capabilities within a single framework, unified models can genuinely grasp the deep relationships between visual and textual information, enabling more intelligent and flexible interactions and task execution in complex real-world scenarios. \n\nTo build such unified models in an autoregressive framework, existing approaches have explored several distinct paradigms. As illustrated in Fig. 2 (a), the earliest models, e.g., Chameleon [59] and Emu3 [65], utilized VQGAN [18] to discretize images, enabling a shared vocabulary for text and vision within an autoregressive (AR) framework. However, even with large-scale training, they lag behind models like LLaVA [38] in visual understanding tasks. To address this issue, works like LaViT [28] and ILLUME [63] (Fig. 2 (b)) learn the semantic codebook from the pretrained text-aligned semantic encoders [78,84], and employ diffusion models to reconstruct images based on semantic tokens. This improves image-text alignment in MLLM pretraining and achieves strong performance in both understanding and generation tasks. However, the lack of texture preservation in vision tokenizers limits their capability in fine-grained image editing.",
            "score": 0.605686331259538,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1259
                },
                {
                    "start": 1262,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2201
                }
            ],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 314,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 481,
                    "end": 485,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 485,
                    "end": 488,
                    "matchedPaperCorpusId": "254854389"
                },
                {
                    "start": 491,
                    "end": 494,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 537,
                    "end": 541,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 541,
                    "end": 544,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 1487,
                    "end": 1491,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 1755,
                    "end": 1759,
                    "matchedPaperCorpusId": "263889455"
                },
                {
                    "start": 1868,
                    "end": 1872,
                    "matchedPaperCorpusId": "257767223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95263671875
        },
        {
            "corpus_id": "274597781",
            "title": "The Narrow Gate: Localized Image-Text Communication in Vision-Language Models",
            "text": "The rise of foundation models [1] trained on vast amounts of text has transformed natural language processing (NLP), \n\nshowing that a single large language model (LLM) [2] can handle many different linguistic tasks [3][4][5]. The rich set of features encoded in LLM embeddings has been then used as an effective prior knowledge both for text-conditional image generation [6][7][8] and image understanding [9][10][11][12][13]. Recently, the availability of large open datasets [14,15] and improved techniques to align text and image embeddings [16] have also enabled the creation of multimodal models that can both understand and generate visual content within a single architecture [17][18][19][20]. This unification allows a richer understanding of the visual world, as generative tasks often require insight into the fundamental concepts and relationships within the data [21]. For example, a model that generates images from text descriptions must grasp the semantic content of those images to ensure they faithfully reflect the details and intent of the text [22,23]. As a result, research has rapidly progressed to integrate multiple modalities into a unified framework with increasingly deeper multimodal fusion. \n\nEarly approaches used cross-attention modules between modality-specific encoders [17]. Further advancements highlighted the importance of using a pre-trained LLM backbone combined with lightweight projection layers, often fine-tuning the LLM to refine the multimodal representations [19,24,25]. More recently, research has shown success in training multimodal-output vision-language models (VLMs) from scratch [20,26], achieving performance close to VLMs that only output text on visual understanding tasks.",
            "score": 0.6016878827604403,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 119,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1728
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 33,
                    "matchedPaperCorpusId": "237091588"
                },
                {
                    "start": 215,
                    "end": 218,
                    "matchedPaperCorpusId": "6401679"
                },
                {
                    "start": 374,
                    "end": 377,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 405,
                    "end": 408,
                    "matchedPaperCorpusId": "244954250"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 412,
                    "end": 416,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 543,
                    "end": 547,
                    "matchedPaperCorpusId": "263605855"
                },
                {
                    "start": 682,
                    "end": 686,
                    "matchedPaperCorpusId": "258822817"
                },
                {
                    "start": 690,
                    "end": 694,
                    "matchedPaperCorpusId": "267750101"
                },
                {
                    "start": 874,
                    "end": 878,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 1302,
                    "end": 1306,
                    "matchedPaperCorpusId": "258822817"
                },
                {
                    "start": 1504,
                    "end": 1508,
                    "matchedPaperCorpusId": "267750101"
                },
                {
                    "start": 1511,
                    "end": 1514,
                    "matchedPaperCorpusId": "261696650"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97509765625
        },
        {
            "corpus_id": "278327487",
            "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
            "text": "Unified multimodal models aim to build a single architecture capable of both understanding and generating data across multiple modalities. These models are designed to process diverse forms of input (e.g., text, image, video, audio) and produce outputs in one or more modalities in a unified manner. A typical unified multimodal framework can be abstracted into three core components: modalityspecific encoders that project different input modalities into a representation space; a modality-fusion backbone that integrates information from multiple modalities and enables cross-modal reasoning; and modality-specific decoders that generate output in the desired modality (e.g., text generation or image synthesis). \n\nIn this section, we primarily focus on unified multimodal models that support vision-language understanding and generation, i.e., models that take both image and text as input and produce either text or image as output. As shown in Fig. 5, existing unified models can be broadly categorized into three main types: diffusion models, autoregressive models, and fused AR + diffusion models. For autoregressive models, we further classify them based on their modality encoding methods into four subcategories: pixel-based encoding, semantic-based encoding, learnable query-based encoding, and hybrid encoding. Each of these encoding strategies represents different ways of handling visual and textual data, leading to varying levels of integration and flexibility in the multimodal representations. Fused AR + diffusion models are divided into two subcategories based on modality encoding: pixel-based encoding and hybrid encoding. These models combine aspects of both autoregressive and diffusion techniques, offering a promising approach to more unified and efficient multimodal generation. \n\nIn the following sections, we will delve deeper into each category: Section 3.1 explores diffusion-based models, discussing their unique advantages in terms of generating high-quality images and text from noisy representations. Section 3.2 focuses on autoregressive-based models, detailing how different encoding methods impact their performance in vision-language tasks. Section 3.3 covers fused AR + diffusion models, examining how the combination of these two paradigms can enhance multimodal generation capabilities.",
            "score": 0.6012701686887088,
            "section_title": "UNIFIED MULTIMODAL MODELS FOR UNDER-STANDING AND GENERATION",
            "char_start_offset": 20720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1805
                },
                {
                    "start": 1808,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2179
                },
                {
                    "start": 2180,
                    "end": 2328
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "258959284",
            "title": "Generating Images with Multimodal Language Models",
            "text": "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.",
            "score": 0.5937684388250244,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98779296875
        },
        {
            "corpus_id": "271974843",
            "title": "Pixels to Prose: Understanding the art of Image Captioning",
            "text": "Unified Architecture -Normal approaches to combining vision and language tasks typically involve creating separate architectures and objectives for each task. However, there's a growing interest in developing a unified framework that can handle various tasks within a single architecture. \n\nVarious vision-language tasks can be accomplished by unifying several image and text related tasks as a single objective, such as text-generation or language modeling, the authors in [14] introduce VL-T5 and VL-BART which unifies various image-text tasks with a unified text-generation objective, they use pre trained language models for text generation and eliminate the need for task-specific hand crafted architectures and objectives. \n\nThey created a label text by encoding textual and visual inputs using a transformer encoder-decoder architecture. The decoder attends to prior tokens and the encoder outputs to predict the probability of the future text tokens, while the encoder integrates text and visual embeddings to create joint representations. \n\nWith input text and image taken into account, the model parameters are trained to minimize the negative log-likelihood of label text tokens [14]. This process seeks to generate text that maintains coherence and meaning irrespective of the input format-image, audio, or video. The focus lies on generating text that accurately reflects the information conveyed by the various input modalities, maintaining relevance and coherence throughout the generated content [14]. \n\nFrozen -this method uses the pre-trained large language model for various multi-modal vision language tasks. The architecture of frozen consists of a visual encoder which takes raw image as input, the vision encoder's outputs are linearly mapped and reshaped to generate a sequence of continuous embeddings,so that these visual representations can be fed into the pre-trained language models.Frozen is different than other models in a way that to apply this method to various vision language tasks does not need updating the weights of the language transformer as activations move through the model, the system becomes more adept at handling tasks that involve both vision and language, gradually adapting and enhancing its performance on multi-modal challenges. Only the visual encoder's parameters are updated during training.",
            "score": 0.5923802853332528,
            "section_title": "Case Studies of various VLP models:",
            "char_start_offset": 78152,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 728
                },
                {
                    "start": 731,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1047
                },
                {
                    "start": 1050,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1517
                },
                {
                    "start": 1520,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 2282
                },
                {
                    "start": 2283,
                    "end": 2348
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8701171875
        },
        {
            "corpus_id": "269043091",
            "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions",
            "text": "Notably, not all multimodal models are VLMs; for instance, text-to-image models like Midjourney and DALL-E [70] lack a language generation component, underscoring the diverse landscape of multimodal AI landscape.The general architecture of a VLM consists of an image and text encoder to generate the embeddings which are then fused in an image-text fusion layer and this fused vector is passed through an LLM to generate the final visually aware generated text.The working of a VLM is shown in Figure -2 In this survey paper, we have categorized VLMs based on their \u2022 Text Generation with Multimodal Input: Within this classification, we explore models that excel in generating textual content while leveraging multimodal inputs, thereby incorporating diverse forms of information.\n\n\u2022 Multimodal Output with Multimodal Input: This category delves into models that exhibit proficiency in generating multimodal outputs by processing multimodal inputs.This involves the synthesis of diverse modalities, such as visual and textual elements, to produce comprehensive and coherent results.We have shown this broad classification in the figure-1. 1omparative Analysis We have conducted an extensive analysis of several Vision-and-Language Models (VLMs) across ten widely recognized benchmark datasets, spanning tasks such as Visual Question Answering (VQA) and image captioning.The results of this analysis are presented in Table 1.Additionally, we have evaluated the perception and cognition capabilities of these VLMs using the Multimodal Model Evaluation (MME) benchmark, with findings summarized in Table 2. Furthermore, a comparative examination of various VLMs on video question-answering datasets is detailed in Table 3.\n\n2.1.Vision-Language Understanding CLIP [69]: CLIP, introduced by OpenAI, is a neural network proficient in grasping visual concepts through natural language guidance.It seamlessly identifies visual categories on diverse benchmarks, mirroring the \"zero-shot\" capabilities seen in GPTpowered models.By scaling a basic contrastive pre-training task, it achieved competitive zero-shot performance on diverse image classification datasets.CLIP gives a much more robust performance than the fine-tuned deep learning vision models in general for classification tasks.",
            "score": 0.5839632811951326,
            "section_title": "Introduction",
            "char_start_offset": 2209,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 212,
                    "end": 461
                },
                {
                    "start": 461,
                    "end": 781
                },
                {
                    "start": 783,
                    "end": 949
                },
                {
                    "start": 949,
                    "end": 1141
                },
                {
                    "start": 1141,
                    "end": 1371
                },
                {
                    "start": 1371,
                    "end": 1425
                },
                {
                    "start": 1425,
                    "end": 1720
                },
                {
                    "start": 1722,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1888
                },
                {
                    "start": 1888,
                    "end": 2156
                },
                {
                    "start": 2156,
                    "end": 2282
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 111,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 1761,
                    "end": 1765,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86376953125
        },
        {
            "corpus_id": "258967184",
            "title": "GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction",
            "text": "Vision and Language Model. In the quest to achieve multimodal models capable of addressing both language and vision tasks, several studies [16][17][18][19][20][21] have explored methods to enable language models to comprehend visual input. These include techniques such as transforming images into discrete textual representations [16,17] or projecting continuous image features into the textual feature space [22][23][24][25]. Concurrently, other research has been dedicated to the development of generalist models [18,19,21,20], which permit a model to simultaneously input images and text, eliminating the necessity for a projection process. For instance, OFA [26] devised a unified sequence-to-sequence decoding architecture applicable to language and object detection tasks. Similarly, Pixel2Pixel [20] converted the outcome of visual comprehension tasks into a series of discrete tokens akin to language tasks. Gato [21] brought together a range of vision and control tasks into a sequential prediction issue, while UViM [27] and Unified-IO [19] advocated for the learned discrete codes as a means to unify an",
            "score": 0.5784323882057553,
            "section_title": "Related Work",
            "char_start_offset": 4787,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1115
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 143,
                    "matchedPaperCorpusId": "246035276"
                },
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "237485500"
                },
                {
                    "start": 147,
                    "end": 151,
                    "matchedPaperCorpusId": "244799261"
                },
                {
                    "start": 155,
                    "end": 159,
                    "matchedPaperCorpusId": "249674493"
                },
                {
                    "start": 331,
                    "end": 335,
                    "matchedPaperCorpusId": "246035276"
                },
                {
                    "start": 335,
                    "end": 338,
                    "matchedPaperCorpusId": "237485500"
                },
                {
                    "start": 410,
                    "end": 414,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 418,
                    "end": 422,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 516,
                    "end": 520,
                    "matchedPaperCorpusId": "244799261"
                },
                {
                    "start": 526,
                    "end": 529,
                    "matchedPaperCorpusId": "249674493"
                },
                {
                    "start": 663,
                    "end": 667,
                    "matchedPaperCorpusId": "246634906"
                },
                {
                    "start": 803,
                    "end": 807,
                    "matchedPaperCorpusId": "249674493"
                },
                {
                    "start": 1027,
                    "end": 1031,
                    "matchedPaperCorpusId": "248964962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87744140625
        },
        {
            "corpus_id": "274597781",
            "title": "The Narrow Gate: Localized Image-Text Communication in Vision-Language Models",
            "text": "This study explores how multimodal-output and text-only output VLMs exchange information between visual and textual tokens. We focus on models that fuse image and text tokens from the start of training and can generate images and text within the same architecture. Chameleon [20] and Emu3 [26] serve as representatives of this class. Since Wang et al. [26] released two fine-tuned versions of Emu3 -Emu3-Gen for image generation and Emu3-chat for image understanding -we fine-tune Emu3-Gen on a mixture of image-text datasets to perform well on both tasks. This enables a direct comparison with Chameleon, which natively supports the joint generation of images and text. Fine-tuning details for Emu3 are provided in the Supplementary Material (SM) in Sec. 7.1. For text-only output VLMs, we analyze Llava-onevision-7B [46] and Pixtral-12B [13], fine-tuned on Qwen2-7B and Mistral Nemo 12B, respectively. Additionally, we also analyze Janus-1.3B [36], a multimodal-output model fine-tuned on a DeepSeek-LLM backbone. Janus uses a SigLIP image encoder and retains the pre-trained unembedding matrix for text generation. Unlike Chameleon and Emu3, which use a unified VQ-GAN encoder-decoder for multimodal tasks, Janus has an encoding-decoding stucture specialized in image understanding tasks which is different from the one used for image generation. This makes its architecture closer to Llava and Pixtral. \n\nAll the models are decoder-only and trained with nexttoken prediction loss. In Chameleon, Emu3, and Janus, the loss is computed on both image and text tokens, while in Llava-onevision and Pixtral, it applies only to text tokens. In all the cases, a special end-of-image token ([EOI]) marks the transition from image to text. Additional details of the model architectures is provided in Table SM3 of the Supplementary Material.",
            "score": 0.5733533074972442,
            "section_title": "Model Architectures",
            "char_start_offset": 7451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1835
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "278327487",
            "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
            "text": "In recent years, the rapid advancement of large language models (LLMs), such as LLaMa [1], [2], PanGu [3], [4], Qwen [5], [6], and GPT [7], has revolutionized artificial intelligence. These models have scaled up in both size and capability, enabling breakthroughs across diverse applications. Alongside this progress, LLMs have been extended into multimodal domains, giving rise to powerful multimodal understanding models like LLaVa [8], Qwen-VL [9], [10], InternVL [11], Ovis [12], and GPT4 [13]. These models have expanded their capabilities beyond simple image captioning to performing complex reasoning tasks based on user instructions. On the other hand, image generation technology has also experienced rapid development, with models like SD series [14], [15] and FLUX [16] now capable of producing high-quality images that adhere closely to user prompts. \n\nThe predominant architectural paradigm for LLMs and multimodal understanding models is autoregressive gener- ation [17], which relies on decoder-only structures and nexttoken prediction for sequential text generation. In contrast, the field of text-to-image generation has evolved along a different trajectory. Initially dominated by Generative Adversarial Networks (GANs) [18], image generation has since transitioned to diffusion-based models [19], which leverage architectures like UNet [14] and DiT [20], [21] alongside advanced text encoders such as CLIP [22] and T5 [23]. Despite some explorations into using LLM-inspired architectures for image generation [24], [25], [26], diffusionbased approaches remain the state-of-the-art in terms of performance currently. \n\nWhile autoregressive models lag behind diffusion-based methods in image generation quality, their structural consistency with LLMs makes them particularly appealing for developing unified multimodal systems. A unified model capable of both understanding and generating multimodal content holds immense potential: it could generate images based on complex instructions, reason about visual data, and visualize multimodal analyses through generated outputs.",
            "score": 0.5726672793110983,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1634
                },
                {
                    "start": 1637,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2092
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 138,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 434,
                    "end": 437,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 467,
                    "end": 471,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 756,
                    "end": 760,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 762,
                    "end": 766,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 1238,
                    "end": 1242,
                    "matchedPaperCorpusId": "1033682"
                },
                {
                    "start": 1310,
                    "end": 1314,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 1355,
                    "end": 1359,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1368,
                    "end": 1372,
                    "matchedPaperCorpusId": "254854389"
                },
                {
                    "start": 1425,
                    "end": 1429,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1437,
                    "end": 1441,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1534,
                    "end": 1538,
                    "matchedPaperCorpusId": "270560593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97998046875
        },
        {
            "corpus_id": "270514555",
            "title": "Automatic, Meta and Human Evaluation for Multimodal Summarization with Multimodal Output",
            "text": "Among the above models, only the HAN model could output both texts and images.Other models could only generate text-only summaries.To have multimodal outputs, we further design different algorithms to select the images from the source image set.Given a multimodal input, there is a reference summary image set (with N images).We edit this reference set with the following methods:",
            "score": 0.5712152684014717,
            "section_title": "RQ2: Meta-Evaluation",
            "char_start_offset": 10915,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 78,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 245
                },
                {
                    "start": 245,
                    "end": 326
                },
                {
                    "start": 326,
                    "end": 380
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.798828125
        },
        {
            "corpus_id": "267311889",
            "title": "InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model",
            "text": "Benefiting from existing large language models and VLMs as the visual encoder, recent Multimodal Large Language Models (MLLMs) [12,14,15,24,28,58] achieve visual perception, understanding and reasoning abilities, show superb performance in diverse vision-language tasks. A series of studies [2,5,9,10,20,21,42,46,49,62,64,80,86,92,97,98,100] have explored further improve the MLLM in different dimensions, such as instruction tuning [11,49,98], efficient fine-tuning [33], highresolution image inputs [6,82,83], hallucination mitigation [34,87,99], image generation [23,30,74,89], 3D understanding [63] and image-text comprehension and composition [95]. \n\nTo enable highly customizable content creation, our model is designed for free-form text-image composition and comprehension based on MLLMs. We use Intern-LM2 as the LLM and CLIP ViT-Large as the visual encoder and propose a new partial LoRA to align the text-image modalities. Given flexible and multi-modal user inputs such as specifications, outlines, and reference images, our model is capable of generating high-quality interleaved text-image written content.",
            "score": 0.564690941740794,
            "section_title": "Related Works",
            "char_start_offset": 9786,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1120
                }
            ],
            "ref_mentions": [
                {
                    "start": 467,
                    "end": 471,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 504,
                    "end": 507,
                    "matchedPaperCorpusId": "265034288"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92578125
        },
        {
            "corpus_id": "273403986",
            "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
            "text": "Unifying multimodal understanding and generation capabilities within a single model is a critical milestone toward artificial general intelligence (AGI). Towards this goal, recent advancements (Liu et al., 2024b;Zhu et al., 2023a) in multimodal large language models (MLLMs) have made significant progress in integrating visual reasoning and understanding with natural language interfaces. However, developing a unified framework that excels at both comprehending and generating multimodal content remains a significant challenge in the field of artificial intelligence. \n\nRecent studies (Sun et al., 2023;Ge et al., 2024b) have explored MLLM's potential for visual generation, beyond the previously well-explored visual understanding and reasoning with MLLMs. These approaches enable MLLMs to process image-text inputs and produce either textual outputs or semantic-level visual tokens. In the case of image generation, these visual tokens are subsequently transformed into pixel-space images using diffusion-based decoders. Such unified frameworks empower MLLMs to perform a wide spectrum of tasks within a single framework, ranging from detailed visual analysis to creative image synthesis. \n\nHowever, existing MLLM-based methods (Sun et al., 2023;2024b) face a common challenge in the trade-off between diversity for text-to-image generation and high controllability for tasks such as image editing. Previous methods mostly rely on single-granular features extracted from a visual encoder and neglect the varying granularity requirements of different tasks. On the one hand, generating diverse images reflecting the real world from text descriptions requires features that encode coarse semantic concepts. Such features are fed as conditions into the diffusion-based image decoder, allowing the diffusion model to generate diverse images that semantically align with the text prompt. On the other hand, tasks demanding precise control over output images, such as image editing and inpainting, require the LLMs to predict fine-grained features that encode rich, detailed visual information for the image decoder. This dichotomy presents a significant challenge for current MLLM-based methods, which typically generate single-granular feature representations for all tasks.",
            "score": 0.5622713842952402,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2275
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 212,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9638671875
        },
        {
            "corpus_id": "273025657",
            "title": "Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks",
            "text": "Multimodal Large Language Models (MLLMs). Many approaches have been proposed for building MLLMs, leveraging different architectural designs. A widely adopted approach is the decoder-only architecture, exemplified by LLaVA [41], Emu2 [61], and Intern-VL [9]. These models typically incorporated a visual encoder to encode images, a vision-language connector to project visual features into the language feature space, and a language model that processes both visual and textual information jointly. Another line of work employed cross-attention architectures where encoded image features are integrated with textual tokens via crossattention layers, as seen in Flamingo [1], OpenFlamingo [3] and CogVLM [66]. Such a design allows models to retain the benefits of a fully intact language model but introduces new parameters to manage the visual-textual interplay. \n\nText-rich MLLMs. Text-rich images are traditionally processed in pipelines [18,60], where an OCR module first recognized text from the image, followed by processing through a language model. To improve efficiency and avoid error propagation, with the advent of MLLMs, end-to-end approaches become more popular recently. For instance, LLaVAR [75] utilized a dataset of 400K instances with OCR-enhanced text to outperform LLaVA on various textrich VQA tasks. Subsequent models such as UReader [70], TextMonkey [44], and Mplug-DocOwl-1.5 [16] recognized the importance of high-resolution encoding for accurate text comprehension, so they adopted strategies that cropped single images into multiple sub-images to preserve the original resolution during visual encoding. However, these approaches are primarily trained on single-image data, and struggle to generalize effectively to multi-image scenarios. Furthermore, the straightforward partitioning technique encounters challenges with multi-image inputs, as the sequence length rapidly increases with the number of images.",
            "score": 0.5615309616433143,
            "section_title": "Related Work",
            "char_start_offset": 5450,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1935
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 702,
                    "end": 706,
                    "matchedPaperCorpusId": "265034288"
                },
                {
                    "start": 943,
                    "end": 946,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 1355,
                    "end": 1359,
                    "matchedPaperCorpusId": "263829804"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83544921875
        },
        {
            "corpus_id": "276422421",
            "title": "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs",
            "text": "Multimodal Large Language Models (MLLMs) have advanced the field by enabling unified multimodal understanding and generation within a single autoregressive framework [37,38]. In terms of multimodal comprehension, models like Flamingo [1] process visual data interleaved with text, utilizing a gated cross-attention layer to encode    inputs and produce free-form textual output. BLIP-2 [20] introduces the Q-Former architecture, which maps images into a hidden space aligned with text tokens in LLMs, while LLaVA [21] simplifies this framework further with a projector and explores instruction tuning within the multimodal domain. Despite these advancements, current MLLMs primarily emphasize text generation and lack the capability to synthesize multimodal outputs (e.g., images). To address this, DreamLLM [6] integrates an LLM backbone with a   diffusion model to enable image generation as a multimodal output. Emu2 [31] scales this architecture to 37B parame-ters, demonstrating strong multimodal in-context learning and the ability to handle complex tasks requiring real-time reasoning, such as visual prompting and object-grounded generation. Chameleon [33] proposes a stable training strategy from the ground up, featuring an alignment process and architectural parameterization tailored to early-fusion, token-based, mixed-modal settings. Nevertheless, most existing approaches overlook the relational dynamics between text and images, limiting their applicability to multimodal content generation tasks on multimodal attributed graphs (MMAGs).",
            "score": 0.5604713211678088,
            "section_title": "Multimodal Large Language Models",
            "char_start_offset": 21994,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1553
                }
            ],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 237,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 386,
                    "end": 390,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 513,
                    "end": 517,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 920,
                    "end": 924,
                    "matchedPaperCorpusId": "266374640"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97900390625
        },
        {
            "corpus_id": "276902413",
            "title": "ARMOR: Empowering Multimodal Understanding Model with Interleaved Multimodal Generation Capability",
            "text": "Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, and often struggle to generate interleaved text-image. We present ARMOR, a resource-efficient and pure autoregressive framework that achieves both understanding and generation by fine-tuning existing multimodal large language models (MLLMs). Specifically, ARMOR extends existing MLLMs from three perspectives: (1) For model architecture, an asymmetric encoder-decoder architecture with a forward-switching mechanism is introduced to unify embedding space integrating textual and visual modalities for enabling natural text-image interleaved generation with minimal computational overhead. (2) For training data, a meticulously curated, high-quality interleaved dataset is collected for fine-tuning MLLMs. (3) For the training algorithm, we propose a ``what or how to generate\"algorithm to empower existing MLLMs with multimodal generation capabilities while preserving their multimodal understanding capabilities, through three progressive training stages based on the collected dataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs with promising image generation capabilities, using limited training resources. Our code will be released soon at https://armor.github.io.",
            "score": 0.556345558912799,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97216796875
        },
        {
            "corpus_id": "263608981",
            "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
            "text": "The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.",
            "score": 0.5545162870271416,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "266326262",
            "title": "From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models",
            "text": "The future sees generative models that seamlessly integrate multiple modalities-text, image, sound, video, and more-offering a holistic understanding of the world and generating content that overcomes the limitations of current models. Recent advancements have already led to transformers capable of generating not just text, but also image, audio, and video [91]. These multimodal models are expected to evolve into sophisticated systems capable of processing and understanding inputs from various modalities simultaneously. \n\nIn the future, we anticipate the emergence of single applications and more advanced multimodal models. These systems would not only understand inputs from different sensory channels-such as visual, auditory, and textual-but also generate outputs in various forms, moving well beyond mere text generation. The integration of these modalities in a single model offers a more comprehensive approach to understanding complex real-world scenarios and creating more nuanced and contextually relevant outputs.",
            "score": 0.54734336055686,
            "section_title": "Multimodal Models",
            "char_start_offset": 57486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 525
                },
                {
                    "start": 528,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1030
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 363,
                    "matchedPaperCorpusId": "249642175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85009765625
        },
        {
            "corpus_id": "270702315",
            "title": "Directed Domain Fine-Tuning: Tailoring Separate Modalities for Specific Training Tasks",
            "text": "Most recently, the advent of multimodal models has significantly expanded the frontiers of Artificial Intelligence research and application, showcasing remarkable versatility and efficiency across many tasks that were once the exclusive domain of single-modality large language models (LLMs).Unlike their predecessors, which primarily processed and generated text-based information, multimodal models are adept at understanding, interpreting, and generating outputs that span across multiple forms of data, including but not limited to text, images, audio, and video.This capability not only enhances the models' comprehension and interaction with the world but also aligns more closely with human cognitive processes, which are inherently multimodal.Integrating multiple data types allows these models to perform a broader spectrum of tasks with enhanced accuracy, creativity, and contextual understanding, thus opening new avenues for AI applications in more nuanced and complex scenarios.\n\nMultimodal models have found application in diverse tasks, illustrating their flexibility and the breadth of their utility.Some of the most notable applications include visual question answering (VQA), where the model responds to text-based questions about visual content; image captioning, which involves generating descriptive text for images; and audio-visual speech recognition, where the model uses both visual and auditory signals to improve speech recognition accuracy.Additionally, these models have been successfully employed in sentiment analysis by analyzing text and vocal tones, in object detection and recognition by integrating spatial data with visual inputs, and in machine translation, where visual context enhances the accuracy of text translations.\n\nAnother application of multimodal models is the generation of step-by-step instructions for complex tasks.This involves not just understanding a text-based query but also analyzing accompanying visual data to produce a coherent, easily understandable sequence of instructions.A multimodal model needs to have a sufficient temporal understanding of events and actions taking place within video inputs.Models should also be able to capture a detailed understanding of the relationships among different actions presented.Additionally, the reasoning ability to work through the relationships between different tools, ingredients, or materials is necessary for a meaningful understanding of inputted tasks.This capability represents a significant leap forward in models' potential to assist with practical, everyday tasks, offering personalized guidance that considers the unique aspects of each scenario.",
            "score": 0.5449754275436481,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 567
                },
                {
                    "start": 567,
                    "end": 751
                },
                {
                    "start": 751,
                    "end": 991
                },
                {
                    "start": 993,
                    "end": 1116
                },
                {
                    "start": 1116,
                    "end": 1469
                },
                {
                    "start": 1469,
                    "end": 1761
                },
                {
                    "start": 1763,
                    "end": 1869
                },
                {
                    "start": 1869,
                    "end": 2039
                },
                {
                    "start": 2039,
                    "end": 2163
                },
                {
                    "start": 2163,
                    "end": 2281
                },
                {
                    "start": 2281,
                    "end": 2464
                },
                {
                    "start": 2464,
                    "end": 2663
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85888671875
        },
        {
            "corpus_id": "271533594",
            "title": "GP-VLS: A general-purpose vision language model for surgery",
            "text": "VLMs are language models designed to process both visual and textual information. These models combine LLMs with vision processing to enable interactions between image and text data using a language backbone 15 . Typically, VLMs consist of a vision encoder f v (I) for processing images, a text encoder f t (T ) for handling language inputs, and a multimodal fusion component f m (v,t) to integrate information from both sources. The vision encoder often employs convolutional neural networks or transformer-based architectures to extract features, while the text encoder typically uses transformer",
            "score": 0.5431656937420456,
            "section_title": "Vision Language Models",
            "char_start_offset": 6187,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 598
                }
            ],
            "ref_mentions": [
                {
                    "start": 208,
                    "end": 210,
                    "matchedPaperCorpusId": "257913547"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8828125
        },
        {
            "corpus_id": "273233512",
            "title": "Exploring Foundation Models in Remote Sensing Image Change Detection: A Comprehensive Survey",
            "text": "The vision-Language model (VLM) is a model that can process both visual and linguistic information and fuses multimodal information across modalities [54]. This cross-modal learning capability enables the VLM to perform numerous sophisticated tasks, such as image quizzing [55]- [58], multimodal sentiment analysis [59]- [61], and visual reasoning [62], [63]. The following are the current common vision-language model structures: \n\n1) Dual-Encoder Architecture: The input image and text are processed by an image encoder based on CNN [64] or vision transformer [51] and a text encoder using a language model like Bert [19], respectively. These encoders transform image and text data into feature vectors, which are subsequently matched across modalities by contrast learning in a shared embedding space. A prime example of this is CLIP [52], which realizes cross-modal tasks such as image retrieval [65]- [67] and audio-text retrieval [68], [69] by massive imagetext contrast learning in order to guarantee that the image and the corresponding text description are close together in the embedding space. \n\n2) Cross-Modality Transformer: The model simultaneously processes image and text data through a shared Transformer model. While exploiting the self-attention mechanism, the model captures the relationship between vision and verbal modality, allowing deep interaction between image and textual information. For instance, ViLBERT [70] utilizes independent encoders for initial image and text processing, and then implements more complex interactions in a cross-modal transformer, while UNITER [71] processes multimodal information through a joint transformer. This architecture is particularly suitable for complex cross-modal tasks and can effectively model long-distance dependencies. \n\n3) Autoregressive Generative Models: This type of model predicts and generates the content of one modality on the basis of another modality through stepwise generation. Such models generate visual or linguistic data by autoregressive means and are specifically suited for multimodal generative tasks.",
            "score": 0.5423064666953715,
            "section_title": "E. Vision-Language Model",
            "char_start_offset": 17599,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 430
                },
                {
                    "start": 433,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 1104
                },
                {
                    "start": 1107,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1791
                },
                {
                    "start": 1794,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 154,
                    "matchedPaperCorpusId": "257913547"
                },
                {
                    "start": 273,
                    "end": 277,
                    "matchedPaperCorpusId": "257683248"
                },
                {
                    "start": 279,
                    "end": 283,
                    "matchedPaperCorpusId": "258236427"
                },
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "271464410"
                },
                {
                    "start": 321,
                    "end": 325,
                    "matchedPaperCorpusId": "264981523"
                },
                {
                    "start": 354,
                    "end": 358,
                    "matchedPaperCorpusId": "264591439"
                },
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "14542261"
                },
                {
                    "start": 837,
                    "end": 841,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 900,
                    "end": 904,
                    "matchedPaperCorpusId": "250210301"
                },
                {
                    "start": 906,
                    "end": 910,
                    "matchedPaperCorpusId": "266214872"
                },
                {
                    "start": 942,
                    "end": 946,
                    "matchedPaperCorpusId": "250607505"
                },
                {
                    "start": 1435,
                    "end": 1439,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 1598,
                    "end": 1602,
                    "matchedPaperCorpusId": "216080982"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95556640625
        },
        {
            "corpus_id": "270319618",
            "title": "A survey on advancements in image-text multimodal models: From general techniques to biomedical implementations",
            "text": "Such a configuration facilitates effective multitask pretraining and enhances BLIP's capability for transfer learning, making it a versatile tool in bridging visual and textual modalities.\n\nLLaVA represents an end-to-end trained, large-scale image-text multimodal model that integrates a visual encoder with a language model (LLM) to facilitate a universal understanding of both vision and language.Its primary objective is to effectively leverage the capabilities of pretrained language models and visual models.As depicted in Figure 10, LLaVA exemplifies the single-stream architecture approach.The model selects Vicuna as its language model due to its exceptional performance in language tasks, as evidenced by publicly available checkpoints.For visual input   , LLaVA utilizes the pre-trained CLIP visual encoder ViT-L/14, generating visual features   = (  ).The model emphasizes grid features both before and after the last Transformer layer.Through a simple linear layer, LLaVA connects image features to the word embedding space.Specifically, it employs a trainable projection matrix  to convert   into language embedding tokens   , matching the dimensionality of the word embedding space in the language model.This efficient projection method facilitates rapid experimentation focused on data.The exploration of image-text multimodal model architectures has led to the identification of two principal configurations: single-stream and dual-stream [215,322].The single-stream approach integrates visual and textual data within a unified processing framework, optimizing the use of pre-trained components for enhanced efficiency.In contrast, the dual-stream configuration processes each modality separately before combining them, employing techniques such as contrastive learning and mutual attention mechanisms for a deeper image-text multimodal understanding.These methodologies underscore the field's varied strategies for the integration of visual and textual information, marking significant progress towards developing systems that achieve a comprehensive understanding of the complex interplay between vision and language.",
            "score": 0.5416198802680031,
            "section_title": "[CLS]",
            "char_start_offset": 54974,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 190,
                    "end": 399
                },
                {
                    "start": 399,
                    "end": 513
                },
                {
                    "start": 513,
                    "end": 597
                },
                {
                    "start": 597,
                    "end": 745
                },
                {
                    "start": 745,
                    "end": 863
                },
                {
                    "start": 863,
                    "end": 947
                },
                {
                    "start": 947,
                    "end": 1036
                },
                {
                    "start": 1036,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1301
                },
                {
                    "start": 1301,
                    "end": 1465
                },
                {
                    "start": 1465,
                    "end": 1635
                },
                {
                    "start": 1635,
                    "end": 1867
                },
                {
                    "start": 1867,
                    "end": 2135
                }
            ],
            "ref_mentions": [
                {
                    "start": 1455,
                    "end": 1460,
                    "matchedPaperCorpusId": "247762352"
                },
                {
                    "start": 1460,
                    "end": 1464,
                    "matchedPaperCorpusId": "220364511"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "274305717",
            "title": "MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding",
            "text": "Unified Visual Language Models Pioneering efforts have made significant strides by enabling multimodal understanding and generation within language models. In the realm of generating visual content using VLM, many works [18,19,25,50,72,74] have integrated VLMs with diffusion models [48] to achieve high-quality visual outputs. It is important to note that VLMs inherently lack the capability to directly produce visual content, and the quality of the generated images heavily relies on the performance of the diffusion models. For example, Emu [50] uses the output of the LLM as a condition for the pretrained diffusion model and then generates images with the diffusion model. Transfusion [74] combines the language modeling loss function with diffusion to train a single transformer. \n\nOther works like Chameleon [52], Show-o [62] and Emu3 [59] have tried to directly adopt the VQ tokenizer to encode images for both multimodal understanding and generation. However, since these visual tokenizers do not contain semantic information, aligning visual tokens with language tokens becomes difficult, and these models usually yield suboptimal performance in multimodal understanding tasks. Janus [60] uses separate encoders for understanding and generation, but it increases the complexity of the model. VILA-U [61] combines contrastive and reconstruction loss to align visual and text tokens, but it has convergence problems, requiring a specific training recipe and large-scale image-text pairs from COYO-700M [5] dataset. \n\nIn this work, we explore a semantic-aware discrete encoding method for image reconstruction and generation. Our work reconstructs Siglip's visual features, which are well aligned with text, making the training process simpler and demonstrating outstanding performance in both visual understanding and generation tasks.",
            "score": 0.5412271806212523,
            "section_title": "Related Work",
            "char_start_offset": 6699,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1523
                },
                {
                    "start": 1526,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1844
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "263889455"
                },
                {
                    "start": 233,
                    "end": 236,
                    "matchedPaperCorpusId": "267750101"
                },
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "271909855"
                },
                {
                    "start": 283,
                    "end": 287,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 691,
                    "end": 695,
                    "matchedPaperCorpusId": "271909855"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "271050462",
            "title": "ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation",
            "text": "This approach traces back to BEiT (Bao et al., 2021), with OpenAI's DALL-E (Ramesh et al., 2021) exemplifying text-to-image generation based on similar principles.These methods rely heavily on vector quantization (VQ) models (Van Den Oord et al., 2017; Esser et al., 2021), which combine ResNet-based encoders and decoders with a discrete codebook.During image encoding, the VQ model transforms the image from pixel space to latent space representations, then maps these representations to codebook IDs using a nearest-neighbor search.These IDs serve as input tokens for a Transformer model, which models conditional probabilities and predicts sequences.The VQ decoder then reconstructs images from the generated sequences.This autoregressive, discrete image encoding and decoding approach has been validated in multiple studies for producing high-quality images (Zhu et al., 2024;Yu et al., 2024), effectively modeling inter-image dependencies (Bai et al., 2023), and enhancing image consistency (Pan et al., 2024).LWM (Liu et al., 2024) and Chameleon (Team, 2024) extend this concept to image-text multimodal tasks, using streamlined architectures to handle tasks involving both images and text.Compared to other methods, unified token-based modeling significantly reduces model complexity, facilitating seamless inference and the generation of interleaved image-text sequences without additional components.\n\nANOLE, building on the foundation of Chameleon, facilitates Chameleon's image and multimodal generation capabilities with efficient fine-tuning.ANOLE retains the inherent advantages of Chameleon's architecture while producing high-quality images and maintaining coherent image-text sequences.Tab. 1 highlights ANOLE's characteristics as an autoregressive, diffusion-free, native token-based model, emphasizing its capability to support multimodal generation, as well as its simplicity and efficiency compared to more complex frameworks.",
            "score": 0.5393863369175953,
            "section_title": "Rich Resources for Accessibility:",
            "char_start_offset": 5071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 163,
                    "end": 348
                },
                {
                    "start": 348,
                    "end": 535
                },
                {
                    "start": 535,
                    "end": 654
                },
                {
                    "start": 654,
                    "end": 723
                },
                {
                    "start": 723,
                    "end": 1016
                },
                {
                    "start": 1016,
                    "end": 1197
                },
                {
                    "start": 1197,
                    "end": 1410
                },
                {
                    "start": 1412,
                    "end": 1556
                },
                {
                    "start": 1556,
                    "end": 1704
                },
                {
                    "start": 1704,
                    "end": 1948
                }
            ],
            "ref_mentions": [
                {
                    "start": 75,
                    "end": 95,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 997,
                    "end": 1015,
                    "matchedPaperCorpusId": "253734226"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "271333958",
            "title": "Harmonizing Visual Text Comprehension and Generation",
            "text": "Visual text comprehension and generation tasks such as scene text detection and recognition [70,54,31,66,53,52,24,49], document understanding [55,23], visual question answering (VQA) [26,15,27,30,50], key information extraction (KIE) [55,23], visual text generation, editing, and erasure [57,6,5] are consistently of significant value for both academic research and practical applications. Recently, remarkable advancements have been achieved in visual text comprehension and generation, driven by the evolution of Multimodal Large Language Models (MLLMs) and diffusion models. Foremost text-centric MLLMs [64,20,27,30] utilize a cohesive framework to comprehend text-rich images comprehensively, whereas diffusion-based approaches [57,6,5] introduce innovative modifications to enhance visual text generation capabilities. As depicted in Figure 1, text-centric MLLMs and diffusion models are capable of handling language and vision modalities adeptly, with MLLMs generating texts and diffusion models producing images. However, integrating language and vision generation capabilities within a large multimodal model for visual text scenarios remains unexplored. This paper focuses on the simultaneous manipulation of language and vision generations to further streamline the processing of diverse text-centric multimodal tasks. In the general multimodal domain, some pioneering efforts [48,16,71,56] empower MLLMs with the ability to generate images beyond texts, vastly extending the versatility of multimodal models. Such advancements inspire us to develop a text-centric multimodal generative model. Our foundational model follows these approaches, incorporating a VIT-based image encoder, a text tokenizer, an LLM, a text detokenizer, and a diffusion-based image decoder. \n\nPrevious works [48,71,56] and our pilot experiments (Figure 2) have shown that multimodal generation often leads to a notable decline in performance due to the substantial inconsistency between language and vision modalities in the generation space. Prior studies [48,16,71,56] commonly rely on modality-specific supervised fine-tuning to bolster generative capacities.",
            "score": 0.5381787211134061,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1776
                },
                {
                    "start": 1779,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2148
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 96,
                    "matchedPaperCorpusId": "265351529"
                },
                {
                    "start": 96,
                    "end": 99,
                    "matchedPaperCorpusId": "247778841"
                },
                {
                    "start": 99,
                    "end": 102,
                    "matchedPaperCorpusId": "255416140"
                },
                {
                    "start": 102,
                    "end": 105,
                    "matchedPaperCorpusId": "257232534"
                },
                {
                    "start": 105,
                    "end": 108,
                    "matchedPaperCorpusId": "251451612"
                },
                {
                    "start": 108,
                    "end": 111,
                    "matchedPaperCorpusId": "251040216"
                },
                {
                    "start": 111,
                    "end": 114,
                    "matchedPaperCorpusId": "259951310"
                },
                {
                    "start": 114,
                    "end": 117,
                    "matchedPaperCorpusId": "258826110"
                },
                {
                    "start": 142,
                    "end": 146,
                    "matchedPaperCorpusId": "254275326"
                },
                {
                    "start": 146,
                    "end": 149,
                    "matchedPaperCorpusId": "248228056"
                },
                {
                    "start": 234,
                    "end": 238,
                    "matchedPaperCorpusId": "254275326"
                },
                {
                    "start": 238,
                    "end": 241,
                    "matchedPaperCorpusId": "248228056"
                },
                {
                    "start": 292,
                    "end": 294,
                    "matchedPaperCorpusId": "258762187"
                },
                {
                    "start": 736,
                    "end": 738,
                    "matchedPaperCorpusId": "258762187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "273403986",
            "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
            "text": "Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm - from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA.",
            "score": 0.5371689445286382,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88037109375
        },
        {
            "corpus_id": "273403986",
            "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
            "text": "Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm -from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA.",
            "score": 0.5371309503414954,
            "section_title": "ABSTRACT",
            "char_start_offset": 11,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1172
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87890625
        },
        {
            "corpus_id": "266174392",
            "title": "SmartEdit: Exploring Complex Instruction-Based Image Editing with Multimodal Large Language Models",
            "text": "The exceptional open-sourced LLaMA [7,34] significantly enhances the performance of vision tasks with the aid of Large Language Models (LLMs). Pioneering works such as LLaVA and MiniGPT-4 [25,42] have improved imagetext alignment through instruction-tuning. While numerous MLLM-based studies have demonstrated their robust capabilities across a variety of tasks, primarily those reliant on text generation (e.g., human-robot interaction, complex reasoning, science question answering, etc.), GILL [20] serves as a bridge between MLLMs and diffusion models. It learns to process images with LLMs and is capable of generating coherent images based on the input texts. SEED [10] presents an innovative image tokenizer to enable LLM to process and generate images and text concurrently. SEED-2 [11] further refines the tokenizer by aligning the generation embedding with the image embedding of unCLIP-SD, which allows for better preservation of rich visual semantics and reconstruction of more realistic images. Emu [33] can be characterized as a multimodal generalist, trained with the next-token-prediction objective. CM3Leon [39] proposes a multi-modal language model that is capable of executing text-to-image and image-to-text generation. It employs the CM3 multi-modal architecture that is fine-tuned on di-verse instruction-style data, and utilizes a training method adapted from text-only language models.",
            "score": 0.5367527567049595,
            "section_title": "LLM with Diffusion Models",
            "char_start_offset": 9463,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1409
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "265498814",
            "title": "SEED-Bench-2: Benchmarking Multimodal Large Language Models",
            "text": "In recent years, Large Language Models (LLMs) [8,13,46,47,55] have exhibited remarkable capabilities to understand, reason, and generate texts across a variety of openended tasks. Leveraging the strong generality of LLMs, Multimodal Large Language Models (MLLMs) [3,9,19,26,29,31,32,36,37,42,43,48,48,53,62,68,70] have demonstrated exceptional capabilities in comprehending multimodal data through predicting open-form texts. \n\nRecent work [11,17,18,34,54,60] further empower LLMs with the ability of generating images beyond texts (acting like a combination of GPT-4V [1] and DALL-E 3 [5]), since they contend that the premise for the emergence of multimodal capabilities is that text and image can be represented and processed interchangeably in a unified autoregressive Transformer. However, despite the extensive capabilities of MLLMs, existing MLLM benchmarks [4,15,38,61,63] primarily focus on evaluating single image-text comprehension, thus failing to fully demonstrate the progress and limitations of current MLLMs. The lag of benchmarks behind the rapid development of MLLMs hinders the exploration and evolution of models. \n\nIn this work, we categorize the capabilities of MLLMs into hierarchical levels ranging from L 0 to L 4 based on the modalities they can accept and generate, as depicted in Fig. 1. Building upon LLMs, the lowest-tier capability L 0 involves generating texts given text inputs, while the highest-tier capability L 4 entails producing open-form interleaved image and text output given arbitrary interleaved image-text inputs. Reaching the capability L 4 is a crucial milestone on the path towards General Artificial Intel-ligence (AGI) since a human-level AI should be able to effortlessly digest and create multimodal content. In the capability pyramid, higher levels inherently include the capabilities of lower tiers. This hierarchical categorization not only clearly illustrates the current progress of MLLMs, but also provides a well-defined roadmap for future research.",
            "score": 0.5360400206334819,
            "section_title": "Introduction",
            "char_start_offset": 1364,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1133
                },
                {
                    "start": 1136,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 277,
                    "end": 280,
                    "matchedPaperCorpusId": "256390509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96826171875
        },
        {
            "corpus_id": "265498814",
            "title": "SEED-Bench-2: Benchmarking Multimodal Large Language Models",
            "text": "We categorize the capabilities of MLLMs into hierarchical levels from L 0 to L4, based on input and output modalities, where higher level encompasses lower capability tiers, as illustrated in Fig. 1. SEED-Bench-2 covers the assessment of MLLMs up to L 3 . The detailed categorization of capability level is illustrated as below, Level L 0 : Building upon LLMs, the most fundamental capability of MLLMs generating text based on provided text inputs, which does not necessitate specific evaluation within the MLLM benchmark. \n\nLevel L 1 : MLLMs at this capability level should possess the ability to comprehend multimodal inputs in a fixed format, i.e., image or multiple images (video input can be regarded as multiple images) and then texts. Current MLLM benchmarks only evaluate this capability level with single image and text as inputs. \n\nLevel L 2 : MLLMs at this capability level should be able to understand multimodal inputs with open-form interleaved image-text data, which aligns with the multimodal inputs encountered in real-life scenarios. \n\nLevel L 3 : Besides the inherent ability of LLMs to generate texts, MLLMs at this capability level should also be proficient in producing images, as advanced MLLMs are expected to process and represent multimodal content on both input and output sides. \n\nLevel L 4 : MLLMs at the highest capability level should possess the ability to process and generate interleaved",
            "score": 0.535748447385459,
            "section_title": "SEED-Bench-2 3.1. Hierarchical Capability Levels",
            "char_start_offset": 8938,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 839
                },
                {
                    "start": 842,
                    "end": 1051
                },
                {
                    "start": 1054,
                    "end": 1306
                },
                {
                    "start": 1309,
                    "end": 1421
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91845703125
        },
        {
            "corpus_id": "271097376",
            "title": "SEED-Story: Multimodal Long Story Generation with Large Language Model",
            "text": "Interleaved image-text data is ubiquitous on the internet, characterized by multiple images interspersed with text.In recent years, there has been a surge of interest in generating interleaved image-text content [52,16,1,11,49], driven by the remarkable advancements in image generation [43,26,9,36,54] and open-form text generation [53,48,59].This has given rise to Multimodal Story Generation, an intriguing and valuable task that involves the generation of narrative texts and vivid images in an interleaved manner.This task goes beyond traditional text-based storytelling by fusing text and visuals to enrich narratives, creating a more immersive storytelling experience where visuals and text dynamically interact and complement each other.\n\nHowever, multimodal story generation poses significant challenges due to the complexity of the inputs and the high demands on the quality of the outputs.Firstly, this task necessitates a thorough comprehension of interleaved data, where text is not only abstract and narrative in nature, but also deeply intertwined with complex images.The model must be adept at deciphering the intricate relationships between images and texts to maintain a coherent narrative flow.Secondly, this task requires the generation of not only a plausible text plot, but also visually captivating images that are consistent in characters and styles.The model should be capable of achieving coherence in the generation of both text and visuals, ensuring an engaging storytelling output.\n\nRecently, Multimodal Large Language Models (MLLMs) [23,60,39,5,28,58,26,22] have showcased powerful comprehension abilities in understanding multimodal data, which makes them ideally suited for interleaved image-text content in multimodal stories.Consequently, we introduce SEED-Story, a novel approach that builds upon the MLLM to harness its comprehension strength, while further equipping it with the capability to generate coherent images align with the narrative texts.\n\nSpecifically, following previous work [45,16], we utilize a pre-trained image tokenizer and detokenizer, which can decode realistic images with SD-XL [41] by taking the features of a pre-trained ViT as input.",
            "score": 0.5353821048248539,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 115,
                    "end": 344
                },
                {
                    "start": 344,
                    "end": 518
                },
                {
                    "start": 518,
                    "end": 745
                },
                {
                    "start": 747,
                    "end": 900
                },
                {
                    "start": 900,
                    "end": 1083
                },
                {
                    "start": 1083,
                    "end": 1213
                },
                {
                    "start": 1213,
                    "end": 1374
                },
                {
                    "start": 1374,
                    "end": 1510
                },
                {
                    "start": 1512,
                    "end": 1759
                },
                {
                    "start": 1759,
                    "end": 1986
                },
                {
                    "start": 1988,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 291,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 296,
                    "end": 299,
                    "matchedPaperCorpusId": "232428282"
                },
                {
                    "start": 1563,
                    "end": 1567,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1584,
                    "end": 1587,
                    "matchedPaperCorpusId": "259287020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92333984375
        },
        {
            "corpus_id": "278327487",
            "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
            "text": "Multimodal understanding models refer to LLM-based architectures capable of receiving, reasoning over, and gener- ating outputs from multimodal inputs [47]. These models extend the generative and reasoning capabilities of LLMs beyond textual data, enabling rich semantic understanding across diverse information modalities [42], [48]. Most efforts of existing methods focus on vision-language understanding (VLU), which integrates both visual (e.g., images and videos) and textual inputs to support a more comprehensive understanding of spatial relationships, objects, scenes, and abstract concepts [49], [50], [51]. A typical architecture of multimodal understanding models is illustrated in Fig. 2. These models operate within a hybrid input space, where textual data are represented discretely, while visual signals are encoded as continuous representations [52]. Similar to traditional LLMs, their outputs are generated as discrete tokens derived from internal representations, using classification-based language modeling and task-specific decoding strategies [8], [53]. \n\nEarly VLU models primarily focused on aligning visual and textual modalities using dual-encoder architectures, wherein images and text are first encoded separately and then jointly reasoned over via aligned latent representations, including CLIP [22], ViLBERT [54], VisualBERT [55], and UNITER [56]. Although these pioneering models established key principles for multimodal reasoning, they depended heavily on region-based visual preprocessing and separate encoders, limiting the scalability and generality of the mode. With the emergence of powerful LLMs, VLU models have progressively shifted toward decoder-only architectures that incorporate frozen or minimally fine-tuned LLM backbones. These methods primarily transform image embeddings through a connector with different structures, as illustrated in Fig. 2. Specifically, MiniGPT-4 [57] utilized a single learnable layer to project CLIP-derived image embeddings into the token space of Vicuna [58].",
            "score": 0.5344124418784048,
            "section_title": "Multimodal Understanding Model",
            "char_start_offset": 4530,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 155,
                    "matchedPaperCorpusId": "259243718"
                },
                {
                    "start": 323,
                    "end": 327,
                    "matchedPaperCorpusId": "271750640"
                },
                {
                    "start": 605,
                    "end": 609,
                    "matchedPaperCorpusId": "268249175"
                },
                {
                    "start": 1065,
                    "end": 1068,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1070,
                    "end": 1074,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1324,
                    "end": 1328,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1338,
                    "end": 1342,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 1372,
                    "end": 1376,
                    "matchedPaperCorpusId": "216080982"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94384765625
        },
        {
            "corpus_id": "276929408",
            "title": "Recognition-Synergistic Scene Text Editing",
            "text": "In response to the notable progress of large language models in natural language processing [1,2,6], the field of multimodal large language models (MLLM) has made significant strides in recent years. MLLMs leverage both natural language and visual inputs, allowing these models to understand and manipulate visual data guided by textual descriptions. This dual-modality capability builds upon foundational image generation models, such as GANs [12] and diffusion models [15], but advances them by incorporating language as a critical component in model design. Recent works [1,11,26,38,40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs [3,13,29] offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation. Inspired by these methods, our approach integrates the multi-modal language model RS-STE, which is specialized in scene text editing.",
            "score": 0.5333905798872862,
            "section_title": "MLLM for Image Generation and Editing",
            "char_start_offset": 6474,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1336
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 99,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 470,
                    "end": 474,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 583,
                    "end": 586,
                    "matchedPaperCorpusId": "266374640"
                },
                {
                    "start": 966,
                    "end": 969,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 972,
                    "end": 975,
                    "matchedPaperCorpusId": "253581838"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "260775783",
            "title": "Multimodal Pretrained Models for Verifiable Sequential Decision-Making: Planning, Grounding, and Perception",
            "text": "Multimodal Pretrained Models. Multimodal pretrained models (also referred to as foundation models [7] or base models [24]) are capable of processing, understanding, and generating data across multiple formats, such as images, text, and audio. These models are pretrained on large training datasets, and they have demonstrated strong empirical performance across a variety of tasks, such as question-answering and next-word prediction, even without further task-specific fine-tuning [2]. \n\nThe Generative Pretrained Transformer (GPT) series of models [2,23] consists of the most well-known multimodal pretrained models that can generate natural language or other data formats. In addition to GPT, pretrained models such as PaLM [5], BLOOM [28], Codex [4], and Megatron [31] also have the capability of generating outputs in natural language or other formats. Language generation is the core capability of these models, which we will use in the rest of the paper. Hence we denote this category of multimodal pretrained models as GLMs. \n\nVision-language models such as CLIP [24], Yolo [25], and the Segment Anything Model [14] are another type of multimodal pretrained model. CLIP takes an image and a set of texts as inputs, and measures the image-text consistency. Yolo, R-CNN [26] and Segment Anything Model are object detection models, which take an image and a set of words that describe objects, and classify whether the objects appear in the image. These models are capable of processing and understanding texts and images but are not capable of content generation. Finite State Automaton. A finite state automaton (FSA) is a tuple A = \u27e8\u03a3, \u0393, ,  0 , , \u27e9 where \u03a3 is the input alphabet (the set of input symbols), \u0393 is the output alphabet (the set of output symbols),  0 \u2208  is the initial state,  :  \u00d7 \u03a3 \u00d7  \u2192 {0, 1} is the transition function, and  :  \u00d7 \u03a3 \u00d7  \u2192 \u0393 is the output function. \n\nWe use  to denote the set of atomic propositions, which we use to define the input alphabet, \u03a3 2  .",
            "score": 0.5312392009015466,
            "section_title": "PRELIMINARIES",
            "char_start_offset": 5319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 486
                },
                {
                    "start": 489,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1032
                },
                {
                    "start": 1035,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1888
                },
                {
                    "start": 1891,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 101,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 117,
                    "end": 121,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 482,
                    "end": 485,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 550,
                    "end": 553,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1071,
                    "end": 1075,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1082,
                    "end": 1086,
                    "matchedPaperCorpusId": "206594738"
                },
                {
                    "start": 1276,
                    "end": 1280,
                    "matchedPaperCorpusId": "10328909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9287109375
        },
        {
            "corpus_id": "258959284",
            "title": "Generating Images with Multimodal Language Models",
            "text": "Multimodal Language Models Several prior works have developed multimodal language models which process image and text inputs to generate text outputs. Frozen [56] showed that it is possible to finetune a visual encoder to map images into the hidden space of a text-only LLM, and that this exhibits compelling few-shot, captioning, and question answering abilities. Other methods improve upon this approach by introducing adapters [19], scaling up model and data sizes [4,64], improving the visual encoder [4,33], finetuning on instructions [35], and training unified models on multi-task objectives [36,63,42]. CM3 [2,62] trained multimodal LMs on HTML webpages consisting of interleaved images and text. Many state-of-the-art models also require significant computational resources to train. For example, Flamingo [4] is trained on 1535 TPUs for 15 days, while RA-CM3 [62] use 256 GPUs for 5 days. In contrast, our efficient adaptation method is trained on 2 GPUs for 2 days. The most similar work to our approach is FROMAGe [31], which trains a multimodal language model capable of processing arbitrarily interleaved image and text inputs to",
            "score": 0.5312292531151044,
            "section_title": "Related Work",
            "char_start_offset": 3794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1143
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95361328125
        },
        {
            "corpus_id": "270440191",
            "title": "MirrorCheck: Efficient Adversarial Defense for Vision-Language Models",
            "text": "A.1 Visual-Language Models (VLMs) \n\nHumans possess the remarkable ability to seamlessly integrate information from various sources concurrently. For instance, in conversations, we adeptly interpret verbal cues, body language, facial expressions, and intonation. Similarly, VLMs demonstrate proficiency in processing such multimodal signals, allowing machines to comprehend and generate image-related content that seamlessly merges visual and textual components. Contemporary VLM architectures such as CLIP [Radford et al., 2021] predominantly leverage transformer-based models [Vaswani et al., 2023, Dosovitskiy et al., 2021] for processing both images and text due to their effectiveness in capturing long-range dependencies. At the heart of the transformers lies the multi-head attention mechanism, which plays a pivotal role in these models' functionality. \n\nTo enable multimodal comprehension, VLMs typically comprise three key components: (i) an Image Model responsible for extracting meaningful visual features from visual data, (ii) a Text Model designed to process natural language, and (iii) a Fusion Mechanism to integrate representations from both modalities. Encoders in VLMs can be categorized based on their fusion mechanisms into Fusion encoders [Li et al., 2020, 2021b, 2019b, Su et al., 2019], which directly combine image and text embeddings, Dual encoders [Radford et al., 2021, Li et al., 2022, 2023b, Jia et al., 2021], which process modalities separately before interaction, and Hybrid methods [Singh et al., 2021, Bao et al., 2022] that leverage both approaches. Furthermore, fusion schemes for cross-modal interaction can be classified into single-stream [Li et al., 2020, 2019b, Su et al., 2019, Bao et al., 2022, Singh et al., 2021] and dual-stream [Li et al., 2021b] architectures.",
            "score": 0.5302321781266919,
            "section_title": "A Related Work",
            "char_start_offset": 31083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 36,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1808
                }
            ],
            "ref_mentions": [
                {
                    "start": 506,
                    "end": 528,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1261,
                    "end": 1277,
                    "matchedPaperCorpusId": "215754208"
                },
                {
                    "start": 1277,
                    "end": 1284,
                    "matchedPaperCorpusId": "236034189"
                },
                {
                    "start": 1375,
                    "end": 1396,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1516,
                    "end": 1535,
                    "matchedPaperCorpusId": "244954250"
                },
                {
                    "start": 1535,
                    "end": 1554,
                    "matchedPaperCorpusId": "241035439"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93212890625
        },
        {
            "corpus_id": "271328975",
            "title": "Videogamebunny: Towards Vision Assistants for Video Games",
            "text": "Large multimodal models (LMMs) enhance large language models (LLMs) by incorporating additional modalities such as images or audio, enabling them to process multimodal inputs and generate textual outputs.The role of the language model is to comprehend user instructions and produce responses based on the additional modality inputs provided.Standard approaches to create LMMs involve combining pre-trained models with different modalities via projection layers.These layers can be implemented using simple mechanisms such as multilayer perceptrons (MLP) [24,27] or transformer layers [25].Alternatively, a resampler module like Perceiver [3,20,21] or Qformer [10,60] selectively chooses features to reduce the number of visual tokens based on the context and instruction, enhancing efficiency and maintaining performance.\n\nIn this study, we focus on LMMs that accept input images and text to produce responses, particularly using the LLaVA-style architecture [29], which is one of the most popular methods [14,26,28,33].This architecture employs an MLP layer to integrate vision tokens with a language model.",
            "score": 0.5295641114553555,
            "section_title": "Large multimodal models",
            "char_start_offset": 2083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 204,
                    "end": 341
                },
                {
                    "start": 341,
                    "end": 461
                },
                {
                    "start": 461,
                    "end": 589
                },
                {
                    "start": 589,
                    "end": 821
                },
                {
                    "start": 823,
                    "end": 1020
                },
                {
                    "start": 1020,
                    "end": 1108
                }
            ],
            "ref_mentions": [
                {
                    "start": 554,
                    "end": 558,
                    "matchedPaperCorpusId": "258999820"
                },
                {
                    "start": 558,
                    "end": 561,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 584,
                    "end": 588,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 638,
                    "end": 641,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 641,
                    "end": 644,
                    "matchedPaperCorpusId": "232110866"
                },
                {
                    "start": 659,
                    "end": 663,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 959,
                    "end": 963,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8974609375
        },
        {
            "corpus_id": "248721934",
            "title": "Deep learning and synthetic media",
            "text": "increasingly easy to guide image generation directly with text. DALL-E, a new multimodal Transformer model trained on a dataset of text-image pairs, is capable of generating plausible images in a variety of styles simply from a text description of the desired output . DALL-E's outputs can exhibit complex compositional structure corresponding to that of the input text sequences, such as \"An armchair in the shape of an avocado\", \"a small red block sitting on a large green block\", or \"an emoji of a baby penguin wearing a blue hat, red gloves, green shirt, and yellow pants\". DALL-E has been developed jointly with another multimodal model, called CLIP, capable of producing a natural language caption for any input image . Using CLIP to steer the generation process, it is also possible to produce images with GANs from natural language descriptions of the desired output (Galatolo et al., 2021;Patashnik et al., 2021;see Fig. 5).\n\nIf these trends continue-and there is no reason for them to slow down significantly as hardware improvement and architectural breakthroughs continue to spur larger and more efficient models, it is only a matter of time before DL algorithms allow us to generate high-resolution stylized or photorealistic samples of arbitrary scenes that are consistently indistinguishable from human-made outputs. In the domain of static visual media, that goal is already within sight for medium to large resolutions (around 1024x1024 pixels at the time of writing). The domain of dynamic visual media presents a significantly greater challenge, as the spatiotemporal consistency of the scene needs to be taken into account. Nonetheless, it is plausible that we will be able to synthesize realistic and coherent video scenes at relatively high resolution in the short to medium term, beyond mere face swapping in existing videos.",
            "score": 0.5276576050287031,
            "section_title": "Totally synthetic DLSAM",
            "char_start_offset": 21091,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 875,
                    "end": 898,
                    "matchedPaperCorpusId": "231749958"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84033203125
        },
        {
            "corpus_id": "258959284",
            "title": "Generating Images with Multimodal Language Models",
            "text": "Autoregressive language models (LMs) and large language models (LLMs) trained on text corpora have shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the effectiveness of grounding text-only LMs to images for vision-and-language tasks [56,4,29,33,31,35], to embodied settings for robotics [3,18], offline reinforcement learning [48], and more. These methods typically keep most of the LLM weights frozen. This allows them to leverage the capabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn from in-context examples [9], more effectively process longer context, and condition on inputs more strongly. \n\nIn this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49]) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs [52], in contrast to other methods which require interleaved image-text data [4,2]. Our approach is computationally efficient and does not require running the image generation model at training time. To achieve strong image generation performance, we propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special FROMAGe Retrieval only multimodal language model. Image outputs limited to the candidate retrieval set.",
            "score": 0.5264562935828627,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1869
                }
            ],
            "ref_mentions": [
                {
                    "start": 1180,
                    "end": 1184,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1328,
                    "end": 1332,
                    "matchedPaperCorpusId": "51876975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "273234038",
            "title": "EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models",
            "text": "Text-to-Image Generation. To advance the overall quality of text-to-image generation, research efforts have been invested in exploring architectural improvement [10,14,15,16] and generation paradigm advancement [17,18,3], etc. Diffusion models stand out as the de facto text-to-image generation paradigm [19,20,1,3,21,22,23,24,25], noted for its scalability and stability [26]. They benefit numerous downstream tasks, spanning image editing [27,28], video generation [29,30,31,32], 3D content generation [33,34], etc. Through the use of highly descriptive and aligned image-text pairs at a substantial scale, text-to-image models that excel in resolutions, safety control, and the capability to render accurate scenes are obtained, e.g., Imagen [1], Midjourney [4], DALL\u2022E 3 [2], Stable Diffusion 3 [3], and Ideogram [13]. However, the exceptional capabilities of most advanced models have led providers to restrict access, typically offering only APIs, which limits their widespread and equitable use. In this paper, we aim to fill this gap by leveraging advanced VLMs to direct base model to replicate the functionality of advanced models. In contrast, some works propose to motivate models to learn from their self-generated images [35,36]. \n\nEvaluating T2I Generation with VLMs. Some automatic evaluation methods [37,38,39] are propoesd to combine the LLMs and VQA models to evaluate the generated contents. Thanks to the substantial advancement of large language models [40,41,42,43,44,45], the capabilities of VLMs are largely boosted [46,47,48,49,50,51]. Utilizing these enhanced capabilities, methods building on VLMs are designed to facilitate the evaluation of text-to-image generation [52,53,54,37,55].",
            "score": 0.5258628224381883,
            "section_title": "Related Works",
            "char_start_offset": 7627,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1243
                },
                {
                    "start": 1246,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1713
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 165,
                    "matchedPaperCorpusId": "254854389"
                },
                {
                    "start": 165,
                    "end": 168,
                    "matchedPaperCorpusId": "3719281"
                },
                {
                    "start": 211,
                    "end": 215,
                    "matchedPaperCorpusId": "54482423"
                },
                {
                    "start": 304,
                    "end": 308,
                    "matchedPaperCorpusId": "258959002"
                },
                {
                    "start": 311,
                    "end": 313,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 372,
                    "end": 376,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 441,
                    "end": 445,
                    "matchedPaperCorpusId": "244909410"
                },
                {
                    "start": 474,
                    "end": 477,
                    "matchedPaperCorpusId": "265609343"
                },
                {
                    "start": 477,
                    "end": 480,
                    "matchedPaperCorpusId": "266053833"
                },
                {
                    "start": 508,
                    "end": 511,
                    "matchedPaperCorpusId": "257279850"
                },
                {
                    "start": 745,
                    "end": 748,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 775,
                    "end": 778,
                    "matchedPaperCorpusId": "264403242"
                },
                {
                    "start": 1235,
                    "end": 1239,
                    "matchedPaperCorpusId": "265506678"
                },
                {
                    "start": 1317,
                    "end": 1321,
                    "matchedPaperCorpusId": "257636562"
                },
                {
                    "start": 1475,
                    "end": 1479,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1557,
                    "end": 1560,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.896484375
        },
        {
            "corpus_id": "271924334",
            "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation",
            "text": "In recent years, an increasing number of studies (Ge et al., 2024;Wu et al., 2023b;Tang et al., 2024;Ye et al., 2024a;Dong et al., 2024;Aiello et al., 2024) have focused on unified multimodal language models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;Sun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive modeling to generate images. SEED-X (Ge et al., 2024) proposes a unified and versatile foundation system capable of handling both multimodal understanding and generation tasks. In this approach, continuous image representations from CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model (LLM) to perform next-word prediction and image representation regression. DreamLLM (Dong et al., 2024) also explores the potential of enabling multimodal comprehension and creation based on LLMs. Chameleon (Team, 2024) introduces a family of token-based mixed-modal models capable of both comprehending and generating images. This approach represents all modalities as discrete tokens and utilizes a unified transformer-based architecture and trains the model from scratch in an end-to-end manner. Compared to this work, we also adopt discrete tokens to represent all modalities. In contrast, we utilize a discrete diffusion process instead of autoregressive modeling for visual generation.",
            "score": 0.5245112408502469,
            "section_title": "UNIFIED VISION-LANGUAGE FOUNDATION MODEL",
            "char_start_offset": 11116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1416
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 101,
                    "matchedPaperCorpusId": "258822817"
                },
                {
                    "start": 118,
                    "end": 136,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 636,
                    "end": 658,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 809,
                    "end": 827,
                    "matchedPaperCorpusId": "261975252"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "270391793",
            "title": "VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks",
            "text": "Conventional MLLMs. With the advancement of large language models (LLMs) [145,146,21,215,171,37,172,13,170,9,103,7,59,43,22], multimodal large language models (MLLMs) have also gained significant momentum recently. Notable commercial models include GPT-4V [2], Gemini series [169,150], Claude-3 [10], and Qwen-VL-Max [14], known for their outstanding performance. \n\nEarly open-source MLLMs like InstructBLIP [42], LLaVA [107] and MiniGPT-4 [223] fine-tune on instruction-following datasets. InternVL [34,33] series models align a large-scale vision encoder with LLMs and perform comparably to commercial models. Efficient MLLMs [100,228,38] have also studied. However, these models only can output text, restricting their applications. \n\nExtension of MLLMs' Text Output. To extend MLLMs to downstream tasks, models like Kosmo-2 [140], Shikra [27], VisionLLM [182], Ferret [201,212], and All-Seeing V2 [180] achieve this using specially-designed tokens or encoding coordinates as text tokens. Despite these advancements, using LLMs solely as visual decoders falls short of resolving the fine-grained visual context needed for precise detection and segmentation. The other line of works focus on broadening the modality scope. AnyGPT [210] builds a multimodal text-centric dataset for any-to-any multimodal generation (text, image, speech, music) with sequence modeling. Chameleon [168] uses fully token-based representations for both texts and images, capable of understanding and generating interleaved imagetext sequences. CM3leon [5,205] are autoregressive models for text-to-image and image-to-text tasks. All these works could unify image understanding and generation in one network. Our model can support more vision and vision-language tasks. \n\nMLLMs w/ Downstream Tools.",
            "score": 0.5222914342844441,
            "section_title": "Multimodal Large Language Model",
            "char_start_offset": 5416,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 363
                },
                {
                    "start": 366,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1748
                },
                {
                    "start": 1751,
                    "end": 1777
                }
            ],
            "ref_mentions": [
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 420,
                    "end": 425,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 440,
                    "end": 445,
                    "matchedPaperCorpusId": "258291930"
                },
                {
                    "start": 858,
                    "end": 863,
                    "matchedPaperCorpusId": "258762579"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88671875
        },
        {
            "corpus_id": "262898422",
            "title": "Jointly Training Large Autoregressive Multimodal Models",
            "text": "Autoregressive text-to-image models, as exemplified by works such as Yu et al. (2023;2022), have made remarkable strides in generating highly detailed images, paralleling the achievements of Diffusion Models Nichol et al. (2022); Ramesh et al. (2022); Rombach et al. (2022). These models bear architectural resemblance to Large Language Models (LLMs), yet their training regimen is tailored for paired image-text data. LLMs on the other hand (Brown et al., 2020;Zhang et al., 2022;Touvron et al., 2023) are limited to text-based output, thus lacking multimodal generative capabilities despite their proficiency in textual tasks. The subfield of Multimodal Large Models has emerged in recent years Tsimpoukelli et al. (2021); Alayrac et al. (2022); Li et al. (2022a) in the quest to bring together the disparate strengths of vision and language models. Despite important advances in this direction, these models still predominantly generate one modality, thereby constraining their expressiveness. This study aspires to break this limitation by developing a multimodal model capable of generating integrated text and image outputs. \n\nTo achieve this objective, we conduct a comprehensive empirical investigation into the fusion of two specialized autoregressive, decoder-only, large transformer models, each designed for unique tasks (one for text-to-image and a text only model). We introduce a set of methods under the umbrella of the Joint Autoregressive Mixture (JAM) framework. In building this framework, we take advantage of the inherent architectural compatibility of autoregressive text-to-image models with LLMs, allowing us to do deep model fusion and joint training in ways which would otherwise not be possible. Our modular and data-efficient solution allows for deep, rapid and effective integration of continually evolving large models, using less than 1% of the original pretraining data for both parent models. \n\nOur contributions to this study are twofold.",
            "score": 0.5179802691243277,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1926
                },
                {
                    "start": 1929,
                    "end": 1973
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 273,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 442,
                    "end": 462,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 697,
                    "end": 723,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 725,
                    "end": 746,
                    "matchedPaperCorpusId": "252199400"
                },
                {
                    "start": 748,
                    "end": 765,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96826171875
        },
        {
            "corpus_id": "272911245",
            "title": "MIO: A Foundation Model on Multimodal Tokens",
            "text": "In this paper, we introduce MIO, a novel foundation model built on multimodal tokens, capable of understanding and generating speech, text, images, and videos in an end-to-end, autoregressive manner. While the emergence of large language models (LLMs) and multimodal large language models (MM-LLMs) propels advancements in artificial general intelligence through their versatile capabilities, they still lack true any-to-any understanding and generation. Recently, the release of GPT-4o has showcased the remarkable potential of any-to-any LLMs for complex real-world tasks, enabling omnidirectional input and output across images, speech, and text. However, it is closed-source and does not support the generation of multimodal interleaved sequences. To address this gap, we present MIO, which is trained on a mixture of discrete tokens across four modalities using causal multimodal modeling. MIO undergoes a four-stage training process: (1) alignment pre-training, (2) interleaved pre-training, (3) speech-enhanced pre-training, and (4) comprehensive supervised fine-tuning on diverse textual, visual, and speech tasks. Our experimental results indicate that MIO exhibits competitive, and in some cases superior, performance compared to previous dual-modal baselines, any-to-any model baselines, and even modality-specific baselines. Moreover, MIO demonstrates advanced capabilities inherent to its any-to-any feature, such as interleaved video-text generation, chain-of-visual-thought reasoning, visual guideline generation, instructional image editing, etc.",
            "score": 0.5177152996681138,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "274777367",
            "title": "The Language of Motion: Unifying Verbal and Non-verbal Language of 3D Human Motion",
            "text": "Recent years have witnessed the rise of language models [7,14,54,55,66,85], primarily leveraging transformer architectures [61] that process text tokens as input and generate text tokens. Building upon these advancements, substantial efforts have expanded into multimodal language models capable of handling various types of input and output, with notable examples including BLIP-2 [32], LLaVA [42], and VideoChat [35]. Furthermore, the scope of multimodal language models (MM-LLMs) has broadened to include modality-specific outputs, as demonstrated by models like GILL [27] and SpeechGPT [74]. Efforts such as LLaVA [42] and AudioGPT [22] are advancing towards seamless any-to-any modality conversion, with the goal of emulating human-like cognitive abilities in multimodal contexts. Inspired by this line of work, we propose a new framework aimed at unifying verbal and non-verbal language within language models. Our framework takes text, speech, and motion data as input and generates human motion or text as output, further exploring the potential synergy between different tasks and modalities to enhance the performance of human motion generation.",
            "score": 0.51708805067084,
            "section_title": "Multimodal Language Models",
            "char_start_offset": 6303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1155
                }
            ],
            "ref_mentions": [
                {
                    "start": 56,
                    "end": 59,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 62,
                    "end": 65,
                    "matchedPaperCorpusId": "263835355"
                },
                {
                    "start": 123,
                    "end": 127,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 382,
                    "end": 386,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 571,
                    "end": 575,
                    "matchedPaperCorpusId": "258959284"
                },
                {
                    "start": 636,
                    "end": 640,
                    "matchedPaperCorpusId": "258309430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8623046875
        },
        {
            "corpus_id": "276580677",
            "title": "A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models",
            "text": "In our paper, we investigate the most common generative VLMs which are developed by connecting a vision encoder (e.g., CLIP) to a large language model through a bridge module. This bridge module (e.g., a few MLP layers (Liu et al., 2023a) or a Q-former (Li et al., 2023a)) is then trained on large-scale image-text pairs. Frozen (Tsimpoukelli et al., 2021) is one of the first works to take advantage of a large language model in image understanding tasks (e.g., few-shot learning). Follow-up works such as MiniGpt (Zhu et al., 2023), BLIP variants (Li et al., 2023b) and LLava (Liu et al., 2023a) improved on Frozen by modifying the scale and type of the training data, as well as the underlying architecture. In recent times, much focus has been geared toward curating high-quality imagetext pairs encompassing various vision-language tasks. Qwen (Yang et al., 2024a), Pixtral (Agrawal et al., 2024) and Molmo (Deitke et al., 2024) are some of the recent multimodal language models focusing on high-quality image-text curated data. Multimodal language models have various realworld applications, such as VQA, and image captioning. \n\nNote. We acknowledge the emergence of unified transformer-based multimodal models capable of both image generation and multimodal understanding, such as (Xie et al., 2024a;Team, 2024;Dong et al., 2024). However, we exclude these from our discussion due to the absence of mechanistic interpretability studies on them. Besides, another variant of model architecture, which is designed to generate interleaved images and text, such as GILL (Koh et al., 2024), combines an MLLM and a diffusion model into one system. We will classify such a model based on its analyzed components.",
            "score": 0.515675890458705,
            "section_title": "Generative Vision-Language Models",
            "char_start_offset": 9045,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1132
                },
                {
                    "start": 1135,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1711
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 271,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1572,
                    "end": 1590,
                    "matchedPaperCorpusId": "258959284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "263137930",
            "title": "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model",
            "text": "Large Language Models (LLMs), known for their substantial size and complexity, have significantly enhanced the capacity of machines to understand and articulate human language. The progress in LLMs has also led to notable advancements in the vision-language domain [1,2,3,4], bridging the gap between image encoders and LLMs to combine their reasoning capabilities. Prior multimodal LLM research has concentrated on models that combine text and one other modality [3,5], such as text and image models, or has centered on proprietary language models that are not open sourced [2,4]. To tackle the previously mentioned challenges, we introduce Any-Modality Augmented Language Model (AnyMAL) -a collection of multi-modal encoders trained to transform data from various modalities, including images, videos, audio, and IMU motion sensor data, into the text embedding space of an LLM. To achieve this, we extend the work by [1] to (1) more capable instruction-tuned LLMs (i.e. LLaMA-2-70B-chat [6]), (2) larger pre-trained modality encoders, and (3) advanced projection layers to handle variable input lengths. The model output examples are shown in Figure 1, and an illustration of the overall methodology is shown in Figure 2. \n\nThe key contributions of the work are as follows: \n\n\u2022 We present an efficient and scalable solution for building Multimodal LLMs. We provide projection layers pre-trained on large datasets with diverse modalities (e.g. 200M images, 2.2M audio, 500K IMU time-series, 28M videos) all aligned to the same LLM (LLaMA-2-70B-chat), thus enabling interleaved multimodal in-context prompting. \n\n\u2022 We further fine-tune the model with the multimodal instruction set across three modalities (image, video, and audio) covering diverse unconstrained tasks beyond simple QA domains. The dataset features high-quality manually collected instruction data, which we thus also use as a benchmark for complex multimodal reasoning tasks.",
            "score": 0.5142720062282163,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1275
                },
                {
                    "start": 1278,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1610
                },
                {
                    "start": 1613,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 268,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 268,
                    "end": 270,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 575,
                    "end": 578,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 919,
                    "end": 922,
                    "matchedPaperCorpusId": "235658331"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94189453125
        },
        {
            "corpus_id": "266573642",
            "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
            "text": "integrating multiple modalities. Overall, the architecture enables the integration of diverse modalities into a large language model for multimodal understanding and response generation. \n\nPandaGPT is a model that integrates multimodal encoders from ImageBind and language models from Vicuna to perform instruction-following tasks across six modalities: image/video, text, audio, depth, thermal, and IMU. It demonstrates the ability to connect information from different modalities and compose their semantics naturally, enabling tasks such as image description generation, story writing inspired by videos, and answering questions about audios. PandaGPT's training on aligned image-text pairs allows it to display emergent cross-modal capabilities for data other than image and text, paving the way for holistic understanding of inputs across different modalities. \n\nLAMM introduces the Language-Assisted multimodal (LAMM) dataset, framework, and benchmark, aiming to facilitate the and evaluation of multimodal large language models (MLLMs). The main contributions include the comprehensive dataset and benchmark covering a wide range of vision tasks for 2D and 3D vision, a detailed methodology for constructing multimodal instruction tuning datasets, and a primary MLLM training framework optimized for modality extension. Additionally, the paper provides baseline models, extensive experimental observations, and analysis to accelerate future research in the field of multimodal language models. \n\nLLaVAR As shown in Figure 28, SEED is a discrete image tokenizer that converts 2D raster-ordered features into a sequence of causal semantic embeddings, which are further discretized into quantized visual codes with causal dependency. These visual codes are then decoded into generation embeddings aligned with the latent space of a pre-trained model, allowing for the generation of realistic images. SEED enables Large Language Models to perform scalable multimodal autoregression on interleaved visual and textual data, thus unifying multimodal comprehension and generation tasks within a single framework. \n\nOtterHD introduces OtterHD-8B model, which addresses the limitations of fixed-resolution inputs in Large Multimodal Models (LMMs). It leverages the Fuyu-8B ar-Fig. 28: Illustration of SEED-LLaMA [137].",
            "score": 0.5137879786948205,
            "section_title": "Visual Assistant",
            "char_start_offset": 68635,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 186
                },
                {
                    "start": 189,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1500
                },
                {
                    "start": 1503,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2111
                },
                {
                    "start": 2114,
                    "end": 2244
                },
                {
                    "start": 2245,
                    "end": 2277
                },
                {
                    "start": 2278,
                    "end": 2315
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "276421723",
            "title": "Natural Language Generation from Visual Sequences: Challenges and Future Directions",
            "text": "The Vision-to-Language Bridge Some V2L model architectures utilize an intermediate module that bridges the input and output modalities for effectively conditioning the text generation on the extracted visual features. Different models operationalize this module with different degrees of complexity. Earlier approaches for several multiimage-to-text tasks condition the text generation process by directly fusing vision encoder outputs with the language decoder input (Kim et al., 2018). Some architectures employ cross-attention mechanisms to focus on the relevant parts of the visual features at various temporal positions during decoding (Yao et al., 2015). However, approaches that adopt pre-trained models-e.g., CLIP-ViT-L (Radford et al., 2021) as the visual model-tend to employ learnable intermediate layers for aligning and converting outputs of the vision encoder into a format that the language decoder can understand. \n\nIn some of the proposed models, this intermediate module is a single linear layer that transforms the visual features into a common shared space, which can be used by the language decoder (Ko et al., 2023;Liu et al., 2023). In other models, advanced transformer-based projectors such as a Q-Former (Li et al., 2023c) are used for their ability to leverage cross-modal interactions effectively (Han et al., 2024). In essence, Q-Former uses dynamic query vectors that are pre-trained to attend to both visual and textual representations, enhancing its ability to generalize and perform well (relative to a single linear layer) across different tasks. Besides these popular methods for adapting multimodal information, some approaches make use of graph neural networks for capturing relationships between objects in the images at different temporal positions and words in the corresponding sentences of the text (Zhang and Peng, 2019). While there is no definitive way to design this intermediate module, recent work has compared the two approaches, i.e., using cross-attention between modalities or using a multimodal projector for transforming vision encoder features into the language space, and found that the latter leads to a stable/improved per-formance of models (Lauren\u00e7on et al., 2024b).",
            "score": 0.5129156990992405,
            "section_title": "4.2",
            "char_start_offset": 19973,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 929
                },
                {
                    "start": 932,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 2226
                }
            ],
            "ref_mentions": [
                {
                    "start": 641,
                    "end": 659,
                    "matchedPaperCorpusId": "623318"
                },
                {
                    "start": 728,
                    "end": 750,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1120,
                    "end": 1137,
                    "matchedPaperCorpusId": "264438945"
                },
                {
                    "start": 1230,
                    "end": 1248,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1325,
                    "end": 1343,
                    "matchedPaperCorpusId": "269293975"
                },
                {
                    "start": 1841,
                    "end": 1863,
                    "matchedPaperCorpusId": "184487531"
                },
                {
                    "start": 2200,
                    "end": 2225,
                    "matchedPaperCorpusId": "269587869"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85888671875
        },
        {
            "corpus_id": "278394475",
            "title": "Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation",
            "text": "Visual understanding and generation have traditionally been handled by separate models: vision language models [49,50] for understanding and diffusion models [20,25,28,69] for generation. Recently, efforts to unify the two tasks into a single foundation model have gained momentum, driven by the potential to produce higher quality, more consistent multimodal content [2,85]. \n\nUnified Framework A central challenge in building unified models is the misalignment between text and visual modalities. State-of-the-art vision-language models (VLMs) are typically built on GPT-style large language models (LLMs) [7,63,64], and trained via next-token prediction [4,35,81]. In contrast, high-performance visual generation models are based on diffusion models [18,32]. Recent efforts attempted to unify visual generation within an auto-regressive (AR) framework by using next-token prediction. For example, the Emu series [72,73] consider images as continuous feature vectors concatenated with text embeddings, allowing a language model [77] to generate both text tokens and image vectors autoregressively. Similarly, Chameleon [74] replaces continuous image features with discrete visual tokens. However, AR methods often struggle to generate high-quality images [8,39,75], compared to diffusion-based approaches. \n\nOther approaches embed diffusion modules directly into an LLM backbone. For example, recent approaches such as Show-O [88] and TransFusion [96], are able to generate text token-by-token, and synthesize an images through a diffusion process, by sharing a transformer backbone [60,78]. This can improve the performance of visual generation, but might sacrifice the understanding capability, due to the conflicts of two tasks learning with shared parameters [44,45]. In Mogao, we adopt the MMDiT architecture [18], which decouples two tasks by using separate text and visual parameters to reduce such cross-modal conflicts. \n\nUnified Representation Another challenge lies in the differing requirements of visual representations for understanding and generation [55,62]. Features optimized for understanding may be suboptimal for generation, making visual tokenizer design critical.",
            "score": 0.5119988296519868,
            "section_title": "Related Work",
            "char_start_offset": 3738,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1307
                },
                {
                    "start": 1310,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1930
                },
                {
                    "start": 1933,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2188
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 115,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 115,
                    "end": 118,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 168,
                    "end": 171,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 368,
                    "end": 371,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 608,
                    "end": 611,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 753,
                    "end": 757,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 1260,
                    "end": 1263,
                    "matchedPaperCorpusId": "270560593"
                },
                {
                    "start": 1263,
                    "end": 1266,
                    "matchedPaperCorpusId": "268876071"
                },
                {
                    "start": 1585,
                    "end": 1589,
                    "matchedPaperCorpusId": "254854389"
                },
                {
                    "start": 1816,
                    "end": 1820,
                    "matchedPaperCorpusId": "268247980"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9189453125
        },
        {
            "corpus_id": "273962846",
            "title": "Autoregressive Models in Vision: A Survey",
            "text": "To further enhance the efficacy of visual in-context learning, CoTVL (Ge et al., 2023) successfully applies visual chain-of-thought prompt tuning for vision-language modeling, and performs better in tasks that require more reasoning abilities. These models highlight the potential for autoregressive frameworks to serve as foundational architectures in multi-modal applications, promoting a more holistic approach to understanding and generating content across modalities. \n\nThe previously mentioned multimodal tokenization involves encoding images into tokens, allowing Large Language Models (LLMs) to process both visual and language signals in a unified space. Visionto-Language (V2T) Tokenizer (Zhu et al., 2024a) maps image patches to discrete tokens that correspond to LLM vocabularies, enabling tasks like inpainting and deblurring. Similarly, the Multimodal Cross-Quantization VAE (MXQ-VAE) (Lee et al., 2022b) encodes both image and text inputs as tokens, generating coherent multimodal outputs. These approaches improve image-text generation, though they typically predict pixels in a fixed order without considering random generation strategies. \n\nIn summary, the evolution of visual autoregressive models in the realm of multi-modality underscores their growing importance and versatility. From early integrations to sophisticated native architectures, these models continue to push the boundaries of what's possible in generating and understanding multimodal content. Future research directions may focus on improving model efficiency, scalability, and the ability to handle an even broader array of modalities.",
            "score": 0.51152617178875,
            "section_title": "The Framework for Unifying Multimodal Understanding and Generation",
            "char_start_offset": 88771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 472
                },
                {
                    "start": 475,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1156
                },
                {
                    "start": 1159,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1624
                }
            ],
            "ref_mentions": [
                {
                    "start": 698,
                    "end": 717,
                    "matchedPaperCorpusId": "268363788"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91064453125
        },
        {
            "corpus_id": "261660318",
            "title": "Evaluation and Enhancement of Semantic Grounding in Large Vision-Language Models",
            "text": "We illustrate one representative architecture of LVLMs in Figure 1, which typically consists of the following components: \n\n1. a visual encoder that extracts features from the input image; 2. a projector that projects visual features to the language embedding space; 3. a tokenizer that tokenizes textual input into textual tokens and maps them into language embedding space; 4. a decoder-only LLM (e.g., LLaMA) that generates textual responses based on the multimodal inputs. \n\nThe overall output generation process of a LVLM F \u0398 can be formally described by where \u0398 are parameters of the LVLM, X v denotes the visual input, X q denotes the textual input, and Y denotes the generated output sequence. Specifically, given a visual input X v , a visual encoder f \u03d5 firstly extracts visual features by \n\nwhere A trainable projector, denoted as g \u03c9 , is then applied to convert Z v into language embedding space, which can be defined by \n\nwhere H v \u2208 R l v \u00d7h , and \u03c9 are trainable parameters of g \u03c9 . h denotes the dimension of language embedding space. In practise, an efficient implementation of g \u03c9 can be a projec- \n\nMore sophisticated projectors are also proposed, such as Qformer (Li et al. 2023b). \n\nGiven the input textual query X q , a tokenizer k \u03c8 is employed to tokenize and map them into \"textual tokens\" by: \n\nwhere H q \u2208 R l q \u00d7h , and \u03c8 are trainable parameters of k \u03c8 . It is worth noting that H q and projected H v have the same dimensionality R h as the input of the large language model p \u03b8 . \n\nSimilarly, the length of textual tokens varies depending on the choice of tokenizer, as words are chunked into subwords. Moreover, special tokens [s], [/s] can be added to indicate the span of visual tokens and textual tokens. Therefore, we denote the process of LVLM to prepare the multimodal input of its LLM component as: \n\nOnce the multimodal input features are processed, the LLM component of an LVLM is responsible for generating responses.",
            "score": 0.5107340466548127,
            "section_title": "Preliminaries",
            "char_start_offset": 4797,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 124,
                    "end": 476
                },
                {
                    "start": 479,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 933
                },
                {
                    "start": 936,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1116
                },
                {
                    "start": 1119,
                    "end": 1202
                },
                {
                    "start": 1205,
                    "end": 1319
                },
                {
                    "start": 1322,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1837
                },
                {
                    "start": 1840,
                    "end": 1959
                }
            ],
            "ref_mentions": [
                {
                    "start": 1184,
                    "end": 1200,
                    "matchedPaperCorpusId": "256390509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80712890625
        },
        {
            "corpus_id": "266362535",
            "title": "A Survey of Reasoning with Foundation Models: Concepts, Methodologies, and Outlook",
            "text": "Text-to-image Generation. Stable Difusion [236] integrates cross-attention layers to the model architecture, transforming difusion models into robust and adaptable generative models for diverse conditional inputs like text and bounding boxes. The application of Latent Difusion Models (LDMs) represents a signiicant breakthrough in image inpainting, while also delivering impressive results in unconditional content generation, super-resolution image generation, and other tasks. Notably, LDMs ofer substantial reductions in computational demands compared to pixel-based difusion models, while maintaining highly competitive performance. DALL\u2022E \u00a7 [19] is an advanced AI system that has the capability to generate realistic images and artwork based on natural language descriptions. Likewise, Midjourney is another AI system that specializes in generating images based on natural language descriptions, which are referred to as \"prompts\u017e. By leveraging the power of AI, Midjourney \u00a7 can translate textual prompts into visual compositions, providing a visual representation of the given description. ImageGen [238] leverages the capabilities of expansive transformer language models for text comprehension and combines this with the eicacy of difusion models for creating high-quality images. PixArt [34] is a Transformer-driven Text-to-Image (T2I) difusion model. It rivals leading image generation systems such as Imagen, SDXL, and Midjourney in terms of quality, approaching the benchmarks set by commercial applications. \n\nMultimodal-to-text Generation. Flamingo-80B [4] comprises a family of Visual Language Models (VLMs) equipped with in-context few-shot learning capabilities. These models undergo thorough evaluation across a wide array of tasks, including open-ended ones like visual question-answering and captioning, as well as closed-ended tasks such as multiple-choice visual question-answering. Frozen [274] accomplishes few-shot learning ability within a multimodal context by preserving the language capabilities of a Language Model (LM) while incorporating visual information as a preix. Frozen achieves this by freezing the LM and training a separate vision encoder to represent images. In the Frozen approach, visual information is represented as a sequence of embeddings, serving as a visual preix.",
            "score": 0.5105964744947828,
            "section_title": "Generation.",
            "char_start_offset": 63456,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1522
                },
                {
                    "start": 1525,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2102
                },
                {
                    "start": 2103,
                    "end": 2202
                },
                {
                    "start": 2203,
                    "end": 2316
                }
            ],
            "ref_mentions": [
                {
                    "start": 647,
                    "end": 651,
                    "matchedPaperCorpusId": "264403242"
                },
                {
                    "start": 1107,
                    "end": 1112,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1569,
                    "end": 1572,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1914,
                    "end": 1919,
                    "matchedPaperCorpusId": "235658331"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9287109375
        },
        {
            "corpus_id": "270440387",
            "title": "EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal Prompts",
            "text": "Developing Text-to-Image Capability.Through ELLA's training paradigm, we have developed a text-to-image model endowed with robust text-to-image capabilities.As illustrated in the first row of Figure 4, ELLA can generate images that strictly adhere to instructions, which forms the foundation for EMMA's multi-modal guidance.\n\nSelective Modular Feature Training.To bolster the stability and enhance the final performance of the training process, we have integrated several innovative design elements into the network architecture.For example, the alternating structure between the Perceiver Resampler and the Assemblable Gated Perceiver Resampler is designed to limit the feature space of the network's intermediate layers.This prevents image information from imparting excessive prior knowledge that might compromise the text's control and disrupt the final generation outcomes.The Assemblable Gated Perceiver Resampler includes separated gates that enable the incorporation of additional features into a few trainable embeddings.\n\nAssembling Modules for Multi-Condition Image Generation.After establishing strong models for each individual condition, we have devised an innovative approach that enables the model to The process can be mathematically expressed as:\n\nIn this manner, various conditions can be applied to the image generation process without the need for further training.",
            "score": 0.51053004964292,
            "section_title": "Image Generation with Multiple Conditions",
            "char_start_offset": 11437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 36,
                    "end": 157
                },
                {
                    "start": 157,
                    "end": 324
                },
                {
                    "start": 326,
                    "end": 361
                },
                {
                    "start": 361,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 722
                },
                {
                    "start": 722,
                    "end": 878
                },
                {
                    "start": 878,
                    "end": 1030
                },
                {
                    "start": 1032,
                    "end": 1088
                },
                {
                    "start": 1088,
                    "end": 1264
                },
                {
                    "start": 1266,
                    "end": 1386
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73291015625
        },
        {
            "corpus_id": "274860024",
            "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
            "text": "Over the past few years, we have seen significant progress in multimodal generative models capable of understanding and generating interleaved text and images in arbitrary sequences (Dong et al., 2023;Koh et al., 2024;Lin et al., 2024b). Models like Transfusion (Zhou et al., 2024), Chameleon (Team, 2024b), and Unified-IO (Lu et al., 2022(Lu et al., , 2024) ) demonstrate the potential of unified architectures that seamlessly handle both image and text modalities. However, these models typically train from scratch, demanding significant computational resources to achieve proficiency across all modalities. The computational cost of mastering even a single modality is substantial-training a state-of-the-art text-only large language models (LLMs) like Llama-3 (Dubey et al., 2024) requires training over 15 trillion tokens. \n\nGiven these computational demands, we investigate an alternative paradigm that reuses and adapts existing pretrained LLMs (Ge et al., 2023;Sun et al., 2023;Wu et al., 2024b). We address a fundamental research question: How to preserve the text-only performance of pretrained LLMs while equipping them with visual understanding and generation abilities? Our experiments show that naive finetuning of pretrained text-only LLMs on multimodal data leads to significant degradation of their language processing capabilities. \n\nTo address this challenge, we introduce LMFusion, a framework that enhances a pretrained text-only LLM, Llama-3 (Dubey et al., 2024) with multimodal capabilities by building upon the recipe of Transfusion (Zhou et al., 2024). Drawing from recent and parallel work on modality separation (Shen et al., 2023;Chen et al., 2023;Liang et al., 2024;Liu et al., 2024a), LMFusion integrates the original Llama modules pretrained for language processing while introducing additional dedicated transformer modules for visual understanding and generation tasks.",
            "score": 0.5102285217352757,
            "section_title": "Introduction",
            "char_start_offset": 2915,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 828
                },
                {
                    "start": 831,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1903
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 218,
                    "matchedPaperCorpusId": "258959284"
                },
                {
                    "start": 323,
                    "end": 339,
                    "matchedPaperCorpusId": "249848272"
                },
                {
                    "start": 339,
                    "end": 360,
                    "matchedPaperCorpusId": "266573555"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95947265625
        },
        {
            "corpus_id": "267406379",
            "title": "Can MLLMs Perform Text-to-Image In-Context Learning?",
            "text": "Consequently, we focus on models capable of processing both text and multiple images. We consider two types of MLLMs: (i) proficient in generating both text and images, including Emu (Sun et al., 2023b), GILL (Koh et al., 2023), and SEED-LLaMA (Ge et al., 2023b), and (ii) those limited to text generation, including GPT-4V (OpenAI, 2023), LLaVA (Liu et al., 2023b), and Qwen-VL (Bai et al., 2023b). For text-only MLLMs, we evaluate their capacity to infer visual outputs by prompt-ing them to describe the anticipated image. Conversely, for MLLMs capable of image generation, we not only elicit image outputs but also ask for descriptive text, ensuring an apple-to-apple comparison with text-only models. \n\nImage-to-Text ICL in MLLMs. Most existing work on M-ICL focuses on the image-to-text generation, i.e., I2T-ICL, which involves mapping from high-dimensional input (i.e., images) to low-dimensional output (i.e., text). In particular, Tsimpoukelli et al. (2021) were the first to extend ICL from the text domain to the multimodal domain, focusing on image-to-text generation such as visual question-answering (see Figure 1(c)). Alayrac et al. (2022) introduced Flamingo, an MLLM that achieves state-of-the-art performance in a variety of image and video understanding tasks using I2T-ICL with 32 demonstrations, implying the efficacy of I2T-ICL in performance enhancement in their model. In contrast, Monajatipoor et al. (2023) explores whether the in-context capabilities of LLMs can be seamlessly extended to I2T-ICL by incorporating a visual encoder. Chen et al. (2023b) conducted a systematic study on the importance of visual and textual information in I2T-ICL.",
            "score": 0.5094237382423028,
            "section_title": "Related Works",
            "char_start_offset": 13033,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1672
                }
            ],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 365,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 941,
                    "end": 967,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 1134,
                    "end": 1155,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1407,
                    "end": 1433,
                    "matchedPaperCorpusId": "259064260"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9130859375
        },
        {
            "corpus_id": "271693411",
            "title": "A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks",
            "text": "These fusion processes often employ pre-trained LLMs, which, while initially designed for textual data, are adapted to handle and synthesize multimodal inputs through advanced feature projection and serialization techniques. \n\nMultimodal Output Decoder: Lastly, the multimodal output decoder reconverts the fused, integrated multimodal information back into a usable form tailored to specific tasks, such as Image captioning, the decoder might generate descriptive text based on visual inputs. Video understanding tasks, it could produce annotations or summaries combining both visual and auditory data. Each decoder is meticulously designed to optimize accuracy and quality, ensuring that the output precisely reflects the combined insights gained from the integrated modalities. \n\nTo summarize, the sophisticated architecture of multimodal large language models empowers them to tackle complex tasks by harnessing and synthesizing data across text, images, and audio. This capability not only enhances the performance of AI applications but also opens up new avenues for innovation in how we understand and interact with technology.",
            "score": 0.5088131217178818,
            "section_title": "B. Main Components of Multimodal Large Language Models",
            "char_start_offset": 11790,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 227,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 780
                },
                {
                    "start": 783,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1134
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "271709458",
            "title": "REVISION: Rendering Tools Enable Spatial Fidelity in Vision-Language Models",
            "text": "Generative vision-language models [36,44] represent a significant step towards developing multimodal systems that bridge the gap between computer vision and natural language processing. Text-to-image (T2I) models [5,38] convert text prompts to high-quality images, while multimodal large language models (MLLMs) [25,48] process images as inputs, and generate rich and coherent natural language outputs in response. As a result, these models have found diverse applications in robotics [45], image editing [17], image-to-image translation [31], and more. However, recent studies [20] and benchmarks such as Fig. 1: Text-to-Image models struggle to generate images that faithfully represent the spatial relationships mentioned in the input prompt. We develop REVISION, an efficient rendering pipeline that enables a training-free and guidance-based mechanism to address this shortcoming. Our method results in improvements in spatial reasoning for T2I models for three dimensional relationships demonstrated by consistently higher scores on VISOR and T2I-CompBench benchmarks. \n\nDALL-Eval [8], VISOR [15], and T2I-CompBench [18] have found that generative vision-language models suffer from a common mode of failure -their inability to correctly reason over spatial relationships. We postulate that the lack of spatial understanding in generative visionlanguage models is a result of the lack of guidance from image-text datasets. Compared to T2I models, graphics rendering tools such as Blender allow deterministic and accurate object placement, but are limited by their lower visual detail and photorealism and do not have intuitive workflows such as T2I models where users can generate images by simply typing a sentence. To get the best of both worlds, in this work, we develop REVISION, a Blender-based image rendering pipeline which enables the synthesis of images with 101 3-dimensional object (assets), 11 spatial relationships, diverse backgrounds, camera perspectives, and lighting conditions. REVISION parses an input text prompt into assets and relationships and synthesizes the scene using Blender to exactly match the input prompt in terms of both objects and their spatial arrangement.",
            "score": 0.5082168723430773,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1074
                },
                {
                    "start": 1077,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 38,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 216,
                    "end": 219,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 312,
                    "end": 316,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "251252882"
                },
                {
                    "start": 538,
                    "end": 542,
                    "matchedPaperCorpusId": "256616002"
                },
                {
                    "start": 578,
                    "end": 582,
                    "matchedPaperCorpusId": "264805345"
                },
                {
                    "start": 1087,
                    "end": 1090,
                    "matchedPaperCorpusId": "253510037"
                },
                {
                    "start": 1122,
                    "end": 1126,
                    "matchedPaperCorpusId": "259847295"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7578125
        },
        {
            "corpus_id": "258947627",
            "title": "Semantic Composition in Visually Grounded Language Models",
            "text": "Vision-language deep learning research [46] synthesizes advances in computer vision and natural language processing to build models capable of solving multimodal tasks like imagetext matching, image2text captioning, and text2image generation. There are a number of methods for learning joint representations of vision and language, but most leverage the transformer architecture [90]. \n\nEncoder models generally either use a dual unimodal encoder architecture [70], a single cross-modal encoder [17], or a combination [80]. Image-conditioned text generation models often consist of an image encoder that feeds into a causal transformer decoder [43]. Textbased image generation models are quite varied but often condition an image diffusion model on the output of a text encoder [73].",
            "score": 0.5080137013510895,
            "section_title": "Vision-Language Deep Learning",
            "char_start_offset": 6163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 384
                },
                {
                    "start": 387,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 783
                }
            ],
            "ref_mentions": [
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 495,
                    "end": 499,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 518,
                    "end": 522,
                    "matchedPaperCorpusId": "244954250"
                },
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "232035663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95556640625
        },
        {
            "corpus_id": "269187998",
            "title": "Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent",
            "text": "The rapid advancement of artificial intelligence has revolutionized the way we interact with technology, giving rise to the development of sophisticated AI agents capable of performing complex tasks and making decisions based on various forms of input, including natural language [53,44,51,15,20] and visual information [9,23].These agents have the potential to automate a wide range of processes, from simple tasks like image recognition and language translation to more complex endeavors such as medical diagnosis [18,34] and autonomous navigation [13,16].At the heart of these AI agents are multimodal language models, which enable them to understand and generate human-like responses by processing and integrating multiple modalities of data, such as text, images, and even audio or video.\n\nMultimodal language models represent a significant leap forward from traditional language models, which primarily focus on processing and generating text.By incorporating visual information, these models can better understand the context and semantics of the input data, leading to more accurate and relevant outputs.For example, a multimodal language model trained on a dataset of images and their corresponding captions would be able to generate more descriptive and contextually appropriate captions for new images compared to a text-only model.The ability to process and integrate multiple modalities of data is crucial for the development of multimodal AI agents, as it allows them to perform tasks that require an understanding of both language and visual information, such as visual question answering, image-guided navigation, and multimodal sentiment analysis.\n\nOne of the key challenges in developing multimodal language models is effectively encoding visual information into a format that can be processed by the model.This is typically achieved through the use of convolutional neural networks (CNNs) [26] or transformer-based architectures [50], such as the Vision Transformer (ViT) [3].CNNs have been widely used in computer vision tasks for their ability to extract hierarchical features from images, allowing the model to learn increasingly complex representations of the input data.On the other hand, transformer-based architectures like ViT have recently gained popularity due to their ability to capture long-range dependencies and model global context, which is particularly useful for understanding the relationships between objects in an image.",
            "score": 0.5079053147336985,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 327
                },
                {
                    "start": 327,
                    "end": 558
                },
                {
                    "start": 558,
                    "end": 793
                },
                {
                    "start": 795,
                    "end": 949
                },
                {
                    "start": 949,
                    "end": 1112
                },
                {
                    "start": 1112,
                    "end": 1343
                },
                {
                    "start": 1343,
                    "end": 1664
                },
                {
                    "start": 1666,
                    "end": 1825
                },
                {
                    "start": 1825,
                    "end": 1995
                },
                {
                    "start": 1995,
                    "end": 2194
                },
                {
                    "start": 2194,
                    "end": 2461
                }
            ],
            "ref_mentions": [
                {
                    "start": 290,
                    "end": 293,
                    "matchedPaperCorpusId": "260499677"
                },
                {
                    "start": 293,
                    "end": 296,
                    "matchedPaperCorpusId": "251872344"
                },
                {
                    "start": 320,
                    "end": 323,
                    "matchedPaperCorpusId": "3617644"
                },
                {
                    "start": 516,
                    "end": 520,
                    "matchedPaperCorpusId": "259303296"
                },
                {
                    "start": 520,
                    "end": 523,
                    "matchedPaperCorpusId": "258841310"
                },
                {
                    "start": 550,
                    "end": 554,
                    "matchedPaperCorpusId": "265308931"
                },
                {
                    "start": 554,
                    "end": 557,
                    "matchedPaperCorpusId": "259924488"
                },
                {
                    "start": 1908,
                    "end": 1912,
                    "matchedPaperCorpusId": "214803074"
                },
                {
                    "start": 1991,
                    "end": 1994,
                    "matchedPaperCorpusId": "232417054"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90869140625
        },
        {
            "corpus_id": "262898422",
            "title": "Jointly Training Large Autoregressive Multimodal Models",
            "text": "In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.",
            "score": 0.5078988503273124,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.986328125
        },
        {
            "corpus_id": "272368331",
            "title": "CV-Probes: Studying the interplay of lexical and world knowledge in visually grounded verb understanding",
            "text": "BLIP (Li et al., 2022) employs a visual transformer and BERT, integrating these through a Multimodal Mixture of Encoder-Decoder (MED) architecture. The MED operates in three modes: an unimodal encoder for separate image and text encoding, an image-grounded text encoder that incorporates visual information into text encoding via a cross-attention layer, and an image-grounded text decoder with causal self-attention layers for generating textual descriptions from images. Pretraining involves three objectives: contrastive learning, image-text matching, and language modeling. The CapFilt method enhances training data quality by generating and filtering synthetic captions to reduce noise in web-sourced image-text pairs. BLIP is pre-trained on MS-COCO, Visual Genome, Conceptual Captions, Conceptual 12M, and SBU Captions. Hard negative mining is performed, similarly to BLIP. \n\nFLAVA (Singh et al., 2022) uses a unified framework for processing unimodal and multimodal data. The architecture comprises a visual and textual transformer integrated through a shared multimodal encoder. As input to the multimodal encoder, the model uses learned linear projections of the hidden state vectors from the two modalities, concatenating these with an additional [CLS M] token. FLAVA's pre-training involves multiple objectives: unimodal objectives like masked image modeling and masked language modeling, and multimodal objectives, including image-text matching, contrastive learning, and masked multimodal modeling. The model is pre-trained on datasets like MS-COCO, SBU Captions, Localized Narratives, Wikipedia Image Text Visual Genome, ImageNet, Conceptual Captions, Conceptual Captions 12M, Red Caps, and YFCC100M. The model is intended for unimodal (vision-only, text-only) and multimodal tasks. \n\nBLIP2 (Li et al., 2023) This model introduces a Querying Transformer (Q-Former) to bridge the gap between a frozen image encoder and a frozen large language model (LLM).",
            "score": 0.5067966979365808,
            "section_title": "Models",
            "char_start_offset": 14120,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 879
                },
                {
                    "start": 882,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1796
                },
                {
                    "start": 1799,
                    "end": 1968
                }
            ],
            "ref_mentions": [
                {
                    "start": 5,
                    "end": 22,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 888,
                    "end": 908,
                    "matchedPaperCorpusId": "244954250"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92333984375
        },
        {
            "corpus_id": "273403725",
            "title": "Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation",
            "text": "In recent years, multimodal large models have made significant advancements in both understanding and generation domains [20,51]. In the field of multimodal understanding, researchers follow the design of LLaVA [51] by using a vision encoder as a bridge to enable large language models (LLMs) to understand images. In the field of visual generation, diffusion-based approaches [9,20,20,67] have seen notable success. More recently, some works have explored autoregressive methods for vision generation [73,79], achieving performance comparable to diffusion models. To build more powerful and generalist multimodal models, researchers have sought to combine multimodal understanding and generation tasks [75,77,94]. For instance, some studies have attempted to connect multimodal understanding models with pretrained diffusion models [27,28,75]. For example, Emu [75] uses the output of the LLM as a condition for a pretrained diffusion model, and then relies on the diffusion model to generate images. However, strictly speaking, this approach cannot be considered a truly unified model, because the visual generation functionality is handled by the external diffusion model, while the multimodal LLM itself lacks the capability to directly generate images. \n\nOther approaches [77,85,86,94]  Figure 1 | Multimodal understanding and vision generation results from our Janus. Janus outperforms the previous state-of-the-art unified multimodal models as well as some task-specific multimodal understanding models, while also demonstrating strong visual generation capabilities. The image resolution is 384 \u00d7 384. Best viewed on screen. \n\nderstanding and generation tasks, which improves instruction-following for visual generation, unlocks potential emergent abilities, and reduces model redundancy. Such methods typically use a single vision encoder to process inputs for both two tasks. However, the representations required by multimodal understanding and generation tasks differ significantly. In multimodal understanding tasks, the purpose of the vision encoder is to extract high-level semantic information (e.g., object categories or visual attributes within an image). The output of understanding task not only involves extracting information from images but also involves complex semantic reasoning.",
            "score": 0.5067681846039375,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1632
                },
                {
                    "start": 1635,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2305
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 125,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 125,
                    "end": 128,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 211,
                    "end": 215,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 380,
                    "end": 383,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 383,
                    "end": 386,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 386,
                    "end": 389,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "269791516",
            "title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models",
            "text": "Recent multimodal foundation models are very widely adopted but still model different modalities separately, often using modality specific encoders or decoders. This can limit their ability to integrate information across modalities and generate multimodal documents that can contain arbitrary sequences of images and text. In this paper, we present Chameleon, a family of mixed-modal foundation models capable of generating and reasoning with mixed sequences of arbitrarily interleaved textual and image content (Figures 2-4). This allows for full multimodal document modeling, which is a direct generalization of standard multimodal tasks such as image generation, understanding and reasoning over images, and text-only LLMs. Chameleon is instead designed to be mixed-modal from inception and uses a uniform architecture trained from scratch in an end-to-end fashion on an interleaved mixture of all modalities, i.e., images, text, and code. \n\nOur unified approach uses fully token-based representations for both image and textual modalities (Figure 1). By quantizing images into discrete tokens, analogous to words in text, we can apply the same transformer architecture to sequences of both image and text tokens, without the need for separate image/text encoders (Alayrac et al., 2022;Liu et al., 2023b;Lauren\u00e7on et al., 2023) or domain-specific decoders (Ramesh et al., 2022;Jin et al., 2023;Betker et al., 2023). This early-fusion approach, where all modalities are projected into a shared representational space from the start, allows for seamless reasoning and generation across modalities. However, it also presents significant technical challenges, particularly in terms of optimization stability and scaling. \n\nWe address these challenges through a combination of architectural innovations and training techniques. We introduce novel modifications to the transformer architecture, such as query-key normalization and revised placement of layer norms, which we find to be crucial for stable training in the mixed-modal setting (Section 2.3).",
            "score": 0.5063740497990757,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 1268,
                    "end": 1290,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1398,
                    "end": 1418,
                    "matchedPaperCorpusId": "264403242"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.966796875
        },
        {
            "corpus_id": "253802096",
            "title": "Retrieval-Augmented Multimodal Language Modeling",
            "text": "We present a retrieval-augmented multimodal model that can retrieve and generate both text and images. As illustrated in Figure 1, given an input multimodal document (i.e., arbitrary sequence of text/images), we use a retriever that retrieves relevant multimodal documents from an external memory, and uses a generator to refer to the retrieved documents and make predictions for the input document (i.e., generate the continuation). We design the multimodal retriever as a dense retriever with a mixed-modal encoder that can encode combinations of text and images (e.g., pretrained CLIP; \u00a73.2). We build the retrieval-augmented generator using the CM3 Transformer architecture, and we prepend the retrieved documents to the main input document that we feed into the generator ( \u00a73.3). We describe how we train this model and use it for text-to-image or image-to-text generation in \u00a73.4. \n\nNotably, our resulting model, Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate combinations of text and images, which is the most general capability among existing multimodal models (Table 1). Moreover, while we build on existing techniques such as CLIP and CM3, we are the first to establish a method to unify them into a performant retrieval-augmented model through extensive analyses of design choices ( \u00a7C.3).",
            "score": 0.5060725423797264,
            "section_title": "Approach",
            "char_start_offset": 6721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 887
                },
                {
                    "start": 890,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1349
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9912109375
        },
        {
            "corpus_id": "276742446",
            "title": "WeGen: A Unified Model for Interactive Multimodal Generation as We Chat",
            "text": "To evaluate WeGen as a unified framework, 1) we first assess its text-to-image generation capability through quantitative metrics, as this forms the foundation for all visual generation tasks. 2) We then demonstrate the framework's versatility through comprehensive case studies across a wide range of tasks, Detailed quantitative evaluations for specific tasks and more case studies are provided in \u00a74.3 and the supplementary materials. Text-to-Image Generation. In the realm of text-to-image generation, there are two primary technical approaches: one relies on pure diffusion models, while the other leverages multimodal large language models (MLLMs). Our model falls into the latter category, where we have achieved stateof-the-art (SOTA) performance among MLLM-based approaches, achieving an FID score of 9.39 and a CLIP-T score of 0.308, as shown in Table 1. Notably, our model accomplishes this with less training data and reduced computational cost, while also supporting a wide range of tasks within a unified framework. Compared to diffusion-based models, our model's performance is comparable in terms of metrics, but it offers the advantage of supporting multiple tasks and possessing both language and visual understanding capabilities. Case Studies on Diverse Tasks. As shown in Figure 3, we demonstrate WeGen's capabilities across a wide range of tasks, including text-to-image generation, subject-driven generation, condition-based generation (canny, depth, pose), style transfer, super-resolution, inpainting, outpainting, and various editing operations. These qualitative results highlight our model's versatility across diverse visual generation tasks. Detailed quantitative evaluations for specific tasks can be found in \u00a74.3 and the supplementary materials, along with additional case studies.",
            "score": 0.5059561349685963,
            "section_title": "Unified Multi-Task Generation",
            "char_start_offset": 17550,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1814
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.935546875
        },
        {
            "corpus_id": "266210376",
            "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
            "text": "VL-GPT aims to process the vision and language understanding and generation within a single transformer model in a unified way, similar to GPT handles language tasks. It is capable of perceiving the interleaved multi-modal data and generating content across various modalities. By employing unified modeling, our VL-GPT can conduct autoregressive pre-training on web-scale multimodal corpora, thereby holding the potential to serve as a powerful foundation model in the multimodal research community. \n\nArchitecture As depicted at the bottom of Fig. 1, our VL-GPT comprises five components: a large vision-language transformer model M, an image tokenizer E v , a text tokenizer E t , an image detokenizer D v and a text detokenizer D t . In comparison to a language model, VL-GPT incorporates additional image tokenizer and image detokenizer elements. \n\nGiven any interleaved image-text data, the image tokenizer and the text tokenizer initially encode them into a multimodal sequence. More specifically, the image tokenizer E v converts each image into N continuous visual embeddings x v . Additionally, two special tokens [IMG] and [/IMG] are appended at the beginning and end of the visual embeddings, respectively. The visual embeddings are then combined with the discrete text tokens encoded by the text tokenizer E t to form a interleaved multimodal sequence v = (v 1 , v 2 , . . . , v n ), where v i can be either a discrete text token or a continuous visual embedding. The multimodal sequence v is then fed into the large VL model M for unified auto-regressive modeling. \n\nThe output embedding M(v i ) can be flexibly transformed into a text embedding through a language modeling head for the predefined vocabulary or into a visual embedding with a separate regression head. During training, the selection of the transformed head depends on whether the target for the current embedding is a text token or a visual embedding.",
            "score": 0.5045350455543993,
            "section_title": "VL-GPT",
            "char_start_offset": 13294,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1578
                },
                {
                    "start": 1581,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1932
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "270068104",
            "title": "The Evolution of Multimodal Model Architectures",
            "text": "In contrast, there are significantly fewer multimodal models capable of generating output modalities other than text.Multimodal output generation is one of the primary challenge in the multimodal domain.Type-C and Type-D multimodal architectures are at the forefront of development for any-to-any multimodal models.The representative models are highlighted in Figure 7.These dominant multimodal model architectures address some, though not all, challenging aspects of multimodal generation.Type-D simplifies the training process by utilizing input tokenization, enabling the use of a standard auto-regressive objective function for model training.However, it still faces limitations in addressing the challenge of accommodating large data sizes and computational demands necessary for building multimodal generative models.Type-C tackles the challenges associated with data and resources by leveraging pretrained components and integrating them with efficient connectors/adapters.Yet, the training process remains challenging because of the diverse objective functions associated with different components in the model architecture.\n\nAt present, there are three primary approaches for constructing any-to-any multimodal models: the first involves utilizing the end-to-end trainable Type-D model architecture, the second entails leveraging the end-to-end trainable Type-C architecture, and the third method employs a combination of Type-C with agents, which is non-end-to-end trainable.Type-D architecture: Models generate multimodal outputs using tokenizers.Unified-IO, Unified-IO 2 and 4M are the models from Type-D which enable any-to-any multimodal model development.Type-C architecture: Models generate multimodal outputs without using tokenizers.NExt-GPT, CoDI and CoDI-2 models belong to Type-C assisting in any-to-any multimodal model development.Type-C + agents: In this method, a Type-C multimodal model is trained to generate specific text outputs with a general format to aid the frozen pretrained modality decoder models (like text-to-image models, text-to-video models) for multimodal generation.ModaVerse follows this process for creating an any-to-any multimodal model.",
            "score": 0.5042659554861526,
            "section_title": "Type-D: Tokenized Early Fusion (TEF)",
            "char_start_offset": 16612,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 203
                },
                {
                    "start": 203,
                    "end": 315
                },
                {
                    "start": 315,
                    "end": 369
                },
                {
                    "start": 369,
                    "end": 490
                },
                {
                    "start": 490,
                    "end": 647
                },
                {
                    "start": 647,
                    "end": 823
                },
                {
                    "start": 823,
                    "end": 980
                },
                {
                    "start": 980,
                    "end": 1132
                },
                {
                    "start": 1134,
                    "end": 1485
                },
                {
                    "start": 1485,
                    "end": 1558
                },
                {
                    "start": 1558,
                    "end": 1670
                },
                {
                    "start": 1670,
                    "end": 1751
                },
                {
                    "start": 1751,
                    "end": 1854
                },
                {
                    "start": 1854,
                    "end": 2109
                },
                {
                    "start": 2109,
                    "end": 2184
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9033203125
        },
        {
            "corpus_id": "270619856",
            "title": "Improving Visual Commonsense in Language Models via Multiple Image Generation",
            "text": "6 and Eq. 7.\n\nto the input text and uses them as contextual input to the language model [Tan and Bansal, 2020, Lu et al., 2022, Wang et al., 2023].Similarly, Tang et al. [2021] employs a knowledge distillation approach to fuse visual knowledge.Other works [Zhang et al., 2023, Guo et al., 2023, Li et al., 2023b] distill visual knowledge from multimodal embedding methods such as CLIP [Radford et al., 2021] into text-only language models.Another set of works utilizes pre-trained text-to-image generative models.In the context of diffusion-based text-to-image models, Z-LaVi [Yang et al., 2022] leverage generated visuals that match possible label predictions of a given text-only language model.Our method, instead, considers visuals that match the input text.LiVE [Tang et al., 2023] introduces a vision-text plug-and-play vision-text fusion layer, which is inserted within transformed blocks of pre-trained language models (LMs).iNLG [Zhu et al., 2023] uses generated images as additional visual supervision to guide the language model in text generation, where the visual input is provided as an additional input to the LM in the form of a visual prefix.Unlike LiVE and iNGL, which integrate visual knowledge as input to the LM or as an integrated layer, we, instead, use the output of an unmodified pre-trained LLM together with an encoding of a generated image, using a late-fusion layer.This enables our model to focus on the input text, while also using visual information if needed.Second, instead of using a specialized attention-like mechanism or a mapping network, our work aggregates scores simply by averaging predictions made using different generated images, obtained from variations of the input text.This enables our method to use a diverse set of predictions obtained using diverse visual \"experts\" and gauge its final prediction towards the more confident predictions.\n\nMultiple Generations Agreement.",
            "score": 0.5039954363657113,
            "section_title": "Related Work",
            "char_start_offset": 4435,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 14,
                    "end": 147
                },
                {
                    "start": 147,
                    "end": 244
                },
                {
                    "start": 244,
                    "end": 439
                },
                {
                    "start": 439,
                    "end": 513
                },
                {
                    "start": 513,
                    "end": 697
                },
                {
                    "start": 697,
                    "end": 762
                },
                {
                    "start": 762,
                    "end": 933
                },
                {
                    "start": 933,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1395
                },
                {
                    "start": 1395,
                    "end": 1492
                },
                {
                    "start": 1492,
                    "end": 1719
                },
                {
                    "start": 1719,
                    "end": 1889
                },
                {
                    "start": 1891,
                    "end": 1922
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 176,
                    "matchedPaperCorpusId": "235743020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74755859375
        },
        {
            "corpus_id": "272911245",
            "title": "MIO: A Foundation Model on Multimodal Tokens",
            "text": "To enable multimodal generation in MM-LLMs, a straightforward approach is to allow these models to call external multimodal generation tools, such as Stable Diffusion (Rombach et al., 2022) or text-to-speech (TTS) tools (Shen et al., 2023;Li et al., 2024c;OpenAI et al., 2023). However, as highlighted in the Gemini technical report (Team et al., 2023), relying on an intermediate natural language interface can limit the model's ability to express images. If a model cannot natively output images, it will not be able to generate images with prompts of interleaved sequences of image and text. This claim is in line with our distinction between descriptive image generation and context-aware image generation, as discussed in \u00a72.2. \n\nAs a result, recent works focus on the unification of multimodal understanding and generation in a single model (i.e., any-to-any MM-LLMs), enabling the generation of multimodal tokens without natural language as an interface. These models typically follow different approaches, depending on how images are represented in both input and output sides. For example, the Discrete-In-Discrete-Out (DIDO) approach has been explored in works such as SEED-LLaMA (Ge et al., 2023b), AnyGPT (Zhan et al., 2024), and Chameleon (Team, 2024). Continuous-In-Discrete-Out (CIDO) methods have been implemented in models like DaVinCi (Diao et al., 2023), Gemini (Team et al., 2023), and Unified-IO 2 (Lu et al., 2023). The Continuous-In-Continuous-Out (CICO) approach is used in models such as Emu (Sun et al., 2023c;a), and DreamLLM (Dong et al., 2023).",
            "score": 0.5037792140359129,
            "section_title": "ANY-TO-ANY MM-LLMS",
            "char_start_offset": 27639,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1573
                }
            ],
            "ref_mentions": [
                {
                    "start": 1353,
                    "end": 1372,
                    "matchedPaperCorpusId": "256901258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81591796875
        },
        {
            "corpus_id": "264804088",
            "title": "CoLLM: Integrating Collaborative Embeddings Into Large Language Models for Recommendation",
            "text": "Among the progress in the field of LLM, the endeavors that resonate most closely with our work involve the exploration of multimodal LLMs [38][39][40][41]. For instance, MiniGPT4 [39] combines a frozen visual encoder with a frozen advanced language model, revealing that aligning visual features with large language models enables advanced multi-modal capabilities like generating detailed image descriptions and creating websites from hand-drawn drafts. Palm-E [41] aims to integrate real-world continuous sensor inputs into language models, creating a connection between words and sensory information. This integration enables the development of embodied language models capable of addressing robotic challenges. These works aim to leverage the robust generation and comprehension capabilities of LLM to process textual data and map information from other modalities such as vision and audio to the textual modality, thereby achieving a large multimodal model with language as its primary carrier, which is similar to our motivations.",
            "score": 0.5031259989436739,
            "section_title": "Multimodal LLM",
            "char_start_offset": 8411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 1036
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 146,
                    "matchedPaperCorpusId": "258291930"
                },
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "258762683"
                },
                {
                    "start": 150,
                    "end": 154,
                    "matchedPaperCorpusId": "257364842"
                },
                {
                    "start": 179,
                    "end": 183,
                    "matchedPaperCorpusId": "258291930"
                },
                {
                    "start": 462,
                    "end": 466,
                    "matchedPaperCorpusId": "257364842"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8955078125
        },
        {
            "corpus_id": "274305717",
            "title": "MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding",
            "text": "Based on semantic discrete encoding, we propose a unified vision-language modeling named MUSE-VL. The structure of MUSE-VL is shown in the Fig. 2. The image is pre-processed into visual tokens {q 1 , q 2 , ..., q i } of length (h \u00d7 w) through SDE tokenizer, while the textual data is converted through the text tokenizer. To achieve joint modeling of language and vision, it is sufficient to simply extend the embedding layer of existing LLMs to incorporate newly added visual token IDs. This modification enables seamless integration of multimodal inputs within the model's architecture. To distinguish visual tokens, two special tokens <soi>and <eoi>are added to mark the start and end of visual tokens respectively. The training objective of the model remains a simple autoregressive task, without any modifications to the LLM's architecture or training loss. \n\nIn this work, we adopt Yi-1.5 [67] and Qwen-2.5 [53,65], which perform well on language tasks, as the base LLM. It is crucial to emphasize that the inherent alignment of SDE tokenzier with language and the unified autoregressive architecture enables our model to integrate effortlessly with the most LLMs. This integration is achieved using only minimal image-text data and does not require any architecture modifications. In contrast, previous approaches, such as Chameleon and Emu3 [52,59], necessitated alterations to the model architecture and required extensive image and language data to train the LLM from scratch. For the image generation task, the order of images and texts in the image caption data is reversed here, enabling the model to generate visual tokens based on the descriptions. \n\nPrompt: {system text} {image caption} Target: <soi >{vision tokens} <eoi > \n\nThe system text is randomly sampled from a set of image generation instructions, such as \"Please generate an image.\", \"Show me a photo.\", etc. At the inference stage, the user provides a prompt for generating an image, and the model will predict the corresponding image tokens. Then, the predicted visual tokens are converted to the image by the image decoder.",
            "score": 0.5017797749545918,
            "section_title": "Unified Vision-Language Modeling",
            "char_start_offset": 12465,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1740
                },
                {
                    "start": 1743,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2103
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80859375
        },
        {
            "corpus_id": "267897956",
            "title": "A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends",
            "text": "In recent epochs, the domain of large language models (LLMs) for natural language processing has witnessed a precipitous progression. Prototypes such as BERT, GPT-3, and PaLM have demonstrated exceptional aptitude in language apprehension and generation, courtesy of self-supervised pretraining on voluminous text corpora [85]. As LLMs expand in magnitude and range, incorporating additional modalities beyond text is a burgeoning field of study. Multimodal LLMs strive to amalgamate language, vision, and other sensory inputs within a singular model architecture. They hold the potential to attain a more holistic understanding of the world by concurrently learning representations across diverse data types [96]. A significant hurdle is the effective fusion of the strengths of CNNs for computer vision and transformer architectures for language modeling. \n\nOne strategy involves employing a dual-stream architecture with distinct CNN and transformer encoders interacting via coattentional transformer layers [97]. The CNN extracts visual features from images, providing contextual information that can guide language generation and comprehension. The transformer architecture models the semantics and syntax of text. Their interaction enables the generation of captions based on image content or the retrieval of pertinent images for textual queries. Alternative methods directly incorporate CNNs within the transformer architecture as visual token encoders that operate with text token encoders [98]. The CNN projections of image patches are appended to text token embeddings as inputs to the transformer layers. This unified architecture allows for end-to-end optimization of parameters for both vision and language tasks. Self-supervised pretraining continues to be vital for multimodal LLMs to learn effective joint representations before downstream task tuning. Contrastive learning objectives that predict associations between modalities have proven highly effective [99]. Models pre-trained on large datasets of image-text pairs have demonstrated robust zeroshot transfer performance on multimodal tasks. \n\nAs multimodal LLMs increase in scale, the efficient combination of diverse convolution types and attention mechanisms will be crucial. Compact CNN architectures could help to reduce the cost of computing. Sparse attention and memory compression techniques can assist with scalability.",
            "score": 0.5014845515179497,
            "section_title": "H. Evolution of Language Models and Multimodal LLMs",
            "char_start_offset": 60107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2114
                },
                {
                    "start": 2117,
                    "end": 2251
                },
                {
                    "start": 2252,
                    "end": 2321
                },
                {
                    "start": 2322,
                    "end": 2401
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96630859375
        },
        {
            "corpus_id": "274656320",
            "title": "Olympus: A Universal Task Router for Computer Vision Tasks",
            "text": "Recent advancements in large language models (LLMs) [7,73] have catalyzed the development of multimodal large language models (MLLMs) [2, 3, 5, 11, 16, 34, 35, 37, 44-46, 48, 54, 57, 63, 72, 82]. Pioneering multimodal large language models (MLLMs), such as MiniGPT-4 [97], have demonstrated impressive capabilities in processing and integrating multiple modalities. Models like Kosmos-2 [57], LLaVA [48] and LLaVA-OneVision [34] have further enhanced the visual cognitive abilities of MLLMs. Additionally, approaches including LLaVA-Phi [100], Mo-Figure 2. Given the user prompts, a trainable MLLM can perform routing across a wide range of specified models. In this concept, MLLMs can solve multimodal understanding tasks (e.g., VQA) with its inherited capacity, while MLLMs can allocate appropriate specialized models to address multimodal generative and classic vision tasks (e.g., image generation and depth estimation), then aggregate the results and deliver a response to the user. \n\nbileVLM [12], and Mipha [99] focus on refining training methodologies and architectural frameworks to develop more efficient and lightweight MLLMs. Although these models excel in visual perception and multimodal understanding, they are predominantly limited to generating text-based outputs, which restricts their effectiveness across a broader range of vision tasks involving images, videos, and 3D content generation. In this work, we adopt a multimodal model structure following Mipha [99].",
            "score": 0.5011185569466569,
            "section_title": "Vision-Language Understanding",
            "char_start_offset": 6690,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1483
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 55,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 399,
                    "end": 403,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91845703125
        },
        {
            "corpus_id": "271213320",
            "title": "ICCV23 Visual-Dialog Emotion Explanation Challenge: SEU_309 Team Technical Report",
            "text": "Multimodal learning in the intersection of visual and linguistic data has seen significant advancements in recent years. This area of research focuses on developing models that can understand and generate content by integrating information from both visual inputs, such as images or videos, and textual descriptions. These models leverage the complementary nature of visual and textual data to perform a wide range of tasks, including but not limited to image captioning, visual question answering, and cross-modal information retrieval. \n\nOne of the key innovations in this field has been the development of models that are trained on vast datasets of image-text pairs. [12,7,8] These models employ contrastive learning techniques to align the representations of images and their corresponding textual descriptions in a shared embedding space. By doing so, they learn to associate specific visual features with relevant linguistic concepts, enabling them to understand complex queries and content with a high degree of accuracy. Moreover, the use of transformer-based [13] architectures has further enhanced the capabilities of these models. Transformers provide a flexible and powerful framework for modeling sequential data, and their adoption in multimodal learning has allowed for the effective integration of visual and textual inputs. This has led to significant improvements in tasks requiring nuanced understanding and generation of content, such as generating descriptive captions for images that accurately reflect their content and context or answering questions based on visual information. \n\nThe emergence of Large Language Models (LLMs) has also marked a significant milestone in the field of artificial intelligence, particularly in natural language processing. Models like GPT [1], BERT [4], and others have demonstrated exceptional abilities in generating and understanding text, significantly advancing the capabilities of AI in understanding human languages. These models have been instrumental in a variety of applications, including text generation, translation, and semantic analysis, showcasing their versatility and powerful computational abilities. However, their primary limitation lies in their text-only nature, confining their applicability to tasks that do not require understanding or generating content in other modalities such as images, speech, and videos. \n\nTo bridge this gap, the research and development of Large Vision Language Models (LVLMs) [9,2,10,15,14] have gained momentum.",
            "score": 0.5004319600243394,
            "section_title": "Related Work",
            "char_start_offset": 2249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 537
                },
                {
                    "start": 540,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2174
                },
                {
                    "start": 2175,
                    "end": 2391
                },
                {
                    "start": 2394,
                    "end": 2519
                }
            ],
            "ref_mentions": [
                {
                    "start": 677,
                    "end": 679,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95068359375
        },
        {
            "corpus_id": "248240054",
            "title": "Opal: Multimodal Image Generation for News Illustration",
            "text": "In Opal, we created a workflow that composed a language generation model (GPT-3) with a multimodal image generation model (VQGAN+CLIP). These are both large pretrained models that have different computational sensibilities. While text-to-image models can reach a near infinite number of visual concepts through text, users can find it taxing to come up with the right visual language on their own. Having a language generation model to complement the text-to-image model in Opal helped because language generation models are capable of returning any number of diverse linguistic trajectories at once. As we found in Study 1, it is remarkably challenging and taxing for a human to come up with a large, diverse set of associations. Even though GPT-3 could not beat out the human benchmark in terms of performance, what GPT-3 can do is significantly mitigate human effort by suggesting concepts. \n\nHaving access to many different possibilities helped mitigate the odds of a user running into a poor quality text-to-image generation. The magnitude of possible text-to-image generations was contended well with the magnitude of possible language model generations-the strengths of the language model addressed a problem of the image generation model, and the image model extended the capabilities of the language generation model into the image domain. \n\nIn composing GPT-3 with VQGAN+CLIP, we were able to take article text, expand it into networks of concepts, and translate those concepts into prompts of rich visual language. When multiple models are integrated together into one workflow, they must be able to find common conceptual spaces such that the output of one can easily be transformed into the input of another. In Opal, this conceptual space was visual language: concrete subjects and styles that were salient with color and artistic techniques. Other examples of conceptual spaces could be the multimodal spaces between language and 3D shape or emotion and music.",
            "score": 0.500233004475175,
            "section_title": "Multiple general-purpose AI models can integrate into one workflow",
            "char_start_offset": 55313,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 893
                },
                {
                    "start": 896,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1975
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83935546875
        },
        {
            "corpus_id": "259847295",
            "title": "T2I-CompBench++: An Enhanced and Comprehensive Benchmark for Compositional Text-to-Image Generation",
            "text": "Multimodal Large Language Models enable users to instruct LLMs to analyze user-provided image inputs. By integrating the visual modality, MLLMs enhance the capability of language-only systems, providing them with new interfaces to address a variety of tasks. However, the multimodal abilities of MLLMs regarding the compositional text-to-image generation remain unclear. Within this context, we analyze the abilities of three types of MLLMs, namely MiniGPT-4, ShareGPT4V [15] and GPT-4V [16], focusing on their performance in compositional problems. \n\nEvaluation methods of MLLMs. By aligning a pretrained visual encoder with a frozen large language model, multimodal large language models such as MiniGPT-4 [77] have demonstrated great abilities in vision-language cross-modal understanding. But the current MiniGPT-4 model exhibit limitations such as inaccurate understanding of images and hallucination issues. ShareGPT4V [15] is a large-scale image-text dataset featuring 1.2 million detailed captions characterized by richness and diversity. GPT-4V, a state-of-the-art MLLM, is developed on the foundation of the state-of-the-art LLM, GPT-4 [81], and trained extensively on a large-scale dataset containing multimodal information [81]. Employing MLLM as an evaluation metric, we submit generated images to the model and assess their alignment with the provided text prompt. This evaluation involves soliciting predictions for the image-text alignment score. \n\nPrompt template for MLLM evaluation. We leverage MLLMs as an evaluation metric by feeding the generated images to the model and asking two questions with Chain-of-Thought [78]: \"describe the image\" and \"predict the imagetext alignment score\". We detail the prompts used for MLLM evaluation metric. For each sub-category, we ask two questions in sequence: \"describe the image\" and \"predict the image-text alignment score\". Specifically, Table III shows the prompts for evaluating attribute binding (color, shape, texture).",
            "score": 0.499644881720289,
            "section_title": "D. MLLM-based Evaluation Metric",
            "char_start_offset": 29754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1462
                },
                {
                    "start": 1465,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 1986
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.931640625
        },
        {
            "corpus_id": "271097376",
            "title": "SEED-Story: Multimodal Long Story Generation with Large Language Model",
            "text": "With the remarkable advancements in image generation and open-form text generation, the creation of interleaved image-text content has become an increasingly intriguing field. Multimodal story generation, characterized by producing narrative texts and vivid images in an interleaved manner, has emerged as a valuable and practical task with broad applications. However, this task poses significant challenges, as it necessitates the comprehension of the complex interplay between texts and images, and the ability to generate long sequences of coherent, contextually relevant texts and visuals. In this work, we propose SEED-Story, a novel method that leverages a Multimodal Large Language Model (MLLM) to generate extended multimodal stories. Our model, built upon the powerful comprehension capability of MLLM, predicts text tokens as well as visual tokens, which are subsequently processed with an adapted visual de-tokenizer to produce images with consistent characters and styles. We further propose multimodal attention sink mechanism to enable the generation of stories with up to 25 sequences (only 10 for training) in a highly efficient autoregressive manner. Additionally, we present a large-scale and high-resolution dataset named StoryStream for training our model and quantitatively evaluating the task of multimodal story generation in various aspects.",
            "score": 0.4991484933689947,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.935546875
        },
        {
            "corpus_id": "275757141",
            "title": "Towards Data-Centric AI: A Comprehensive Survey of Traditional, Reinforcement, and Generative Approaches for Tabular Data Transformation",
            "text": "Furthermore, designing scalable architectures capable of handling both vertical and horizontal data partitioning efficiently will be instrumental in advancing privacy-conscious feature engineering. \n\nLLMs and Multimodality for Feature Generation. Large Language Models (LLMs) and multimodal systems are opening new frontiers in feature generation by processing diverse data types, including text, images, and structured tabular data. These systems hold immense potential for cross-domain feature engineering. However, challenges persist in encoding tabular data effectively within LLM architectures, which are traditionally optimized for textual inputs. \n\nAdditionally, integrating multimodal data while preserving the unique characteristics of each modality remains a complex task. Future efforts should focus on designing advanced embedding techniques specifically tailored to tabular data, enabling its seamless integration into LLM frameworks. Methods for aligning features across modalities must ensure that semantic relationships are preserved, thereby enhancing the utility of multimodal representations. Efficient fine-tuning and knowledge transfer mechanisms for tabular tasks will further expand the practicality and adoption of LLM-based feature generation systems.",
            "score": 0.4986937457901419,
            "section_title": "Challenges and Future Research Directions",
            "char_start_offset": 79456,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 200,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1276
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76318359375
        },
        {
            "corpus_id": "267547631",
            "title": "Real-world robot applications of foundation models: a review",
            "text": "While individual language or image processing has limited capabilities, combining vision and language allows us to build diverse foundation models. These Vision-Language Models (VLMs) are trained with the Internet-scale massive datasets as well as LLMs. For instance, CLIP [28] is trained on 400 million image-text pair datasets from the Internet. When classifying VLMs from the network input and output, they mainly fall into the following four categories. \n\nVision + Language \u2192 Latent involves converting images and text into latent space vectors, which allows one to calculate the similarity between them for downstream tasks such as retrieval. Prominent models include CLIP [28] and GLIP [57]. While CLIP [28] computes correspondences between a single image-text pair, GLIP [57] can calculate correspondences between multiple regions of an image with their text descriptions. \n\nVision + Language \u2192 Language enables tasks like image captioning (IC), visual question answering (VQA), and visual entailment (VE). Prominent models include BLIP2 [58], Flamingo [59], OFA [60], and Unified-IO [61]. BLIP2 is dedicated to IC and VQA, whereas OFA and Unified-IO serve as a foundation model capable of VQA, IC, Visual Grounding (VG), Text-to-Image Generation (TIG), and more within a single network. Flamingo supports in-context learning for both vision and language. GPT-4 Vision (GPT-4V) [27] can analyze images that users input into the model and generate the answer as texts, which can apply to IC, VWA, and VE. Extending GPT-4V, GPT-4o (omni) is a multimodal model that can reason across audio, vision, and text. \n\nVision + Language \u2192 Vision facilitates tasks like image editing and image generation through language. Prominent models include Stable Diffusion [62], DALL-E [63], and the previously mentioned OFA [60] and Unified-IO [61]. They incorporate Transformer-based architectures and diffusion architectures into the models. \n\nVision + Language \u2192 Recognition enables tasks like semantic segmentation, instance segmentation, and bounding box extraction from images and language.",
            "score": 0.49859669011993935,
            "section_title": "Foundation Models for Vision and Language",
            "char_start_offset": 11191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 879
                },
                {
                    "start": 882,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1612
                },
                {
                    "start": 1615,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1931
                },
                {
                    "start": 1934,
                    "end": 2084
                }
            ],
            "ref_mentions": [
                {
                    "start": 273,
                    "end": 277,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 678,
                    "end": 682,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 709,
                    "end": 713,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1060,
                    "end": 1064,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1070,
                    "end": 1074,
                    "matchedPaperCorpusId": "246634906"
                },
                {
                    "start": 1091,
                    "end": 1095,
                    "matchedPaperCorpusId": "249848272"
                },
                {
                    "start": 1760,
                    "end": 1764,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1773,
                    "end": 1777,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 1812,
                    "end": 1816,
                    "matchedPaperCorpusId": "246634906"
                },
                {
                    "start": 1832,
                    "end": 1836,
                    "matchedPaperCorpusId": "249848272"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8984375
        },
        {
            "corpus_id": "269635620",
            "title": "Similarity Guided Multimodal Fusion Transformer for Semantic Location Prediction in Social Media",
            "text": "Pre-trained language models such as BERT[5] leverage deep transformer architectures to encode text in a contextual manner, offering versatile applications for text encoding across various domains.The prevailing text encoders in VLP models primarily employ pretrained language models since they possess the capability to learn better text semantics and capture linguistic knowledge.Popular choices include BERT[5], RoBERTa [20], ELECTRA [4], ALBERT [14], and DeBERTa [10].Notably, RoBERTa achieves the most robust performance in various downstream tasks.\n\nAlthough existing methods are capable of extracting effective feature representations from both images and text, when confronted with multimodal features, it is crucial to establish interconnections between the features of images and text in advance to mitigate modality heterogeneity.Given CLIP's ability to achieve multimodal consistency by sharing an encoding space between images and text, it ensures that similar image-text pairs are closer to each other in the encoding space, resulting in a more robust and expressive representation for multimodal tasks.Hence, we em-",
            "score": 0.4982034929415876,
            "section_title": "Related work",
            "char_start_offset": 8916,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 196,
                    "end": 381
                },
                {
                    "start": 381,
                    "end": 471
                },
                {
                    "start": 471,
                    "end": 553
                },
                {
                    "start": 555,
                    "end": 840
                },
                {
                    "start": 840,
                    "end": 1116
                },
                {
                    "start": 1116,
                    "end": 1129
                }
            ],
            "ref_mentions": [
                {
                    "start": 436,
                    "end": 439,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 466,
                    "end": 470,
                    "matchedPaperCorpusId": "219531210"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92724609375
        },
        {
            "corpus_id": "277606609",
            "title": "Vision LLMs: Bridging Language and Visual Understanding - Case study of IndoAI",
            "text": "Vision language models, according to Nvidia [11] are multimodal AI systems built by combining a large language model (LLM) with a vision encoder, giving the LLM the ability to \"see.\" With this ability, VLMs can process and provide advanced understanding of video, image, and text inputs supplied in the prompt to generate text responses. \n\nOverall architecture of the proposed by authors [12] Vision LLM consists of three parts: a unified language instruction designed to accommodate both vision and vision-language tasks, an image tokenizer that encodes visual information guided by language instructions and an LLM-based open ended task decoder that executes diverse tasks defined by language instructions. \n\nThus, Vision LLMs typically consist of two main components and LLM:",
            "score": 0.4979811467622276,
            "section_title": "Architecture of Vision LLMs",
            "char_start_offset": 1813,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 337
                },
                {
                    "start": 340,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 778
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82568359375
        },
        {
            "corpus_id": "263608981",
            "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
            "text": "The development of large-scale vision-and-language models is significantly impacting a wide range of fields like automated dialogue systems and digital content creation. With the surge in research and development in this domain, the current state-of-the-art Large Language Models (LLMs) (OpenAI, 2023;Chiang et al., 2023;Ouyang et al., 2022) and vision-and-language models such as (Wu et al., 2023a;Li et al., 2023c;Tsimpoukelli et al., 2021;Alayrac et al., 2022) fall short in generating coherent multimodal outputs. This limitation becomes particularly evident in tasks that demand an integrated handling of vision and language, essential for the next generation Large Language Models (LLMs). \n\nOur work, as illustrated in Fig. 1, seeks to address these shortcomings by enhancing the integration of text and image generation in LLMs. The challenges in developing a multimodal LLM capable of interleaved vision and language generation are manifold. First, LLMs typically lack mechanisms to directly produce images, prompting us to introduce \"generative vokens\" that bridge the gap between textual and visual feature spaces. Second, the constraint of data scarcity, especially in visionand-language tasks (Sharma et al., 2018) lacking extensive detailed descriptions of images (Huang et al., 2016), is countered by our unique description-free training approach. Third, maintaining both image-text and image-image consistency poses a significant challenge, which we address through dual-loss strategies. Finally, as we push forward the boundaries with LLMs, the large memory requirements urge us to devise more efficient end-to-end strategies and create an efficient training pipeline accessible for the community, especially in downstream tasks. \n\nSpecifically, to overcome these challenges, we present MiniGPT-5, a novel approach for interleaved vision-and-language generation. By combing the Stable Diffusion with LLMs through special visual tokens (Tan & Bansal, 2020) -\"generative vokens\", we develop a new approach for multimodal generation. Our two-stage training methodology emphasizes a description-free foundational phase, enabling effective model training even with limited caption-grounded images.",
            "score": 0.49733178869551975,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1745
                },
                {
                    "start": 1748,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2208
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 341,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 416,
                    "end": 442,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 442,
                    "end": 463,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1205,
                    "end": 1226,
                    "matchedPaperCorpusId": "51876975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8818359375
        },
        {
            "corpus_id": "274305669",
            "title": "VLM-HOI: Vision Language Models for Interpretable Human-Object Interaction Analysis",
            "text": "Integrating vision and language has been a long-standing goal in artificial intelligence. Early work focused on image captioning [39], generating textual descriptions of image contents. Recent years have seen rapid progress in visual question answering [1,2], enabled by large-scale datasets [7] and deep neural encoder-decoder models [43]. \n\nMore advanced vision-language tasks require a tighter integration between the visual and linguistic modalities. This has led to a surge of interest in unified multimodal representation models that can process both images and text within a single framework [3]. \n\nOne line of work explores joint embedding models to learn aligned vector representations for image regions and language fragments [15,19]. However, these approaches do not explicitly model interactions between modalities. \n\nRecently, pretrained language models, such as BERT [11] and GPT [4], have significantly influenced the vision-language domain. Researchers have extended these models to handle multimodal inputs, leveraging their contextual understanding of text. More recent methods like VisualBERT [23] and LXMERT [38] utilize pretrained BERT weights for joint image-text comprehension. \n\nUnified architectures like ViLBERT [30], VL-BERT [36], UNITER [8], BLIP [21] integrate masked language modeling objectives alongside paired image-text prediction tasks within a single Transformer model. These models set new stateof-the-art results on downstream tasks like visual question answering (VQA), visual reasoning, and image retrieval. \n\nSpecifically, we utilize BLIP as our vision-language model backbone. BLIP uses a flexible multimodal encoder-decoder model that can handle both understanding and generation tasks. It also improves training data quality through generating new image captions and filtering noise. These advantages lead to strong performance. \n\nInstruction-based VLMs like GPT-4V [33], LLaVA [28], and InstructBLIP [10] are a new type of large vision-language model that are trained to follow natural language instructions and prompts to perform various tasks. A key advantage these models have over traditional fine-tuning is they can adapt to new tasks without needing gradient updates or lots of specific training data. The instruction format allows rapid adaptation.",
            "score": 0.4963214839671234,
            "section_title": "Vision-Lauguage Model",
            "char_start_offset": 7308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 603
                },
                {
                    "start": 606,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1200
                },
                {
                    "start": 1203,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1547
                },
                {
                    "start": 1550,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1872
                },
                {
                    "start": 1875,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2252
                },
                {
                    "start": 2253,
                    "end": 2300
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 133,
                    "matchedPaperCorpusId": "1169492"
                },
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "3753452"
                },
                {
                    "start": 256,
                    "end": 258,
                    "matchedPaperCorpusId": "3180429"
                },
                {
                    "start": 335,
                    "end": 339,
                    "matchedPaperCorpusId": "1055111"
                },
                {
                    "start": 599,
                    "end": 602,
                    "matchedPaperCorpusId": "10137425"
                },
                {
                    "start": 736,
                    "end": 740,
                    "matchedPaperCorpusId": "8517067"
                },
                {
                    "start": 894,
                    "end": 897,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1238,
                    "end": 1242,
                    "matchedPaperCorpusId": "208637516"
                },
                {
                    "start": 1265,
                    "end": 1268,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 1275,
                    "end": 1279,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96923828125
        },
        {
            "corpus_id": "271946796",
            "title": "IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model with Multimodal Capabilities",
            "text": "Overview. As illustrated in Figure 2, our approach enables the simultaneous execution of two high-quality workflows post-deployment: one for multimodal interactions and the other for text-only conversations. Both workflows leverage the transformer layers of the large language model. The multimodal interaction workflow encompasses: (1) an image encoder and a projector, utilized for extracting high-quality image features and achieving vision-language alignment, respectively, (2) the transformer layers of the large language model, which remain frozen during training, and (3) the Figure 2: Overview of the proposed architecture, which mainly consists of two workflows: the Multimodal Workflow and the Text-only Workflow. The multimodal workflow, beyond the necessary image encoder and projector, integrates the Inner-Adaptor Architecture, including insertion layers, an embedding layer, and a language model head. Both workflows share the same large language model. The number of insertion layers is variable, where N \u2264 M . In this context, MM denotes MultiModal, EL stands for Embedding Layer, and LH represents the Language model Head. inner-adaptor architecture, which comprises insertion layers, an embedding layer, and a language model head specifically designed for multimodal inputs. Conversely, the text-only conversation workflow solely employs the constituent elements of the original language model, without resorting to the specialized multimodal components. \n\nImage Encoder and Projector. Following LLava-1.5 (Liu et al. 2024a), we utilize the CLIP ViT-L/14 (Radford et al. 2021) image encoder with an input resolution of 336px. Subsequently, we employ a vision-language projector composed of a two-layer MLP to integrate the vision features with LLMs. \n\nLarge Language Model. We employ the Llama3-8B (Meta 2024) as the base language model throughout the training process. \n\nInner-Adaptor Architecture. To achieve multimodal comprehension, it is essential to integrate trainable parameters into MLLMs.",
            "score": 0.495898961195525,
            "section_title": "Methodology",
            "char_start_offset": 7686,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1473
                },
                {
                    "start": 1476,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1768
                },
                {
                    "start": 1771,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1888
                },
                {
                    "start": 1891,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2017
                }
            ],
            "ref_mentions": [
                {
                    "start": 1525,
                    "end": 1543,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 1574,
                    "end": 1594,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9423828125
        },
        {
            "corpus_id": "259165371",
            "title": "Training Multimedia Event Extraction With Generated Images and Captions",
            "text": "Recent advances in generative models have propelled data augmentation research to a new level. On textual tasks, one approach is to generate additional textual training data [10,16,36,37,57,62,69]. Another, multimodal approach is to generate visual data to complement existing textual data [31,33,66,76], which improves performance on textual tasks. For example, [31] generates visual data for machine translation. [76] uses generated images to guide text generation tasks, such as text completion, story generation, and concept-totext generation. In addition, [33,66] integrate synthetic images into language models to enhance the solution of plain language understanding tasks under low-resource settings. Unlike previous studies that address unimodal problems by synthesizing multimodal data, our work use the generated data to tackle multi-modal tasks. Doing so places a stringent requirement on the quality of generated data, as we need to train encoders in both modalities with the generated data. This necessitates overcoming the domain shifts between generated and real data. To the best of our knowledge, this is the first work to utilize bidirectional cross-modality data generation models for multimodal tasks.",
            "score": 0.49536526688579147,
            "section_title": "Cross-modality Generative Data Augmentation.",
            "char_start_offset": 7600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1221
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 178,
                    "matchedPaperCorpusId": "257219618"
                },
                {
                    "start": 190,
                    "end": 193,
                    "matchedPaperCorpusId": "238857304"
                },
                {
                    "start": 290,
                    "end": 294,
                    "matchedPaperCorpusId": "221819393"
                },
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "248240059"
                },
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "253098783"
                },
                {
                    "start": 363,
                    "end": 367,
                    "matchedPaperCorpusId": "221819393"
                },
                {
                    "start": 561,
                    "end": 565,
                    "matchedPaperCorpusId": "248240059"
                },
                {
                    "start": 565,
                    "end": 568,
                    "matchedPaperCorpusId": "253098783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8916015625
        },
        {
            "corpus_id": "270370808",
            "title": "One Perturbation is Enough: On Generating Universal Adversarial Perturbations against Vision-Language Pre-training Models",
            "text": "VLP models are pre-trained on massive image-text pairs to learn the semantic correlations across modalities and serve diverse multimodal user demands [8,13]. We next illustrate the basis of VLP models from multiple perspectives. \n\nArchitectures. Based on the ways of multimodal fusion, the architectures of VLP models can be classified into two types: single-stream and dual-stream architectures. Single-stream architectures [9,25] directly concatenate the text and image features, and calculate the attention in the same Transformer block for multimodal fusion. \n\nOn the contrary, dual-stream architectures [24,38] separately feed the text and image features to different Transformer blocks and leverage the cross-attention mechanism for multimodal fusion. Generally, single-stream architectures are more parameter-efficient than dual-stream architectures since they adopt the same set of parameters in a Transformer block for the text and image modalities. \n\nPre-training Objectives. The pre-training objectives for VLP models mainly include masked feature completion, multimodal feature matching, and specific downstream objectives. Masked feature completion [9] encourages VLP models to predict the deliberately masked tokens based on the remaining unmasked tokens. Multimodal feature matching [23] pre-trains VLP models to precisely predict whether the given image-text pairs are matched. Specific downstream objectives [2] directly utilize the training objectives of downstream tasks (e.g., visual question answering) for pre-training VLP models. \n\nDownstream Tasks. In this paper, we mainly consider the following multimodal downstream tasks: (1) Image-text retrieval (ITR) [42]: finding the most matched image for the given text and vice versa, including image-to-text retrieval (TR) and text-to-image retrieval (IR). ( 2) Image caption (IC) [4]: generating the most suitable descriptions for the given image. (3) Visual grounding (VG) [20]: locating specific regions in the image that correspond with the given textual descriptions. (4) Visual entailment (VE) [43]: analyzing the input image and text and predicting whether their relationship is entailment, neutral, or contradiction.",
            "score": 0.49524862453672525,
            "section_title": "Vision-Language Pre-training Models",
            "char_start_offset": 5787,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 228
                },
                {
                    "start": 231,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2193
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 153,
                    "matchedPaperCorpusId": "246996617"
                },
                {
                    "start": 425,
                    "end": 428,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 608,
                    "end": 612,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 612,
                    "end": 615,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1162,
                    "end": 1165,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 1298,
                    "end": 1302,
                    "matchedPaperCorpusId": "236034189"
                },
                {
                    "start": 1425,
                    "end": 1428,
                    "matchedPaperCorpusId": "3753452"
                },
                {
                    "start": 1850,
                    "end": 1853,
                    "matchedPaperCorpusId": "51613554"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.845703125
        },
        {
            "corpus_id": "270319618",
            "title": "A survey on advancements in image-text multimodal models: From general techniques to biomedical implementations",
            "text": "Text Encoder\n\nFigure 7: CLIP architecture(adapted [93]).\n\nSome researchers have found that although current VLP improves performance on a variety of joint visual and language downstream tasks.These methods rely heavily on image feature extraction processes, most of which involve region supervision (such as object detection) and convolutional architectures.Therefore, there are problems in the inference efficiency and speed of the model.Vision-and-Language Transformer (ViLT) is thus proposed, which has a concise architecture as a VLP model with minimal visual embedding pipelines and follows a single-stream approach.The model initializes the interactive converter weights from a pre-trained ViT instead of BERT, and the processing of visual input is greatly simplified to the same convolution free way as for text input [37].As shown in Figure8, the CLIP mentioned above belongs to Figure8(a), which uses a separate but equally expensive transformer embedder for each mode.Whereas, as shown in Figure8(b), the embedding layer of ViLT's raw pixels is shallow and computatively as small as text markup, the architecture focuses most of the computation on modal interaction modeling.BLIP (Bootstrapping Language-Image Pretraining), introduced by Salesforce in 2022 [40], exemplifies the singlestream architecture within the realm of image-text multimodal frameworks, as depicted in Figure9.This model innovatively combines understanding and generation tasks into a unified approach by leveraging cross-modal encoders and decoders, thereby achieving state-of-the-art performance in a variety of visual and language tasks.Central to BLIP's architecture is the image-text Multimodal Mixture of Encoder-Decoder (MED) structure.This comprehensive setup comprises two unimodal encoders-one for images and another for text-alongside an image-grounded text encoder and decoder.Such a configuration facilitates effective multitask pretraining and enhances BLIP's capability for transfer learning, making it a versatile tool in bridging visual and textual modalities.",
            "score": 0.4949918374151241,
            "section_title": "[CLS]",
            "char_start_offset": 53103,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 14,
                    "end": 56
                },
                {
                    "start": 58,
                    "end": 192
                },
                {
                    "start": 192,
                    "end": 358
                },
                {
                    "start": 358,
                    "end": 439
                },
                {
                    "start": 439,
                    "end": 621
                },
                {
                    "start": 621,
                    "end": 830
                },
                {
                    "start": 830,
                    "end": 978
                },
                {
                    "start": 978,
                    "end": 1185
                },
                {
                    "start": 1185,
                    "end": 1392
                },
                {
                    "start": 1392,
                    "end": 1622
                },
                {
                    "start": 1622,
                    "end": 1725
                },
                {
                    "start": 1725,
                    "end": 1871
                },
                {
                    "start": 1871,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 825,
                    "end": 829,
                    "matchedPaperCorpusId": "231839613"
                },
                {
                    "start": 1267,
                    "end": 1271,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88818359375
        },
        {
            "corpus_id": "263620748",
            "title": "Kosmos-G: Generating Images in Context with Multimodal Large Language Models",
            "text": "Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of\"image as a foreign language in image generation.\"This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of\"image as a foreign language in image generation.\"The code can be found at https://aka.ms/Kosmos-G",
            "score": 0.49471678861211466,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83544921875
        },
        {
            "corpus_id": "277065986",
            "title": "Unified Modeling Language Code Generation from Diagram Images Using Multimodal Large Language Models",
            "text": "Large Language Models (LLMs) have demonstrated impressive capabilities in understanding and generating human-like text. Advanced transformer-based architectures such as BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and T5 (Raffel et al., 2020) capture contextual relationships within text, enabling tasks like summarization, question answering, and code generation. However, LLMs process only textual data, limiting their utility in domains requiring multimodal understanding, such as image or video analysis. Researchers developed MM-LLMs that incorporate visual, auditory, and textual data into unified frameworks to address these limitations. Models like CLIP (Radford et al., 2021) align visual embeddings with textual embeddings, enabling cross-modal tasks such as visual question answering (VQA) and zero-shot image classification. For example, CLIP (Contrastive Language-Image Pretraining) learns shared visual and textual semantics from large-scale datasets of image-text pairs, effectively bridging linguistic context and visual perception. MM-LLMs leverage these multimodal capabilities to tackle tasks that traditional LLMs cannot address, such as interpreting UML diagrams and generating corresponding executable code. MM-LLMs align diagram structures with text-based outputs by combining visual encoders with language models, enhancing accuracy and flexibility for applications like UML-to-code automation.",
            "score": 0.4945990740507372,
            "section_title": "Addressing the Problem",
            "char_start_offset": 4189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1429
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 253,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97314453125
        },
        {
            "corpus_id": "266693616",
            "title": "Taking the Next Step with Generative Artificial Intelligence: The Transformative Role of Multimodal Large Language Models in Science Education",
            "text": "For example, this can be achieved by dissecting video into audio and a series of images, then encoding and embedding each element separately to capture the context of the video format [Zhang et al. 2023c]. Taking a more comprehensive approach, multimodal inputs can also move beyond mere dual-modality and include additional sensor data [Driess et al. 2023] or speech [Chen et al. 2023]. An example of this advancement is PandasGPT [Su et al. 2023], which integrates six different modalities as input for LLMs. \n\nDespite the proficiency of LLMs in processing multimodal context, they are currently mostly designed to output information solely in textual format. By combining visual foundation models [Wu et al. 2023a], audio foundation models [Huang et al. 2023], or both [Shen et al. 2023], LLMs can act as a sophisticated 'controller' between data types. This allows them to intelligently invoke specific AI models as needed, marking a preliminary step toward generating multimodal outputs. \n\nEnhancements in multimodal capabilities, such as more advanced structures for audio processing [Zhang et al. 2023a], have the potential to significantly boost performance. The concept of a singular model capable of handling text, images, audio, and video for both input and output is still relatively nascent. Recently, Wu et al. published NExT-GPT as a general-purpose any-to-any open-source MLLM [Wu et al. 2023b] and OpenAI released GPT-4 Vision [OpenAI 2023] and GPT-4 Turbo1 , marking an important further development in this research. Also, Google announced an new LLM-based AI system they named Gemini, which is, according to Google \"built from the ground up for multimodality -reasoning seamlessly across text, images, video, audio, and code\" [DeepMind 2023]. This kind of reasoning in combination with other types of deep learning and explainability approaches [Borisov et al. 2022a;Rombach et al. 2022;Wu et al. 2020] has the potential to advance science [Wong et al. 2023] and education.",
            "score": 0.4944257230282954,
            "section_title": "Large Generative AI Models",
            "char_start_offset": 9553,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 510
                },
                {
                    "start": 513,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 992
                },
                {
                    "start": 995,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1993
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78271484375
        },
        {
            "corpus_id": "276422421",
            "title": "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs",
            "text": "Multimodal Large Language Models (MLLMs) [6,20,22,31] have made significant progress in recent years, allowing the comprehension and generation of diverse data modalities including text and images. However, in real-world * Equal contribution. scenarios, there exists a pervasive graph-structured relationships between texts and images. Such graph-structured relationship can be described as \"Multimodal Attributed Graphs\" (MMAGs) [17,27,40], where nodes are associated with image and text information. For example, the artwork graph [25] is composed of nodes that include images (pictures) and text (titles), with edges representing shared genres and authorship. This structure uniquely represents each artwork in relation to thousands of others within the graph, providing a context that extends beyond simple language descriptions or image references. While MLLMs have demonstrated outstanding comprehension and generation capability for text and image data, it is questionable how they could utilize the structural information on MMAGs. \n\nIn this context, we formulate the problem of multimodal content generation on MMAGs which tasks MLLMs with producing both a textual description and an accompanying image for a new node based on the graph connectivity and node attributes. This task focuses on generating text-image pairs for a node from MMAGs, reflecting a wide range of practical applications. For example, generating an image and a text for a product node linked to others through copurchase edges in an e-commerce MMAG is equivalent to recommending [5,24] potential future products to users. Likewise, creating an image and a title for a virtual artwork node in the art MMAG is comparable to creating virtual artwork [7,14] that reflects the subtle styles of various artists and genres. \n\nHowever, directly adopting MLLMs on MMAGs for multimodal content generation presents several challenges: (1) Graph Size Explosion: Although MMAGs provide sub-arXiv:2502.11925v1 [cs.AI] 17 Feb 2025 stantial context for image and text generation, inputting the entire local subgraph structure to a model is impractical due to the exponential increase in size with additional hops, leading to excessively long context sequences.",
            "score": 0.494001005095486,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1797
                },
                {
                    "start": 1800,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2225
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 47,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 47,
                    "end": 50,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 50,
                    "end": 53,
                    "matchedPaperCorpusId": "266374640"
                },
                {
                    "start": 533,
                    "end": 537,
                    "matchedPaperCorpusId": "195848773"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8994140625
        },
        {
            "corpus_id": "269043091",
            "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions",
            "text": "The rise of Large Language Models (LLMs) signifies the dawn of a transformative period in Artificial Intelligence, fundamentally restructuring the entire domain.Research labs, spanning both academia and industry, are actively engaged in a competitive race to advance the capabilities of LLMs.Yet, a notable limitation has come to the forefront -these models are confined to processing a singular modality of data, specifically text.This constraint underscores a pivotal challenge in the ongoing pursuit of refining LLMs to operate seamlessly across multiple modalities, marking a crucial avenue for further innovation in the realm of AI.\n\nNatural intelligence excels in processing information across multiple modalities, encompassing written and spoken language, visual interpretation of images, and comprehension of videos.This innate ability to seamlessly integrate diverse sensory inputs enables humans to navigate the complexities of the real world.For artificial intelligence to emulate human-like cognitive functions, it must similarly embrace multimodal data processing.This imperative is not merely technological but essential for providing AI systems with contextual awareness and adaptability in real-world scenarios.\n\nIn response to these limitations, researchers have pioneered a cutting-edge class of neural models known as Vision-Language Models (VLMs).These models intricately combine visual and textual information, showcasing a remarkable proficiency in comprehending and generating content that involves both images and text.Designed to excel in tasks such as responding to visual queries, image captioning and generating images based on textual descriptions, VLMs exhibit a versatile capability.Their seamless integration of visual and linguistic modalities positions them at the forefront of technological advancements, allowing them to navigate the intricate interplay between images and text with unparalleled finesse.\n\nIn recent times, major research labs have been consistently introducing innovative VLMs, including DeepMind's Flamingo, Salesforce's BLIP, and OpenAI's CLIP.Examples like GPT-4 (V) and Gemini showcase how chatbots are evolving within the realm of VLMs.Notably, not all multimodal models are VLMs; for instance, text-to-image models like Midjourney and DALL-E [70] lack a language generation component, underscoring the diverse landscape of multimodal AI landscape.",
            "score": 0.4936269433129021,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 432
                },
                {
                    "start": 432,
                    "end": 637
                },
                {
                    "start": 639,
                    "end": 824
                },
                {
                    "start": 824,
                    "end": 953
                },
                {
                    "start": 953,
                    "end": 1077
                },
                {
                    "start": 1077,
                    "end": 1227
                },
                {
                    "start": 1229,
                    "end": 1367
                },
                {
                    "start": 1367,
                    "end": 1543
                },
                {
                    "start": 1543,
                    "end": 1714
                },
                {
                    "start": 1714,
                    "end": 1940
                },
                {
                    "start": 1942,
                    "end": 2099
                },
                {
                    "start": 2099,
                    "end": 2194
                },
                {
                    "start": 2194,
                    "end": 2406
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89501953125
        },
        {
            "corpus_id": "266362535",
            "title": "A Survey of Reasoning with Foundation Models: Concepts, Methodologies, and Outlook",
            "text": "In the Frozen approach, visual information is represented as a sequence of embeddings, serving as a visual preix. MAGMA [68] follows a similar approach to Frozen by incorporating a new image preix encoder while keeping the language model frozen. It trains a series of Visual Language models capable of generating text autoregressively from combined visual and textual inputs. Visual ChatGPT [302] and  represent advancements in extending chatbot capabilities to encompass multimodal applications that support both image and text prompts. Visual ChatGPT builds upon the foundation of ChatGPT and incorporates visual models. It incorporates a Prompt Manager that manages the histories of various visual foundation models, enabling a comprehensive multimodal conversation experience. On the other hand, GPT-4 takes a diferent approach by accepting prompts that consist of both images and texts. This lexibility empowers users to specify vision and language tasks by generating text outputs in response to arbitrarily interlaced text and image prompts. Microsoft has also proposed a series of Multimodal Foundation Models, including Kosmos-1 [113] and Kosmos-2 [208]. These models further contribute to the development of multimodal capabilities and facilitate rich interactions involving both images and text. Furthermore, there are ongoing eforts to adapt GPT to speciic domains, such as BiomedGPT [343], which focuses speciically on biomedical research. These domain-speciic adaptations aim to enhance the language model's performance and applicability within specialized ields.",
            "score": 0.49359489038801063,
            "section_title": "Generation.",
            "char_start_offset": 65659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1577
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 124,
                    "matchedPaperCorpusId": "245117852"
                },
                {
                    "start": 1157,
                    "end": 1162,
                    "matchedPaperCorpusId": "259262263"
                },
                {
                    "start": 1396,
                    "end": 1401,
                    "matchedPaperCorpusId": "258947331"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87890625
        },
        {
            "corpus_id": "258959284",
            "title": "Generating Images with Multimodal Language Models",
            "text": "GILL is the first multimodal language model capable of conditioning on image-and-text inputs to generate meaningful images interleaved with text. Hence, our experiments primarily focus on evaluating its ability to produce novel images (Sec. 4.1). Our results show that GILL improves over Stable Diffusion [49] on tasks that require processing long-form text such as dialogue and discourse. We also benchmark the performance of models in deciding whether to retrieve or generate (see appendix). GILL is capable of generating text, retrieving images, and generating images. Despite being more general than prior work [56,4,31], we find that GILL performs comparably to or better than existing multimodal LMs on contextual image retrieval and text generation tasks (see Sec. 5).",
            "score": 0.493552565347128,
            "section_title": "Experiments",
            "char_start_offset": 16923,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 775
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 309,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.984375
        },
        {
            "corpus_id": "268091311",
            "title": "SoK: Exploring the Potential of Large Language Models for Improving Digital Forensic Investigation Efficiency",
            "text": "Traditional vision assistant systems face limitations in image processing or recognition, as they are typically trained on fixed types of datasets. However, with the emergence of LLMs, this paradigm has changed to the use of raw text as a source of supervision [92,93]. Research on visual recognition language models is experiencing exponential growth, with the number of models exceeding 1,500 in 2023 [94]. Radford et al. [92] introduced a novel method called Contrastive Language Image Pre-training (CLIP). This method is efficient and capable of performing a wide range of tasks during pre-training. It enables a model to learn a shared representation space for both images and text, facilitating a deeper understanding of the relationships between the two modalities. Ramesh et al. [95] proposes a model for text-to-image generation, capable of generating images as combinations derived from textual input or sentences. Moreover, with the model named Generating Images with Large Language Models (GILL), it becomes feasible to generate text, retrieve images, generate novel images, and interleave the results into coherent multimodal dialogues [96]. VisionLLM is a framework leveraging LLMs for diverse vision tasks with unified language instruction, demonstrating generality and flexibility [97]. It incorporates a language-guided image tokeniser and an LLM-based task decoder, capable of handling open-ended tasks based on provided language instructions [97]. \n\nVisual instruction tuning leverages language-only models, such as GPT-4, to generate multimodal language-image instruction following data. This data is then utilised to instructiontune large multimodal models, such as Large Language and Vision Assistant (LLaVA) [98,99,6]. The open source LLaVA project introduces an end-to-end trained model, integrating a vision encoder with an LLM. Notably, LLaVA showcases multimodal chat capabilities. LLaVa has the capability to interact with images, provide detailed descriptions and respond to queries with a reported accuracy of 92.53% [98]. This shows its effectiveness in understanding and generating contextually relevant information on visual content [98].",
            "score": 0.4931044916553169,
            "section_title": "Vision Assistance",
            "char_start_offset": 24754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1466
                },
                {
                    "start": 1469,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2171
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 268,
                    "matchedPaperCorpusId": "244714558"
                },
                {
                    "start": 403,
                    "end": 407,
                    "matchedPaperCorpusId": "257913547"
                },
                {
                    "start": 1149,
                    "end": 1153,
                    "matchedPaperCorpusId": "258959284"
                },
                {
                    "start": 1297,
                    "end": 1301,
                    "matchedPaperCorpusId": "258762579"
                },
                {
                    "start": 1461,
                    "end": 1465,
                    "matchedPaperCorpusId": "258762579"
                },
                {
                    "start": 1735,
                    "end": 1738,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87060546875
        },
        {
            "corpus_id": "277065932",
            "title": "Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey",
            "text": "The release of models such as GPT-4V [181], Gemini 2.0 [7], and Claude3 [194] has demonstrated remarkable capabilities in multimodal understanding, sparking significant interest in MLLMs within the research community. Initial investigations into MLLMs focused on developing robust language models capable of interpreting multimodal content and generating textual responses. In the domain of image-text understanding, notable progress has been achieved with Visual Large Language Models (VLLMs) such as BLIP2 [195], OpenFlamingo [196], MiniGPT-4 [197], and LLaVA [13]. \n\nConcurrently, advancements in video-text understanding have emerged, with significant contributions from VideoChat [198] and Video-ChatGPT [17]. Audio and speech comprehension have also garnered attention, exemplified by models like Qwen-Audio [199,16] and LLaSM [200]. A noteworthy development is VideoLLaMA [18], which leverages Qformer [195] to enable both audio and video understanding. In simple terms, mainstream MLLMs typically follow a consistent model architecture by processing multimodal embeddings or tokens into the decoder structure and generating contextually relevant outputs in an autoregressive manner, as shown in the left of Figure 4. \n\nParallel to these works about multimodal understanding, research also explored multimodal content generation. In image generation, models such as Kosmos-2 [201], GILL [202], Emu [203], and MiniGPT-5 [204] have achieved breakthroughs. Audio generation has seen advancements with SpeechGPT [205,206] and AudioPaLM [207], while video generation research, including CogVideo [208], VideoPoet [209], Video-Lavit [210], and StreamingT2V [211], has laid the groundwork for multimodal content creation. The recent introduction of GPT-4o [212], capable of both understanding and generating images and audio, has shifted attention toward \"any-to-any\" paradigm models.",
            "score": 0.4928079299180789,
            "section_title": "Multimodal LLMs",
            "char_start_offset": 12087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 567
                },
                {
                    "start": 570,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1884
                }
            ],
            "ref_mentions": [
                {
                    "start": 508,
                    "end": 513,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 562,
                    "end": 566,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 909,
                    "end": 914,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1394,
                    "end": 1399,
                    "matchedPaperCorpusId": "258959284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "corpus_id": "271310255",
            "title": "I Know About \"Up\"! Enhancing Spatial Reasoning in Visual Language Models Through 3D Reconstruction",
            "text": "Vision Language Models (VLMs). VLMs amalgamate computer vision and natural language processing technologies to comprehend and generate correlations between images and natural language [10]. VLMs can accept image input to generate a corresponding response in text [33]. In terms of training architecture, recent works include joint pretraining architectures (e.g. OFA [45] and VLMo [2]), which train image and text data jointly, and image-to-text mapping architectures (e.g., PaLI [7]), which map an image encoder to a well-pretrained text encoder. The latter approach has gained more popularity. VLMs exhibit cross-modal understanding capabilities, enabling them to extract information from images and translate it into natural language, or retrieve information from natural language and generate corresponding images [6]. This wide range of capabilities broadens the potential of VLMs (e.g., GlaMM [38] and MiniGPT-4 [56]) in understanding and generating associations between images and language, such as Image Captioning [53], Visual Question Answering [48], and Multi-modal Translation [21]. In this study, we aim to explore the visual-spatial recognition capabilities of these VLMs. \n\nVisual Spatial Reasoning. Visual spatial reasoning refers to the cognitive ability to understand and manipulate the spatial relationships of objects or elements in a given environment [26]. This reasoning ability involves not only recognizing the properties of individual entities, but also understanding the complex relationships and structures between them. Recent works have aimed to benchmark the problem of visual spatial reasoning, notably VSR [26] and Whatsup_vlm [17]. VSR [26] evaluates the ability of VLMs using text-image pairs to describe various visual spatial relationships, while Whatsup_vlm [17] assesses the visual spatial reasoning of VLMs through specific prepositions and perspectives. Although these methods provide valuable insights into the visual spatial reasoning capabilities of VLMs, they remain somewhat limited. In our work, we employ 3D reconstruction to more comprehensively test the visual spatial reasoning ability of VLMs. 3D Reconstruction.",
            "score": 0.4927372218117558,
            "section_title": "Related Work",
            "char_start_offset": 4330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1186
                },
                {
                    "start": 1189,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2164
                }
            ],
            "ref_mentions": [
                {
                    "start": 367,
                    "end": 371,
                    "matchedPaperCorpusId": "246634906"
                },
                {
                    "start": 818,
                    "end": 821,
                    "matchedPaperCorpusId": "248218567"
                },
                {
                    "start": 1023,
                    "end": 1027,
                    "matchedPaperCorpusId": "235692795"
                },
                {
                    "start": 1055,
                    "end": 1059,
                    "matchedPaperCorpusId": "247011309"
                },
                {
                    "start": 1089,
                    "end": 1093,
                    "matchedPaperCorpusId": "201058752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.904296875
        },
        {
            "corpus_id": "270440960",
            "title": "AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language Models",
            "text": "\u2022 Problem.This task involves solving high school-level problems across various disciplines such as Mathematics, Physics, Chemistry, and Biology.To differentiate from the OCR category, we ensure that most of these problems cannot be correctly answered based solely on the text in the images.\n\n\u2022 Comparison.This task requires models to identify differences between two sub-images or compare objects in different locations.Models need to locate objects and analyze the differences between them.\n\n\u2022 Writing.Writing based on images requires comprehensive analysis and creative abilities.\n\nIn this task, the model is required to compose poems, advertisements, speeches, and other forms of text based on the images.\n\nDialogue context.In real-world user scenarios, models are typically required to generate responses based on dialogue context.However, this remains an underexplored area in existing multimodal benchmarks.We broadly categorize this capability into two tasks:\n\n\u2022 Coherence.In practical applications, users prefer that VLMs retain and utilize the entire dialogue context to generate coherent responses.This task focuses on evaluating the model's ability to follow instructions and incorporate information from dialogue history.\n\n\u2022 Incoherence.When a model provides incorrect or irrelevant responses, users usually highlight these errors and expect the model to correct its replies.This task assesses the model's self-correcting abilities when users indicate its mistakes.",
            "score": 0.4925867366070205,
            "section_title": "Dataset composition",
            "char_start_offset": 7013,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 10,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 290
                },
                {
                    "start": 292,
                    "end": 305
                },
                {
                    "start": 305,
                    "end": 420
                },
                {
                    "start": 420,
                    "end": 491
                },
                {
                    "start": 493,
                    "end": 503
                },
                {
                    "start": 503,
                    "end": 582
                },
                {
                    "start": 584,
                    "end": 708
                },
                {
                    "start": 710,
                    "end": 727
                },
                {
                    "start": 727,
                    "end": 835
                },
                {
                    "start": 835,
                    "end": 913
                },
                {
                    "start": 913,
                    "end": 966
                },
                {
                    "start": 968,
                    "end": 980
                },
                {
                    "start": 980,
                    "end": 1108
                },
                {
                    "start": 1108,
                    "end": 1233
                },
                {
                    "start": 1235,
                    "end": 1249
                },
                {
                    "start": 1249,
                    "end": 1387
                },
                {
                    "start": 1387,
                    "end": 1477
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5576171875
        },
        {
            "corpus_id": "273185972",
            "title": "VISTA: A Visual and Textual Attention Dataset for Interpreting Multimodal Models",
            "text": "Vision-Language Models (VLMs): In recent years, substantial progress has been made in research on integrating language and vision. Numerous tasks now combine language-ranging from words, phrases, and sentences to paragraphs and full documents-with visual data, typically in the form of images or videos. Initially, much of the work focused on linking low-level linguistic units, such as words, with images or videos to create visual-semantic embeddings (Barnard et al., 2003;Frome et al., 2013;Kiros et al., 2014b;Liu et al., 2015;Cao et al., 2016;Tsai et al., 2017;Guo et al., 2018;Mogadala et al., 2018b;Wang et al., 2019;Kim et al., 2020). These embeddings are valuable for various downstream applications, as well as for understanding adversarial attacks (Wu et al., 2019) to enhance model robustness. \n\nMore recently, leading research labs have consistently introduced cutting-edge VLMs, such as OpenAI's CLIP (Radford et al., 2021), Salesforce's BLIP (Li et al., 2022a), and DeepMind's Flamingo (Alayrac et al., 2022). Notable examples like GPT-4 (OpenAI et al., 2024) and Gemini (Team et al., 2024) highlight the ongoing evolution of chatbots within the VLM space. However, not all multimodal models are VLMs; for instance, text-to-image models like DALL-E (Ramesh et al., 2021), Stable Diffusion (Rombach et al., 2022) and Midjourney (https://www.midjourney.com/) do not have a language generation component, illustrating the varied landscape of multimodal AI. A typical VLM architecture includes separate image and text encoders to generate embeddings, which are then combined in an image-text fusion layer, and the fused vector is processed by a large language model (LLM) to produce the final visually-aware text output.",
            "score": 0.4907483711377677,
            "section_title": "Related Work",
            "char_start_offset": 1837,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1731
                }
            ],
            "ref_mentions": [
                {
                    "start": 915,
                    "end": 937,
                    "matchedPaperCorpusId": "6941275"
                },
                {
                    "start": 1001,
                    "end": 1023,
                    "matchedPaperCorpusId": "227238841"
                },
                {
                    "start": 1086,
                    "end": 1105,
                    "matchedPaperCorpusId": "15019293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8974609375
        },
        {
            "corpus_id": "275156991",
            "title": "Similarity based task timely termination method for image based intelligent agents",
            "text": "In real-world scenarios, most tasks are often overly complex, typically involving information that goes beyond simple text and encompasses multimodal content. The next key research direction for agents based on large language models is to master the ability to process and generate multimodal information. This capability will enable agents to perform more powerful functions and is crucial for achieving human-like intelligence levels. Compared to single-modal agents, such as image-based agents, text-based agents, audio-based agents, and video-based agents, multimodal agents face challenges that are more complex and severe. \n\nToday, there are many efficient models or methods for processing multimodal data, with Contrastive Language-Image Pre-training (CLIP) and Flamingo being the most prominent. CLIP 52 is the first model that can be generalized to multiple image classification tasks through zero-shot and few-shot learning. The biggest highlight of CLIP is its ability to map data of different modalities, text, and images, to a shared vector space. This shared multimodal vector space makes tasks like text-to-image and image-to-text much easier. Unlike CLIP, Flamingo can generate text responses. Flamingo 53 is like CLIP with the addition of a language model, enabling it to generate corresponding text tokens based on the images and text it sees. Multimodal agents built using such models or methods can better accomplish complex tasks in real-life scenarios. \n\nAt the same time, there is a range of technologies and algorithms that can enhance the performance of agents in the real world. For instance, the COT 54 series of algorithms can be fully utilized to further improve the perception, memory, and reasoning capabilities of agents. In terms of perception, whether it's feedback from the environment or instructions from humans, using COT algorithms allows the agent to gradually receive information, better understand it, and based on this understanding, recognize intentions and convert them into the next task process. Regarding memory, when agents face multiple rounds of interaction with the environment, COT enables them to efficiently store and retrieve long-term memory by iteratively accessing text memory information. In reasoning, by referencing the approach of COT, agents can decompose tasks and make plans and decisions step by step, thereby enhancing the reliability of problem-solving by the agent.",
            "score": 0.4906902785332509,
            "section_title": "Further expansion of multimodal agents",
            "char_start_offset": 20086,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 628
                },
                {
                    "start": 631,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1474
                },
                {
                    "start": 1477,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2248
                },
                {
                    "start": 2249,
                    "end": 2435
                }
            ],
            "ref_mentions": [
                {
                    "start": 809,
                    "end": 811,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8896484375
        },
        {
            "corpus_id": "276774735",
            "title": "A Multimodal Symphony: Integrating Taste and Sound through Generative AI",
            "text": "In recent years, cross-modal generative models have advanced significantly, inspired by an increasing interest in developing systems capable of seamlessly integrating and translating information across diverse sensory modalities. This evolution is driven by the increasing capabilities of artificial intelligence, particularly within the realm of generative models, which have demonstrated potential in producing coherent and contextually relevant outputs across a multitude of domains. The advancement of cross-modal generative models is grounded in foundational research within the disciplines of cognitive neuroscience and experimental psychology, which have long investigated the interactions among different sensory modalities. These models endeavor to emulate the human faculty of perceiving and interpreting multisensory information, a process that is inherently complex and nuanced. By utilizing large-scale datasets and advanced machine learning techniques, researchers have initiated the creation of models capable of generating outputs that reflect the intricate interrelations among modalities such as vision, sound, and taste. Several notable multimodal generative models have emerged, illustrating the substantial capabilities inherent within this domain. Text-to-image generation models, such as DALL\u2022E (Ramesh et al., 2021) and Stable Diffusion (Rombach et al., 2022), are capable of rendering detailed images from textual descriptions. Text-to-audio models, including MusicLM (Agostinelli et al., 2023), translate text prompts into music or soundscapes, presenting intriguing possibilities for the fields of entertainment and virtual environments. Although still at a nascent stage, text-to-video generation (generating both video and audio) is anticipated to offer significant benefits for media content production and simulation environments (Singer et al., 2022). \n\nIn contrast, image-to-text models (Radford et al., 2021;Li et al., 2022;Alayrac et al., 2022) transform visual data into descriptive narratives, thereby facilitating tasks such as automated captioning and providing assistance to individuals with visual impairments. Audio-to-text models, which have been widely implemented in speech-to-text applications, have historically served the domains of transcription and virtual assistance (Bahar et al., 2019). Recent developments in generative models have enabled more nuanced and context-sensitive analyses of spoken language.",
            "score": 0.4901900209458302,
            "section_title": "Cross-modal generative models",
            "char_start_offset": 11741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1883
                },
                {
                    "start": 1886,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2339
                },
                {
                    "start": 2340,
                    "end": 2457
                }
            ],
            "ref_mentions": [
                {
                    "start": 1318,
                    "end": 1339,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 1920,
                    "end": 1942,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1942,
                    "end": 1958,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 2318,
                    "end": 2338,
                    "matchedPaperCorpusId": "204791999"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86083984375
        },
        {
            "corpus_id": "273969525",
            "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
            "text": "Large language models (LLMs) have demonstrated remarkable capabilities in learning diverse knowledge and generalizing to new scenarios [1,7,8,69,91]. Leveraging these capabilities, researchers have developed sophisticated models specialized in image comprehension [2,15,47,49,56,58] and text-to-image generation [23,73,76,79]. \n\nThe field has recently shifted toward creating unified systems capable of handling both tasks simultaneously. One prominent direction involves utilizing pre-trained text-to-image models for high-quality generation while training LLMs to generate conditions for these models [19,[25][26][27]87]. However, this approach introduces architectural complexity and potentially constrains the model's capabilities through maintaining separate LLM and generative components. Alternative approaches [88,97,99,100,108] propose training a single LLM for both tasks, typically incorporating either diffusion models [32,83] or vector-quantized autoregressive models [22,86]. \n\nOur approach builds upon recent breakthroughs in rectified flow models [3,23,55,61,62], which provide a simple framework for generative modeling while delivering exceptional empir-  ical performance [23,36,45]. Building on these advances, we propose JanusFlow, a powerful unified multimodal model that seamlessly integrates rectified flow with LLM architecture. Following a minimalist design principle, our architecture requires only a lightweight encoder and decoder to adapt the LLM for rectified flow operations. To optimize JanusFlow's performance, we implement two key strategies: First, we maintain separate vision encoders for understanding and generation tasks, preventing task interference and thus enhancing comprehension capabilities. Second, we align the intermediate representations between generation and understanding modules during training, strengthening semantic coherence in the generation process. \n\nJanusFlow shows state-of-the-art performances in both multimodal comprehension and text-to-image generation compared to existing unified approaches, and even outperforms several specialized methods.",
            "score": 0.4900186929088897,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 326
                },
                {
                    "start": 329,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1909
                },
                {
                    "start": 1912,
                    "end": 2110
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 267,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 267,
                    "end": 270,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 273,
                    "end": 276,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 276,
                    "end": 279,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 279,
                    "end": 281,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 312,
                    "end": 316,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 316,
                    "end": 319,
                    "matchedPaperCorpusId": "259341735"
                },
                {
                    "start": 322,
                    "end": 325,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 603,
                    "end": 607,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 619,
                    "end": 622,
                    "matchedPaperCorpusId": "259765944"
                },
                {
                    "start": 931,
                    "end": 935,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 981,
                    "end": 985,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 1063,
                    "end": 1066,
                    "matchedPaperCorpusId": "252668615"
                },
                {
                    "start": 1066,
                    "end": 1069,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 1069,
                    "end": 1072,
                    "matchedPaperCorpusId": "252734897"
                },
                {
                    "start": 1072,
                    "end": 1075,
                    "matchedPaperCorpusId": "252111177"
                },
                {
                    "start": 1075,
                    "end": 1078,
                    "matchedPaperCorpusId": "261697392"
                },
                {
                    "start": 1191,
                    "end": 1195,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 1198,
                    "end": 1201,
                    "matchedPaperCorpusId": "259275061"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9814453125
        },
        {
            "corpus_id": "276929205",
            "title": "WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation",
            "text": "Unified multimodal models aim to construct general-purpose models capable of processing both textual and visual inputs, and performing cross-modal generation and understanding. These models [4,8,16,17,21,25,35,38,39,41,[43][44][45]48] are typically built upon powerful large language models (LLMs) [54] and extend next-token prediction [2] to image generation: the LLM generates visual tokens, and a VQ-VAE [42] or Diffusion model serves as a detokenizer. Moreover, Transfusion [55] and Show-O [49] demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework. D-DiT [22] achieves both Text-to-Image (T2I) and Imageto-Text (I2T) tasks using an end-to-end diffusion model. A crucial question concerning unified multimodal models is whether their understanding and generation capabilities can mutually enhance each other. Some studies [41,45] have provided evidence supporting this phenomenon. However, in contrast to the rich and comprehensive benchmarks for multimodal understanding, T2I benchmarks are often relatively",
            "score": 0.4894732914888311,
            "section_title": "Unified Multimodal Models",
            "char_start_offset": 6123,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1083
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96630859375
        },
        {
            "corpus_id": "269791516",
            "title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models",
            "text": "We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",
            "score": 0.4892621430495129,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98291015625
        },
        {
            "corpus_id": "274597781",
            "title": "The Narrow Gate: Localized Image-Text Communication in Vision-Language Models",
            "text": "Multimodal Vision Language Models. Many approaches have been proposed to adapt decoder-only LLMs trained with autoregressive loss for image understanding [10-13, 32, 33] and multimodal generation tasks [16, 19-21, 24-26, 34-37]. These models consist of an LLM, an image encoder, an image decoder (for multimodal-output VLMs), and adapters between the LLM and visual components. The success of diffusion-based image generation has led to the widespread use of stable diffusion as the decoding architecture in multimodal-output models [16,19,21,24,25,35]. For the image encoding part, most architectures use a vision transformer (ViT), often that of CLIP [16,19,21,24,25,[34][35][36], due to the effectiveness of ViT embeddings in encoding image semantics [38]. The outputs from the visual encoder are either directly projected into the LLM embedding space [21,24,25,38] or, in the case of multimodal generation, often mapped to a discrete set of tokens [16,19,34]. A drawback of these methods is the complexity of encoding architecture. Moreover, the image-text contrastive loss can make the image representations resemble textual ones excessively [33], with text often favored as the preferred modality [17,19,25]. Alternative approaches use VQ-VAEs [39,40] to encode and decode the images and train the LLMs from scratch on multimodal datasets [20,26]. This strategy fuses the visual and textual representations early in training instead of using complex pipelines to bridge the modality gap [32,38,41,42] and eliminates the need for adapters, greatly simplifying the architecture design. \n\nSpecial Tokens, Memory Tokens, Registers. The importance of special tokens to store and redistribute global information was emphasized by Burtsev et al. [31].",
            "score": 0.4888816519575641,
            "section_title": "Related Works",
            "char_start_offset": 4131,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 35,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1750
                }
            ],
            "ref_mentions": [
                {
                    "start": 533,
                    "end": 537,
                    "matchedPaperCorpusId": "263605855"
                },
                {
                    "start": 537,
                    "end": 540,
                    "matchedPaperCorpusId": "267750101"
                },
                {
                    "start": 540,
                    "end": 543,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 546,
                    "end": 549,
                    "matchedPaperCorpusId": "261696650"
                },
                {
                    "start": 549,
                    "end": 552,
                    "matchedPaperCorpusId": "266374640"
                },
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "263605855"
                },
                {
                    "start": 657,
                    "end": 660,
                    "matchedPaperCorpusId": "267750101"
                },
                {
                    "start": 660,
                    "end": 663,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 666,
                    "end": 669,
                    "matchedPaperCorpusId": "261696650"
                },
                {
                    "start": 669,
                    "end": 673,
                    "matchedPaperCorpusId": "263889455"
                },
                {
                    "start": 673,
                    "end": 677,
                    "matchedPaperCorpusId": "266374640"
                },
                {
                    "start": 855,
                    "end": 859,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 862,
                    "end": 865,
                    "matchedPaperCorpusId": "261696650"
                },
                {
                    "start": 952,
                    "end": 956,
                    "matchedPaperCorpusId": "263605855"
                },
                {
                    "start": 956,
                    "end": 959,
                    "matchedPaperCorpusId": "267750101"
                },
                {
                    "start": 959,
                    "end": 962,
                    "matchedPaperCorpusId": "263889455"
                },
                {
                    "start": 1147,
                    "end": 1151,
                    "matchedPaperCorpusId": "270703300"
                },
                {
                    "start": 1203,
                    "end": 1207,
                    "matchedPaperCorpusId": "258822817"
                },
                {
                    "start": 1207,
                    "end": 1210,
                    "matchedPaperCorpusId": "267750101"
                },
                {
                    "start": 1210,
                    "end": 1213,
                    "matchedPaperCorpusId": "261696650"
                },
                {
                    "start": 1254,
                    "end": 1257,
                    "matchedPaperCorpusId": "247628171"
                },
                {
                    "start": 1493,
                    "end": 1497,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1500,
                    "end": 1503,
                    "matchedPaperCorpusId": "270357343"
                },
                {
                    "start": 1503,
                    "end": 1506,
                    "matchedPaperCorpusId": "247244904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.935546875
        },
        {
            "corpus_id": "277043666",
            "title": "DynRsl-VLM: Enhancing Autonomous Driving Perception with Dynamic Resolution Vision-Language Models",
            "text": "Since the introduction of the Transformer architecture in 2017, vision-language models have advanced rapidly. The Transformer's powerful sequence modeling and global attention mechanisms revolutionized natural language processing and laid the groundwork for multimodal integration. Models like ViLBERT [20] and LXMERT [26] applied the Transformer to dual-stream processing of vision and language, separately encoding visual and textual information before fusing them, establishing the basis for multimodal learning. Subsequently, VisualBERT [17] and VL-BERT [25] further advanced the field by integrating visual and textual data within a unified Transformer architecture, enabling more effective multimodal representations. Later, models like CLIP [24] and ALIGN [13] employed large-scale cross-modal contrastive learning to map images and texts into a shared embedding space, significantly improving performance in zero-shot and few-shot learning tasks. In multimodal dialogue and question answering, Visual Dialog [6] enabled models to understand and generate image descriptions in multi-turn conversations. More recent work, such as Microsoft's VLMo [4], introduced a unified vision-language model capable of handling multiple tasks like visual question answering and image captioning. \n\nRecent research emphasizes multimodal feature alignment, crucial for enhancing model performance. For instance, BLIP-2 [15] introduces the Q-Former module-a lightweight query Transformer that extracts languagerelated information from visual features, achieving more efficient and fine-grained alignment. These advancements in pre-training and feature alignment have not only improved vision-language models across various tasks but also revitalized the development of multimodal artificial intelligence.",
            "score": 0.48883357377971415,
            "section_title": "Vision-Language Models",
            "char_start_offset": 3631,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1794
                }
            ],
            "ref_mentions": [
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "201103729"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96044921875
        },
        {
            "corpus_id": "277043716",
            "title": "Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models",
            "text": "Generation Models: Large Vision-Language Models (LVLMs) integrate vision-language modality information to generate final language outputs. This evolution is marked by the integration of pre-trained vision encoders and large language models (LLMs), enabling LVLMs to process and generate language based on visual inputs. Recent advancements include architectures that employ learnable queries to distill visual information and align it with LLM-generated text, as well as models like LLaVA [30,31], InternVL [6], Ovis [33], and Qwen [43,45], which use projection layers to bridge visual features and textual embeddings. Additionally, the commercial closed-source LVLMs Claude-3.5-Sonnet (Anthropic) [2] and GPT-4o (OpenAI) [36] garner significant attention in contemporary society due to their advanced capabilities and widespread applications. Concretely, the application of LVLMs in VLP tasks extends to scenarios such as medical diagnosis [19,49], business operations [20,37], and education [9]. For Image-to-Image (I2I) Generation Models, previous architectures such as GANs [16], VAEs [24], and their variants [17,22,25] demonstrate performance to a certain extent. However, diffusion-based models, particularly DDPM [18] and its variants [28,35,48], have gained prominence due to their superior performance. Among these, CLIP-guided diffusion models, such as DALL-E 2 (UnCLIP) [41] and IP-Adapter [50], integrate the CLIP vision encoder [39] to enhance visual semantic perception, enabling the generation of highly realistic, diverse, and semantically rich images. These models have become dominant in both research and commercial applications. Concurrently, the development of Multimodal Large Language Models (MLLMs) like GPT-4 (OpenAI) [36] and Dreamina (ByteDance) [4]. While I2I tasks can also be expanded to fields such as artistic creation [47,51], fundamental scientific exploration [3,26], and historical archaeology [5,21].",
            "score": 0.4879677972187103,
            "section_title": "Related Works",
            "char_start_offset": 5358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1938
                }
            ],
            "ref_mentions": [
                {
                    "start": 941,
                    "end": 945,
                    "matchedPaperCorpusId": "267657686"
                },
                {
                    "start": 945,
                    "end": 948,
                    "matchedPaperCorpusId": "270371398"
                },
                {
                    "start": 974,
                    "end": 977,
                    "matchedPaperCorpusId": "271051104"
                },
                {
                    "start": 993,
                    "end": 996,
                    "matchedPaperCorpusId": "270703507"
                },
                {
                    "start": 1078,
                    "end": 1082,
                    "matchedPaperCorpusId": "1033682"
                },
                {
                    "start": 1121,
                    "end": 1124,
                    "matchedPaperCorpusId": "222291664"
                },
                {
                    "start": 1243,
                    "end": 1247,
                    "matchedPaperCorpusId": "266191471"
                },
                {
                    "start": 1247,
                    "end": 1250,
                    "matchedPaperCorpusId": "251765371"
                },
                {
                    "start": 1250,
                    "end": 1253,
                    "matchedPaperCorpusId": "261558570"
                },
                {
                    "start": 1442,
                    "end": 1446,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.875
        },
        {
            "corpus_id": "268363835",
            "title": "Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation",
            "text": "Text-to-image diffusion models [2,9,29,38,41,44] are capable of generating images based on user prompts.These models consist of two main components, namely a language model and a vision model.The language module is responsible for understanding the text input provided by the user, extracting contextual information, and injecting it into the vision module to generate the desired image.Text-to-image diffusion models have paved the way for various exciting research areas, including image editing [3,22,27], controllable image generation [28,53,54], personalized object generation [12,16,40], as well as other interesting applications [5,13,15].In the extensive exploration of diffusion models, researchers have utilized different language models and vision models.For instance, Stable Diffusion [38] employs CLIP text encoder [31] as its language model and U-Net as its vision model.Imagen [41] utilizes T5 [34] as its language model, which claims to enhance both sample fidelity and image-text alignment.ParaDiffusion [50] focuses on paragraph-to-image generation and leverages the powerful semantic understanding capability of Llama-2 [48] to comprehend lengthy sentences.PixArt [6], on the other hand, utilizes a ViT [11] as its vision model and achieves high image fidelity while being trained at a lower cost.\n\nAfter training on a large dataset of text-image pairs [42], the language and vision models in the text-to-image diffusion model become closely intertwined.This tight coupling ensures a strong alignment between the provided text description and the generated image, but at the same time also limits the flexibility of the diffusion model.For instance, if a more advanced language or vision model becomes available, it may have the potential to enhance the text-to-image task.However, decoupling the language and vision modules in existing text-to-image diffusion models and replacing a module with a new one is nontrivial.Therefore, this paper explores the dilemma faced by text-to-image generation and proposes a framework that enables efficient integration of various language models and generative vision models.",
            "score": 0.486948889436253,
            "section_title": "Text-to-Image Diffusion Models",
            "char_start_offset": 7796,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 104,
                    "end": 192
                },
                {
                    "start": 192,
                    "end": 387
                },
                {
                    "start": 387,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 766
                },
                {
                    "start": 766,
                    "end": 885
                },
                {
                    "start": 885,
                    "end": 1007
                },
                {
                    "start": 1007,
                    "end": 1176
                },
                {
                    "start": 1176,
                    "end": 1316
                },
                {
                    "start": 1318,
                    "end": 1473
                },
                {
                    "start": 1473,
                    "end": 1655
                },
                {
                    "start": 1655,
                    "end": 1792
                },
                {
                    "start": 1792,
                    "end": 1939
                },
                {
                    "start": 1939,
                    "end": 2132
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 36,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 36,
                    "end": 39,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 39,
                    "end": 42,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 42,
                    "end": 45,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 498,
                    "end": 501,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 501,
                    "end": 504,
                    "matchedPaperCorpusId": "252918469"
                },
                {
                    "start": 504,
                    "end": 507,
                    "matchedPaperCorpusId": "245704504"
                },
                {
                    "start": 543,
                    "end": 546,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 589,
                    "end": 592,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 636,
                    "end": 639,
                    "matchedPaperCorpusId": "258762187"
                },
                {
                    "start": 639,
                    "end": 642,
                    "matchedPaperCorpusId": "258108187"
                },
                {
                    "start": 642,
                    "end": 645,
                    "matchedPaperCorpusId": "259501509"
                },
                {
                    "start": 797,
                    "end": 801,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 828,
                    "end": 832,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 892,
                    "end": 896,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 909,
                    "end": 913,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1183,
                    "end": 1186,
                    "matchedPaperCorpusId": "263334265"
                },
                {
                    "start": 1222,
                    "end": 1226,
                    "matchedPaperCorpusId": "225039882"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.859375
        },
        {
            "corpus_id": "266210376",
            "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
            "text": "Multimodal Pre-training in the Pre-LLM Era. Prior research efforts primarily concentrated on model architecture to facilitate the fusion and interaction of cross-model data [6,50,52]. The success of transformers in language models [42] and ViT [10] inspired the development of unified multi-modal modeling [27,44]. Although images and language can be processed by a unified model with shared parameters, they often have distinct training objectives. It is worth mentioning that the BEiT series [2,45] successfully adapted the masked language modeling objective from BERT [8] to vision and multimodal pre-training. \n\nMultimodal Pre-training in the LLM Era. Building upon pre-trained large language models (LLMs) [33,40,41,54], recent studies have effectively developed multimodal language models capable of processing image and text inputs to generate text outputs [1, 22,24,25,56]. Another challenge for large multimodal models is generating multimodal content beyond language. Several efforts, such as Visual ChatGPT [47] and HuggingGPT [38], have achieved this by connecting LLMs with other generation tools within an LLM integration framework, e.g., LangChain. However, these systems exhibit instability and limited room for further optimization. To enable LLMs to generate images with optimization, M-VADER [46] aligns the semantic consistence between an LLM and a diffusion decoder by training them on image-text pair data. GILL [18] achieves more complex interleaved image-text generation by mapping the embedding spaces of the LLM to text-to-image generation models. NExT-GPT [48] extends this concept to additional modalities, such as audio and video. DreamLLM [9] facilitates passing the differential gradient from image diffusion models to language models, enabling the generation of free-form interleaved content. Following similar methods, Kosmos-G [30] enhances the fidelity of generated images in context through a compositional instruction tuning task. \n\nIn contrast to our VL-GPT, these studies mainly focus on  leveraging existing LLMs and exploring the integration of current image encoders and image generation models into LLMs.",
            "score": 0.4866208630921012,
            "section_title": "Related Work",
            "char_start_offset": 6155,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 43
                },
                {
                    "start": 44,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 613
                },
                {
                    "start": 616,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 2147
                }
            ],
            "ref_mentions": [
                {
                    "start": 310,
                    "end": 313,
                    "matchedPaperCorpusId": "246634906"
                },
                {
                    "start": 711,
                    "end": 715,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9443359375
        },
        {
            "corpus_id": "255394080",
            "title": "Progressive Text-to-Image Generation",
            "text": "The task of text-to-image generation aims to create natural and consistent images from the input text and has received extensive research interest. Recently, latent autoregressive (AR) generation frameworks have achieved great success in advancing the start-of-the-arts, by learning knowledge and patterns from a large-scale multimodal corpus [35,42,64]. Generally, they treat the task as a form of language modeling and use Transformer-like [59] architectures to learn the relationship between language inputs and visual outputs. A key component of these approaches is the conversion of each image into a sequence of discrete units through the use of a VQ-VAE [57] based image tokenizer, e.g., VQ-GAN [16,63], RQ-VAE [32] and ViT VQ-GAN [63]. Visual tokenization essentially unifies the view of text and images so that both can be treated simply as sequences of discrete tokens and is adaptable to sequenceto-sequence models. To that end, DALL-E [47], CogView [12], RQ-Transformer [32], and Parti [65] employ autoregressive models to learn text-to-image task from a large collection of potentially noisy text-image pairs [5,17,27]. In particular, [62] further expand on this AR over AR modeling approach to support arbitrarily-sized image generation. \n\nAnother research line for text-to-image generation involves diffusion-based methods, such as GLIDE [38], DALL-E 2 [46], stable diffusion [49], RQ-Transformer [32], and Imagen [51]. These models pursue to directly generate images or latent image features with diffusion process [11,22] and produce high-quality images with great aesthetic appeal. Even so, discrete sequence modeling for text-to-image generation remains appealing given extensive prior work on large language models [4] and advances in discretizing other modalities, such as video and audio, as cross-language tokens [3].",
            "score": 0.4863860761369274,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1840
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 350,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 702,
                    "end": 706,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 718,
                    "end": 722,
                    "matchedPaperCorpusId": "247244535"
                },
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 982,
                    "end": 986,
                    "matchedPaperCorpusId": "247244535"
                },
                {
                    "start": 1122,
                    "end": 1125,
                    "matchedPaperCorpusId": "231951742"
                },
                {
                    "start": 1128,
                    "end": 1131,
                    "matchedPaperCorpusId": "231879586"
                },
                {
                    "start": 1391,
                    "end": 1395,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1412,
                    "end": 1416,
                    "matchedPaperCorpusId": "247244535"
                },
                {
                    "start": 1531,
                    "end": 1535,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1535,
                    "end": 1538,
                    "matchedPaperCorpusId": "219955663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91845703125
        },
        {
            "corpus_id": "273350685",
            "title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding",
            "text": "Multimodal Large Language Models (MLLMs) integrate visual representations from images with linguistic embeddings from text, thereby enhancing the models' capabilities in comprehending and generating language descriptions of visual content. Most open-source MLLMs employ architectures that include a pre-trained vision encoder, an LLM, and a crossmodal connector. Early models, such as the BLIP series [1], [36], utilized the Q-Former module to align text and images, thus improving multimodal capabilities. Flamingo [37] employed a gated cross-attention mechanism to integrate images and text. LLaVA-1.5 [3] adopted an MLP projector to connect the pre-trained vision encoder with the LLM. InternVL [4], [6] employed a dynamic resolution strategy, segmenting images into tiles and encoding both the tiles and the thumbnail view together. It then uses a pixel shuffle operation to reduce the number of visual tokens before integrating these features with the LLM through an MLP projector. \n\nAdditionally, private MLLMs such as the Gemini series [38], [39], GPT-4V [7], and Claude-3V series [40], along with open-source MLLMs like MiniGPT-4 [23], Qwen-VL [5], CogVLM [41], the VisionLLM series [42], [43], and the All-Seeing series [44], [45], among others [46], have demonstrated robust multimodal capabilities. These models exemplify the powerful ability of MLLMs to understand, generalize, and reason with multimodal information, consistently setting new benchmarks in multimodal tasks.",
            "score": 0.48624043838698516,
            "section_title": "II. RELATED WORK A. Multimodal Large Language Model",
            "char_start_offset": 6868,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1486
                }
            ],
            "ref_mentions": [
                {
                    "start": 401,
                    "end": 404,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 406,
                    "end": 410,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 516,
                    "end": 520,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 604,
                    "end": 607,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 698,
                    "end": 701,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 1191,
                    "end": 1195,
                    "matchedPaperCorpusId": "258762579"
                },
                {
                    "start": 1229,
                    "end": 1233,
                    "matchedPaperCorpusId": "260438589"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9599609375
        },
        {
            "corpus_id": "273403516",
            "title": "H2OVL-Mississippi Vision Language Models Technical Report",
            "text": "Early VLMs focused on connecting vision encoders to language models through trainable connectors, allowing models to align visual and textual representations. Notable examples include Flamingo [1] and BLIP-2 [2], which achieved strong results in tasks such as image captioning and visual question answering (VQA) by leveraging pre-trained vision and language components. LLaVA [3] extended this approach by introducing multimodal instruction tuning, enabling models to follow human instructions across visual tasks, such as interactive dialogue about images. This capability set a new benchmark for multimodal interaction and improved the model's ability to transfer knowledge across tasks. \n\nFurther advancements were made with models like PaLI [4], Florence-2 [5], and Unified-IO 2 [6], which jointly trained vision and language components instead of relying on frozen pre-trained encoders. This joint training approach improved the model's performance on complex, cross-modal tasks such as document parsing and visual reasoning. \n\nDecoder-only models, like Fuyu [7] and CM3 [8], streamlined the architecture by using a single transformer to process both image and text inputs. This simplification increased training and inference efficiency, making these models attractive for scenarios where computational resources are limited. \n\nRecently, encoder-decoder models, like Qwen2-VL [9] utilize a Naive Dynamic Resolution mechanism, enabling it to process images at varying resolutions by dynamically adjusting the number of visual tokens. This allows the model to handle complex visual tasks such as detailed image captioning and OCR with improved efficiency and accuracy. Similarly, InternVL 1.5 [10] adopts a high-resolution strategy, breaking down images into tiles, which improves the model's ability to capture fine details across a range of vision tasks. Other models, such as Mini-Monkey [11], tackle high-resolution image processing challenges by introducing multi-scale adaptive cropping, which allows models to capture small or irregularly shaped objects more accurately.",
            "score": 0.48611288723831575,
            "section_title": "Related Works",
            "char_start_offset": 1897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1031
                },
                {
                    "start": 1034,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1332
                },
                {
                    "start": 1335,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2082
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90771484375
        },
        {
            "corpus_id": "277740693",
            "title": "Investigating Vision-Language Model for Point Cloud-based Vehicle Classification",
            "text": "Vision-language models (VLMs) are a class of multimodal generative models designed to process and under-arXiv:2504.08154v1 [cs.CV] 10 Apr 2025 stand both visual and textual data. These models take image and text inputs and generate text-based outputs, which enables a wide range of applications. Large VLMs demonstrate strong zero-shot performance and generalize effectively across diverse image types-including documents, web pages, and photographs-and support tasks such as image-based chat, instruction-driven recognition, visual question answering, document understanding, and image captioning [13]. Some advanced VLMs, e.g., DeepSeek-VL, also incorporate spatial reasoning, allowing them to detect, segment, and localize objects within an image [14]. When prompted, they can generate bounding boxes or segmentation masks, identify specific subjects, and answer questions about spatial relationships. The capabilities of VLMs vary significantly based on their training data, image encoding strategies, and architectural design, which leads to diverse strengths across different applications.",
            "score": 0.4860012619040731,
            "section_title": "Vision Language Model",
            "char_start_offset": 2234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1095
                }
            ],
            "ref_mentions": [
                {
                    "start": 598,
                    "end": 602,
                    "matchedPaperCorpusId": "266174746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83544921875
        },
        {
            "corpus_id": "276782523",
            "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
            "text": "Unified multimodal generative models aim to bridge the gap between understanding and generation tasks, and integrate vision and language into a single framework, enabling the model to learn shared representations across modalities [1,2,15,19,59,62,71,72,74,76,77,79,85]. SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder [45] are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM [15] extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities. Our work leverages the advancements made by these foundational models and explores how to adapt the foundational model to specific domains.",
            "score": 0.48554158297839234,
            "section_title": "Unified Multimodal Generative Models",
            "char_start_offset": 6395,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1413
                }
            ],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 236,
                    "matchedPaperCorpusId": "262898422"
                },
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 242,
                    "end": 245,
                    "matchedPaperCorpusId": "266374640"
                },
                {
                    "start": 373,
                    "end": 377,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 517,
                    "end": 521,
                    "matchedPaperCorpusId": "261975252"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9658203125
        },
        {
            "corpus_id": "267750101",
            "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling",
            "text": "To enable cross-modal perception in LLM, a common approach is to connect pre-trained encoders of other modalities as adaptors. However, these models are often limited to text generation. \n\nTo empower LLMs with multimodal generation capabilities, Tang et al. (2023b) introduces a frozen text-to-image diffusion model and learns the mapping between the LLM's embeddings and the diffusion model. Sun et al. (2023a) utilizes continuous embeddings to represent the image, calculating either a loss for the next token prediction or the next visual embedding regression. In contrast, SEED-LLaMA (Ge et al., 2023b) trains an image discretization tokenizer to encode the original image into discrete tokens. Through a unified next token prediction task, it achieves unified image understanding and generation. Similarly, in the field of speech, SpeechGPT (Zhang et al., 2023a) enables LLMs to have inherent cross-modal conversation capabilities through discrete speech representation. VideoPoet (Kondratyuk et al., 2023) employs a decoder-only transformer architecture that processes multimodal inputs -including images, videos, text, and audio, and is capable of generating videos and audio. \n\nTo achieve multimodal generation across various modalities on LLMs, NExT-GPT (Wu et al., 2023) utilizes existing high-performance encoders and decoders, connected by a small number of projection layer parameters. However, NExT-GPT does not train the LLM, which may result in suboptimal performance. Moreover, its representation of multimodal input and output lacks a unified form, which poses challenges in unified training and inference.",
            "score": 0.48523350234578866,
            "section_title": "Multimodal Large Language Models",
            "char_start_offset": 5116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 186
                },
                {
                    "start": 189,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1183
                },
                {
                    "start": 1186,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1624
                }
            ],
            "ref_mentions": [
                {
                    "start": 846,
                    "end": 867,
                    "matchedPaperCorpusId": "258762683"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89794921875
        },
        {
            "corpus_id": "277043335",
            "title": "Small Vision-Language Models: A Survey on Compact Architectures and Techniques",
            "text": "Figure 15: Cobra Architecture: Integrates a Mamba backbone with 64 Mamba blocks for processing language and visual inputs. It combines linear transformations, state space models (SSM), Swish activations, and convolutional layers to handle complex data. The architecture uses tokenizers and MLPs to process language instructions and image inputs (X v ) from models like DINOv2 and SigLIP, concatenating responses (R 1 and R 2 ) to generate coherent language outputs (X a ). This design ensures efficient and effective multimodal processing for tasks involving both text and images. \n\nCobra [111], it employs a vision encoder that fuses features from DINOv2 [64], which captures low-level spatial details, and SigLIP [106], which provides semantic visual representations. These features are combined into a compact representation. A projector, implemented as a learnable module, aligns the visual and textual modalities by mapping visual features into the language embedding space. Cobra's backbone is the Mamba language model, which utilizes a stack of SSM-based layers, including convolutional components, selective propagation mechanisms, residual connections, and Root Mean Square Normalization (RMSNorm). The architecture processes input sequences auto-regressively, effectively integrating visual and text tokens to generate natural language responses. Key contributions include Cobra's ability to achieve inference speeds 3\u00d7 to 4\u00d7 faster than leading models like LLaVA-Phi [116] and MobileVLM [16], while maintaining or exceeding performance on various benchmarks. It reduces parameter count by 48% without sacrificing accuracy, demonstrating efficiency on both open-ended tasks (e.g., VQA-v2, TextVQA [81]) and closed-set tasks (e.g., VSR [109], POPE [52]). Limitations include a dependence on vision encoder choice, the negative impact of lightweight down-sample projectors (LDPv2), the importance of training strategies including the number of training epochs and projector initialization, the influence of model parameter size on the grounding ability, and sensitivity to prompt order, particularly when using OCR tokens.",
            "score": 0.484736734310574,
            "section_title": "Mamba-Based Models",
            "char_start_offset": 33916,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 2130
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "259164940",
            "title": "Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation",
            "text": "Recently, the most important breakthrough was made by ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b), which unveiled the emerging potential of the conversation between human and artificial intelligence system. ChatGPT serves as a chatbot that operates with language as both input and output, while GPT-4 is a multimodal model capable of accepting both image and text inputs and producing text outputs. \n\nA successful multimodal generative model should excel in both textual and visual reasoning, generating high-quality text and image feedback. Visual ChatGPT (Chenfei Wu & Duan, 2023) is a pioneering work that combines ChatGPT with a series of pre-trained visual foundation models, enabling text-image chat. Another relevant work, FROMAGe (Jing Yu Koh, 2023), also involves image-text inputs and outputs for  The model is required to generate a re-created image (M) and a textual feedback (A) conditioned on the visual input (V) and text-based user query (T), and the language-based explanation is also given for those instructions that cannot be executed and the prohibited instructions. \n\n\u2022 We consider the issue of accountability in multimodal generative models by embedding pre-set rules as supervised signals in our datasets. This enables the VLMs to learn to reject human instructions in multimodal conversations. \n\n\u2022 We propose a two-stage training procedure for training the image auto-encoder and auto-regressive transformer, aiming to enable the models to learn how to reject human instructions. All of our models are trained from scratch, and their training took 350\u223c900 GPU days on our constructed datasets. \n\n\u2022 We provide extensive qualitative and quantitative results, evaluating the quality of generated images, the accuracy of answers, and the model's ability to handle uncertainty and incomplete queries. \n\nThe remainder of this paper is organized as follows. Section 2 presents detailed information about our datasets. In Section 3, we introduce our models and training process. We then present extensive experiments and detailed analysis in Section 4 to validate our proposed method. Section 5 provides an overview of related work, and Section 6 concludes the paper. Section 7 pertains to acknowledgments.",
            "score": 0.48471766378614933,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 1093
                },
                {
                    "start": 1096,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1324
                },
                {
                    "start": 1327,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1624
                },
                {
                    "start": 1627,
                    "end": 1826
                },
                {
                    "start": 1829,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 1941
                },
                {
                    "start": 1942,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2190
                },
                {
                    "start": 2191,
                    "end": 2229
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95458984375
        },
        {
            "corpus_id": "270688372",
            "title": "Holistic Evaluation for Interleaved Text-and-Image Generation",
            "text": "Large Multimodal Models for Interleaved Generation The advent of large multimodal models (LMMs) (Koh et al., 2023;Sun et al., 2023a) has significantly advanced the field of interleaved textand-image generation. Previous models such as DALL-E (Ramesh et al., 2021) and Stable Diffusion (Podell et al., 2023) have demonstrated impressive capabilities in generating high-quality images conditioned on textual descriptions. However, previous focus has predominantly been on unidirectional generation tasks, either from text to image or image to text, without considering the interleaved generation scenarios where text and images are seamlessly integrated within the same output. Recent works have begun to address this gap, with the LMMs extended with diffusion models, exploring the generation of mixed text and image outputs (Koh et al., 2023;Sun et al., 2023b;Dong et al., 2024;Tian et al., 2024;Zhan et al., 2024;Chen et al., 2023). These models leverage advanced architectures and training techniques to enhance their ability to produce coherent and contextually relevant interleaved content. Despite these advancements, the evaluation of such models remains an underexplored area, with most evaluations still relying on separate assessments of text and image quality or simplistic reference-based metrics. Our proposed INTERLEAVEDBENCH benchmark aims to bridge this gap by providing a holistic evaluation framework tailored specifically for interleaved text-and-image generation.",
            "score": 0.4842476342124168,
            "section_title": "Related Work",
            "char_start_offset": 5358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1482
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 114,
                    "matchedPaperCorpusId": "258959284"
                },
                {
                    "start": 242,
                    "end": 263,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 824,
                    "end": 842,
                    "matchedPaperCorpusId": "258959284"
                },
                {
                    "start": 860,
                    "end": 878,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 896,
                    "end": 914,
                    "matchedPaperCorpusId": "267750101"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97021484375
        },
        {
            "corpus_id": "270765959",
            "title": "FigCLIP: A Generative Multimodal Model with Bidirectional Cross-attention for Understanding Figurative Language via Visual Entailment",
            "text": "The Multimodal Figurative Language (MFL) Shared Task can be treated as a classification and generation problem.Given an <image, claim> pair, a MFL model is required to align image-claim representations, learn a binary classification function F c to predict entailment or contradiction labels by following Eq. 1, and learn a generation function F g to generate explanations by following Eq. 2.\n\n(1) V , where d denotes the dimension of 768.This claim-guided multimodal vector achieves an effective interaction of observing details in images based on text.Similarly, the image vector v image serves as Q, and the claim vector v claim serves as K and V .They are fed into the fusion module and then a imageguided multimodal vector v image-gui multi is calculated by the same cross-attention calculation process.This image-guided multimodal vector achieves an effective interaction of observing details in text based on images.These two mentioned-above steps share parameters, enhancing the alignment of figurative image-text representations.After that, the v claim-gui multi and the v image-gui multi are concatenated and input to a binary linear-layer classifier to predict a label of entailment or contradiction.\n\nThe original representation space of CLIP is inconsistent with that of GPT-2.GPT-2 relies on a 50257-dimensional vocabulary to generate text, while the CLIP multimodal space is 768dimensional.For generating explanation, the Fig-CLIP model needs to match the low-dimensional multimodal representations to 50257 dimensions in a mapping space.Inspired by ClipCap (Mokady et al., 2021), we stack multiple linear layers of different dimensions as a projector.This projector is composed of three sets of linear layers of (768*2\u21922048), (2048\u21924096), (4096\u219250257).In order to further compress the size of parameters to reduce training costs, the parameters of this (4096\u219250257) linear layer are frozen and treated as a fixed matrix.This is the reason why FigCLIP is more lightweight than ClipCap, despite their similar model architectures.",
            "score": 0.4840647832006208,
            "section_title": "Task formulation",
            "char_start_offset": 6890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 111,
                    "end": 392
                },
                {
                    "start": 394,
                    "end": 439
                },
                {
                    "start": 439,
                    "end": 554
                },
                {
                    "start": 554,
                    "end": 651
                },
                {
                    "start": 651,
                    "end": 808
                },
                {
                    "start": 808,
                    "end": 923
                },
                {
                    "start": 923,
                    "end": 1038
                },
                {
                    "start": 1038,
                    "end": 1211
                },
                {
                    "start": 1213,
                    "end": 1290
                },
                {
                    "start": 1290,
                    "end": 1405
                },
                {
                    "start": 1405,
                    "end": 1553
                },
                {
                    "start": 1553,
                    "end": 1667
                },
                {
                    "start": 1667,
                    "end": 1768
                },
                {
                    "start": 1768,
                    "end": 1936
                },
                {
                    "start": 1936,
                    "end": 2043
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62158203125
        },
        {
            "corpus_id": "271218251",
            "title": "Visualization Literacy of Multimodal Large Language Models: A Comparative Study",
            "text": "Recently, models that accept multiple modalities as input are gaining increasing popularity.From the early work, e.g., CLIP [26], that jointly embedded image and text to models designed for input in one modality and output in another, such as text-to-image [28,27] and image-to-text [37] models.In the context of this work, we focus on multi-modal large language models (LLM) [36] that can understand both image and text inputs for text generation.LLMs are often referred to as foundation models due to their extensive capabilities and scale.They often possess a wide range of knowledge, allowing them to understand implicit context and common sense.\n\nSince humans interact with their environment through multi-modal sensory input, the evolution from text-only LLM systems to multimodal LLMs significantly increases the potential usage cases for LLMs, prompting many applications [35].Since the initial introduction of GPT4-V, many state-of-the-art MLLMs have been introduced such as the recent update to GPT4-o from OpenAI, multimodal support for Claude 3 [3] from Anthropic, and Gemini family from Google.On the open weight model side, many attempts have been made to develop similar capabilities, such as LLAVA [18], and CogVLM [30].",
            "score": 0.48399356652039915,
            "section_title": "Multimodal Vision Language Models",
            "char_start_offset": 8099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 92,
                    "end": 295
                },
                {
                    "start": 295,
                    "end": 448
                },
                {
                    "start": 448,
                    "end": 542
                },
                {
                    "start": 542,
                    "end": 650
                },
                {
                    "start": 652,
                    "end": 885
                },
                {
                    "start": 885,
                    "end": 1107
                },
                {
                    "start": 1107,
                    "end": 1236
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 128,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 257,
                    "end": 261,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1214,
                    "end": 1218,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92919921875
        },
        {
            "corpus_id": "270870796",
            "title": "From Efficient Multimodal Models to World Models: A Survey",
            "text": "Multimodal models have demonstrated significant potential and application prospects in processing and understanding data from different modalities.By analyzing existing multimodal models, it is evident that they have made remarkable progress in generating images, videos, audio, and 3D models.These models achieve cross-modal generation and transformation by handling data from various modalities such as text, images, videos, or audio.\n\nFor example, the DALL-E series models (including DALL-E [98], DALL-E 2 [82], DALL-E 3 [99]) developed by OpenAI since 2021 have evolved to generate high-quality images based on textual descriptions.DALL-E models leverage the power of large-scale datasets and transformer architectures to understand and generate highly detailed and creative images from textual inputs.Each iteration has improved on the previous, with DALL-E 3 incorporating more advanced techniques to produce even more coherent and higher-quality images.\n\nMidjourney [100], launched in 2022, focuses on highquality artistic image generation and is widely used in creative design.This model emphasizes generating visually appealing and artistically valuable images, making it popular among artists and designers.\n\nGoogle's Imagen and Imagen 2 [79], introduced in 2022 and 2023, respectively, have further enhanced image generation capabilities by including editing features.Imagen models allow users to make modifications to generated images, making the process of creating and refining images more interactive and user-friendly.The field of video generation has also seen significant breakthroughs.OpenAI and PKU-YuanGroup released Sora and Open-Sora in 2023, capable of generating highly realistic videos.These models extend the capabilities of image generation to the temporal domain, allowing for the creation of dynamic and lifelike video content.\n\nAILab-CVC's VideoCrafter and VideoCrafter2 [101], released in 2023 and 2024 respectively, further optimized video generation and editing functionalities.These models integrate advanced video editing capabilities, making it easier to create and modify video content.Google's AudioLM [102], launched in 2022, excels in audio generation, producing natural sounds, music, and human speech.",
            "score": 0.48351997063962643,
            "section_title": "A. Multimodal Generative Models",
            "char_start_offset": 60680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 147,
                    "end": 293
                },
                {
                    "start": 293,
                    "end": 436
                },
                {
                    "start": 438,
                    "end": 636
                },
                {
                    "start": 636,
                    "end": 806
                },
                {
                    "start": 806,
                    "end": 960
                },
                {
                    "start": 962,
                    "end": 1085
                },
                {
                    "start": 1085,
                    "end": 1217
                },
                {
                    "start": 1219,
                    "end": 1379
                },
                {
                    "start": 1379,
                    "end": 1534
                },
                {
                    "start": 1534,
                    "end": 1604
                },
                {
                    "start": 1604,
                    "end": 1712
                },
                {
                    "start": 1712,
                    "end": 1857
                },
                {
                    "start": 1859,
                    "end": 2012
                },
                {
                    "start": 2012,
                    "end": 2124
                },
                {
                    "start": 2124,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 494,
                    "end": 498,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 524,
                    "end": 528,
                    "matchedPaperCorpusId": "255570036"
                },
                {
                    "start": 973,
                    "end": 978,
                    "matchedPaperCorpusId": "251953684"
                },
                {
                    "start": 1248,
                    "end": 1252,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1902,
                    "end": 1907,
                    "matchedPaperCorpusId": "267028095"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "269293048",
            "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
            "text": "Architecture The Phi-3.5-Vision (4.2B parameters) is a multimodal model designed to process an image/multi-image and a textual prompt as inputs, and subsequently generate textual outputs. This model is composed of two primary components: an image encoder, i.e., CLIP ViT-L/14 [RKH + 21] and a transformer decoder, i.e., phi-3.5-mini. The visual tokens, once extracted by the image encoder, are then combined with text tokens in an interleaved way (no particular order for image and text tokens). To accommodate high-resolution images and various aspect ratios, a dynamic cropping strategy [DZZ + 24b] is utilized to split the input image into a 2d array of blocks, where the tokens of the blocks are concatenated to represent the whole image. For multi-image input, we simply concatenated tokens from each images together.",
            "score": 0.4832813062743565,
            "section_title": "Technical Specifications",
            "char_start_offset": 14337,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 822
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63427734375
        },
        {
            "corpus_id": "277313203",
            "title": "ImageGen-CoT: Enhancing Text-to-Image In-context Learning with Chain-of-Thought Reasoning",
            "text": "Human intelligence excels at learning novel concepts through contextual observation and adapting to new inputs. When presented with a series of interleaved text-image examples-such as \"a leather-bound book\", followed by \"a leather apple\"-and then asked to generate an image for the query \"a box,\" humans intuitively infer the implicit pattern of \"leather\" and apply it to the new query, resulting in \"a leather box\". This reasoning ability to learn novel concepts from multimodal contexts underpins creative problem-solving. Existing unified Multimodal Large Language Models (unified MLLMs) [5,8,9,14,18,28,34] have demonstrated remarkable capabilities in multimodal understanding and generation within a single model architecture. Given their ability to process and generate across modalities similar to human cognition, it is natural to investigate whether these models can exhibit reasoning capabilities comparable to those of humans. To evaluate this, we adopt the Text-to-Image In-Context Learning (T2I-ICL) task [40], which requires models to process interleaved textimage inputs and generate coherent outputs by learning from multimodal contexts (Figure 1). Despite the impressive capabilities of unified MLLMs, our experiments reveal that they struggle to replicate this reasoning capability, often failing to grasp contextual relationships or preserve compositional consistency in T2I-ICL tasks. \n\nTo overcome these challenges, building upon the demonstrated success of CoT prompting in enhancing complex task processing for LLMs, we propose a novel framework that involves a structured thought process called ImageGen-CoT prior to image generation. Our key insight is that explicitly generating reasoning steps before image synthesis helps unified MLLMs better understand multimodal contexts and produce more coherent outputs. However, these models often produce disorganized and incoherent thought processes, leading to suboptimal performance. To address these limitations, we first propose an automated dataset construction pipeline to generate ImageGen-CoT datasets, where each sample consists of a pair of ImageGen-CoT and a corresponding image.",
            "score": 0.4828607494586953,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1404
                },
                {
                    "start": 1407,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2159
                }
            ],
            "ref_mentions": [
                {
                    "start": 601,
                    "end": 604,
                    "matchedPaperCorpusId": "266573555"
                },
                {
                    "start": 604,
                    "end": 607,
                    "matchedPaperCorpusId": "266374640"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88671875
        },
        {
            "corpus_id": "276902610",
            "title": "Diagram-Driven Course Questions Generation",
            "text": "This performance on vision-language tasks through its streamlined architecture and parameter-efficient design. DeepSeek-VL [20] introduces a balanced approach to vision-language modeling that builds upon the DeepSeek language model series. The model emphasizes maintaining strong language capabilities while developing visual understanding, featuring high-resolution processing capabilities and a carefully curated training strategy. Through its systematic scaling methodology from 1B to 7B parameters, it achieves competitive performance in practical applications while maintaining efficiency in multi-modal processing. \n\nInternVL2 [3] introduces a progressive alignment training strategy that scales efficiently from small to large language models while supporting diverse input modalities (text, images, video, and medical data). Built upon In-ternVL 1.5, the model incorporates an expanded training dataset covering multiple tasks (captioning, VQA, detection, grounding, and OCR) and employs VisionLLMv2 technology to enable various output formats including images, bounding boxes, and masks. This comprehensive approach allows the model to handle hundreds of visionlanguage tasks while maintaining expert-level performance. \n\nClosed-Source Large vision-language models: GLM4-V is a large-scale multimodal language model developed by ZhiPu, featuring outstanding visual understanding and language generation capabilities. It employs a unified pre-training framework, capable of handling multiple modalities such as text, images, and audio. GLM4-V demonstrates strong performance in tasks like visual question answering, image description, and cross-modal reasoning, and can quickly adapt to new scenarios through fewshot learning. The model supports both Chinese and En-glish, excelling in multilingual understanding and generation. \n\nClaude 3.5 Sonnet is a next-generation multimodal assistant developed by Anthropic, exhibiting exceptional performance in both visual and language understanding. It utilizes an innovative neural network architecture that allows for deep comprehension of image content and complex reasoning. The model has strict controls in terms of safety and ethics, capable of identifying and filtering inappropriate content. A notable feature is its strong contextual understanding and coherent conversational abilities. \n\nGPT-4o is the latest large language model developed by OpenAI, possessing powerful multimodal understanding and generation capabilities. It can process image and text inputs and perform complex reasoning and knowledge integration.",
            "score": 0.48228091212604046,
            "section_title": "Conclusion",
            "char_start_offset": 21645,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1836
                },
                {
                    "start": 1839,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2129
                },
                {
                    "start": 2130,
                    "end": 2250
                },
                {
                    "start": 2251,
                    "end": 2346
                },
                {
                    "start": 2349,
                    "end": 2485
                },
                {
                    "start": 2486,
                    "end": 2579
                }
            ],
            "ref_mentions": [
                {
                    "start": 633,
                    "end": 636,
                    "matchedPaperCorpusId": "266521410"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "266191741",
            "title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
            "text": "Trained on a 3T multilingual corpus, they excel in language understanding, commonsense reasoning, and reading comprehension. \n\n\u2022 BLIP2 (Li et al., 2023b): This visual-language model is proposed by Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. BLIP-2 leverages frozen pre-trained image encoders and large language models (LLMs) by training a lightweight, 12-layer Transformer encoder in between them, achieving excellent performance in various vision-language tasks \n\n\u2022 LLaVA (Liu et al., 2024): LlaVA (Language-Image LLaMA) is a multimodal model combining language and image data. It extends the LLaMA architecture to handle both modalities, enabling tasks like image captioning, visual question answering, and imagebased text generation. \n\n\u2022 Qwen-VL series (Bai et al., 2023): Qwen-VL (Qwen Large Vision Language Model) is the multimodal version of the large model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-VL accepts image, text, and bounding box as inputs, outputs text, and bounding box. \n\n\u2022 InternLM-XComposer2-VL (Dong et al., 2024): InternLM-XComposer2 is a cuttingedge vision-language model excelling in free-form text-image composition and comprehension, crafting content from diverse inputs like outlines, detailed specs, and reference images. Using a Partial LoRA (PLoRA) approach, it balances vision understanding and text composition. \n\nProprietary LLMs: \n\n\u2022 ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b): OpenAI's ChatGPT and GPT-4 are advanced iterations of the GPT series. ChatGPT is tailored for interactive tasks, while GPT-4 is the most proficient in the series and supports image input. \n\n\u2022 PaLM 2 (Anil et al., 2023): PaLM 2 is an advanced language model that excels in multilingual and reasoning capabilities, offering greater computational efficiency than its predecessor, PaLM.",
            "score": 0.48221741341542845,
            "section_title": "Conclusion and Discussion",
            "char_start_offset": 11812,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 127,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1379
                },
                {
                    "start": 1382,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1642
                },
                {
                    "start": 1645,
                    "end": 1837
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 153,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 480,
                    "end": 498,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 763,
                    "end": 781,
                    "matchedPaperCorpusId": "263625818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96044921875
        },
        {
            "corpus_id": "269983697",
            "title": "From Text to Pixel: Advancing Long-Context Understanding in MLLMs",
            "text": "The success of Large Language Models (LLMs) [35,44,1,8] has significantly impacted various fields, notably Multimodal Large Language Models (MLLMs) [37,26,2,31]. And there is a burgeoning interest in enhancing LLMs to handle longer context [48,4,15], for example, the recent GPT-4O [38] can support up to 128k tokens, paving the way to unlock many real-world applications from long-document understanding, summarization to document translation, among others. In many applications involving long-form documents that integrate images and text, there is a significant demand for the strong long-context understanding ability of MLLMs. As shown in Figure 2, the long context in the multimodal domain falls into two main categories: 1) long-form inputs consisting of multiple text-rich images, and 2) long-form text outputs. In the first category, multiple images increase the context length with image tokens and additional text tokens if the images are text-rich. This requires the model to efficiently integrate textual data with multiple images and reason across them. In the second category, the model must produce coherent and attentive long responses to the input context, avoiding irrelevant or hallucinated content and minimizing reliance on the model knowledge without considering the specific multimodal context. \n\nThe existing MLLMs [26,23,31] leverage pretrained LLMs [5,43] and inherit their advanced language understanding capabilities. Although these MLLMs demonstrate strong performance across various vision-language benchmarks [29,49], their effectiveness in long-form multimodal contexts is less explored. This issue becomes significant in tasks with very long input or output, which may exceed the context length limit (e.g., 2048 tokens for LLaMA) and increase computational overhead. \n\nWhile only a few MLLMs [37,34] are capable of handling multiple images in the multimodal context, efficiency emerges as another critical challenge. \"A picture is worth a thousand words\", for human, it is more natural to fully utilize our bandwidth to process an image than words. However, this might not be the case for models.",
            "score": 0.48161176971814124,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1801
                },
                {
                    "start": 1804,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 155,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1340,
                    "end": 1344,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83837890625
        },
        {
            "corpus_id": "270199384",
            "title": "StrucTexTv3: An Efficient Vision-Language Model for Text-rich Image Perception, Comprehension, and Beyond",
            "text": "We found that many existing multimodal large language models already possess the ability to accurately recognize text content in images.However, for the task of image translation, which requires both perception and understanding abilities, their performance is suboptimal.Therefore, we aimed to verify whether incorporating relevant training data during the training phase could enable the model to acquire image translation capabilities.We synthesized 1M training and 1000 testing data for image translation based on the WMT18 Chinese to English corpus and evaluated the model's performance.As shown in Table 8, compared to models that were not supervised-trained on the relevant task, our model demonstrated a significant improvement in image translation performance, demonstrating the potential of multimodal large language models in this task.This technical report has introduced an efficient Vision-Language model, namely StrucTexTv3, which effectively tackles the problems of text-rich image perception and comprehension.The proposed combination of a multi-scale reduced transformer and MG-Sampler successfully solves the challenges of high-resolution input and rich representation learning for text-rich images.Subsequently, a systematic organization of tasks related to both perception and comprehension abilities of text-rich images, as well as high-quality data collection, was carried out to train a highly competitive unified large mulitmodal model.StrucTexTv3 stands out among numerous multimodal models equipped with LLM decoder of approximately 1.8B parameters, and it also makes the deployment of edge devices feasible.\n\nOur method offers promising avenues for future research.(1) To enrich the comprehension of textrich videos and multi-page scanned PDFs, we need to broaden the image-level context, enabling the extraction of visual tokens from multiple images.(2) To advance toward Artificial General Intelligence (AGI), we must augment our learning data with a wider range of images, corpora, and real user instructions.This will empower our model with more universal capabilities, such as GUI understanding and mathematical problem analysis.\n\n(3) To study the scaling laws of large visionlanguage models on text-rich images, we must validate the effectiveness of larger datasets and model parameter sizes.These efforts will pave the way for more comprehensive and intelligent text-rich image understanding in the future.",
            "score": 0.4815289718220087,
            "section_title": "TEXT IMAGE TRANSLATION",
            "char_start_offset": 21948,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 272
                },
                {
                    "start": 272,
                    "end": 438
                },
                {
                    "start": 438,
                    "end": 592
                },
                {
                    "start": 592,
                    "end": 847
                },
                {
                    "start": 847,
                    "end": 1027
                },
                {
                    "start": 1027,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1461
                },
                {
                    "start": 1461,
                    "end": 1635
                },
                {
                    "start": 1637,
                    "end": 1693
                },
                {
                    "start": 1693,
                    "end": 1879
                },
                {
                    "start": 1879,
                    "end": 2040
                },
                {
                    "start": 2040,
                    "end": 2162
                },
                {
                    "start": 2164,
                    "end": 2326
                },
                {
                    "start": 2326,
                    "end": 2441
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7119140625
        },
        {
            "corpus_id": "271534005",
            "title": "LLaVA-Read: Enhancing Reading Ability of Multimodal Language Models",
            "text": "What is 21% for? Boxes Words Figure 1: Model overview of LLaVA-Read, a multimodal LLM with dual encoders to handle both visual objects and texts. Given a text-rich image, the visual-text encoder extracts texts and their location information, feeding them to the OCR tokenizer. ViT-based low-resolution encoder (e.g., 336\u00d7336) focuses on the global visual information and convolution-based encoder (e.g., 768\u00d7768) focuses on visual details. The high-resolution encoder merges its information into low-resolution encoders, as not all details are useful in answering a question. powerful screenshot LMs to unlock complex tasks such as chart understanding and UI navigation. Multimodal LLMs for text-rich images can extract visual texts, which is similar to the visual text understanding problem. The major difference is that multimodal LLMs not only need to comprehend visual texts but also visual objects and their relationship. Inspired by previous work [47], LLaVA-Read performs an visual text understanding analysis of multimodal LLMs on synthetic data, revealing their impressive capability on shorter scene text understanding but limited proficiency in comprehending large amounts of textual content within a text-rich image. This observation motivates us to add an additional visual-text encoder to enhance reading ability of multimodal LLMs. \n\n3 LLaVA-Read: Enabling LLaVA to Read LLaVA-Read is designed to enhance the comprehension of textual information within images, particularly in text-rich images. An overview of the model is shown in Figure 1. LLaVA-Read comprises multiple visual encoders, a visual-text encoder, and a large language model (LLM) serving as the decoder. Given an input image X v , the visual encoders generate visual features \n\nwhere f v consists of two visual encoders. Subsequently, we employ a multi-layer perceptron (MLP) projection g to transform Z v into visual tokens H v = g(Z v ) for the large language model. Notably, H v shares the same embedding dimensions as the text tokens used by the LLM tokenizer.",
            "score": 0.481338048775803,
            "section_title": "User Instructions:",
            "char_start_offset": 7065,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1755
                },
                {
                    "start": 1758,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2044
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72802734375
        },
        {
            "corpus_id": "271709823",
            "title": "Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining",
            "text": "Multimodal Large Language Models Recent large language models (LLMs) (Chowdhery et al., 2022;Touvron et al., 2023;Brown et al., 2020;Achiam et al., 2023;Team et al., 2023) demonstrate strong instruction-following and reasoning capabilities, coupled with extensive world knowledge. To extend LLMs' expertise from the text domain to multimodal domains such as images and videos, prior works (Liu et al., 2023;Lin et al., 2023a;Maaz et al., 2023;Lin et al., 2023b) have aligned pretrained encoders from various modalities with LLMs by curating multimodal instruction tuning datasets. Although these multimodal large language models (MLLMs) have exhibited powerful visual understanding capabilities, their multimodal functionality is primarily limited to perceiving the visual world, exemplified by tasks such as visual question answering and image captioning, rather than generating visual outputs. Another line of research (Koh et al., 2024;Dong et al., 2024;Sun et al., 2023;Wu et al.) has proposed augmenting MLLMs with the ability to generate images, videos, and audio from text instructions. These approaches introduce additional visual tokens for generation and align these generative tokens as conditional information with a pretrained generator, such as Stable Diffusion (Esser et al., 2024;Podell et al., 2023) for text-to-image generation. Consequently, the generation capabilities heavily rely on the external expert generator rather than MLLMs themselves, resulting in inconsistent and inferior generation results. To combine the strength of both approaches, our model aims to learn both understanding and generation of images using an MLLM with native multimodal capabilities, drawing inspiration from Chameleon (Team, 2024a), a mixed-modal early-fusion foundation model.",
            "score": 0.4813120636292394,
            "section_title": "B. Related Work",
            "char_start_offset": 27737,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1781
                }
            ],
            "ref_mentions": [
                {
                    "start": 921,
                    "end": 939,
                    "matchedPaperCorpusId": "258959284"
                },
                {
                    "start": 939,
                    "end": 957,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 974,
                    "end": 984,
                    "matchedPaperCorpusId": "261696650"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96044921875
        },
        {
            "corpus_id": "277741397",
            "title": "VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop Question Answering",
            "text": "Large language models (LLMs) have demonstrated exceptional capabilities in understanding and generating human language, significantly advancing the field of natural language processing across diverse applications. Building on these developments, vision-language models have been introduced to extend the utility of LLMs by incorporating visual inputs. These models enable multimodal understanding and support a wide range of tasks such as image captioning, visual reasoning, and optical character recognition. \n\nRecent progress has led to the emergence of multimodal large language models (MLLMs) [37], [34], which have become a prominent paradigm for addressing multimodal tasks. MLLMs typically integrate a pre-trained language model with a vision encoder within a unified architecture. Instruction-following capability is considered essential for handling various downstream tasks, and therefore instructiontuned LLMs [36], [38] are frequently adopted as the language backbone in these models. In parallel, visual encoders [39], [40] are utilized to extract semantically rich representations from visual data. To enable seamless interaction between visual and textual modalities, a projector module is commonly used to map visual features into the language embedding space. This configuration allows the language model to jointly process and reason over multimodal inputs. \n\nWhile MLLMs exhibit strong generalization capabilities and have achieved state-of-the-art performance on several vision-language benchmarks, their application to domainspecific tasks such as multimodal multi-hop question answering (MMQA) [1], [2] remains limited. MMQA introduces distinct challenges, including open-domain retrieval from heterogeneous modalities and multi-step reasoning over diverse information sources. Existing MLLM architectures are often not optimized for fine-grained modality alignment or scalable context retrieval, which are critical for accurate and contextually grounded answer generation in MMQA. \n\nDespite the progress in MLLMs, there remains a lack of unified frameworks specifically tailored for MMQA tasks. Current models either rely heavily on pre-processing pipelines or depend on modality-specific transformations, which may lead to information loss and suboptimal cross-modal reasoning. Moreover, most MLLMs are not pretrained with objectives explicitly designed for visual reasoning in multi-hop settings.",
            "score": 0.4807148714569963,
            "section_title": "B. Multimodal Large Language Models",
            "char_start_offset": 12205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1375
                },
                {
                    "start": 1378,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 2003
                },
                {
                    "start": 2006,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2301
                },
                {
                    "start": 2302,
                    "end": 2421
                }
            ],
            "ref_mentions": [
                {
                    "start": 597,
                    "end": 601,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 603,
                    "end": 607,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 921,
                    "end": 925,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 927,
                    "end": 931,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 1026,
                    "end": 1030,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1032,
                    "end": 1036,
                    "matchedPaperCorpusId": "257767223"
                },
                {
                    "start": 1621,
                    "end": 1624,
                    "matchedPaperCorpusId": "237385768"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95703125
        },
        {
            "corpus_id": "272310282",
            "title": "AdaptVision: Dynamic Input Scaling in MLLMs for Versatile Scene Understanding",
            "text": "Using GPT-4, it generates detailed image-text descriptions that are then subjected to manual verification to ensure data quality. With this high-quality dataset, the model employs a two-stage training strategy and significantly outperforms previous methods [3]. Building on these foundational works, subsequent models such as LLaVA-1.5 [13] and MiniGPT-v2 [22], along with other innovative works such as mPLUG-Owl [23], InstructBLIP [24], and Shikra [25], have further advanced the multimodal research field. \n\nThe above methods primarily focus on natural scenes and lack text Optical Character Recognition (OCR) capabilities for environments rich in text, leading to difficulties in handling text-related tasks. This limitation is particularly evident in textheavy scenes, where the performance of these models further declines. To address this issue, Multimodal Large Language Models (MLLMs) optimized specifically for text-rich scenes have been developed.",
            "score": 0.4797858405638451,
            "section_title": "A. MLLMs for Natural Scene Images",
            "char_start_offset": 7265,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 508
                },
                {
                    "start": 511,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 958
                }
            ],
            "ref_mentions": [
                {
                    "start": 257,
                    "end": 260,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 433,
                    "end": 437,
                    "matchedPaperCorpusId": "258615266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87451171875
        },
        {
            "corpus_id": "278129712",
            "title": "Token Sequence Compression for Efficient Multimodal Computing",
            "text": "Large Multimodal Models (LMMs) have enabled processing and reasoning across various modalities, including images, video, and audio. LMMs excel in visual reasoning by integrating image, video, and text processing in VLMs [1,8,9,[11][12][13]. These models integrate visual information with textual semantics, allowing the model to perform tasks like visual question answering, image captioning, and multimodal inference. \n\nHowever, the promising performance of VLMs largely relies on the large number of visual tokens generated during encoding. For instance, in LLaVA-1.5 [12], the model generates 576 visual tokens, and in LLaVA-NeXT [24], a 672x672 image can generate over 2880 visual tokens, while the text tokens are typically fewer ( 10s per sentence). This disparity, combined with the quadratic scaling of attention operations with token count O(T 2 ), results in significant Figure 1. Architecture of auto-regressive visual language models. computational overhead as the number of tokens increases. \n\nThe challenge becomes even more pronounced as we scale to incorporate additional modalities, such as video or audio, further increasing the token count and computational requirements [19,21]. \n\nThis research focuses on addressing the challenge of hardware-efficient multimodal computing by investigating multimodal prompt compression. The central question of this paper is: How can we encode a multimodal prompt into a tokenized sequence that reduces the computational cost of multimodal inference, while maintaining satisfactory model accuracy? In this paper, we examine existing and novel visual token selection methods. In particular, we consider both methods that assign importance scores to visual tokens for selection, and methods that do not. We underline surprising trends and inaccurate popular assumptions, and conclude with our ongoing and future directions in visual sequence compression.",
            "score": 0.4786676663189927,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 418
                },
                {
                    "start": 421,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1004
                },
                {
                    "start": 1007,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1907
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 227,
                    "matchedPaperCorpusId": "256390509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.716796875
        },
        {
            "corpus_id": "277043335",
            "title": "Small Vision-Language Models: A Survey on Compact Architectures and Techniques",
            "text": "For the textual modality, the input text is tokenized into subword units, and the resulting embeddings are processed similarly through the transformer layers. The fusion of these modalities typically occurs in one of two ways: early fusion or late fusion. A key innovation in these models is the use of cross-attention mechanisms, where the textual features attend to the visual features and vice versa. This enables the model to focus on the most relevant parts of each modality when performing tasks such as visual question answering (VQA) or image captioning. By allowing the model to learn both intra-modal (within the same modality) and inter-modal (between modalities) relationships, transformers can effectively handle complex multimodal data. The transformer architecture in sVLMs provides a powerful and flexible framework for integrating visual and textual information. \n\nIts ability to model long-range dependencies, capture contextual relationships, and fuse multimodal data efficiently makes it an ideal choice for vision-language tasks. TinyGPT-V [103] introduces several key technical contributions, including an efficient architecture that integrates the compact Phi-2 [73] language model with pre-trained vision encoders, a novel training methodology focused on small pre-trained backbones, and resource-efficient training and inference capabilities. The model addresses challenges such as the closed-source nature and high computational demands of large MLLMs, aiming to achieve comparable performance to larger models while using significantly fewer resources. TinyGPT-V's architecture consists of a visual encoder using EVA (ViT) [25], projection layers including Q-Former layers and linear projections, and the Phi-2 language model as the backbone, incorporating various normalization techniques and LoRA [35] for efficient fine-tuning. The model is evaluated on benchmarks such as GQA Hudson [39], VSR [109], IconVQ [62], VizWiz [32], and Hateful Memes (HM) [44], using a diverse set of datasets across its training stages. The ablation study highlights the potential challenges in training smaller language models for multimodal tasks, which is one of the possible drawbacks.",
            "score": 0.47850719438935185,
            "section_title": "Transformer-Based Models",
            "char_start_offset": 16745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 879
                },
                {
                    "start": 882,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 1938,
                    "end": 1942,
                    "matchedPaperCorpusId": "239885946"
                },
                {
                    "start": 1951,
                    "end": 1955,
                    "matchedPaperCorpusId": "150368646"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "269354784",
            "title": "RS-LLaVA: A Large Vision-Language Model for Joint Captioning and Question Answering in Remote Sensing Imagery",
            "text": "Vision-language models represent a category of models that integrate computer vision and NLP techniques to achieve a comprehensive understanding of visual and textual data.The field of research in this area has witnessed significant advancements, encompassing a wide range of techniques and architectures that aim to fuse vision and language, thereby enhancing performance and capabilities across various tasks.\n\nOne approach adopted by these models involves learning generic multi-modal representations from a large corpus of image-text pairs.Models such as CLIP [46] and ALIGN [47] employ dual encoders trained with a contrastive objective on extensive imagetext datasets, enabling them to model the interaction between modalities.However, these models do not natively support multi-modal generation tasks (e.g., image captioning and VQA).Other models, like BLIB [44] and CoCa [45], combine contrastive and generative pre-training to support both cross-modal alignment and multi-modal generation tasks.\n\nAnother paradigm of vision-language models are the generalized models that are trained to jointly perform multiple tasks without task-specific fine-tuning.For instance, UniT [48] unifies different tasks in a single model by designing a decoder with per-task query embedding and task-specific output heads.OFA [49] formulates different visionlanguage tasks as sequence-to-sequence tasks and trains a transformer model on them without task-specific layers.GIT [50] is a multi-tasking transformer-based model that conditions the decoder on vision inputs and text to generate the output text.Flamingo [51] extends sequence-to-sequence models to support interleaved image, video and text inputs for generating the output text.\n\nBuilding upon the success of GPT models, GPT-4 [14] has exhibited impressive capabilities in engaging in multi-modal dialogues with humans.The open-source project LLaVA [15] aims to replicate this performance by aligning visual representations with the input space of the LLM.By leveraging the original self-attention mechanism within the LLM, LLaVA enables effective processing of visual information alongside textual input.Multimodal-GPT [52] utilizes gated cross-attention layers to facilitate seamless interactions between images and text.",
            "score": 0.4784316035455418,
            "section_title": "Vision-Language Models in General Computer Vision",
            "char_start_offset": 13494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 172,
                    "end": 411
                },
                {
                    "start": 413,
                    "end": 544
                },
                {
                    "start": 544,
                    "end": 733
                },
                {
                    "start": 733,
                    "end": 841
                },
                {
                    "start": 841,
                    "end": 1004
                },
                {
                    "start": 1006,
                    "end": 1161
                },
                {
                    "start": 1161,
                    "end": 1311
                },
                {
                    "start": 1311,
                    "end": 1460
                },
                {
                    "start": 1460,
                    "end": 1594
                },
                {
                    "start": 1594,
                    "end": 1727
                },
                {
                    "start": 1729,
                    "end": 1868
                },
                {
                    "start": 1868,
                    "end": 2005
                },
                {
                    "start": 2005,
                    "end": 2154
                },
                {
                    "start": 2154,
                    "end": 2272
                }
            ],
            "ref_mentions": [
                {
                    "start": 564,
                    "end": 568,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 865,
                    "end": 869,
                    "matchedPaperCorpusId": "221279805"
                },
                {
                    "start": 879,
                    "end": 883,
                    "matchedPaperCorpusId": "226602850"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "271218677",
            "title": "OpenPSG: Open-set Panoptic Scene Graph Generation via Large Multimodal Models",
            "text": "Since the introduction of large models like the GPT series [1], recent years have witnessed rapid development in large multimodal models [21,28,30].Benefiting from its nature of connecting vision and language, LMMs have significantly advanced various downstream tasks, ranging from computer vision [11,42] to natural language processing [17,31,35].Early multimodal models, such as CLIP [30], trained on image-text paired datasets through contrastive learning to align the visual and textual information.Subsequently, owing to enlightenment of the autoregressive prediction in large language models [1,34], there has been an explosive growth of LMMs [21,27,28]; by introducing mechanisms that can transform visual information into large language models, they facilitate the communication between visual and textual information.Furthermore, this has endowed LMMs with the capability to generate free text, leading to substantial improvements in numerous multimodal tasks.In this paper, we leverage LMMs to design a multimodal relation decoder to predict relations in an open-set scenarios.",
            "score": 0.4784262197328272,
            "section_title": "Large Multimodal Models",
            "char_start_offset": 6208,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 148,
                    "end": 348
                },
                {
                    "start": 348,
                    "end": 503
                },
                {
                    "start": 503,
                    "end": 826
                },
                {
                    "start": 826,
                    "end": 969
                },
                {
                    "start": 969,
                    "end": 1087
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 144,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 341,
                    "end": 344,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 656,
                    "end": 659,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88427734375
        },
        {
            "corpus_id": "261975252",
            "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
            "text": "This approach avoids altering the output space of MLLMs. Raw images are then decoded by the SD image decoder conditioned on these semantics. In this fashion, the pretrained SD acts as the score function (Ho et al., 2020). The image posterior is thus modeled by direct sampling in the pixel space, facilitated by score distillation (van den Oord et al., 2018;Poole et al., 2023). \n\nii. Interleaved Generative Pre-Training (I-GPT) DREAMLLM is trained to generate interleaved multimodal corpora from the internet (Zhu et al., 2023b), both encoding and decoding interleaved image-text multimodal inputs. Unlike encoding multimodal inputs as in existing methods, decoding interleaved multimodal outputs is challenging due to the complex interleaving layout structures and the long-context requirement of images. Our approach tackles the interleaved layout learning using a unique <dream> token that predicts the placement of images within texts. Harnessing DREAMLLM's causal nature, all contents are generated with history multimodal contexts of any length. This interleaved generative pretraining (I-GPT) inherently forms all joint, marginal, and conditional distributions of images and texts in the document, leading to a learning synergy that grounds DREAMLLM's comprehension in creation and vice versa. \n\nExtensive experiments across various vision-language comprehension, content creation, and languageonly tasks demonstrate DREAMLLM's superior performance as a zero-shot multimodal generalist. \n\nFor instance, DREAMLLM-7B achieves an 8.46 FID on MS-COCO and sets a new standard with 49.1/35.9 scores on MMBench and MM-Vet evaluations, respectively. Moreover, we delve into the learning synergy between comprehension and creation, revealing decent in-context generation capabilities. With I-GPT pretraining, DREAMLLM generates interleaved documents following human prompts after supervised fine-tuning on instruction-following data curated with GPT-4. To our knowledge, this work is the first to enable MLLMs to create free-form interleaved content with a learning synergy on both sides.",
            "score": 0.47831457933787613,
            "section_title": "body",
            "char_start_offset": 2053,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1301
                },
                {
                    "start": 1304,
                    "end": 1494
                },
                {
                    "start": 1497,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2087
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 220,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 340,
                    "end": 358,
                    "matchedPaperCorpusId": "27706557"
                },
                {
                    "start": 358,
                    "end": 377,
                    "matchedPaperCorpusId": "252596091"
                },
                {
                    "start": 510,
                    "end": 529,
                    "matchedPaperCorpusId": "258170467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8583984375
        },
        {
            "corpus_id": "258947258",
            "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs",
            "text": "We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.",
            "score": 0.4782816320729848,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95654296875
        },
        {
            "corpus_id": "272693949",
            "title": "NVLM: Open Frontier-Class Multimodal LLMs",
            "text": "There are various architectural designs for constructing multimodal LLMs (MLLMs) using existing LLMs and vision encoders [4; 66; 79; 147; 8]. We discuss the two most common architectures. \n\nDecoder-only MLLMs. Decoder-only architectures are popular mainly for their simplicity and unified handling of all modalities by aligning other modality tokens into the text token embedding space. It also facilitates the extension to generating other modalities [35; 108]. The notable examples of decoder-only multimodal LLMs include LLaVA [79; 78; 80; 65], InternVL [19; 18; 111], and Cambrian-1 [139]. In these models, image tokens from the vision encoder are projected into the text-embedding space via a projector module, e.g., position-wise multi-layer perceptron (MLP), and then directly fed into the decoder-only LLM, just like the text tokens. Some variants, such as Qwen-VL [8], utilize more advanced modules, e.g., Perceiver [48], to down-sample the image tokens before they are fed into the LLM. \n\nTraining decoder-only multimodal LLMs typically involves two stages: pretraining and supervised fine-tuning (SFT). At the start of pretraining, the randomly initialized MLP or projector module needs to be trained while keeping the LLM frozen to avoid disrupting the LLM's weights [79; 80]. Related work has also shown cases where both the projector and vision encoder are jointly trained during the pretraining stage [18; 8]. Due to the limited capacity of the MLP or projector module, the LLM need to be unfrozen during multimodal supervised fine-tuning (SFT) to achieve good performance on vision-language tasks [71]. The vision encoder is typically kept frozen during the SFT stage. There are some exceptions, though, where the entire multimodal LLM is trained end-to-end [65], usually with smaller vision encoder [167].",
            "score": 0.47776078685045936,
            "section_title": "Architectural Designs",
            "char_start_offset": 15012,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 187
                },
                {
                    "start": 190,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 996
                },
                {
                    "start": 999,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1822
                }
            ],
            "ref_mentions": [
                {
                    "start": 1613,
                    "end": 1617,
                    "matchedPaperCorpusId": "266174746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "263608981",
            "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
            "text": "Figure 1: MiniGPT-5 is a unified model for interleaved vision-and-language comprehension and generation. Besides the original multimodal comprehension and text generation abilities, MiniGPT-5 can provide appropriate, coherent multimodal outputs. \n\ninnovative methods (Ge et al., 2023;Sun et al., 2021;Koh et al., 2023;Sun et al., 2023b;Yu et al., 2023;Aiello et al., 2023;Wu et al., 2023c). For instance, CM3Leon (Yu et al., 2023) presents a retrieval-augmented, decoder-only architecture designed for both text-to-image and image-to-text applications. Similarly, Emu (Sun et al., 2023b) employs the pretrained EVA-CLIP (Sun et al., 2023a) model to convert images into one-dimensional features and fine-tunes the LLAMA (Touvron et al., 2023) model to generate cohesive text and image features through autoregressive techniques. \n\nOn the other hand, NextGPT (Wu et al., 2023c), GILL (Koh et al., 2023) and SEED (Ge et al., 2023) explore the concept of mapping vokens into the text feature space of a pretrained Stable Diffusion model; GILL and NextGPT employ an encoder-decoder framework, while SEED utilizes a trainable Q-Former structure. In contrast to these approaches, our model takes a more direct route by aligning voken features with visual information. Additionally, we introduce several training strategies to enhance image quality and contextual coherence.",
            "score": 0.47758371556864093,
            "section_title": "Multimodal Input Multimodal Output",
            "char_start_offset": 6796,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1366
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90087890625
        },
        {
            "corpus_id": "263889455",
            "title": "Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization",
            "text": "Given an image-text pair, the 2D image can be tokenized into a 1D sequence with causal dependency and then concatenated with text tokens to form a multi-modal sequence y = (y 1 , y 2 , .., y S ). For differentiating between two modalities, two special tokens [IMG] and [/IMG] are inserted into the beginning and end of the image token sequence respectively, indicating the start and end of image content. To empower LaVIT with the capability to generate both text and images, we employ two different concatenation forms, i.e., [image, text] and [text; image]. When the image is used as a condition (on the left) to generate text, we use the continuous visual features Xr from the token merger instead of quantized visual embeddings as the input to LLMs. Such a design mitigates the loss of detailed information caused by vector quantization, which is crucial for fine-grained multimodal understanding tasks like visual question answering. Our LaVIT adopts the general Language Modeling (LM) objective to directly maximize the likelihood of each multi-modal sequence in an auto-regressive manner: \n\nSince both image and text are already represented as discrete token IDs, we can use the cross-entropy to supervise the token prediction at each location for both two modalities with a shared prediction head. The complete unification in representation spaces and the training paradigms can help LLMs better learn multi-modal interaction and alignment. When LaVIT is pre-trained, it possesses the capacity to perceive images akin to a foreign language, comprehending and producing them like text. Nevertheless, most of the existing works merely regard images as prompts to guide the generation of text with no supervision, restricting them to solely performing image-to-text tasks.",
            "score": 0.4774851866741837,
            "section_title": "STAGE-2: UNIFIED GENERATIVE MODELING",
            "char_start_offset": 16080,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1777
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9326171875
        },
        {
            "corpus_id": "274023123",
            "title": "Spider: Any-to-Many Multimodal LLM",
            "text": "Large Language Models (LLMs) such as Vicuna [9], LLaMA [58], ChatGPT [42], and GPT-4 [1] have demonstrated human-level proficiency in language understanding and generation. However, as the demand for more complex, real-world applications grew, the need for integrating LLMs with multiple types of input and output modalities (e.g., text, images, audio, video) became apparent. This evolution has led to the rise of Multimodal LLMs (MLLMs), which extend LLMs' capabilities by incorporating multimodal perception modules [2,23,28,30,37,53,54,66,73]. \n\nThe development of MLLMs marks a significant ad-vancement in enabling comprehensive understanding and generation across various modalities. Initially, models like LLaVA1.5 [36] and MiniGPT-4 [73] were capable of processing only two modalities: text and images. Further innovations saw the rise of models like PandaGPT [54], OneLLM [19], Gemini [57], and NExT-GPT [66], which expanded support to four modalities, incorporating text, image, audio, and video into their multimodal frameworks. \n\nHowever, as depicted in Fig. 1(a), these X-to-X (Anyto-Any) MLLMs are restricted to generate pairwise modalities 'Text + X' within a single interaction, such as 'Text + Image' or 'Text + Audio'. For example, when a user asks to generate an image of a dog, the model responds with image output. In subsequent interaction, to get an audio of the dog's bark, the user needs to give a new instruction. These MLLMs based on Multi-Round Dialogue Generation paradigm, require several rounds of user instructions and do not allow for a seamless integration of multiple modalities within a single interaction. Each pair of modalities is handled independently, resulting in a fragmented user experience where the responses feel disjointed rather than cohesive. Another example is in Fig. 7 of Appendix B.",
            "score": 0.4772359347344,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 547
                },
                {
                    "start": 550,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1836
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 88,
                    "matchedPaperCorpusId": "257532815"
                },
                {
                    "start": 519,
                    "end": 522,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 528,
                    "end": 531,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 540,
                    "end": 543,
                    "matchedPaperCorpusId": "261696650"
                },
                {
                    "start": 881,
                    "end": 885,
                    "matchedPaperCorpusId": "265709786"
                },
                {
                    "start": 913,
                    "end": 917,
                    "matchedPaperCorpusId": "261696650"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.912109375
        },
        {
            "corpus_id": "261705943",
            "title": "When Geoscience Meets Foundation Models: Toward a general geoscience artificial intelligence system",
            "text": "C. Large Vision-Language Models 1) Basic Large Vision-Language Models: In the current landscape, FMs have evolved to encompass more than just deep models; they now focus on utilizing vast amounts of data and computational power to tackle diverse problems. The goal is to use a unified model capable of addressing multiple modalities and tasks. Consequently, there is a growing emphasis on training vision-language models [238]. \n\nCLIP [142] is a prominent example that leverages a largescale collection of image-text pairs from the Internet for contrastive learning, enabling the creation of a unified representation of multimodal data. Inspired by CLIP, various multimodal image FMs have been proposed, differing in their model structures [239], [240], feature representations [241], [242], multimodal feature fusion approaches [243], feature alignment loss functions [244], pre-training methods [245], and more. These advancements have significantly improved the performance of multimodal FMs. \n\nIn addition, DALL-E [136] combines the diffusion model with multimodal FMs to generate images from text. SAM [213] introduces a promptable model with training strategies that enable the segmentation of objects using text, points, and lines as prompts. GPT-4 [36] exhibits superhuman capabilities on various professional and academic datasets. mPLUG-2 [246] introduces a multimodule composition network, including text, image, and video. Metatransformer [247] proposed a unified framework performing learning across 12 modalities with unpaired data (e.g., natural language, 2-D images, 3-D point clouds, audio, video, time series, and tabular data). \n\n2) Advanced Large Vision-Language Models for Geoscience: LVLMs have emerged as powerful tools in remote sensing and geoscience, particularly for visual question answering and image understanding tasks. Chappuis et al. [248] pioneered the use of Remote Sensing Visual Question Answering (RSVQA), leveraging advancements in natural language processing and computer vision to analyze satellite images.",
            "score": 0.4771319920935223,
            "section_title": "Segment Anything Data Engine",
            "char_start_offset": 110377,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 427
                },
                {
                    "start": 430,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 995
                },
                {
                    "start": 998,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2047
                }
            ],
            "ref_mentions": [
                {
                    "start": 421,
                    "end": 426,
                    "matchedPaperCorpusId": "257913547"
                },
                {
                    "start": 435,
                    "end": 440,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 740,
                    "end": 745,
                    "matchedPaperCorpusId": "260068316"
                },
                {
                    "start": 747,
                    "end": 752,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 829,
                    "end": 834,
                    "matchedPaperCorpusId": "236034189"
                },
                {
                    "start": 897,
                    "end": 902,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1107,
                    "end": 1112,
                    "matchedPaperCorpusId": "257952310"
                },
                {
                    "start": 1349,
                    "end": 1354,
                    "matchedPaperCorpusId": "256459873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.890625
        },
        {
            "corpus_id": "258426630",
            "title": "A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions",
            "text": "Researchers can explore the integration of ChatGPTs with other modalities such as images and videos to enhance their applications in fields such as education and healthcare. Multimodal integration is an important research direction for language models, as it involves combining multiple modalities of information, such as text, images, and audio, to generate more comprehensive and accurate outputs. Multimodal integration can help Chatbots better understand and respond to complex inputs, and can enable more natural and intuitive interactions between humans and machines. \n\nOne approach to multimodal integration in ChatGPTs is to use a multimodal transformer architecture [60], which incorporates multiple modalities of input into a single transformer model. This approach has been used in a number of applications, such as image captioning (e.g., XGPT [61]) and video question answering (e.g., AVQA [62), with promising results. Another approach is to use multimodal fusion techniques to combine the outputs of separate models trained on different modalities [63]. For example, Zhu et al. used separate models for text and images, and then combined their outputs using a fusion approach based on self-attention [64]. In addition to these approaches, there is ongoing research into developing new methods for multimodal integration in language models. For example, recent work has explored the use of reinforcement learning to learn how to weight different modalities of input based on their relative importance [65]. Another area of research is developing methods to incorporate multimodal inputs that are not synchronous, such as when text and audio inputs are recorded separately [66]. In general, multimodal integration is an important research direction for language models, as it enables more flexible and powerful interactions with users and can improve the accuracy and comprehensiveness of the model's output.",
            "score": 0.4768936241001086,
            "section_title": "Multimodal Integration",
            "char_start_offset": 41874,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1921
                }
            ],
            "ref_mentions": [
                {
                    "start": 856,
                    "end": 860,
                    "matchedPaperCorpusId": "211817758"
                },
                {
                    "start": 903,
                    "end": 907,
                    "matchedPaperCorpusId": "239054406"
                },
                {
                    "start": 1063,
                    "end": 1067,
                    "matchedPaperCorpusId": "257122091"
                },
                {
                    "start": 1515,
                    "end": 1519,
                    "matchedPaperCorpusId": "252369340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9130859375
        },
        {
            "corpus_id": "269043091",
            "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions",
            "text": "The advent of Large Language Models (LLMs) has significantly reshaped the trajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable limitation, as they are primarily adept at processing textual information. To address this constraint, researchers have endeavored to integrate visual capabilities with LLMs, resulting in the emergence of Vision-Language Models (VLMs). These advanced models are instrumental in tackling more intricate tasks such as image captioning and visual question answering. In our comprehensive survey paper, we delve into the key advancements within the realm of VLMs. Our classification organizes VLMs into three distinct categories: models dedicated to vision-language understanding, models that process multimodal inputs to generate unimodal (textual) outputs and models that both accept and produce multimodal inputs and outputs.This classification is based on their respective capabilities and functionalities in processing and generating various modalities of data.We meticulously dissect each model, offering an extensive analysis of its foundational architecture, training data sources, as well as its strengths and limitations wherever possible, providing readers with a comprehensive understanding of its essential components. We also analyzed the performance of VLMs in various benchmark datasets. By doing so, we aim to offer a nuanced understanding of the diverse landscape of VLMs. Additionally, we underscore potential avenues for future research in this dynamic domain, anticipating further breakthroughs and advancements.",
            "score": 0.47673233280053473,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90771484375
        },
        {
            "corpus_id": "254275326",
            "title": "Unifying Vision, Text, and Layout for Universal Document Processing",
            "text": "Unifying Model Architectures in Multimodal Learning. Unifying model architectures for different modalities, such as vision, language, and speech, is an emergent direction. Inspired by the immense success in natural language processing, computer vision and speech processing, model architectures in multimodal learning is converging to Transformers. One type of works concatenates text token embeddings and projected image patches as the input [6,42] to a multimodal Transformer. Other models uses two-tower or three-tower architecture where each modality is encoded respectively. Projection heads or fusion networks on top of the two-tower architecture generate multimodal representations [38,56]. Unifying Tasks with the Generative Framework. Research on unifying training processes across different tasks and domains recently has made significant progress. [8] finetunes language models with instructions on 1.8k tasks. [7] unifies several vision-language tasks by converting training objectives to sequence generation. [31,49,50] further combines more tasks, e.g., image generation, by converting images and bounding boxes to discrete tokens. Document Artificial Intelligence. LayoutLM [53] pretrains BERT models on document data with masked language modeling and document classification task, with 2D positional information and image embeddings integrated. Subsequent works [15,16,55] also adopt VL-BERT alike architecture and includes additional pretraining tasks, e.g., masked image/region modeling proposed, and leverages the reading order in layout information [12]. [11,29] use a multimodal encoder to model region features extracted by CNN with sentence-level text representations and train with self-supervised objectives. [20] proposes an OCR-free model to directly generate textual output from document images. [36] trains generative language models on both unlabeled and labeled document data using generative training objectives. [10] proposed to model documents as collections of tokens bounding boxes.",
            "score": 0.4767147392830813,
            "section_title": "Related Work",
            "char_start_offset": 6556,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 443,
                    "end": 446,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 446,
                    "end": 449,
                    "matchedPaperCorpusId": "201317624"
                },
                {
                    "start": 689,
                    "end": 693,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 922,
                    "end": 925,
                    "matchedPaperCorpusId": "231802355"
                },
                {
                    "start": 1026,
                    "end": 1029,
                    "matchedPaperCorpusId": "246634906"
                },
                {
                    "start": 1189,
                    "end": 1193,
                    "matchedPaperCorpusId": "209515395"
                },
                {
                    "start": 1378,
                    "end": 1382,
                    "matchedPaperCorpusId": "237485613"
                },
                {
                    "start": 1385,
                    "end": 1388,
                    "matchedPaperCorpusId": "229923949"
                },
                {
                    "start": 1569,
                    "end": 1573,
                    "matchedPaperCorpusId": "247446585"
                },
                {
                    "start": 1575,
                    "end": 1579,
                    "matchedPaperCorpusId": "244950822"
                },
                {
                    "start": 1579,
                    "end": 1582,
                    "matchedPaperCorpusId": "235358528"
                },
                {
                    "start": 1734,
                    "end": 1738,
                    "matchedPaperCorpusId": "250924870"
                },
                {
                    "start": 1824,
                    "end": 1828,
                    "matchedPaperCorpusId": "231951453"
                },
                {
                    "start": 1945,
                    "end": 1949,
                    "matchedPaperCorpusId": "235262539"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88916015625
        },
        {
            "corpus_id": "274860024",
            "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
            "text": "We present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.",
            "score": 0.47621811913564116,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "263620748",
            "title": "Kosmos-G: Generating Images in Context with Multimodal Large Language Models",
            "text": "In recent studies, advancements in text-to-image (T2I) generation, particularly with diffusion models, have shown remarkable progress in producing highly photorealistic, accurate, and diverse images from textual descriptions. Building on the unprecedented success of producing highly accurate images from text descriptions, numerous studies have delved into more sophisticated subject-driven generation to integrate images into text prompts to generate new customized images. \n\nA group of approaches [GAA + 22, RLJ + 22, KZZ + 23, TGCA23, AAF + 23, HHZW23, SHZ + 23] propose to fine-tune the models on each set of reference images, and fail to achieve subject-driven generation through a generalized pre-trained model. [XYF + 23, WZJ + 23, CHL + 23, CHSC22] inject image features into the U-Net of diffusion models. However, such injection methods segregate the guidance for text and images, thereby limiting the effectiveness of joint modeling between the two modalities. Additionally, this approach is challenging to extend to scenarios involving multiple entities. Recent work BLIP-Diffusion [LLH23] learns object representations by synthesizing images through the composition of subjects with random backgrounds. This approach effectively endows it with a zero-shot, subject-driven text-to-image generation capability. However, the specific design of its input template and training data restricts its scalability to multiple entities. \n\nIn contrast to previous methods that work with original CLIP text encoder [RKH + 21], we propose that through leveraging Multimodal Large Language Models (MLLMs) [ADL + 22, HSD + 22, AHR + 22, HDW + 23, LLSH23], most of the challenges in subject-driven generation may be easily resolved. \n\nMLLMs have expanded the perception capabilities of language models to multimodality, enabling them to perceive diverse modalities such as images. The idea of leveraging MLLMs for subject-driven generation presents several advantages: 1) It capitalizes on the inherent vision-language alignment within the MLLM. \n\n2) The MLLM architecture naturally supports interleaved interleaved multi-image and text input.",
            "score": 0.47620837937515464,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1439
                },
                {
                    "start": 1442,
                    "end": 1729
                },
                {
                    "start": 1732,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 2042
                },
                {
                    "start": 2045,
                    "end": 2140
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93359375
        },
        {
            "corpus_id": "265659463",
            "title": "Towards More Unified In-Context Visual Understanding",
            "text": "Multimodal understanding and generation represent an emerging frontier in artificial intelligence that seeks to interpret and synthesize information across various forms of data, such as text, images, sounds, and even more modalities. Inspired by the success of ChatGPT as well as GPT-4 [32,33], recent works primarily concentrate on aligning visual features with the pre-trained LLMS for multimodal comprehension tasks [18,24,26,27,44,45,53,57]. While pre-trained LLMs have empowered systems to follow human instructions for vision-language interactions, their application has been confined to generating textual outputs. \n\nExpanding the horizons of multimodal capabilities, a burgeoning spectrum of studies [15,21,40,42,51,54] are pioneering innovations in both understanding and generative capacities across modalities. IMAGEBIND [15] utilizes the image-paired data to connect five different modalities with a single joint embedding space, demonstrating impressive zero-shot capabilities across these modalities. Oth-erwise, CoDi [42] introduces a composable generation strategy by bridging alignment in the diffusion process, facilitating the synchronized generation of any combination of output modalities, including language, image, video, or audio. Furthermore, NExT-GPT [51] integrates an LLM with multimodal adaptors and diverse diffusion decoders, enabling it to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio with understanding and reasoning. \n\nHowever, these models are not designed for in-context learning, without the benefit of the multiple prompts. \n\nMixture of Experts models. Mixture of Experts (MoEs), which have demonstrated remarkable success in both computer vision [28,35,46] and natural language processing [11,14,22,36,59] with the context of conditional computation. Conditional computation aims to increase the number of model parameters without significantly increasing computational cost by selectively activating relevant parts of the model based on input-dependent factors [6,9]. [36] first provides compelling evidence for the efficacy of MoEs by incorporating MoE layers into LSTM models.",
            "score": 0.4758982478230325,
            "section_title": "Related Works",
            "char_start_offset": 6388,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 622
                },
                {
                    "start": 625,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1504
                },
                {
                    "start": 1507,
                    "end": 1615
                },
                {
                    "start": 1618,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 709,
                    "end": 713,
                    "matchedPaperCorpusId": "258564264"
                },
                {
                    "start": 833,
                    "end": 837,
                    "matchedPaperCorpusId": "258564264"
                },
                {
                    "start": 1743,
                    "end": 1746,
                    "matchedPaperCorpusId": "235417196"
                },
                {
                    "start": 1746,
                    "end": 1749,
                    "matchedPaperCorpusId": "119184034"
                },
                {
                    "start": 1782,
                    "end": 1786,
                    "matchedPaperCorpusId": "245124124"
                },
                {
                    "start": 1786,
                    "end": 1789,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 2055,
                    "end": 2058,
                    "matchedPaperCorpusId": "6083596"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90771484375
        },
        {
            "corpus_id": "258546868",
            "title": "LMEye: An Interactive Perception Network for Large Language Models",
            "text": "Vision-Language Models (VLMs) [1,44] trained on a massive amount of image-text data have shown impressive results in various multimodal understanding and generation tasks. However, training a MLLM (e.g., Flamingo [1], Kosmos-1 [16], and GPT-4 [34]) from scratch is resource-intensive. To alleviate this issue, previous open-source efforts [30,21,8,14] present that we can construct a MLLM based on the text-only large language model (LLM) through transforming the visual information (obtained by frozen pretrained visual encoders [38,7]) into the representation space of LLM. By doing so, LLMs are capable of understanding visual information and performing multimodal humanmachine interaction. Significantly, the whole training process is parameter efficient since it only needs to optimize a few parameters of the vision-to-language feature transformer, similar to popular prefix or prompt tuning approaches [23,17]. \n\nRecent work [30] demonstrates that a learnable linear mapping network can allow LLMs to incorporate the basic global perception information of an image. Different from common VLMs, e.g., Oscar [22] and OFA [44], MLLMs constructed in this way usually perform multimodal generation well [30] because LLMs are capable of powerful contextual understanding, reasoning, and generating capabilities. To step forward this direction, [18] present the model FROMAGe, where they freeze the LLM and visual encoder and fine-tune several linear mapping layers to achieve cross-modality information interactions. It realizes strong zero-shot performances on the contextual image retrieval and multimodal dialogue tasks. [21] propose BLIP-2 with a lightweight Querying Transformer to bridge the vision and language semantic gap for frozen image encoders and large language models. In addition, the multimodal instruction-following tuning approach is recently introduced by [25] and [54] to advance the multimodal interaction capability of LLMs, which show supervisor performances on various multimodal scenarios.",
            "score": 0.47559768578834116,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 2016
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 33,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 213,
                    "end": 216,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 913,
                    "end": 916,
                    "matchedPaperCorpusId": "247618727"
                },
                {
                    "start": 1113,
                    "end": 1117,
                    "matchedPaperCorpusId": "215754208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8828125
        },
        {
            "corpus_id": "278368161",
            "title": "CM1 - A Dataset for Evaluating Few-Shot Information Extraction with Large Vision Language Models",
            "text": "PaliGemma3B is an open-weight LVLM that integrates the SigLIP-So400m vision encoder [29] with the Gemma-2B language model [22], resulting in a model with approximately 3 billion parameters. The architecture uses a Vision Transformer (ViT) [11] encoder to process input images and extract high-dimensional visual features, which are then linearly projected to match the input space of the Gemma-2B language model. This integrated architecture facilitates the seamless processing of multimodal input, allowing the model to effectively handle tasks requiring both visual understanding and natural language generation. The pre-training of PaliGemma follows a multi-stage approach to cultivate robust unimodal and multimodal competencies. First, the SigLIP-So400m vision encoder and the Gemma-2B language model are independently pre-trained on large unimodal datasets to improve the feature extraction and natural language processing capabilities of the model, respectively. The model then undergoes multimodal pre-training on a variety of image-text pairs, focusing on tasks such as image captioning and visual question answering. This phase enables the model to learn complex relationships between visual elements and textual descriptions, which is critical for performance in integrated vision-language tasks. \n\nPaliGemma supports a range of input image resolutions, with the highest resolution of 896px used in this work to capture detailed visual information. The model used has been pre-trained on the DocVQA dataset [21], providing it with advanced capabilities for document-based visual question answering.",
            "score": 0.47462367662496957,
            "section_title": "PaliGemma",
            "char_start_offset": 19440,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1307
                },
                {
                    "start": 1310,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1609
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 88,
                    "matchedPaperCorpusId": "257767223"
                },
                {
                    "start": 239,
                    "end": 243,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 1518,
                    "end": 1522,
                    "matchedPaperCorpusId": "220280200"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9140625
        },
        {
            "corpus_id": "276928288",
            "title": "OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models",
            "text": "In recent years, Large Language Models (LLMs) [2,5,15,59,60] have achieved remarkable advancements, igniting significant research interest in extending their fundamental capabilities to the visual domain. Consequently, researchers have developed a series of Multimodal Large Language Models (MLLMs) for tasks such as multimodal understanding [42,43,75,77] and visual generation [31,55]. \n\nRecent studies have emerged that seek to integrate multimodal understanding with visual generation, aiming to develop unified systems capable of handling both tasks simultaneously. Such designs hold the potential to foster mutual enhancement between generation and understanding, offering a promising pathway toward truly unifying all modalities. Numerous studies have sought to preserve the text generation paradigm of LLMs while exploring the impact [46,64,66,67] of integrating diverse visual generation paradigms, such as diffusion models [24], flow-based generative models [16,40], and vector-quantized autoregressive models [56]. \n\nUnfortunately, the significant domain gap between image and text presents a critical challenge for unified multimodal generative models: preserving generation capabilities without degrading understanding performance requires an extensive volume of image-text pairs for training, as illustrated in Fig. 1. This not only leads to poor training efficiency but also creates a substantial barrier to the broader development of such models, as only a small fraction of researchers possess the resources to undertake such computationally demanding studies. Moreover, most existing unified multimodal generative models rely on Transformerbased LLMs [61]. However, their quadratic computational complexity results in slow inference speeds, rendering them less practical for real-time applications. \n\nThe challenges faced by existing unified multimodal generative models naturally lead us to ponder: can a model be developed that achieves both training efficiency and inference efficiency? \n\nTo address this, we introduce OmniMamba, a novel unified multimodal generative model that requires only 2M image-text pairs for training. Built on the Mamba-2-1.3B [10] model as the foundational LLM with a unified next token prediction paradigm to generate all modali-ties, OmniMamba leverages the linear computational complexity of state space models (SSMs) to achieve significantly faster inference speeds.",
            "score": 0.47421368774414,
            "section_title": "Introduction",
            "char_start_offset": 624,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 386
                },
                {
                    "start": 389,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1815
                },
                {
                    "start": 1818,
                    "end": 2006
                },
                {
                    "start": 2009,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2417
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 51,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 342,
                    "end": 346,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 346,
                    "end": 349,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 378,
                    "end": 382,
                    "matchedPaperCorpusId": "258959284"
                },
                {
                    "start": 382,
                    "end": 385,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 967,
                    "end": 971,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 1668,
                    "end": 1672,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96435546875
        },
        {
            "corpus_id": "271310040",
            "title": "Foundation Models for Autonomous Robots in Unstructured Environments",
            "text": "Multimodality refers to the ability of a model to accept different \"modalities\" or types of inputs, such as images, text, or audio.Vision Language Models (VLMs) are a category of multimodal models that take in both images and text for various tasks, including image captioning, visual question answering, and visual entailment.One widely used VLM is Contrastive Language-Image Pre-training (CLIP) [43], trained on 400 million image-text pair datasets from the internet.CLIP offers a method to compute the similarity between textual descriptions and images.It employs internet-scale image-text pairs data to capture the semantic information between images and text.The CLIP model architecture comprises a text encoder and an image encoder (a modified version of ViT) that are jointly trained to maximize the cosine similarity of the image and text embeddings.Another notable example is BLIP [66], which focuses on multimodal learning by jointly optimizing three objectives during pre-training: Image-Text Contrastive Loss, Image-Text Matching Loss, and Language Modeling Loss.This method leverages noisy web data by bootstrapping captions, enhancing the training process.FILIP [67] concentrates on achieving finer-level alignment in multimodal learning.It incorporates a crossmodal late interaction mechanism that utilizes token-wise maximum similarity between visual and textual tokens.This mechanism guides the contrastive objective and improves the alignment between visual and textual information.\n\nRecent advancements in multimodal foundation models, enabled them to handle a wider range of modalities and tasks.Prominent examples of such models are AudioCLIP [68], CLAP [69], and Whisper.AudioCLIP and CLAP are more focused on encoding sounds into vector space, facilitating similarity computations, while Whisper [70] seeks to specify easily confused words and perform in-context learning.Furthermore, there are foundation models that incorporate spatial information and 3D data into their architectures to enable segmentation and object detection tasks.Some of these models are 3D-LLM [71], OpenScene [72], and SpatialVLM [73].",
            "score": 0.4739271591093951,
            "section_title": "3.4.3-Multi-modality: Large Multimodal Models (LMMs)",
            "char_start_offset": 20937,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 327
                },
                {
                    "start": 327,
                    "end": 469
                },
                {
                    "start": 469,
                    "end": 556
                },
                {
                    "start": 556,
                    "end": 664
                },
                {
                    "start": 664,
                    "end": 858
                },
                {
                    "start": 858,
                    "end": 1075
                },
                {
                    "start": 1075,
                    "end": 1170
                },
                {
                    "start": 1170,
                    "end": 1252
                },
                {
                    "start": 1252,
                    "end": 1386
                },
                {
                    "start": 1386,
                    "end": 1500
                },
                {
                    "start": 1502,
                    "end": 1616
                },
                {
                    "start": 1616,
                    "end": 1693
                },
                {
                    "start": 1693,
                    "end": 1895
                },
                {
                    "start": 1895,
                    "end": 2060
                },
                {
                    "start": 2060,
                    "end": 2134
                }
            ],
            "ref_mentions": [
                {
                    "start": 397,
                    "end": 401,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 890,
                    "end": 894,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 1176,
                    "end": 1180,
                    "matchedPaperCorpusId": "244117525"
                },
                {
                    "start": 1664,
                    "end": 1668,
                    "matchedPaperCorpusId": "235624127"
                },
                {
                    "start": 1675,
                    "end": 1679,
                    "matchedPaperCorpusId": "249605738"
                },
                {
                    "start": 1819,
                    "end": 1823,
                    "matchedPaperCorpusId": "252923993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90478515625
        },
        {
            "corpus_id": "270391394",
            "title": "Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey",
            "text": "Large multimodal models (LMMs) are created to handle and integrate various data types, such as text, images, audio, and video.LMMs extend the capabilities of LLMs by incorporating additional modalities, allowing for a more comprehensive understanding and generation of diverse content.The development of LMMs is driven by the need to more accurately reflect the multimodal nature of human communication and perception.While traditional LLMs like GPT-4 are primarily text-based, LMMs are capable of processing and generating outputs across various data types.For instance, they can interpret visual inputs, generate textual descriptions from images, and even handle audio data, thus bridging the gap between different forms of information.One of the critical advancements in LMMs is the ability to create a unified multimodal embedding space.This involves using separate encoders for each modality to generate data-specific representations, which are then aligned into a cohesive multimodal space.This unified approach allows the models to integrate and correlate information from different sources seamlessly.\n\nNotable examples include Gemini [111], GPT-4V, and ImageBind [110].These models showcase the ability to process text, images, audio, and video, enhancing functionalities such as translation, image recognition, and more.In addition to these well-known models, other emerging models are also making significant strides: BLIP-2 [112] introduces a novel approach to integrate a frozen pre-trained visual encoder with a frozen large language model using a Q-former module.This module employs learnable input queries that interact with image features and the LLM, allowing for effective cross-modal learning.This setup helps maintain the versatility of the LLM while incorporating visual information effectively.LLava [113] is a represent large multimodal model integrating a pre-trained CLIP [116] visual encoder (ViT-L/14), the Vicuna [115] language model, and a simple linear projection layer.",
            "score": 0.4733562840727251,
            "section_title": "Large Multimodal Models",
            "char_start_offset": 7553,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 126,
                    "end": 285
                },
                {
                    "start": 285,
                    "end": 418
                },
                {
                    "start": 418,
                    "end": 558
                },
                {
                    "start": 558,
                    "end": 738
                },
                {
                    "start": 738,
                    "end": 841
                },
                {
                    "start": 841,
                    "end": 996
                },
                {
                    "start": 996,
                    "end": 1109
                },
                {
                    "start": 1111,
                    "end": 1178
                },
                {
                    "start": 1178,
                    "end": 1330
                },
                {
                    "start": 1330,
                    "end": 1578
                },
                {
                    "start": 1578,
                    "end": 1713
                },
                {
                    "start": 1713,
                    "end": 1817
                },
                {
                    "start": 1817,
                    "end": 2001
                }
            ],
            "ref_mentions": [
                {
                    "start": 1172,
                    "end": 1177,
                    "matchedPaperCorpusId": "258564264"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9580078125
        },
        {
            "corpus_id": "276408024",
            "title": "I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models",
            "text": "Recent work on large language models (LLMs) and diffusion transformers (Peebles & Xie, 2023;Forest, 2024a) has inspired unified models for multimodal understanding and generation. These models either finetune LLMs to generate image tokens, which are then decoded into images via diffusion decoders (Ge et al., 2024;Pan et al., 2023;Sun et al., 2023;Koh et al., 2024;Wu et al., 2023;Ye et al., 2024), or integrate text, image, and noise tokens within a transformer architecture (Xiao et al., 2024;Shi et al., 2024). They are typically trained end-to-end with diffusion losses or align output image tokens with CLIP text features using cosine similarity losses (Wu et al., 2023;Ye et al., 2024;Tong et al., 2024). While some methods exhibit preliminary reasoning capabilities, these capabilities remain constrained by the limits of diffusion training paradigms, the availability of reasoning datasets, and the representational limits of CLIP embeddings. In contrast, our method leverages vision-language training to transfer advanced multimodal reasoning capabilities in VLMs to diffusion models.",
            "score": 0.47284699282357534,
            "section_title": "Unified understanding and generation",
            "char_start_offset": 7244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1094
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 92,
                    "matchedPaperCorpusId": "254854389"
                },
                {
                    "start": 349,
                    "end": 366,
                    "matchedPaperCorpusId": "258959284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94384765625
        },
        {
            "corpus_id": "258833460",
            "title": "Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition with Auxiliary Refined Knowledge",
            "text": "In this paper, PGIM enables the integration of multimodal tasks into large language model by converting images into image captions. While PGIM achieves impressive results, we consider this Text-Text paradigm as a transitional phase in the development of MNER, rather than the ultimate solution. \n\nBecause image captions are inherently limited in their ability to fully capture all the details of an image. This issue may potentially be further resolved in conjunction with the advancement of multimodal capabilities in language and vision models (e.g., GPT-4).",
            "score": 0.47272465222891635,
            "section_title": "Limitations",
            "char_start_offset": 23290,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 560
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62646484375
        },
        {
            "corpus_id": "278714599",
            "title": "A Light and Smart Wearable Platform with Multimodal Foundation Model for Enhanced Spatial Reasoning in People with Blindness and Low Vision",
            "text": "Foundation models, encompassing large-scale pretrained models such as Large Language Models (LLMs) and Multi-modal Large Language Models (MLLMs), have revolutionized the field of artificial intelligence by demonstrating exceptional capabilities across a wide range of tasks. These models, trained on vast amounts of data, are capable of understanding and generating human-like text, as well as interpreting visual inputs in a meaningful way. The GPT-3 model, for instance, has shown remarkable proficiency in natural language understanding and generation, enabling applications ranging from automated customer service to complex problem-solving [6]. Similarly, MLLMs like CLIP [29] and VL-BERT [33] have illustrated the potential of integrating visual and linguistic information, allowing for tasks such as image captioning and text-based image generation. \n\nRecent advancements have introduced multimodal models that integrate capabilities across multiple modalities, further enhancing their utility. GPT-4 [24], for instance, extends the capabilities of its predecessors by incorporating more sophisticated multimodal inputs, allowing it to process and generate text, images, and other forms of data simultaneously. This capability significantly enhances its application in fields requiring complex reasoning and contextual understanding across different types of information. LLaVA (Large Language and Vision Assistant) [21] is another recent model that exemplifies the integration of visual and linguistic processing. LLaVA is designed to handle tasks that require understanding and generating coherent responses based on both visual and textual inputs, such as detailed scene descriptions and interactive question-answering involving visual context. \n\nDespite their impressive performance, foundation models face limitations, particularly in specialized domains requiring fine-grained understanding and reasoning. The black-box nature of these models often poses challenges in interpretability and reliability, especially in critical applications like assistive technology for visually impaired individuals. Furthermore, their generalist design may not adequately address specific needs such as spatial reasoning and real-time responsiveness, which are crucial for effective assistive technologies.",
            "score": 0.47259253440194904,
            "section_title": "Foundation Models",
            "char_start_offset": 5730,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 856
                },
                {
                    "start": 859,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1754
                },
                {
                    "start": 1757,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2112
                },
                {
                    "start": 2113,
                    "end": 2303
                }
            ],
            "ref_mentions": [
                {
                    "start": 645,
                    "end": 648,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1423,
                    "end": 1427,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95751953125
        },
        {
            "corpus_id": "276902413",
            "title": "ARMOR: Empowering Multimodal Understanding Model with Interleaved Multimodal Generation Capability",
            "text": "Unified understanding and generation is a crucial direction in the development of vision-and-language models, requiring a model to simultaneously handle understanding tasks (e.g., visual question answering) and generation tasks (e.g., text-to-image generation). Existing Unified Models (UniMs) for understanding and generation, such as Showo [41] and Janus-pro [7], are designed to simultaneously learn both multimodal understanding and generation capabilities. Despite their impressive performance, the training of such models demands substantial computational resources, which significantly hinders their scalability and ability to accommodate personalized modifications. Furthermore, most UniMs struggle to generate interleaved textimage. To this end, we present a resource-efficient autoregressive framework named ARMOR, which fine-tunes existing multimodal large language model (MLLMs) to upgrade them to UniMs from three perspectives, including model architecture, training data and training algorithm. Sample generation from ARMOR is shown in Figure 1. \n\nWe first introduce an asymmetric encoder-decoder architecture into existing MLLMs to enable them output natural interleaved text-image. Specifically, ARMOR fully retains the encoder and decoder in MLLMs while incorporating an asymmetric image decoder, as shown in Figure 2. In doing so, the strong semantic perception and understanding capabilities of the MLLMs are almost preserved, while the asymmetric image decoder enables image generation with minimal computational overhead. To alleviate the long-tail distribution problem across different modalities in the answer space, we propose a forward-switching mechanism to dynamically control which modality's answer space is used for prediction based on the model input. \n\nThen we collect a meticulously curated, high-quality interleaved dataset, and propose a \"What or How to Generate\"  To validate the effectiveness of the proposed ARMOR framework, we incorporate InternVL2.5 [8] into ARMOR (named \"Ours\" here), and conduct extensive experiments on 9 benchmarks to evaluate multimodal understanding and generation capabilities.",
            "score": 0.4724836193161517,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1782
                },
                {
                    "start": 1785,
                    "end": 2141
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96337890625
        },
        {
            "corpus_id": "273346517",
            "title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models",
            "text": "\"True evaluation lies in the seamless interweaving of diverse modalities.\" \n\nMultimodal learning has made remarkable progress with the development of Large Vision-Language Models (LVLMs) (Liu et al., 2023a;Zhu et al., 2023;Dai et al., 2023), which are capable of handling diverse tasks that involve both images and text. Despite their advancements, most of these models are limited to multimodal tasks for text generation, such as visual question answering (VQA) and image captioning, which do not fully reflect the potential of multimodal capacity. To broaden their application, interleaved text-and-image generation has emerged as a critical area of research (Liu et al., 2024). It requires models to generate images and text in any sequence, thereby enhancing the versatility and effectiveness of multimodal systems. It opens up possibilities for various complex applications, such as multi-step inference (Lu et al., 2024;Kazemi et al., 2024), multimodal situational analysis (Yang et al., 2021), and visual storytelling (Huang et al., 2016). \n\nWhile recent LVLMs are evolving to support interleaved text-and-image generation (Team, 2024;Xie et al., 2024;Chern et al., 2024;Zhou et al., 2024), a comprehensive evaluation benchmark is still falling behind due to the following two challenges: Figure 1: Typical samples from the MMIE Benchmark showcase its support for multiple image inputs and outputs, with ground truth provided for every query. MMIE evaluates models across diverse fields, ensuring a comprehensive evaluation of their capabilities. \n\n\u2022 Difficulty in Constructing Modality-Coherent Benchmarks. The first challenge lies in the difficulty of constructing modality-aligned multimodal datasets, where both the input and output contain images and text.",
            "score": 0.4724448289363265,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 77,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1046
                },
                {
                    "start": 1049,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1768
                }
            ],
            "ref_mentions": [
                {
                    "start": 909,
                    "end": 926,
                    "matchedPaperCorpusId": "264491155"
                },
                {
                    "start": 1025,
                    "end": 1045,
                    "matchedPaperCorpusId": "2574224"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93408203125
        },
        {
            "corpus_id": "273901351",
            "title": "V-Gl\u00f3rIA - Customizing Large Vision and Language Models to European Portuguese",
            "text": "Vision and Language are two of the main communication and information perception mediums, serving as fundamental channels through which humans interpret and interact with the world around them. Devising Vision and Language (V&L) models that can seamlessly combine these two modalities is paramount to delivering AI systems capable of addressing tasks such as image captioning and visual question-answering, essential tasks to aid visually impaired individuals, and Imageto-Text and Text-to-Image retrieval, for general multimodal information seeking. Recently, there have been notable advances in vision and language models (Liu et al., 2023;Koh et al., 2023;Kim et al., 2021), which leverage Large Language Models as backbones (Touvron et al., 2023;Brown et al., 2020;Zhang et al., 2022) (LLMs). Most of these advances have been made with models in English or other high-resource languages, leaving behind other lower-resource languages, as is the case of European Portuguese (PT-PT). This evidences the urgent need of having effective customization methologies to deliver V&L LMs, openly available, for PT-PT speakers. This customization process raises two complementary challenges: 1) how to overcome the limited availability of PT-PT multimodal datasets and resources, and 2) how to train a Large Visiwon and Language model, capable of addressing multiple V&L tasks, in PT-PT. \n\nMost LLMs are trained with text-only web scraped data, achieving great performance on a myriad of natural language tasks, but lack an overall understanding of images, thus not having visual reasoning capabilities. Pioneering vision and language models, adopted fully multimodal Transformer-based models (Lu et al., 2019;Yu et al., 2022;Wang et al., 2022), with either singlestream or dual-stream architectures (Bugliarello et al., 2021), pre-trained on image-text pairs.",
            "score": 0.4724028957908115,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1853
                }
            ],
            "ref_mentions": [
                {
                    "start": 624,
                    "end": 642,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 642,
                    "end": 659,
                    "matchedPaperCorpusId": "258947258"
                },
                {
                    "start": 659,
                    "end": 676,
                    "matchedPaperCorpusId": "231839613"
                },
                {
                    "start": 750,
                    "end": 769,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8447265625
        },
        {
            "corpus_id": "270765591",
            "title": "A Textual Modal Supplement Framework for Understanding Multi-Modal Figurative Language",
            "text": "In recent years, advances in language modeling notably improved model comprehension of metaphorical language.broader study of metaphorical language.Their findings indicated that although pre-trained language models could achieve commendable performance after fine-tuning, their performance on a limited number of samples still fell significantly short of human capabilities.\n\nIn addition, with the development of multimedia, there had been an increased focus on generative understanding of multimodal metaphorical language.Hessel et al. investigated visual language models and language-only models for understanding multimodal metaphorical language and found that both types of models had difficulties in all three tasks.Desai et al. introduced an architecture based on a multimodal Transformer, which included a cross-modal attention mechanism focusing on the distinctive features between images and captions.This model obtained relatively high consistency scores in human evaluations.Yosef et al. utilized the state-of-the-art vision and language model CLIP (Radford et al., 2021) to perform on a multi-modal metaphorical language comprehension task and found that it performed relatively poorly.The experimental results showed that the best model was only 22% accurate in the detection task, much lower than the 97% accuracy achieved by humans.This discrepancy was mainly due to the poor performance of model in understanding the connection between metaphorical language and images, with a tendency to prefer partially literal images over metaphorical ones.\n\nThese studies have primarily improved performance through methods such as model finetuning and knowledge enhancement.However, they still face challenges in understanding multimodal metaphorical language.To enhance the capability of visual language models to comprehend metaphorical language, we design prompts to clarify task requirements and employ modal supplement methods to boost the integration of multimodal data, aiming to narrow the gap between models and humans in multimodal metaphor comprehension.",
            "score": 0.47233131720043153,
            "section_title": "Related work",
            "char_start_offset": 2397,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 109,
                    "end": 148
                },
                {
                    "start": 148,
                    "end": 374
                },
                {
                    "start": 376,
                    "end": 523
                },
                {
                    "start": 523,
                    "end": 721
                },
                {
                    "start": 721,
                    "end": 910
                },
                {
                    "start": 910,
                    "end": 986
                },
                {
                    "start": 986,
                    "end": 1198
                },
                {
                    "start": 1198,
                    "end": 1347
                },
                {
                    "start": 1347,
                    "end": 1560
                },
                {
                    "start": 1562,
                    "end": 1679
                },
                {
                    "start": 1679,
                    "end": 1765
                },
                {
                    "start": 1765,
                    "end": 2070
                }
            ],
            "ref_mentions": [
                {
                    "start": 1060,
                    "end": 1082,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.498779296875
        },
        {
            "corpus_id": "266210376",
            "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
            "text": "Driven by the remarkable success of large language models (LLMs) in the field of natural language processing (NLP) [40,41,54], there has been a surge of interest within multimodal community to develop large vision-language (VL) models. One of the promising approaches, exemplified by Flamingo [1], BLIP2 [24], LLAVA [25], have explored how to build large VL models based on powerful pre-trained LLMs. These studies typically adopted a similar architecture: a pre-trained image encoder and an LLM are connected via a trainable connection module, which aligns the image feature and text embeddings, thereby enabling language models to accept images and text as inputs and generate a text sequence. \n\nTo expand the capabilities of generating image in a multimodal context, certain efforts, e.g., Visual ChatGPT [47], attempt to connect LLMs with image generation tools in a cascaded pipeline by transferring text messages, which inevitably introduce instability and noise. Alternatively, another line of research achieves it by optimizing models in an end-to-end manner [9,18,23,30,48]. By aligning the output space with the image diffusion models, VL models can not only perceive but also generate images and text. \n\nA crucial characteristic of large language models is autoregressive modeling [31], i.e., predicting next token, which facilitates language understanding and generation in a unified manner. However, in the aforementioned studies, the inconsistency of image embeddings between LLM's input and output sides compels the model to treat input images and generated images differently, resulting in separate modeling for image understanding and generation. Meanwhile, this discrepancy also obstructs the implementation of autoregressive training loss on image embeddings. \n\nIn this study, we introduce VL-GPT, a large visionlanguage generative pre-trained transformer that enables the unified training of both visual and linguistic data using an auto-regressive objective, as depicted in Fig. 1.",
            "score": 0.4723180431407133,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 695
                },
                {
                    "start": 698,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1212
                },
                {
                    "start": 1215,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1778
                },
                {
                    "start": 1781,
                    "end": 2002
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "267523393",
            "title": "ScreenAI: A Vision-Language Model for UI and Infographics Understanding",
            "text": "Our model architecture as shown in Figure 1 is inspired by the architecture of the PaLI family of models [Chen et al., 2022;Chen et al., 2023a;Chen et al., 2023b], which is composed of a multimodal encoder block with a vision encoder like ViT [Dosovitskiy et al., 2020] and a mT5 [Xue et al., 2020;Raffel et al., 2020] language encoder consuming image and text inputs, followed by an autoregressive decoder. The input image is transformed into a sequence of embeddings by the vision encoder and these embeddings are concatenated with the input text embeddings and fed into the mT5 language encoder. The output of this encoder is passed to the decoder to generate the text output. This generic formulation enables us to use the same model architecture to solve a variety of vision and multimodal tasks that can be recast as a text+image (input) to text (output) problem. Compared to the text input, the image embeddings constitute a significant portion of the input length to the multimodal encoder. \n\nWe further extend PaLI's encoder-decoder architecture to accept various image patching patterns. The original PaLI architecture only accepts a fixed grid pattern of patches for processing the input images. However, the data we encounter in screen-related domains spans a wide variety of resolutions and aspect ratios. To have a single model to work across all screen shapes, it is necessary to use a patching strategy which can work well with images of various shapes. To this end, we borrow a technique introduced in Pix2Struct [ Lee et al., 2023], which allows us to have image patches with arbitrary grid shapes based on the input image shape and a pre-defined maximum number of patches, as shown in Figure 1. This enables us to accommodate input images of various formats and aspect ratios without the need for padding or stretching the image to a fixed shape, making our model more polyvalent to handle both mobile (i.e. portrait) and desktop (i.e. landscape) image formats.",
            "score": 0.47227855860806883,
            "section_title": "Architecture",
            "char_start_offset": 6505,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 1980
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 124,
                    "matchedPaperCorpusId": "231698419"
                },
                {
                    "start": 143,
                    "end": 162,
                    "matchedPaperCorpusId": "6623010"
                },
                {
                    "start": 298,
                    "end": 318,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1532,
                    "end": 1549,
                    "matchedPaperCorpusId": "252762394"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84033203125
        },
        {
            "corpus_id": "265658988",
            "title": "UPOCR: Towards Unified Pixel-Level OCR Interface",
            "text": "The emergence of Transformer [18,72] breaks the boundary between different modalities [10,26,28,29,45,58,96], fostering a broad variety of generalist models [75,88,101]. One category of these models unifies both the input and output as sequences and bridges them with a sequenceto-sequence learning framework. Pix2Seq [12] pioneered in unifying the output vocabulary of natural language and spatial coordinates and demonstrated effectiveness in object detection. Subsequently, Pix2Seq v2 [13] simultaneously tackled multiple tasks with the guidance of specific prompts. Furthermore, OFA [76] discretized both the input and output as token sequences, accomplishing various unimodal and cross-modal vision-language tasks. Similarly, Unified-IO [49] extended the framework to a wider range of tasks and modalities. With the rise of LLMs [9,56,69], numerous studies [1,38,42,91,94,100] connected pretrained ViTs and LLMs for generalist models with stronger reasoning and robustness. Following an image-to-image translation pipeline, MAE-VQGAN [5] treated diverse tasks as inpainting problems. Painter [79] further investigated visual in-context learning which allows it to adapt to unseen tasks. \n\nIn the OCR field, several generalist models have been studied following sequence-to-sequence paradigms. Donut [33] pioneered in employing a prompt-based sequence generation framework for document understanding tasks. UDOP [68] further unified the representation of image, text, and layout modalities. Moreover, Nougat [7] and Kosmos-2.5 [51] focused on translating document images into markup languages, encapsulating multiple document OCR tasks into a single sequence. Based on large multimodal models with pretrained ViTs and LLMs, recent approaches [22,89,90] augment them using OCR-related data and fine-grained visual perception. However, these approaches primarily focused on document scenarios and failed to generate pixels. Moreover, some of them [33,68] still required dataset-specific finetuning.",
            "score": 0.4719932986111018,
            "section_title": "Generalist Model",
            "char_start_offset": 7297,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2000
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 33,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 33,
                    "end": 36,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 86,
                    "end": 90,
                    "matchedPaperCorpusId": "218889832"
                },
                {
                    "start": 90,
                    "end": 93,
                    "matchedPaperCorpusId": "261049745"
                },
                {
                    "start": 93,
                    "end": 96,
                    "matchedPaperCorpusId": "232110866"
                },
                {
                    "start": 96,
                    "end": 99,
                    "matchedPaperCorpusId": "236635379"
                },
                {
                    "start": 99,
                    "end": 102,
                    "matchedPaperCorpusId": "255416140"
                },
                {
                    "start": 102,
                    "end": 105,
                    "matchedPaperCorpusId": "245144274"
                },
                {
                    "start": 157,
                    "end": 161,
                    "matchedPaperCorpusId": "249152323"
                },
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "251105279"
                },
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "244799261"
                },
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "237593074"
                },
                {
                    "start": 488,
                    "end": 492,
                    "matchedPaperCorpusId": "249674493"
                },
                {
                    "start": 587,
                    "end": 591,
                    "matchedPaperCorpusId": "246634906"
                },
                {
                    "start": 742,
                    "end": 746,
                    "matchedPaperCorpusId": "249848272"
                },
                {
                    "start": 834,
                    "end": 837,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 862,
                    "end": 865,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 865,
                    "end": 868,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1097,
                    "end": 1101,
                    "matchedPaperCorpusId": "254246343"
                },
                {
                    "start": 1304,
                    "end": 1308,
                    "matchedPaperCorpusId": "250924870"
                },
                {
                    "start": 1416,
                    "end": 1420,
                    "matchedPaperCorpusId": "254275326"
                },
                {
                    "start": 1949,
                    "end": 1953,
                    "matchedPaperCorpusId": "250924870"
                },
                {
                    "start": 1953,
                    "end": 1956,
                    "matchedPaperCorpusId": "254275326"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6884765625
        },
        {
            "corpus_id": "258615242",
            "title": "Image-to-Text Translation for Interactive Image Recognition: A Comparative User Study with Non-Expert Users",
            "text": "For example, in the case of image input, the model is expected to output bounding boxes and their corresponding object categories for object detection [10,11,12], while the typical output format for semantic segmentation is pixel-level classification maps [13,14,15]. As recent examples of language modeling studies have shown, text output has the advantage of being able to handle a wide variety of tasks in a generic manner. It has been pointed out that large-scale language models are capable of handling a variety of tasks through in-context learning and fine-tuning [16,17], and this is also true for cross-modal cases such as vision-language models [18,19,20]. Image-to-text translation, or image captioning, has been actively studied in the computer vision community as one of the most fundamental image recognition tasks [21,22,23]. Although the phrase \"image captioning\" often refers to the specific task of describing the content of an image, the image-to-text translation model, which converts arbitrary images to text, can express a variety of tasks that go beyond the generic description of the content of the image. For example, the model can be seen as a face detector if it outputs simple texts indicating the location of the faces in the input image. Likewise, when a text describes the shapes of image regions together with the categories they belong to, it can be seen as a rough representation of a semantic segmentation map. Compared to image classification, image-to-text translation backends can potentially extend the capability of interactive image recognition systems. However, there is no prior work focusing on comparing these two algorithms from the IML perspective or designing an IML system based on image-to-text translation. \n\nThe purpose of this work is to examine how the user annotations and the system usability change when a model that produces text output is adopted in the design of an IML system instead of classification. As shown in Fig. 1, this study compares two IML systems for image recognition. One is an image classification system that provides category output, and the other is an image-to-text translation system that provides text output. We conducted a comparative user study using both image classification-and image-to-text translation-based prototype IML systems.",
            "score": 0.47185855295135026,
            "section_title": "Introduction",
            "char_start_offset": 2152,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1757
                },
                {
                    "start": 1760,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2191
                },
                {
                    "start": 2192,
                    "end": 2320
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 158,
                    "matchedPaperCorpusId": "49862415"
                },
                {
                    "start": 158,
                    "end": 161,
                    "matchedPaperCorpusId": "6476085"
                },
                {
                    "start": 256,
                    "end": 260,
                    "matchedPaperCorpusId": "1629541"
                },
                {
                    "start": 260,
                    "end": 263,
                    "matchedPaperCorpusId": "44155333"
                },
                {
                    "start": 263,
                    "end": 266,
                    "matchedPaperCorpusId": "204743865"
                },
                {
                    "start": 571,
                    "end": 575,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 575,
                    "end": 578,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 662,
                    "end": 665,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 829,
                    "end": 833,
                    "matchedPaperCorpusId": "52947736"
                },
                {
                    "start": 833,
                    "end": 836,
                    "matchedPaperCorpusId": "1169492"
                },
                {
                    "start": 836,
                    "end": 839,
                    "matchedPaperCorpusId": "2592435"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5048828125
        },
        {
            "corpus_id": "273962846",
            "title": "Autoregressive Models in Vision: A Survey",
            "text": "The unified framework extends traditional multimodal understanding (Li et al., 2023a;Zhang et al., 2023b;Shi et al., 2023;Xie et al., 2024b;Wan et al., 2024) by enabling both visual and textual output generation. Unlike earlier models that focused solely on interpretation, this framework leverages the integration of Large Language Models (LLMs) to enhance multimodal content generation. The overview of auto-regressive multimodal LLM's application is shown in Figure 10. \n\nFrom pre-LLM era's separate processing of modalities, the evolution to LLMs has allowed for a seamless convergence of language and visual data. This progress supports more coherent and contextually rich outputs, using native multimodal architectures to handle integrated inputs and outputs efficiently. As a result, the Unified Framework facilitates applications that require simultaneous comprehension and generation across modalities, such as interactive storytelling and dynamic content creation. \n\nAutoregressive Vision-Language Fusion Methods. Before the advent of LLMs, several autoregressive models focused on integrating vision and language. Notable among these is OFA (Wang et al., 2022a), which unified tasks across modalities using a shared transformer backbone, demonstrating versatility in handling tasks such as image captioning and visual question answering. Similarly, CogView (Ding et al., 2021) and M6 (Lin et al., 2021), extended the capabilities to Chinese text inputs, offering robust image generation from textual descriptions. ERNIE-ViLG (Zhang et al., 2021) further advanced this by employing a multi-modal pretraining strategy that enhanced the understanding of image-text pairs. Unified-IO (Lu et al., 2022) explored input-output transformations across modalities, providing a unified framework for diverse tasks. \n\nIntegration with LLMs and Diffusion Models. With the rise of LLMs, autoregressive models have increasingly interfaced with diffusion models to enhance multi-modal capabilities. NEXT-GPT (Wu et al., 2023c) exemplifies this trend by combining the autoregressive approach with diffusion processes to improve image synthesis from textual inputs.",
            "score": 0.4715413308580595,
            "section_title": "The Framework for Unifying Multimodal Understanding and Generation",
            "char_start_offset": 82869,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 472
                },
                {
                    "start": 475,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 974
                },
                {
                    "start": 977,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1814
                },
                {
                    "start": 1817,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 67,
                    "end": 85,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 105,
                    "end": 122,
                    "matchedPaperCorpusId": "256415903"
                },
                {
                    "start": 1152,
                    "end": 1172,
                    "matchedPaperCorpusId": "246634906"
                },
                {
                    "start": 1368,
                    "end": 1387,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 1395,
                    "end": 1413,
                    "matchedPaperCorpusId": "14113767"
                },
                {
                    "start": 1691,
                    "end": 1708,
                    "matchedPaperCorpusId": "249848272"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94970703125
        },
        {
            "corpus_id": "271534005",
            "title": "LLaVA-Read: Enhancing Reading Ability of Multimodal Language Models",
            "text": "Notably, H v shares the same embedding dimensions as the text tokens used by the LLM tokenizer. Different from the conventional architecture of multimodal large language models [4], LLaVA-Read incorporates a visual-text encoder f t to better capture textual and layout information, along with a high-resolution encoder for finer visual details. The objective of the visual-text encoder is to extract text from an image, yielding visual-text tokens H t = f t (X v ). Subsequently, we concatenate H v , H t , and H q , feeding them into the large language model to generate the desired response Y. \n\nIn designing LLaVA-Read, we hold the belief that a visual encoder should specialize in processing visual objects, while a lightweight visual-text encoder should focus on extracting text within images. This approach, we believe, enhances the efficiency of the visual components, as text recognition presents distinct patterns compared to visual object detection. Although high-resolution visual encoders can capture finer details, they also generate a larger number of visual tokens. To mitigate additional computational costs associated with employing two visual encoders in LLaVA-Read, we merge the output of these encoders while maintaining the same visual tokens as in LLaVA. More details on architectural design are elaborated in Section 3.1. In essence, LLaVA-Read offers a multimodal LLM framework that leverages multiple visual encoders to improve visual token learning and conversion efficiency. To foster enhanced collaboration between multiple visual encoders, we propose layout-aware tuning during the two-stage training of LLaVA-Read, as discussed in Sections 3.2 and 3.3.",
            "score": 0.47137887681708485,
            "section_title": "User Instructions:",
            "char_start_offset": 9014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1682
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7841796875
        },
        {
            "corpus_id": "274130618",
            "title": "SoK: Unifying Cybersecurity and Cybersafety of Multimodal Foundation Models with an Information Theory Approach",
            "text": "In this section, we first explore how multimodal learning has evolved from unimodal learning and then introduce several typical MFMs. From unimodal to multimodal learning. Multimodal learning extends beyond traditional unimodal learning by integrating data from various modalities like text, images, and audio. In unimodal learning (Figure 2(a)), the model maps input features to output labels within a discrete feature space, focusing on patterns within one data type. It extracts features, converts them into vectors, and then predicts labels based on these vectors. In contrast, multimodal learning (Figure 2(b)) involves mapping and aligning continuous feature spaces across different modalities, such as pairing images with text. Instead of directly predicting labels, multimodal models create unified representations by linking feature spaces from different data types. This allows for more complex tasks as the model can work with more diverse information. Existing MFMs. Within the image/text multimodal space, there are four typical classes of multimodal foundation models (MFMs): feature alignment models, text-to-image generation models, audio-to-text generation models, and models that take both text and image inputs, as summarized in Table    that uses a dual-encoder framework to process images and text separately. It trains on large image-text pairs using contrastive learning [17] to create a shared representation space, excelling in tasks like zero-shot classification and image retrieval. \u2022 Text-to-image generation models (T2I). DALL\u2022E [18] and Stable Diffusion [3] are prominent models in this category, generating images by embedding text prompts into models. These models typically leverage feature alignment techniques, such as CLIP, to convert text into feature vectors that serve as conditional guidance for image generation, enabling the model to synthesize visuals corresponding to the input text. \u2022 Text generation from text and/or image inputs (I/T2T).",
            "score": 0.4713632894080094,
            "section_title": "Multimodal Foundation Models",
            "char_start_offset": 6865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 1984
                }
            ],
            "ref_mentions": [
                {
                    "start": 1394,
                    "end": 1398,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 1558,
                    "end": 1562,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 1584,
                    "end": 1587,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.841796875
        },
        {
            "corpus_id": "275337159",
            "title": "Vision-Driven Prompt Optimization for Large Language Models in Multimodal Generative Tasks",
            "text": "Large Vision-Language Models (LVLMs) represent a significant advancement in multimodal AI by integrating visual model and Language Models [18], [19] into a unified framework. These models are capable of handling a wide range of tasks, including visual understanding, text-image alignment, image captioning, and even multimodal generation [20]. Recent works have expanded their applications and enhanced their architectures to achieve better performance, efficiency, and scalability [21]- [23]. \n\nA primary focus in the development of LVLMs has been the design of architectures that effectively unify language and vision modalities. Recent models have proposed end-to-end frameworks that leverage shared embeddings for both text and images, enabling them to excel at tasks requiring fine-grained multimodal reasoning [24]. Additionally, techniques such as mixture of experts and relational reasoning mechanisms have been introduced to improve scalability and enhance the relational reasoning capabilities of LVLMs [25], [26]. \n\nAnother significant research direction involves improving the handling of long-contextual inputs and outputs, allowing LVLMs to perform better on complex tasks such as document understanding and scene analysis [27]. These advancements enable models to process large amounts of information while maintaining efficiency and coherence [28]. Furthermore, models have been tailored for specialized tasks, including bilingual optical character recognition and text-based grounding, achieving state-of-the-art performance in domain-specific applications [29], [30]. \n\nDespite these advancements, challenges remain in evaluating LVLMs effectively. Current evaluation methodologies often fail to capture the full spectrum of capabilities offered by these models. Recent works have emphasized the importance of developing more comprehensive benchmarks and metrics to evaluate multimodal reasoning, contextual comprehension, and generative quality [31], [32]. \n\nIn summary, the field of LVLMs is rapidly evolving, with continuous innovations driving their applicability to increasingly complex tasks. From architectural advancements to application-specific adaptations, LVLMs are poised to play a central role in the future of AI research.",
            "score": 0.47111388728350256,
            "section_title": "B. Large Vision-Language Models",
            "char_start_offset": 6180,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 493
                },
                {
                    "start": 496,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1585
                },
                {
                    "start": 1588,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1975
                },
                {
                    "start": 1978,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2255
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 148,
                    "matchedPaperCorpusId": "247244544"
                },
                {
                    "start": 338,
                    "end": 342,
                    "matchedPaperCorpusId": "256358384"
                },
                {
                    "start": 488,
                    "end": 492,
                    "matchedPaperCorpusId": "249712198"
                },
                {
                    "start": 816,
                    "end": 820,
                    "matchedPaperCorpusId": "270923961"
                },
                {
                    "start": 1013,
                    "end": 1017,
                    "matchedPaperCorpusId": "267311517"
                },
                {
                    "start": 1359,
                    "end": 1363,
                    "matchedPaperCorpusId": "270391793"
                },
                {
                    "start": 1574,
                    "end": 1578,
                    "matchedPaperCorpusId": "273186005"
                },
                {
                    "start": 1964,
                    "end": 1968,
                    "matchedPaperCorpusId": "268793433"
                },
                {
                    "start": 1970,
                    "end": 1974,
                    "matchedPaperCorpusId": "273963138"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97509765625
        },
        {
            "corpus_id": "270870796",
            "title": "From Efficient Multimodal Models to World Models: A Survey",
            "text": "Fig. 6.General architecture of a multimodal large model.This figure illustrates how different encoders (Text, Vision, Audio) process their respective inputs and align them before projecting into a shared latent space managed by a Large Language Model (LLM).The model then performs diffusion processes to generate or refine outputs, integrating information across multiple modalities to enhance understanding and generation capabilities.complementary visual knowledge.Efficient visual encoding models use techniques like token processing to manage highresolution images without excessive computational burden.High-resolution images are input into lightweight visual encoders, resized, and segmented to generate initial visual tokens.These tokens are compressed by visual token compression modules to reduce computational and storage overhead.Compressed tokens are projected into the language model's feature space by efficient visual-language projectors, aligned with text tokens.A small language model combines and processes these aligned visual features and text tokens, generating language responses.LLaVA-UHD [67] introduces an image modular strategy, dividing images into smaller fragments for efficient encoding, reducing computational load while maintaining perceptual ability.Advances in visual encoders also include MAE (Masked Autoencoders) [68], a self-supervised learning method that learns image representations by masking and reconstructing parts of input images.\n\nText encoders are another crucial component of multimodal models, used to process and understand textual data.Transformers are a common text encoding architecture, with selfattention mechanisms efficiently capturing long-range dependencies in text.BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained model based on Transformers, generating high-quality text representations through bidirectional training on large-scale corpora, widely applied in various natural language processing tasks.\n\nIn audio encoding, AudioCLIP [69] is an effective choice, generating audio representations by combining audio and text information.AudioCLIP uses an architecture similar to CLIP, aligning audio, text, and image features in the same feature space through contrastive learning.This method enhances audio data representation and improves multimodal model performance in audio-text and audio-image tasks.",
            "score": 0.47108735754461806,
            "section_title": "Audio Diffusion",
            "char_start_offset": 46635,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 7
                },
                {
                    "start": 7,
                    "end": 56
                },
                {
                    "start": 56,
                    "end": 257
                },
                {
                    "start": 257,
                    "end": 436
                },
                {
                    "start": 436,
                    "end": 467
                },
                {
                    "start": 467,
                    "end": 608
                },
                {
                    "start": 608,
                    "end": 732
                },
                {
                    "start": 732,
                    "end": 841
                },
                {
                    "start": 841,
                    "end": 979
                },
                {
                    "start": 979,
                    "end": 1102
                },
                {
                    "start": 1102,
                    "end": 1283
                },
                {
                    "start": 1283,
                    "end": 1476
                },
                {
                    "start": 1478,
                    "end": 1588
                },
                {
                    "start": 1588,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1995
                },
                {
                    "start": 1997,
                    "end": 2128
                },
                {
                    "start": 2128,
                    "end": 2272
                },
                {
                    "start": 2272,
                    "end": 2397
                }
            ],
            "ref_mentions": [
                {
                    "start": 1350,
                    "end": 1354,
                    "matchedPaperCorpusId": "243985980"
                },
                {
                    "start": 2026,
                    "end": 2030,
                    "matchedPaperCorpusId": "235624127"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89794921875
        },
        {
            "corpus_id": "270319618",
            "title": "A survey on advancements in image-text multimodal models: From general techniques to biomedical implementations",
            "text": "Multimodal models has begun to adopt an end-to-end training method, which means that the model can learn the characteristics of images and text simultaneously within a single framework, rather than processing them separately.This approach enables the model to better capture the complex relationship between visual and textual information.The high degree of integration and collaboration of this model relies on two key components: text encoder and visual encoder.\n\nThe main responsibility of the text encoder module is to understand and process natural language text.By applying deep learning and natural language processing techniques, especially Transformer architecture-based models such as BERT and GPT, these encoders are able to deeply analyze the semantics and structure of text.They are not only able to identify keywords and phrases in text, but also understand more complex language structures and meanings, allowing the model to understand language information at a deeper level.\n\nMeanwhile, the visual encoder module focuses on extracting and understanding visual information from images or videos [195,196,197].These encoders are usually based on convolutional neural networks (CNN) or Transformerbased vision models that have emerged in recent years, such as Vision Transformer (ViT).Their function is to capture elements such as objects, textures, colors, etc. in the image and convert this visual information into a form that allows the model to be further processed and analyzed.\n\nImage-text multimodal models combine information from image and text modalities.For example, CLIP [38], a visual encoder models image information, while a text encoder models text information.The feature vectors of images and text can be pre-calculated and stored, and the modal interaction is handled through the cosine similarity of the image and text feature vectors.Since CLIP was proposed, it has become a classic in the field of multimodal learning and is widely used as the basic model of today's multimodal models.CLIP uses 400 million pairs of (image, text) data for training through self-supervision.It maps images and text into a shared vector space, allowing the model to understand the semantic relationship between images and text.This approach represents a new way to learn visual models under natural language supervision.",
            "score": 0.4696672102230537,
            "section_title": "Recent Image-text Multimodal Models",
            "char_start_offset": 14620,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 225,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 464
                },
                {
                    "start": 466,
                    "end": 568
                },
                {
                    "start": 568,
                    "end": 787
                },
                {
                    "start": 787,
                    "end": 991
                },
                {
                    "start": 993,
                    "end": 1125
                },
                {
                    "start": 1125,
                    "end": 1299
                },
                {
                    "start": 1299,
                    "end": 1497
                },
                {
                    "start": 1499,
                    "end": 1579
                },
                {
                    "start": 1579,
                    "end": 1691
                },
                {
                    "start": 1691,
                    "end": 1869
                },
                {
                    "start": 1869,
                    "end": 2021
                },
                {
                    "start": 2021,
                    "end": 2109
                },
                {
                    "start": 2109,
                    "end": 2244
                },
                {
                    "start": 2244,
                    "end": 2337
                }
            ],
            "ref_mentions": [
                {
                    "start": 1111,
                    "end": 1116,
                    "matchedPaperCorpusId": "86426900"
                },
                {
                    "start": 1116,
                    "end": 1120,
                    "matchedPaperCorpusId": "230435805"
                },
                {
                    "start": 1120,
                    "end": 1124,
                    "matchedPaperCorpusId": "243985875"
                },
                {
                    "start": 1597,
                    "end": 1601,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94287109375
        },
        {
            "corpus_id": "270285703",
            "title": "Understanding Information Storage and Transfer in Multi-modal Large Language Models",
            "text": "Multimodal Large Language Models.. We consider a MLLM to be a model that takes an image and text as input, and generates a text output [2].Over the last year, such models have made tremendous advances in tasks like VQA and image captioning, including BLIP [15], BLIP-2 [16], Instruct-BLIP [7], LLaVA [18,19], Flamingo [3] and multi-modal Phi-2 (from the Bunny repo) [11].These MLLMs can broadly be categorized into two families based on how their visual information is integrated into the language model: (i) by embedding the vision encoder's output into each layer of the language model with a cross-attention layer (e.g., Flamingo, BLIP) or, (ii) by mapping the vision encoder's output into \"visual tokens\" in the language model's input space (i.e.alongside the text tokens) via a projection layer (e.g., LLaVA, Bunny).Both families are widely used, however, the projection layer family has recently shown stronger performance on popular benchmark [18,19,11].We, therefore, focus our study of information storage and transfer on this model family.",
            "score": 0.46918502581100985,
            "section_title": "Related Works",
            "char_start_offset": 4494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 139,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 750
                },
                {
                    "start": 750,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1049
                }
            ],
            "ref_mentions": [
                {
                    "start": 269,
                    "end": 273,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 289,
                    "end": 292,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 304,
                    "end": 307,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 318,
                    "end": 321,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 954,
                    "end": 957,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87158203125
        },
        {
            "corpus_id": "270319618",
            "title": "A survey on advancements in image-text multimodal models: From general techniques to biomedical implementations",
            "text": "MiniGPT-4 [49] fixes the parameters of the language model and the visual model, and then uses only the projection layer to align the language decoder with LLaMA and the visual encoder with BLIP-2.LRV-Instruction effectively reduces hallucinations produced by LMMs, and maintaining a balanced ratio of positive and negative instances leads to a more robust model [198].MiniGPT-5 [50] combines the Stable Diffusion mechanism with LLM through special visual tokens.In addition to its original multi-modal understanding and text generation capabilities, it can also provide reasonable and coherent multi-modal output.Structure-CLIP [51]proposes a Knowledge Enhanced Encoder (KEE) that takes SGK as input to enhance multi-modal structured representation by integrating scene graph knowledge (SGK).In order to maximize the complementarity of highly similar tasks, CLIPbased MmAP [52] utilizes a gradient-driven task grouping method to divide tasks into several disjoint groups and assign a group-shared parameter to each group.MM-Interleaved [53] achieves direct access to fine-grained image features in the previous context during the generation process by introducing a multi-scale multi-image feature synchronizer module.This module enables models to be pre-trained endto-end on paired and interleaved image-text corpora to more efficiently follow complex cross-modal instructions.\n\nIn the real world, data are not always complete, and situations where certain modalities are missing may be encountered.This results in degraded model performance.Integrating the missing-aware prompt method into Transformerlike multimodal models is straightforward, which can alleviate the impact of missing modalities and enhance the robustness of the models [199,200,201,202,203].\n\nThe technical development of large multimodal models for vision and text has gone through many important stages, and it has essentially formed a unified paradigm that uses large language models as the core and leverages highquality datasets to enhance the fusion of visual and textual modalities.With the development of multimodal technology, modal fusion occupies a core position in the technological advancement of large multimodal models for vision and text.",
            "score": 0.4689700894540991,
            "section_title": "Recent Image-text Multimodal Models",
            "char_start_offset": 21037,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 196,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 462
                },
                {
                    "start": 462,
                    "end": 613
                },
                {
                    "start": 613,
                    "end": 792
                },
                {
                    "start": 792,
                    "end": 1021
                },
                {
                    "start": 1021,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1378
                },
                {
                    "start": 1380,
                    "end": 1500
                },
                {
                    "start": 1500,
                    "end": 1543
                },
                {
                    "start": 1543,
                    "end": 1762
                },
                {
                    "start": 1764,
                    "end": 2060
                },
                {
                    "start": 2060,
                    "end": 2225
                }
            ],
            "ref_mentions": [
                {
                    "start": 1740,
                    "end": 1745,
                    "matchedPaperCorpusId": "257365349"
                },
                {
                    "start": 1745,
                    "end": 1749,
                    "matchedPaperCorpusId": "248118952"
                },
                {
                    "start": 1749,
                    "end": 1753,
                    "matchedPaperCorpusId": "232170317"
                },
                {
                    "start": 1753,
                    "end": 1757,
                    "matchedPaperCorpusId": "248476361"
                },
                {
                    "start": 1757,
                    "end": 1761,
                    "matchedPaperCorpusId": "236459819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82666015625
        },
        {
            "corpus_id": "268733063",
            "title": "A Survey on Large Language Models from Concept to Implementation",
            "text": "Implementing interactive systems presents a significant challenge due to the necessity for the system to generate coherent responses to multimodal input.Earlier AI-driven NLP tools, taking Amazon Alexa as an example, faced difficulties in effectively handling sentences that combined multiple languages.Given a query like \"What is your Lieblingsfarbe?\" -a mix of English and German -the response was often restricted to \"I don't know that one.\"Such challenges arise from the difficulties of navigating diverse language datasets.The task becomes even more complicated when dealing with varied modalities, including both text and visuals.\n\nConsidering the interaction between text and image, the difficulty isn't just about object recognition but also understanding styles and extracting deeper visual meanings.In this context, the GPT-4 with vision (GPT-4V) model, as discussed in OpenAI [2023a,b], merges the capabilities of text-focused and vision-focused models.This model is trained on a combination of textual and visual data and is further optimized using the reinforcement learning from the human feedback approach, ensuring the results resonate with human expectations Ouyang et al. [2022].The GPT-4V's capability to handle multimodal inputs is achieved through a systematic process, specifically -\u2022 Extract image features using the image recognition model.These features encompass pixel values, semantic information, etc.\n\n\u2022 Derive textual features using NLP models.These encapsulate word vectors, syntactic structures, etc.\n\n\u2022 Fusion the extracted image and text features.The integrated features enable GPT-4V to achieve a comprehensive understanding of the context and generate linguistically coherent captions.\n\nBy integrating interactive modes into LLMs, GPT-4V can analyze user-provided visual content, address a broader range of tasks, and enhance user experiences.\n\nThe use of GPT-4V in a real-time analysis showed an interesting result during a Dall-E 3 test.The famous banana painting by Andy Warhol, as shown in Figure 5(a), was fed into GPT-4V, requesting a detailed description.Then the GPT-generated text was fed back into Dall-E 3, which results in a recreated image, as depicted in Figure 5(b).",
            "score": 0.46884129093782095,
            "section_title": "LLMs with Interactive Systems",
            "char_start_offset": 38688,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 153,
                    "end": 303
                },
                {
                    "start": 303,
                    "end": 444
                },
                {
                    "start": 444,
                    "end": 528
                },
                {
                    "start": 528,
                    "end": 636
                },
                {
                    "start": 638,
                    "end": 809
                },
                {
                    "start": 809,
                    "end": 964
                },
                {
                    "start": 964,
                    "end": 1197
                },
                {
                    "start": 1197,
                    "end": 1364
                },
                {
                    "start": 1364,
                    "end": 1429
                },
                {
                    "start": 1431,
                    "end": 1474
                },
                {
                    "start": 1474,
                    "end": 1532
                },
                {
                    "start": 1534,
                    "end": 1581
                },
                {
                    "start": 1581,
                    "end": 1721
                },
                {
                    "start": 1723,
                    "end": 1879
                },
                {
                    "start": 1881,
                    "end": 1975
                },
                {
                    "start": 1975,
                    "end": 2098
                },
                {
                    "start": 2098,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 1176,
                    "end": 1196,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.822265625
        },
        {
            "corpus_id": "273022981",
            "title": "Exploring Social Media Image Categorization Using Large Models with Different Adaptation Methods: A Case Study on Cultural Nature's Contributions to People",
            "text": "LVLMs have emerged as a natural evolution of integrating LLMs with LVMs, enabling seamless multimodal interaction. By utilizing machine-generated instruction-following data during training, these models facilitate more dynamic and interactive communication between humans and machines. LVLMs are capable of processing both visual and textual inputs, generating coherent and contextually relevant responses in natural language. \n\nThese models serve as general-purpose problem solvers, demonstrating good performance across various tasks-particularly when prompt engineering is used to guide the model, and when the task aligns with the domain on which the model was trained.Notable LVLMs include BLIP (Li et al. 2022), one of the earliest unified vision-language encoder-decoder architectures; LLaVA (Large Language and Vision Assistant) (Liu et al. 2023), an end-to-end multimodal model that connects a vision encoder to a language model for versatile visual-language understanding; and GPT-4 (Achiam et al. 2023), a proprietary foundation model capable of multimodal reasoning, accessible exclusively via APIs such as ChatGPT.",
            "score": 0.46867682491285334,
            "section_title": "Large Vision Language Models:",
            "char_start_offset": 14563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 1127
                }
            ],
            "ref_mentions": [
                {
                    "start": 700,
                    "end": 716,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9697265625
        },
        {
            "corpus_id": "278339632",
            "title": "A Comprehensive Survey of Large AI Models for Future Communications: Foundations, Applications and Challenges",
            "text": "3) Large multimodal model: LMMs are capable of simultaneously processing and understanding data from different modalities, such as vision, language, haptic and auditory. These models achieve comprehensive processing and reasoning of multimodal information by integrating the features of various modalities in a unified high-dimensional space. They utilize advanced neural network architectures, such as transformers and diffusion models, to extract features from each modality and optimize their representations through techniques such as contrastive learning and self-supervised learning. By training across multiple modalities, these models are able to understand and relate the semantic relationships between different modalities, thereby demonstrating superior performance in handling complex multimodal data and providing intelligent, efficient solutions. Unlike visual language models (VLMs), LMMs support modalities beyond vision and text [90]. \n\nLMMs show strong capabilities in processing multimodal information. Their core technical features are cross-modal fusion and multimodal representation learning: \n\n\u2022 Multimodal representation learning: Multimodal representation learning is an important technology of LMMs. \n\nIt integrates feature representations of different modalities such as images, speech and text into a unified highdimensional space. First, the LMM uses ViT to extract image features and uses a transformer to extract text and speech features. Then, these high-dimensional vectors are fused through methods such as splicing and weighted summation to form a unified feature representation. This fusion enables the LMM to better understand and associate information from different modalities, improving the performance of multimodal tasks. \u2022 Cross-modal fusion: LMMs integrate multiple data types such as text, images, audio and video through cross-modal fusion technology to achieve deeper understanding and analysis. These LMMs can process data from different modalities at the same time and learn the relationship between them. For example, the LMM can combine images with related text to generate richer descriptions; in video analysis, it can understand the visual content as well as the voice and text information in the video. In addition, these LMMs can also perform cross-modal reasoning and prediction, such as generating images or audio from text. Such capabilities make LMMs widely used in NLP, computer vision, speech recognition and other fields.",
            "score": 0.46866593378768817,
            "section_title": "Multimodal data",
            "char_start_offset": 37040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 951
                },
                {
                    "start": 954,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1114
                },
                {
                    "start": 1117,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2257
                },
                {
                    "start": 2258,
                    "end": 2382
                },
                {
                    "start": 2383,
                    "end": 2484
                }
            ],
            "ref_mentions": [
                {
                    "start": 946,
                    "end": 950,
                    "matchedPaperCorpusId": "265351653"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "264451774",
            "title": "Kiki or Bouba? Sound Symbolism in Vision-and-Language Models",
            "text": "We provide results on additional text-conditioned image generation models, covering various model architectures, pretraining data and input languages. We consider the SOTA text-to-image diffusion models DeepFloyd-IF [69] and Kandinsky [57], as well as the SOTA text-conditioned GAN model GALIP [72]. To evaluate these models, we use the methodology from our main paper. This includes prompts and evaluation metrics, calculated by embedding generated images with CLIP. \n\nWe use the following model checkpoints from Hugging Face Model Hub to evaluate DeepFloyd-IF and Kandinsky: DeepFloyd/IF-I-M-v1.0, kandinsky-community/kandinsky-2-1-prior, kandinsky-community/kandinsky-2-1. For GALIP, we use the official code implementation with the checkpoint trained on the CC12M dataset. 4: Quantitative results for additional multimodal models. We report results for additional SOTA text-to-image models with various architectures and pretraining data sources. We also report the results for the vision-and-language models considered in our work for comparison, as well as the random baseline indicated by (random). \n\nWe use the following inference settings: We run DeepFloyd-IF and Kandinsky in half-precision. \n\nFor DeepFloyd-IF, we only run the first stage of inference which outputs low-resolution (64 \u00d7 64) images. For Kandinsky, we use both prior and image-to-text pipelines with guidance scale set to 1.0 and 768 \u00d7 768 output. All other inference settings use the defaults from the model checkpoint configuration files and the DiffusionPipeline class from the Hugging Face diffusers library. \n\nFor GALIP, we use the default settings provided in the inference notebook in its official repository. For all of these models, we generate a single image for each pseudoword; for \"kiki\" and \"bouba\" generations to calculate \u2206P kb , we generate 10 images for each for each. \n\nIn Table 4, we show quantitative results for these models, which all show significant sound symbolic effects.",
            "score": 0.4679731878766161,
            "section_title": "C.1 Additional Multimodal Models",
            "char_start_offset": 39237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1201
                },
                {
                    "start": 1204,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1588
                },
                {
                    "start": 1591,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1862
                },
                {
                    "start": 1865,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 298,
                    "matchedPaperCorpusId": "256389964"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5703125
        },
        {
            "corpus_id": "272689156",
            "title": "Keypoint-Integrated Instruction-Following Data Generation for Enhanced Human Pose and Action Understanding in Multimodal Models",
            "text": "Our model follows a multimodal approach, integrating both image data and text prompts to facilitate a comprehensive understanding of visual and textual content. As shown in Figure 1, the architecture consists of several key components: \n\nInput Layer: The model accepts two types of inputs: image data and text prompts. Image data, such as photographs containing human actions and poses, are processed by the visual encoder. Text prompts, comprising natural language instructions or questions, are directly fed into the language model. This bifurcation allows the model to handle both visual and linguistic modalities simultaneously, thereby offering richer contextual understanding. \n\nVisual Encoder: The image data is processed through a visual encoder that extracts pertinent visual features. In our model, we utilize the pre-trained CLIP visual encoder [29], which efficiently captures detailed image representations that are crucial for understanding human-centric actions. The visual encoder's output is then projected into a space compatible with the LLaVA core's language processing abilities . \n\nLanguage Model (LLaVA Core): Text prompts bypass the visual encoder and are directly input into the LLaVA core, a language model built upon the Vicuna architecture [30], known for its strong instruction-following capabilities . The language model processes these prompts and the encoded visual features, aligning the semantic representations of both visual and textual data.",
            "score": 0.4675680229576741,
            "section_title": "Overall Architecture",
            "char_start_offset": 18015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 235
                },
                {
                    "start": 238,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1478
                }
            ],
            "ref_mentions": [
                {
                    "start": 856,
                    "end": 860,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91064453125
        },
        {
            "corpus_id": "273940916",
            "title": "Scalable multimodal approach for face generation and super-resolution using a conditional diffusion model",
            "text": "Multimodality integrating text, images, masks, semantic labels or sketches to synthesize face images have been demonstrated in the literature. \n\nXia et al. proposed a framework for text guided face image generation and manipulation using a GAN model 20 . This framework is able to generate high resolution face images from a set of multimodal inputs (images, text, sketches and semantic labels). It relies on manipulating the latent variables of a StyleGAN model 21 to generate a new or altered face image. \n\nNair et al. have formulated an approach for multimodal image generation, including face image generation, using multiple standalone diffusion models each trained for a certain sub-task and associated with a certain modality 22 . During sampling, their corresponding outputs are combined. This approach accepted text, semantic labels and sketches. \n\nHam et al. have devised a method to take advantage of already trained image generation diffusion models and adding the capability for multimodal synthesis to them 23 . This is done by modulating the sampling process through multimodal conditioning modules (MCM), so as to insert information from other inputs into the output image. This method accepted segmentation maps, sketches and texts as input. A similar method was proposed by Huang et al. 24 , but instead of multimodal conditioning modules, a dynamic diffuser was proposed, which selectively enhances or suppresses the influence of a given modality. Huang et al. 's work utilized pretrained diffusion models and was shown to work with images, texts and masks. \n\nRen et al. have presented a flexible, scalable and adaptive face image synthesis model conditioned on a combination of text descriptions, masks, poses, expressions, lightning and sketches 25 . This model was built upon a diffusion model, and by using uni-modal training with modal surrogates and adapting its processes based on entropy, it ensures high-quality outputs and robust inter-modal collaboration. \n\nIt is evident that there an absence of incorporating audio data in current multimodal face image generation techniques, where voice helps synthesize the face image of the speaker. Flexible solutions that can work with any modality, which are architecturally simple and which are scalable are too few in the literature. This work aims to solve these problems.",
            "score": 0.46746447619210285,
            "section_title": "Multimodal conditioned face image generation",
            "char_start_offset": 6932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 145,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1576
                },
                {
                    "start": 1579,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1985
                },
                {
                    "start": 1988,
                    "end": 2167
                },
                {
                    "start": 2168,
                    "end": 2306
                },
                {
                    "start": 2307,
                    "end": 2346
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 252,
                    "matchedPaperCorpusId": "235702618"
                },
                {
                    "start": 463,
                    "end": 465,
                    "matchedPaperCorpusId": "54482423"
                },
                {
                    "start": 733,
                    "end": 735,
                    "matchedPaperCorpusId": "261091932"
                },
                {
                    "start": 1021,
                    "end": 1023,
                    "matchedPaperCorpusId": "257205737"
                },
                {
                    "start": 1305,
                    "end": 1307,
                    "matchedPaperCorpusId": "258236244"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76025390625
        },
        {
            "corpus_id": "272986599",
            "title": "Multimodal Pragmatic Jailbreak on Text-to-image Models",
            "text": "Text-to-image (T2I) models have shown unprecedented capabilities in synthesizing high-quality images that closely adhere to textual prompts [49,45,51]. However, recently the community has raised concerns regarding the potential for these models to generate unsafe content, i.e., images that might be offensive, disturbing, hateful, sexually explicit, or otherwise inappropriate [47,52,42]. Concurrently, a growing body of work focused on the jailbreak on Large Language Models (LLMs) [68,76,37,1] to generate harmful texts through bypassing existing safety protocols. Moreover, Multimodal Large Language Models (MLLMs) [34,24,75], have been shown to be more vulnerable to jailbreak and generate unsafe texts when integrating the visual modality [17,57,41]. \n\nIn this work, we introduce a novel form of jailbreak for T2I models, which we term multimodal pragmatic jailbreak, which triggers T2I models to generate the image with visual typographic texts on it, so-called visual text rendering [25], using prompts like \"an image of <image-generation prompt>, with a sign that says, '<visual text prompt>'\". In this scenario, the image and text content may each be considered safe in isolation, yet their combination can lead to unsafeness. This form of jailbreak is also inspired by pragmatics, which as defined in linguistics [5,18], is considered as the context-based interpretation of language use, where meanings derive not only from the linguistic system itself, but also from contextual factors, like background information, or moving beyond a focus on language (e.g. visual resources), a concept known as multimodal pragmatics [38]. In image generation with visual text by T2I models, the integration of visual and textual information can introduce complex interactions between modalities, such as allusion, exaggeration, sarcasm, and contrast, leading the resulting images to be unsafe, as demonstrated in Fig. 1. This underscores the need for careful consideration of both modalities in the generation process to ensure the safety of the outputs.",
            "score": 0.4674439980244506,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 756
                },
                {
                    "start": 759,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 144,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 147,
                    "end": 150,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 382,
                    "end": 385,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 385,
                    "end": 388,
                    "matchedPaperCorpusId": "258841623"
                },
                {
                    "start": 491,
                    "end": 494,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 623,
                    "end": 626,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 749,
                    "end": 752,
                    "matchedPaperCorpusId": "260203143"
                },
                {
                    "start": 752,
                    "end": 755,
                    "matchedPaperCorpusId": "259244034"
                },
                {
                    "start": 991,
                    "end": 995,
                    "matchedPaperCorpusId": "254877579"
                },
                {
                    "start": 1324,
                    "end": 1327,
                    "matchedPaperCorpusId": "153411858"
                },
                {
                    "start": 1327,
                    "end": 1330,
                    "matchedPaperCorpusId": "11089401"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6396484375
        },
        {
            "corpus_id": "275357957",
            "title": "Feedback-Driven Vision-Language Alignment with Minimal Human Supervision",
            "text": "Vision-Language Models Generative VLMs are multimodal systems processing both text and images. LLaVA-like architectures (Fig. 6, left) integrate a visual encoder v \u03b8 (c v ), text encoder t \u03b8 (c t ), visual-text alignment adapter a \u03b8 , and large language model. The model p \u03b8 generates token trajectories z from conditioning c for various downstream tasks. These systems undergo three training phases: multimodal pre-training, visual-text alignment, and instruction tuning [117,55,47], enabling broad cross-modal capabilities. Vision-Language Grounding Grounding links language descriptions to spatial regions in images. A grounding model g(z, c v ) processes visual c v and textual z inputs to produce open-set detection labels and bounding boxes (Fig. 6, right). While traditional object detection uses fixed-class classification, modern approaches like GLIP and GroundingDINO reframe detection as text-guided grounding. This flexibility enables broader applications in detection and spatial understanding tasks.",
            "score": 0.4670660005940179,
            "section_title": "Visual Projection Grounding",
            "char_start_offset": 7284,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1013
                }
            ],
            "ref_mentions": [
                {
                    "start": 480,
                    "end": 483,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94287109375
        },
        {
            "corpus_id": "266162752",
            "title": "4M: Massively Multimodal Masked Modeling",
            "text": "Large language models have been demonstrated to be capable of performing a diverse range of tasks out of the box [86,12,81,25,45] by training on large datasets with simple objectives [30,84,86,106]. \n\nIn vision, however, many scaling efforts have instead focused on training specialized models on a single task and modality, such as predicting masked RGB pixels [20,31,4,48,118,34], discrete tokens [7,132], or other (deep) features [117,6,82,114,36,70] from RGB inputs. Training models instead on multiple tasks [16,33,63,42,92,11] and modalities [85,122,76,59,133,1,3,57,43], or both [54,103,114,5,44] usually requires modality-specific modeling choices, making it difficult to extend these methods. \n\nWhile some recent works aim to consolidate various modalities and tasks by representing them as images [77,8,115], these approaches have limitations when dealing with modalities that cannot be readily converted into images, such as text or neural network feature maps. Instead, 4M adopts the approach of Pix2Seq [21,22] and Unified-IO [74] which addresses these issues by unifying the representation space on which models are trained through tokenization [110,35,30]. However, unlike methods like Unified-IO which operate on a single RGB image tokenizer, 4M's ability to work with multiple modality-specific tokenizers enables scaling to visual modalities beyond those that can be represented as images, such as neural network feature maps. 4M also builds upon the multimodal masking approach of MultiMAE [5] and extends it beyond image-like modalities. \n\nBoth token-based generative models [88,123,17,18,65] and diffusion models [89,79,95,97] have been mostly limited text-to-image generation.",
            "score": 0.46656105109528984,
            "section_title": "Related Work",
            "char_start_offset": 29223,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 701
                },
                {
                    "start": 704,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1557
                },
                {
                    "start": 1560,
                    "end": 1698
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 117,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 183,
                    "end": 187,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 190,
                    "end": 193,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 193,
                    "end": 197,
                    "matchedPaperCorpusId": "252780443"
                },
                {
                    "start": 362,
                    "end": 366,
                    "matchedPaperCorpusId": "219781060"
                },
                {
                    "start": 366,
                    "end": 369,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 371,
                    "end": 374,
                    "matchedPaperCorpusId": "243985980"
                },
                {
                    "start": 374,
                    "end": 378,
                    "matchedPaperCorpusId": "244346275"
                },
                {
                    "start": 399,
                    "end": 402,
                    "matchedPaperCorpusId": "235436185"
                },
                {
                    "start": 402,
                    "end": 406,
                    "matchedPaperCorpusId": "244117494"
                },
                {
                    "start": 433,
                    "end": 438,
                    "matchedPaperCorpusId": "245218767"
                },
                {
                    "start": 438,
                    "end": 440,
                    "matchedPaperCorpusId": "246652264"
                },
                {
                    "start": 513,
                    "end": 517,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 517,
                    "end": 520,
                    "matchedPaperCorpusId": "102496818"
                },
                {
                    "start": 520,
                    "end": 523,
                    "matchedPaperCorpusId": "8070108"
                },
                {
                    "start": 523,
                    "end": 526,
                    "matchedPaperCorpusId": "237291521"
                },
                {
                    "start": 529,
                    "end": 532,
                    "matchedPaperCorpusId": "248834301"
                },
                {
                    "start": 548,
                    "end": 552,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 552,
                    "end": 556,
                    "matchedPaperCorpusId": "248512473"
                },
                {
                    "start": 556,
                    "end": 559,
                    "matchedPaperCorpusId": "235694621"
                },
                {
                    "start": 559,
                    "end": 562,
                    "matchedPaperCorpusId": "236635379"
                },
                {
                    "start": 562,
                    "end": 566,
                    "matchedPaperCorpusId": "244799261"
                },
                {
                    "start": 568,
                    "end": 570,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 573,
                    "end": 576,
                    "matchedPaperCorpusId": "258564264"
                },
                {
                    "start": 586,
                    "end": 590,
                    "matchedPaperCorpusId": "237204499"
                },
                {
                    "start": 590,
                    "end": 594,
                    "matchedPaperCorpusId": "244954250"
                },
                {
                    "start": 598,
                    "end": 600,
                    "matchedPaperCorpusId": "247939918"
                },
                {
                    "start": 600,
                    "end": 603,
                    "matchedPaperCorpusId": "249712367"
                },
                {
                    "start": 811,
                    "end": 813,
                    "matchedPaperCorpusId": "251979350"
                },
                {
                    "start": 813,
                    "end": 817,
                    "matchedPaperCorpusId": "254246343"
                },
                {
                    "start": 1016,
                    "end": 1020,
                    "matchedPaperCorpusId": "237593074"
                },
                {
                    "start": 1020,
                    "end": 1023,
                    "matchedPaperCorpusId": "249674493"
                },
                {
                    "start": 1039,
                    "end": 1043,
                    "matchedPaperCorpusId": "249848272"
                },
                {
                    "start": 1159,
                    "end": 1164,
                    "matchedPaperCorpusId": "20282961"
                },
                {
                    "start": 1164,
                    "end": 1167,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 1167,
                    "end": 1170,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1509,
                    "end": 1512,
                    "matchedPaperCorpusId": "247939918"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80322265625
        },
        {
            "corpus_id": "274023126",
            "title": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
            "text": "These output modalities also correspond to specific multimodal model architectures, specifically any-totext by LLM-backbone models and any-to-image by diffusionbackbone models. While these surveys have made valuable contributions, they lack a unified framework that spans a broad range of modalities (i.e., text, images, audio, and video) within different generative systems, as shown in Fig. 1.",
            "score": 0.46644915669159726,
            "section_title": "Any-to-Text Models Any-to-Vision Models Any-to-Any Models",
            "char_start_offset": 3538,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 395
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7080078125
        },
        {
            "corpus_id": "274023123",
            "title": "Spider: Any-to-Many Multimodal LLM",
            "text": "To build foundational Multimodal LLMs (MLLMs), researchers align pre-trained encoders from various modalities with the textual space of LLMs, enabling them to process multimodal inputs [2,23,28,30,37,53,73]. For example, Flamingo [2] connects a fixed image encoder to LLMs using cross-attention, while LLaVA [37] links image and word spaces via projection. BLIP-2 [30] uses a Q-Former to translate image queries into LLMs. Similar approaches are applied to videos (e.g., Video-Chat [31], Video-LLaMA [71]) and audios (e.g., SpeechGPT [70]). PandaGPT [54] extends this to six modalities using ImageBind [17]. \n\nHowever, existing MLLMs only perceive multimodal data and cannot generate content in arbitrary modalities. To address this, approaches like Visual-ChatGPT [64], Hug-gingGPT [50], and AudioGPT [22] use LLMs as decisionmakers, incorporating external multimodal encoders and decoders for multimodal input-output. Despite this, discrete text-message-based pipelines can introduce noise and hinder semantic understanding. NExT-GPT [66] overcomes this by learning an end-to-end multimodal input-output LLM, capable of handling any combination of text, image, video, and audio. \n\nHowever, these X-to-X MLLMs are limited to generating pairwise modalities 'Text + X' within a single interaction. In contrast, our proposed X-to-Xs Spider model aims for Any-to-Many Modalities Generation in a single response, supporting arbitrary combinations of a wider range of modalities as shown in Fig. 8, including text, image, audio, video, box, and mask.",
            "score": 0.466275070472813,
            "section_title": "C.2. Large Multimodal Models",
            "char_start_offset": 23157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 607
                },
                {
                    "start": 610,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1545
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 188,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 194,
                    "end": 197,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 230,
                    "end": 233,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 364,
                    "end": 368,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 602,
                    "end": 606,
                    "matchedPaperCorpusId": "258564264"
                },
                {
                    "start": 1036,
                    "end": 1040,
                    "matchedPaperCorpusId": "261696650"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9345703125
        },
        {
            "corpus_id": "266998744",
            "title": "Developing ChatGPT for biology and medicine: a complete review of biomedical question answering",
            "text": "Transformer with a language model, showcasing improvements in data efficiency and transfer learning across various robot embodiments. \n\nOn the language understanding front, GPT-3.5, an enhanced version of GPT-3, addresses limitations in language understanding and generation. With its advanced architecture and the use of prompt engineering and ensemble methods, GPT-3.5 achieves state-of-the-art results on various language tasks and shows promise in medical benchmarks, enabling more accurate and contextually relevant responses. Building upon the successes of its predecessors, GPT-4 (Nori et al, 2023) further improves language generation with an advanced architecture that captures longrange dependencies and excels in understanding multimodal content. This model demonstrates enhanced performance and generates more coherent and contextually relevant text. MiniGPT-v2 (Chen et al, 2023) serves as a unified interface for vision-language tasks, achieving strong performance on visual question answering and grounding. MiniGPT-4 further enhances multimodal capabilities by combining a frozen visual encoder with a large language model. NExT-GPT (Wu et al, 2023) is an end-to-end multimodal language model capable of processing any combination of text, images, videos, and audio. It achieves universal multimodal understanding and supports any-to-any modality input and output. \n\nOverall, these models have made substantial strides in enhancing visual language modeling, effectively tackling various challenges in tasks such as image understanding, language comprehension, and generation within multimodal contexts.",
            "score": 0.4656410870214579,
            "section_title": "combines a Vision",
            "char_start_offset": 29797,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 136,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1618
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9560546875
        },
        {
            "corpus_id": "264405778",
            "title": "Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds",
            "text": "Figure 4 illustrates the overall architecture of our proposed model. Steve-Eye, functioning as a generative model, connects an image-oriented tokenizer f v with the pre-trained LLM backbone \u0398. \n\nWe adopt the image tokenizer, e.g., VQ-GAN (Esser et al., 2021), to encode the raw images I into token embeddings V = {v 1 , v 2 , ..., v n } \u2208 R n\u00d7d , where n denotes the number of visual tokens and d is the dimensionality of each token. We further utilize a lightweight projection module f l with a trainable projection matrix W . This module maps the visual tokens to the same space with text embeddings, yielding V = {v 1 , v2 , ..., vn } \u2208 R n\u00d7 d: \n\nTo effectively process visual-language inputs and generate corresponding outputs, our model integrates the visual codebook C v into the pre-existing language vocabulary C l . This integration leads to the formation of a unified multimodal codebook, denoted as \n\nAdditionally, in order to mark the starting and ending points of visual elements in I/O sequences, we introduce two special tokens, namely <vis> and </vis>. The LLM backbone \u0398 of our Steve-Eye is built upon a decoder-only architecture with casual transformers. Our model employs an auto-regressive prediction mechanism, generating responses based on the provided multimodal input tokens. The resulting response is a mixed sequence of visual and textual tokens, represented as Y = {y 1 , y 2 , ..., y m }. For each embedding y i , we pass it through a linear layer f p followed by a softmax operation, mapping it into a probability distribution of the multimodal vocabulary. The final prediction for the i-th token z i is determined by selecting the token from the multimodal codebook with the highest score: \n\n(2)",
            "score": 0.4655163878484814,
            "section_title": "MODEL ARCHITECTURE",
            "char_start_offset": 15142,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 192
                },
                {
                    "start": 195,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 647
                },
                {
                    "start": 650,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 909
                },
                {
                    "start": 912,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1719
                },
                {
                    "start": 1722,
                    "end": 1725
                }
            ],
            "ref_mentions": [
                {
                    "start": 238,
                    "end": 258,
                    "matchedPaperCorpusId": "229297973"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83935546875
        },
        {
            "corpus_id": "276782523",
            "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
            "text": "Generating visual contents from text and describing them through natural language have been extensively studied as core multimodal tasks. Advanced generative models [3,5,6,8,12,14,17,20,40,41,47,49,50,54,[82][83][84], such as DALL\u2022E [46,48], Stable Diffusion [51], demonstrate remarkable generation capabilities, producing high-quality and diverse contents from textual prompts. Meanwhile, image captioning models [26,28,29,66,67,69], such as mPLUG [34], and BLIP [36], push the boundaries of visual understanding, generating accurate and context-aware descriptions. Additionally, recent advancements in multimodal large language models [35], such as LLaVA [39], MiniGPT-4 [86], and InstructBLIP [11], have significantly improved the ability to understand and reason about visual content. \n\nBesides the powerful foundational generative models, adapting or customizing them attracts increasing interest, which enables more personalized and specific outputs based on user preferences [7,9,18,21,32,33,42,75]. Approaches like DreamBooth [52] enable user-specific customization by fine-tuning generative models with personal data, allowing the generation of content tailored to individual needs or preferences.",
            "score": 0.46522999879878274,
            "section_title": "Multimodal Generation and Understanding",
            "char_start_offset": 5148,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 1206
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 174,
                    "matchedPaperCorpusId": "219781060"
                },
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 183,
                    "end": 186,
                    "matchedPaperCorpusId": "258960192"
                },
                {
                    "start": 198,
                    "end": 201,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 201,
                    "end": 204,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 204,
                    "end": 208,
                    "matchedPaperCorpusId": "257279850"
                },
                {
                    "start": 208,
                    "end": 212,
                    "matchedPaperCorpusId": "263909602"
                },
                {
                    "start": 233,
                    "end": 237,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 414,
                    "end": 418,
                    "matchedPaperCorpusId": "52947736"
                },
                {
                    "start": 418,
                    "end": 421,
                    "matchedPaperCorpusId": "251564151"
                },
                {
                    "start": 421,
                    "end": 424,
                    "matchedPaperCorpusId": "244527510"
                },
                {
                    "start": 427,
                    "end": 430,
                    "matchedPaperCorpusId": "246634906"
                },
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 673,
                    "end": 677,
                    "matchedPaperCorpusId": "258291930"
                },
                {
                    "start": 985,
                    "end": 987,
                    "matchedPaperCorpusId": "259951373"
                },
                {
                    "start": 990,
                    "end": 993,
                    "matchedPaperCorpusId": "257985512"
                },
                {
                    "start": 996,
                    "end": 999,
                    "matchedPaperCorpusId": "254408780"
                },
                {
                    "start": 999,
                    "end": 1002,
                    "matchedPaperCorpusId": "253581838"
                },
                {
                    "start": 1002,
                    "end": 1005,
                    "matchedPaperCorpusId": "244345805"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8330078125
        },
        {
            "corpus_id": "268793702",
            "title": "Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving",
            "text": "Initially designed to operate on sequence data, Transformers [32] achieved state-of-the-art performance for natural language processing tasks.This propelled the development of Large Language Models, which learn general statistical properties of language through pretraining Encoder [9], Encoder-Decoder [28], and Decoder [2,26,31] Transformer architectures on a large corpus of tokens.These pretrained models can then be finetuned for downstream, more specialized language tasks.Dosovitskiy et al. [10] introduced the application of Transformers to image tasks with the Vision Transformer (ViT), which converts images into a sequence representation of image patches that can be processed by Transformers.Vision-Language Models bridge the gap between LLMs and Vision Transformers, encoding images and text into a combined latent representation and then utilizing cross-modal pre-training tasks to learn text and image correlations.This general approach to multimodal learning has sparked a variety of vision-language models.Radford et al. [27] devise a pre-training task of matching text captions with images to develop CLIP, which learns state-of-the-art image representations and exhibits strong zero-shot transfer capabilities for many image classification tasks.BLIP-2 [19] introduces a two stage pretraining process to train a Querying Transformer \"QFormer\" that serves as a intermediary between a frozen image encoder and language model.This approach outperforms much larger vision-language models such as Flamingo [1] and is capable of zero-shot image-to-text generation.Instruct-BLIP [7] builds off BLIP-2 and is a general-purpose VLM that aggregates public vision-language datasets and transforms them into an instruction tuning format.The VLM most similar to the model introduced in this paper is VL-T5 [6], which extends a pre-trained T5 to learn to generate text labels conditioned on a combination of a text and image embedding.Using a pre-trained LLM as a framework for multi-modal tasks harnesses the text generation ability of these models, critical for the question-answering task in our research.",
            "score": 0.46479691261569833,
            "section_title": "Vision-Language Models",
            "char_start_offset": 3345,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 142,
                    "end": 385
                },
                {
                    "start": 385,
                    "end": 479
                },
                {
                    "start": 479,
                    "end": 704
                },
                {
                    "start": 704,
                    "end": 930
                },
                {
                    "start": 930,
                    "end": 1023
                },
                {
                    "start": 1023,
                    "end": 1265
                },
                {
                    "start": 1265,
                    "end": 1442
                },
                {
                    "start": 1442,
                    "end": 1577
                },
                {
                    "start": 1577,
                    "end": 1744
                },
                {
                    "start": 1744,
                    "end": 1940
                },
                {
                    "start": 1940,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 324,
                    "end": 327,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1038,
                    "end": 1042,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1520,
                    "end": 1523,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1812,
                    "end": 1815,
                    "matchedPaperCorpusId": "231802355"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91552734375
        },
        {
            "corpus_id": "273234426",
            "title": "VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning",
            "text": "We have introduced the first comprehensive benchmark suite VL-ICL Bench for multi-modal visionand-language in-context learning. This benchmark suite avoids the issue with the existing main-stream but limited approach to evaluating image-to-text ICL -that ICL provides limited demonstrable benefit over zero-shot inference, and VLLMs learn answer formatting at best rather than any true multi-modal capability. In contrast, VL-ICL Bench tests a wide variety of multi-modal capabilities including both text-to-image and image-to-text generation, fine-grained perception, rule-induction, reasoning, image interleaving, fast concept binding, long context, and shot scaling. We hope this benchmark will inspire model developers to consider all these capabilities in VLLM development, and inform practitioners about the evolution of what VLLM ICL can and cannot do as the field develops. One limitation of this work is that we evaluate only a small number of text-to-image models, in contrast to the more comprehensive set of image-to-text models. This is due to the limited availability of VLLMs capable of handling interleaved inputs and generating images. Developing such models presents a promising direction for future research.",
            "score": 0.46456524207755007,
            "section_title": "Discussion",
            "char_start_offset": 27524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1227
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84130859375
        },
        {
            "corpus_id": "269187998",
            "title": "Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent",
            "text": "On the other hand, transformer-based architectures like ViT have recently gained popularity due to their ability to capture long-range dependencies and model global context, which is particularly useful for understanding the relationships between objects in an image.These architectures enable the model to extract meaningful features from the input image and convert them into a vector representation that can be integrated with the text input.\n\nAnother approach to encoding visual information is through image tokenization, which involves breaking down the image into smaller, discrete units or tokens.This method allows the model to process the image in a manner similar to how it processes text, enabling a more seamless integration of the two modalities.Tokenized image information can be fed into the model alongside the text input, allowing the model to attend to both modalities simultaneously and generate more accurate and contextually relevant outputs.For instance, the DALL-E model developed by OpenAI uses a variant of the VQ-VAE (Vector Quantized Variational Autoencoder) [40,35] to tokenize images, enabling the model to generate novel images based on textual descriptions.\n\nThe development of smaller, more efficient models that are capable of taking action based on a user's query and provided image has significant implications for the future of AI agents.These models can be deployed on resource-constrained devices, such as smartphones or IoT devices, enabling a wider range of applications and use cases.By leveraging the power of multimodal language models, these smaller agents can understand and respond to user queries in a more natural and intuitive manner, while also considering the visual context provided by the user.This opens up possibilities for more engaging and personalized interactions, such as virtual assistants that can provide visual recommendations based on a user's preferences or smart home devices that can adjust settings based on the user's facial expressions.\n\nMoreover, the development of multimodal AI agents has the potential to democratize AI technology, making it more accessible to a broader range of users and industries.Smaller, more efficient models can be trained on less powerful hardware, reducing the computational resources and energy consumption required for deployment.This could lead to the widespread adoption of AI agents in various domains, from healthcare and education to entertainment and e-commerce, ultimately transforming the way we live and work.",
            "score": 0.4644335183179704,
            "section_title": "body",
            "char_start_offset": 2195,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 267,
                    "end": 445
                },
                {
                    "start": 447,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 759
                },
                {
                    "start": 759,
                    "end": 963
                },
                {
                    "start": 963,
                    "end": 1188
                },
                {
                    "start": 1190,
                    "end": 1374
                },
                {
                    "start": 1374,
                    "end": 1525
                },
                {
                    "start": 1525,
                    "end": 1747
                },
                {
                    "start": 1747,
                    "end": 2007
                },
                {
                    "start": 2009,
                    "end": 2176
                },
                {
                    "start": 2176,
                    "end": 2333
                },
                {
                    "start": 2333,
                    "end": 2521
                }
            ],
            "ref_mentions": [
                {
                    "start": 1090,
                    "end": 1093,
                    "matchedPaperCorpusId": "232269702"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89013671875
        },
        {
            "corpus_id": "267760102",
            "title": "Model Composition for Multimodal Large Language Models",
            "text": "In this work, we examine the composition of a set of Multimodal Large Language Models (MLLMs), denoted as {M 1 , M 2 , . . . , M n }, each capable of responding to textual queries with different modalities m = {m 1 , m 2 , . . . , m n }. The core objective is to develop a composition method C that effectively integrates these MLLMs into a singular, more versatile model \n\nThis integration aims to enable the model to process and understand inputs from any combination of modalities in m. For example, by integrating two specialized MLLMs-a vision LLM and an audio LLM-the resulting composite model should not only preserve the individual proficiencies of these models in processing images and audio, respectively, but should also acquire a zero-shot capacity for handling inputs that encompass both visual and auditory information simultaneously.",
            "score": 0.4642308367154766,
            "section_title": "Task Definition",
            "char_start_offset": 6867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 371
                },
                {
                    "start": 374,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 848
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.837890625
        },
        {
            "corpus_id": "270319618",
            "title": "A survey on advancements in image-text multimodal models: From general techniques to biomedical implementations",
            "text": "Q-former is trained in two stages: fixing the image encoder to learn representations of visual-language consistency, and fixing the language large-scale model to enhance the generation capability from vision to language.InstructBLIP [42] proposes a paradigm of instruction fine-tuning based on BLIP-2, using additional instructions to extract more useful visual features.mPLUG-2 [43] is capable of handling various modal inputs such as text, images, and videos.It has achieved leading or comparable performance in more than 30 multimodal and unimodal tasks.Models such as BEIT3 [44] and X-FM [45] undergo joint pre-training on both image and text data, enabling them to learn joint representations of images and text.These models can handle not only tasks that involve only text or images but also complex tasks that require simultaneous processing of both text and images, such as generating image descriptions and answering questions related to images.LLaVA [46] fine-tunes on the generated visual-language instruction data end-to-end, validating the effectiveness of using generated data for instruction-tuning in LMM, and providing practical techniques for constructing general instructions that follow visual agents.OFA [47] unifies the understanding and generation tasks of multimodal and single-modal data into a single simple Seq2Seq generative framework.It undergoes pretraining and fine-tuning using task instructions, without introducing additional task-specific layers.GPT-4 [48]takes this to the next level, boasting a total parameter count of 1.8 trillion, a significant increase compared to GPT-3's 100 billion parameters.This massive parameter scale equips GPT-4 with potent computational capabilities and potential in natural language understanding and generation tasks.It enhances its contextual comprehension, allowing it to handle more complex textual information and possess advanced semantic analysis and knowledge reasoning abilities.MiniGPT-4 [49] fixes the parameters of the language model and the visual model, and then uses only the projection layer to align the language decoder with LLaMA and the visual encoder with BLIP-2.",
            "score": 0.4638395355454648,
            "section_title": "Recent Image-text Multimodal Models",
            "char_start_offset": 19080,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 220,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 461
                },
                {
                    "start": 461,
                    "end": 557
                },
                {
                    "start": 557,
                    "end": 717
                },
                {
                    "start": 717,
                    "end": 954
                },
                {
                    "start": 954,
                    "end": 1221
                },
                {
                    "start": 1221,
                    "end": 1363
                },
                {
                    "start": 1363,
                    "end": 1481
                },
                {
                    "start": 1481,
                    "end": 1637
                },
                {
                    "start": 1637,
                    "end": 1787
                },
                {
                    "start": 1787,
                    "end": 1957
                },
                {
                    "start": 1957,
                    "end": 2153
                }
            ],
            "ref_mentions": [
                {
                    "start": 592,
                    "end": 596,
                    "matchedPaperCorpusId": "250373772"
                },
                {
                    "start": 1225,
                    "end": 1229,
                    "matchedPaperCorpusId": "246634906"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90185546875
        },
        {
            "corpus_id": "271533621",
            "title": "Data Processing Techniques for Modern Multimodal Models",
            "text": "1) Multimodal Generative models: Multimodal models have seen significant development in recent years. While multimodal models often refer to those that incorporate more than one modality, vision-language models are the most extensively studied areas [4], [5]. In this work, we focus on data processing techniques for vision-language models, and some of these concepts can be also applied to models involving other modalities. \n\nThere are two major categories of vision-language models: discriminative models and generative models. Discriminative models typically use early fusion or late fusion architectures to learn meaningful representations for downstream tasks such as classification and ranking. Most early work centered on discriminative models, including ALBEF [6], ALIGN [7], and CLIP [8]. \n\nGenerative models, on the other hand, aim to generate text or image outputs. Generative vision-language models can be further divided into image generation models (e.g., diffusion models) [9] and text generation models (e.g., MLLMs) [10]. Diffusion models, for instance, utilize U-Net [11] and transformer architectures [12] for text-based image generation. MLLMs have emerged following the introduction of large language models (LLMs) [13]. With the success of LLMs, researchers have explored the possibility of incorporating visual information as tokens into LLM models to leverage the previously learned knowledge in LLMs. Influential works in this area include LLaVA [14] and miniGPT-4 [15]. \n\nIn this work, we will focus on the data processing techniques used in diffusion models and MLLMs, because these models represent the forefront of multimodal machine learning, with significant advancements and wide-ranging applications. While our mainly focus on these recent models, we also cover some early discriminative models to provide a comprehensive overview of data processing techniques in the field. \n\n2) Multimodal Dataset: Training datasets play a critical role in both model pretraining and finetuning. A typical dataset for vision-language models consists of pairs of images and text.",
            "score": 0.46354318632088387,
            "section_title": "II. BACKGROUND",
            "char_start_offset": 962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1496
                },
                {
                    "start": 1499,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1908
                },
                {
                    "start": 1911,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2097
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9326171875
        },
        {
            "corpus_id": "258947721",
            "title": "PandaGPT: One Model To Instruction-Follow Them All",
            "text": "Humans possess remarkable abilities to perceive and understand information from diverse sensory modalities, such as seeing a painting and hearing an audio guide.Analogously, to learn simultaneously, holistically, and directly from many different forms of information holds great promise for enabling machines to have a more comprehensive and better understanding of the world.To this end, there has been an emergent interest in developing artificial intelligence (AI) systems capable of perceiving and understanding information from multiple modalities simultaneously in a manner similar to humans.However, much of the prior research has focused on tackling individual modalities in isolation.For instance, while significant progress has been made in text-to-image retrieval and generation [18], visually-grounded instruction following [12,31], and speech understanding and generation [29], these advances have largely been confined to separate combinations of text and other modalities or, at best, a few visual modalities (e.g., image and video).These models are limited in their ability to connect information from different modalities and lack the capacity to perceive and understand multimodal inputs holistically, thereby neglecting the inherent richness and complementary nature of multimodal data.\n\nIn this paper, we present PandaGPT, the first general-purpose model capable of instruction-following data from six modalities.PandaGPT leverages the power of multimodal encoders from ImageBind [8] and the expressive language models from Vicuna [4], demonstrating impressive and emergent cross-modal capabilities across six modalities: image/video, text, audio, depth, thermal, and inertial measurement units (IMU).Crucially, PandaGPT achieves these capabilities despite being only trained on aligned image-text pairs, thanks to the shared embedding space provided by ImageBind.This integration of multimodal information enables PandaGPT to perform a wide range of tasks, including generating detailed descriptions of images, composing engaging stories inspired by videos, and providing accurate answers to questions about audio inputs.Most interestingly, the core innovation of PandaGPT lies in its ability to naturally compose the semantics of multimodal inputs, which enables a rich set of compositional multimodal tasks across different modalities.",
            "score": 0.46330042543859484,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 598
                },
                {
                    "start": 598,
                    "end": 693
                },
                {
                    "start": 693,
                    "end": 1048
                },
                {
                    "start": 1048,
                    "end": 1305
                },
                {
                    "start": 1307,
                    "end": 1433
                },
                {
                    "start": 1433,
                    "end": 1721
                },
                {
                    "start": 1721,
                    "end": 1884
                },
                {
                    "start": 1884,
                    "end": 2142
                },
                {
                    "start": 2142,
                    "end": 2358
                }
            ],
            "ref_mentions": [
                {
                    "start": 790,
                    "end": 794,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74365234375
        },
        {
            "corpus_id": "270877993",
            "title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis",
            "text": "GPT-4 is a large multimodal model capable of accepting both image and text inputs and generating text outputs.\n\nIt demonstrates human-level performance across various professional and academic benchmarks and is currently recognized as the most powerful language model.GPT-3.5-turbo, an early AI language model developed by OpenAI based on GPT-3.5, is the version we used.Claude-3-Sonnet, a large language model developed by Anthropic, is renowned for its powerful multimodal understanding and generation capabilities.Gemini-Pro, another large language model developed by Anthropic, gains attention for its excellent natural language understanding and generation abilities.Mistral, also developed by Anthropic, is noted for its strong multimodal understanding and generation capabilities.\"output\": \"Processor CPU Caterr\" \"id\": \"0\" \"instruction\": \"Parse the following log entry into a template format, replacing variable parts with a wildcard <*>,and focus the answer after the keyword 'Answer'\\nFor example:\\nlog entry:\\nno floppy controllers found,\\nanswer:'no floppy controllers found';\\nlog entry:\\n13 tree receiver 1 in re-synch state event(s) (dcr 0x0185) detected over 4562 seconds,\\nanswer:'<*> tree receiver <*> in re-synch state event(s) (dcr <*>) detected over <*> seconds';\\niog entry:\\n... autorun DONE.,\\nanswer:'... autorun DONE.'; \\nlog entry:\\n2 L3 EDRAM error(s) (dcr 0x0157) detected and corrected over 282 seconds, \\nanswer:'<*> L3 EDRAM error(s) (dcr <*>) detected and corrected over <*> seconds'; \\nlog entry:\\nprobe of vesafb0 failed with error -6,\\nanswer:'probe of vesafb0 failed with error <*>'.\"",
            "score": 0.46304429902749056,
            "section_title": "APPENDIX",
            "char_start_offset": 73867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 112,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 517
                },
                {
                    "start": 517,
                    "end": 672
                },
                {
                    "start": 672,
                    "end": 787
                },
                {
                    "start": 787,
                    "end": 1619
                },
                {
                    "start": 1619,
                    "end": 1620
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71435546875
        },
        {
            "corpus_id": "271039660",
            "title": "M5 - A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks",
            "text": "Large Multi-Modal Models This work focuses on the multimodal counterpart of large language models (LLMs), often referred to as Large Multimodal Models (LMMs). LMMs are language 1 We will release all code and data upon acceptance. models capable of processing and \"understanding\" data other than text. While this generally subsumes images, video, audio, or more, we concentrate on visio-linguistic LMMs, i.e., models that take text and/or images as input and generate textual output. \n\nThe vast majority of open-source LMMs comprise three major components: a pretrained generative LLM as the core, a pretrained vision-encoder model that computes semantically rich image embeddings, and a shallow mapping network that learned to project image embeddings into the text embedding space. One of this architecture's successful open-source implementations with a recent LLM, i.e., the Llama-based Vicuna (Chiang et al., 2023;Touvron et al., 2023), is LLaVA (Liu et al., 2023b), from which many others took inspiration also regarding the training data and process. Besides this, LMMs also exist, which use Cross-Attention (Wang et al., 2023;Bai et al., 2023), Q-Formers (Li et al., 2023;Geigle et al., 2023), Adapters (Eichenberg et al., 2022), or Preceiver Resamplers (Alayrac et al., 2022;Awadalla et al., 2023) to process image embeddings. For an overview including architectural details and the number of parameters of the 18 LMMs' components we employed in this work, please see Table 8. \n\nEvaluation Benchmarks With the recent surge in the research of LLMs and LMMs, analyzing the models' performances is crucial yet challenging.",
            "score": 0.4625967010012271,
            "section_title": "Related Work",
            "char_start_offset": 3205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 482
                },
                {
                    "start": 485,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1627
                }
            ],
            "ref_mentions": [
                {
                    "start": 950,
                    "end": 969,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 1261,
                    "end": 1283,
                    "matchedPaperCorpusId": "248476411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9150390625
        },
        {
            "corpus_id": "271693411",
            "title": "A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks",
            "text": "Multimodal models play a crucial role in image understanding tasks by integrating text and image information to achieve more intelligent and comprehensive understanding and reasoning. In this field, MiniGPT-4 [41], [42] ,Instruct-BLIP [43] ,and Wiki-LLaVA [44] are three highly regarded models, along with other related models such as 3DMIT [45], GroundingGPT [46], ModaVerse [47], Vary-toy [48], LLaVA-MOLE [49], and CogCom [50]. \n\nMiniGPT-4 is a multimodal model based on GPT-4 that combines text and image information, exhibiting excellent image understanding capabilities. MiniGPT-4 adopts a modular architecture design, which allows for rapid adaptation to various LLMs, enabling flexible understanding and reasoning for different types of images. The model utilizes advanced image encoders and text generators to accurately describe and infer information from images, providing strong support for image understanding tasks. \n\nAnother prominent model is InstructBLIP, which demonstrates exceptional image understanding capabilities. With its modular architecture design, InstructBLIP can quickly adapt to different LLMs and perform instruction tuning by freezing the image encoder and the LLM. This approach enables efficient understanding and reasoning of image information. InstructBLIP excels in vision-language instruction tuning, generating rich responses while effectively considering the dialogue history, thus providing impressive performance for image understanding tasks. \n\nBesides MiniGPT-4 and InstructBLIP, there are other noteworthy multimodal models that also play significant roles in image understanding tasks. 3DMIT employs advanced threedimensional information processing technology, enabling deep understanding and analysis of 3D images. GroundingGPT focuses on effectively correlating natural language and visual information to achieve cross-modal understanding and reasoning. Models like ModaVerse, Vary-toy, LLaVA-MOLE, and CogCom also exhibit outstanding image understanding capabilities to varying degrees, each with unique features and advantages, contributing significantly to image understanding tasks. \n\nIn summary, multimodal models play an essential role in image understanding tasks. MiniGPT-4, InstructBLIP, and other related models have shown excellent performance and potential, providing strong support for the development of image understanding tasks.",
            "score": 0.46248969534816203,
            "section_title": "Model Introduction:",
            "char_start_offset": 19708,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 430
                },
                {
                    "start": 433,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 929
                },
                {
                    "start": 932,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1486
                },
                {
                    "start": 1489,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2135
                },
                {
                    "start": 2138,
                    "end": 2220
                },
                {
                    "start": 2221,
                    "end": 2393
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 260,
                    "matchedPaperCorpusId": "269330022"
                },
                {
                    "start": 376,
                    "end": 380,
                    "matchedPaperCorpusId": "266977568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.951171875
        },
        {
            "corpus_id": "270440809",
            "title": "Explore the Limits of Omni-modal Pretraining at Scale",
            "text": "We first investigate several variants of encoder architectures with four data modalities.With our collected data, we pretrain architectures for 300K steps by the same contrastive [4] and maskedgeneration loss functions [26] (details in Appendix B).We take the captioning and retrieval tasks on image, audio, and video modalities as the main evaluation benchmark for designing architectures.\n\nArchitectural Designs.We construct the vanilla architecture from CLIP [4].A text encoder of Transformer [21] takes text inputs and outputs text embeddings z T , and an image encoder of Vision Transformer [45] takes image input x I \u2208 R 3\u00d7H\u00d7W and outputs image embeddings z I , respectively.As shown in Figure 4, we propose 4 architectures for omni-modal learning: i) Modality-specific encoders for each modality, employing individual transformers to extract multimodal embeddings, then fuse them as BEiT-3 [26].ii) BERT (text encoder) as a unified multimodal encoder to extract multimodal embeddings and generates texts.iii) LLM (text decoder) as a unified multimodal encoder and text generator.iv) A ViT as a unified multimodal encoder besides text, and an LLM deals with text embeddings and generation.\n\nEmpirical Discovery.Referring to Table 1, we conclude several guidelines for designing architectures in omni-modal learning: 1) Pure language models are difficult to retrieval tasks.Both (ii) and (iii) deliver a significant performance drop in retrieval tasks.2) No more than 2 Encoders.Comparing (i) with (ii) & (iii), we observe that additional encoders are beneficial for retrieval tasks; however, comparison between (i) and (iv) suggests that discrepancies among multiple encoders can also hinder multimodal alignment.3) Language is an individual branch for alignment.Comparing (ii) & (iii), with (iv), improvements are significant in both retrieval and captioning.\n\nTable 1: Architecture Design of Omni-Modal Learning Paradigm.",
            "score": 0.46247860679901914,
            "section_title": "Architecture Design for Omni-modal Learning",
            "char_start_offset": 10055,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 89,
                    "end": 248
                },
                {
                    "start": 248,
                    "end": 390
                },
                {
                    "start": 392,
                    "end": 414
                },
                {
                    "start": 414,
                    "end": 466
                },
                {
                    "start": 466,
                    "end": 681
                },
                {
                    "start": 681,
                    "end": 902
                },
                {
                    "start": 902,
                    "end": 1011
                },
                {
                    "start": 1011,
                    "end": 1086
                },
                {
                    "start": 1086,
                    "end": 1195
                },
                {
                    "start": 1197,
                    "end": 1217
                },
                {
                    "start": 1217,
                    "end": 1379
                },
                {
                    "start": 1379,
                    "end": 1457
                },
                {
                    "start": 1457,
                    "end": 1484
                },
                {
                    "start": 1484,
                    "end": 1719
                },
                {
                    "start": 1719,
                    "end": 1769
                },
                {
                    "start": 1769,
                    "end": 1866
                },
                {
                    "start": 1868,
                    "end": 1929
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 182,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 462,
                    "end": 465,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.814453125
        },
        {
            "corpus_id": "265302359",
            "title": "Automated Cardiovascular Record Retrieval by Multimodal Learning between Electrocardiogram and Clinical Report",
            "text": "This approach is highly effective for capturing the context and meaning of each token in the input text, enabling the model to better understand the overall meaning of the text. When processing an input text T , the text encoder generates a sequence of embeddings w cls , w 1 , ..., w N , where w cls represents the embedding of the [CLS] token, and the remaining w i represent the embeddings of the individual input tokens. This sequence of embeddings is then passed to the multimodal encoder to be combined with the visual embeddings generated by the vision encoder. \n\nMultimodal Encoder The multimodal encoder is a complex module that plays a critical role in enabling the model to learn the relationships between the visual and textual inputs. To achieve this, it incorporates an additional cross-attention (CA) layer that sits between the self-attention (SA) layer and the feed-forward network (FFN) for each transformer block of the text encoder. By doing so, the model can attend to both the textual and visual inputs and build better representations of the image-text pair. To create a multimodal representation of the image-text pair, the text input is modified by appending a taskspecific [Encode] token at the end of the sequence, which is then fed into the multimodal encoder. The output embedding of this token is used as the final representation of the image-text pair. The embedding layers, CA layers, and FFN share similar functionality between encoding and decoding tasks, which means that they can be shared to improve training efficiency and benefit from multi-task learning. Additionally, the cross-attention layer introduces another set of attention weights to the model, which requires additional computation and increases the number of parameters to be learned. However, this additional complexity is necessary to enable the model to learn Figure 3: The overall architecture of our model, which comprises a vision encoder responsible for processing visual data, a language encoder that focuses on comprehending textual information, and a multimodal encoder that combines the input from both the vision and language encoders to fuse comprehensive representations.",
            "score": 0.46214992817291145,
            "section_title": "Retrieval System",
            "char_start_offset": 15648,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 2185
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85888671875
        },
        {
            "corpus_id": "258959284",
            "title": "Generating Images with Multimodal Language Models",
            "text": "Yes, what about a pencil drawing of one? \n\nWhat about a comic illustration of one? learnt text tokens. We train it by minimizing the l 2 distance between its outputs and the outputs of the text encoder of a text-to-image generation model. This distillation training allows us to use the image decoder of the text-to-image model at inference time. Despite its simplicity, we show that this allows us to outperform the baseline text-to-image generation model on several tasks that measure language context dependence. Finally, to decide whether to produce a retrieved image or a generated one at inference time, we train a decision model that outputs a decision conditioned on the LM hidden representations. This allows us to both generate and retrieve in output sequences, as shown in Fig. 1. \n\nOur experimental results demonstrate that GILL is more effective than Stable Diffusion at processing longer-form text, including dialogue and discourse. We show on dialogue-conditioned image generation that GILL can outperform non-LLM based generation models, and benefit from multimodal context: generating images that match text better than the backbone generation models that we distill from. In addition, GILL can process arbitrarily interleaved image-text inputs, unlike typical text-to-image models which only process text. GILL is the first model capable of outputting retrieved images, novel images, and text -interleaving these for coherent multimodal dialogue generation. 1",
            "score": 0.46205208417024857,
            "section_title": "Show me an illustration of a beaver",
            "char_start_offset": 2300,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 43,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 791
                },
                {
                    "start": 794,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1477
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "271039870",
            "title": "HEMM: Holistic Evaluation of Multimodal Foundation Models",
            "text": "Multimodal machine learning brings unique challenges for ML research due to the heterogeneity between modalities and the interconnections found between them [69].It has inspired many theoretical studies in data heterogeneity and interactions [25], as well as diverse applications in multimedia [44,14,88], affective computing [86], robotics [47], finance [39], HCI [25,82], education [12] and healthcare [80,110].\n\nEvaluation frameworks for multimodal models have significantly shaped the multimodal research landscape, through holistic [57,66] and domain-specific benchmarks [31,28].Recent benchmarks have focused on testing the capabilities of multimodal foundation models, such as MME [29], MMBench [73], LVLM-ehub [111], SEED-Bench [59], Touchstone [7], Mm-vet [120], ReForm-Eval [65], VisIT-Bench [11], FLAVA [45].Other benchmarks focus on evaluating hallucination [21] and applications in medicine [113] and autonomous driving [107].These benchmarks contain many tasks, but without the systematic taxonomy and comprehensiveness that HEMM provides.\n\nMultimodal foundation models are promising foundations for the future of AI, with impressive reasoning [75], interactive dialogue [49], and few-shot generalization abilities [100].These models can be pre-trained (typically with image-text self-supervised learning) and fine-tuned for downstream tasks [63,74,91,67], or based on adapting language models with vision to enable text generation conditioned on images [61,105].Cross-modal transformer architectures have emerged as a popular backbone due to their suitability for both language and image data [17,99].Additionally, composable diffusion models [96] can be used to further generate combinations of output modalities.\n\nAdapting language models for multimodality is another promising approach where frozen models are aligned on both vision and language to generate text from multimodal inputs [127,62,118,109].",
            "score": 0.4619243715479775,
            "section_title": "Related Work",
            "char_start_offset": 18335,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 162,
                    "end": 413
                },
                {
                    "start": 415,
                    "end": 584
                },
                {
                    "start": 584,
                    "end": 819
                },
                {
                    "start": 819,
                    "end": 939
                },
                {
                    "start": 939,
                    "end": 1053
                },
                {
                    "start": 1055,
                    "end": 1235
                },
                {
                    "start": 1235,
                    "end": 1477
                },
                {
                    "start": 1477,
                    "end": 1616
                },
                {
                    "start": 1616,
                    "end": 1729
                },
                {
                    "start": 1731,
                    "end": 1921
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 246,
                    "matchedPaperCorpusId": "16540884"
                },
                {
                    "start": 294,
                    "end": 298,
                    "matchedPaperCorpusId": "244954623"
                },
                {
                    "start": 298,
                    "end": 301,
                    "matchedPaperCorpusId": "249375461"
                },
                {
                    "start": 301,
                    "end": 304,
                    "matchedPaperCorpusId": "680978"
                },
                {
                    "start": 341,
                    "end": 345,
                    "matchedPaperCorpusId": "156051062"
                },
                {
                    "start": 355,
                    "end": 359,
                    "matchedPaperCorpusId": "149720066"
                },
                {
                    "start": 365,
                    "end": 369,
                    "matchedPaperCorpusId": "16540884"
                },
                {
                    "start": 369,
                    "end": 372,
                    "matchedPaperCorpusId": "14879124"
                },
                {
                    "start": 384,
                    "end": 388,
                    "matchedPaperCorpusId": "13858994"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "53110712"
                },
                {
                    "start": 541,
                    "end": 544,
                    "matchedPaperCorpusId": "235899386"
                },
                {
                    "start": 576,
                    "end": 580,
                    "matchedPaperCorpusId": "224960953"
                },
                {
                    "start": 580,
                    "end": 583,
                    "matchedPaperCorpusId": "2044324"
                },
                {
                    "start": 814,
                    "end": 818,
                    "matchedPaperCorpusId": "252782958"
                },
                {
                    "start": 1158,
                    "end": 1162,
                    "matchedPaperCorpusId": "252383606"
                },
                {
                    "start": 1229,
                    "end": 1234,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 1360,
                    "end": 1363,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 1366,
                    "end": 1369,
                    "matchedPaperCorpusId": "259275338"
                },
                {
                    "start": 1468,
                    "end": 1472,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 1608,
                    "end": 1612,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 1612,
                    "end": 1615,
                    "matchedPaperCorpusId": "173990158"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65869140625
        },
        {
            "corpus_id": "273233771",
            "title": "PAR: Prompt-Aware Token Reduction Method for Efficient Large Multimodal Models",
            "text": "Multimodal large language models (MLLMs) represent a significant advancement in artificial intelligence by integrating visual encoders with pre-trained large language models. This integration allows these models to jointly process and understand diverse modalities, such as images and text, enabling more comprehensive and contextually aware analyses. \n\nFor a given image I, the visual encoder f visual transforms it into a series of token representations X I : \n\nHere, X I denotes a set of visual tokens that capture essential visual features and semantic information from the image. This process allows image content to be encoded in a format compatible with the language model, facilitating the effective integration of visual and textual data. \n\nTo ensure that the visual tokens are compatible with the language model's processing framework, a visual projection layer is employed. This layer converts visual tokens X I into language embedding tokens y 1 , y 2 , . . . , y n . The transformation is performed such that these language embedding tokens share the same dimensional space d as the word embeddings used by the language model: \n\nwhere f projection denotes the projection function that aligns the dimensionality of the visual tokens with that of the language embeddings. This alignment ensures that the visual information encoded in y 1 , y 2 , . . . , y n can be processed seamlessly alongside the textual information by the language model. Consequently, the MLMM can effectively integrate and leverage both visual and textual representations, improving its ability to generate accurate and contextually relevant outputs.",
            "score": 0.4617909254246055,
            "section_title": "Preliminaries",
            "char_start_offset": 10284,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 351
                },
                {
                    "start": 354,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1634
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "264590581",
            "title": "Entity Embeddings : Perspectives Towards an Omni-Modality Era for Large Language Models",
            "text": "Recent advancements in capabilities of Large Language Models (LLMs) have paved the way for an era in the field of natural language understanding and generation. A natural extension to the LLM powered natural language understanding would be to incorporate other modalities such as imagery and audio to harness the inherent cognitive strength of language, which acts as an intricate bridge connecting various semantic concepts [1] [2]. This type of a formulation not only allows for beneficial possibilities like extracting information out of given non-textual modalities to perform tasks, but also add new dimensions in LLMs' cognitive prowess through training. The significance of learning from multi-sensory stimuli and interaction to reach human level cognition and beyond is well deliberated [3][4] [29]. \n\nThough it is hard to draw a hard line on the taxonomy, we can define the first image caption generators and image generation models with text based conditioning as early attempts to meld natural language understanding with a different modality [5][6] [7] [8]. Despite their adeptness and popularity, these architectures have focused on a single particular task on processing distinct modalities which limit their flexibility to achieve greater cognitive complexity. Apart from the inability to fuse information from different modalities effectively, another practical limitation is that the modality heads such as imagery expect fixed input size. More importantly, the number of different data points which can be fed for a modality is static, usually one. \n\nRecently, few LLM architectures with multi-modal data processing capability have been proposed with promising results [9][10] [11]. Different from conditioned generative models with cross-attention and early multimodal LLMs, these proposals aim to project data of any modality on the joint latent language space. The central idea of these proposed architectures can be summarized as \"tokenization\" of data from different modalities and utilization of these \"modality tokens\" as regular language tokens allowing interleaving with proper projection and training mechanisms. \n\nIf the current transformer centric architectural framework continues to exist and prosper, we believe that one particular direction that research will follow is this \"any-modal\" tokenized approach.",
            "score": 0.46171461613218456,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 807
                },
                {
                    "start": 810,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1566
                },
                {
                    "start": 1569,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2140
                },
                {
                    "start": 2143,
                    "end": 2340
                }
            ],
            "ref_mentions": [
                {
                    "start": 802,
                    "end": 806,
                    "matchedPaperCorpusId": "7310189"
                },
                {
                    "start": 1054,
                    "end": 1057,
                    "matchedPaperCorpusId": "199465721"
                },
                {
                    "start": 1061,
                    "end": 1064,
                    "matchedPaperCorpusId": "256827727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86962890625
        },
        {
            "corpus_id": "257663918",
            "title": "Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding",
            "text": ", while dual encoder models like CLIP [46] and ALIGN [24] consist of dual text and image encoder models that are jointly trained to produce embeddings in a shared semantic space. FLAVA, a vision-and-language model introduced recently by Singh et al. [58], also includes dual text and image encoders, and is trained on both multimodal objectives involving the alignment of images and text, as well as unimodal objectives. In this work we focus on the text encoder component of dual encoder models, since it may be used after V&L pretraining for text-only tasks.\n\nVarious works have explored the use of multimodal learning to benefit text understanding. Most related to our work is the very recent study of Zhang et al. [72] which investigates the use of unimodal and multimodal models for understanding visual commonsense in text. Their analysis follows a line of related work investigating the contribution of multimodal learning to visual commonsense knowledge in text, since such knowledge is typically not written explicitly in text but is abundantly present in visual information [32,36,67]. Unlike Zhang et al. [72] who only evaluate CLIP with an added set of task-specific learned weights, we are able to probe CLIP and other similar models in the strictly zero-shot setting via our novel Stroop probing method. This allows for directly evaluating properties learned by the models, independent of differences that result, for instance, from specific training configurations. In addition, we also study performance on both visual and nonvisual NLU tasks in order to provide a controlled benchmark.\n\nOther works have investigated the use of multimodal learning for NLU in various contexts. Bruni et al. [7] propose an architecture for integrating text and image-based distributional information to improve performance on tasks where meaning is grounded in perception. Kiela and Bottou [27] show that integrating features extracted from images using CNN with skip-gram representation vectors improves performance on semantic similarity datasets. Lazaridou et al. [33] train visual representations extracted using CNNs together with skip-gram embeddings to integrate visual and textual information performing well in both semantic and vision tasks. Kiela et al. [28] train a sentence embed",
            "score": 0.46091327451455666,
            "section_title": "Related Work",
            "char_start_offset": 8777,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 42,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 53,
                    "end": 57,
                    "matchedPaperCorpusId": "231879586"
                },
                {
                    "start": 250,
                    "end": 254,
                    "matchedPaperCorpusId": "244954250"
                },
                {
                    "start": 1084,
                    "end": 1088,
                    "matchedPaperCorpusId": "1224220"
                },
                {
                    "start": 1088,
                    "end": 1091,
                    "matchedPaperCorpusId": "606971"
                },
                {
                    "start": 1091,
                    "end": 1094,
                    "matchedPaperCorpusId": "6974607"
                },
                {
                    "start": 1707,
                    "end": 1710,
                    "matchedPaperCorpusId": "2618475"
                },
                {
                    "start": 1889,
                    "end": 1893,
                    "matchedPaperCorpusId": "9187880"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.921875
        },
        {
            "corpus_id": "269626102",
            "title": "VisionGraph: Leveraging Large Multimodal Models for Graph Theory Problems in Visual Context",
            "text": "We test widely used powerful commercial LMMs (GPT-4V, Gemini, and Qwen-Plus/Max) and open-sourced LMMs: MiniGPT-4 (Zhu et al., 2023) extends the Q-Former architecture to enhance multimodal interactions.It leverages the shallow transformer approach to align visual features from a frozen visual encoder with the language model, thereby enabling robust multimodal comprehension and generation.\n\nInstructBLIP (Wang et al., 2022) introduces instructionaware visual features by incorporating instructions directly into the Q-Former architecture (Li et al., 2023a).It allows the model to dynamically adapt its multimodal understanding based on explicit instructions provided alongside visual inputs.LLaVA (Liu et al., 2023) takes a distinct approach by utilizing a linear layer to map fine-grained visual features from a frozen vision encoder into the embedding space of the pre-trained LLM.Qwen-VL (Bai et al., 2023) is a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images.\n\nSPHINX (Lin et al., 2023) is a versatile multi-modal large language model (MLLM) with a joint mixing of model weights, tuning tasks, and visual embeddings.InternLM-XComposer (Zhang et al., 2023b) is a vision-language large model that enables advanced image-text comprehension and composition.For closed-source LMMs, GPT-4V(ision) (OpenAI, 2023) from OpenAI is recognized as the most powerful MLLMs to date, surpassing a host of Vicuna-based models, e.g., MiniGPT-4, InstructBLIP, and LLaVA.Besides, Gemini (Team et al., 2023), released by Google, has emerged as a formidable challenger to GPT-4V, exhibiting significant multi-modal capabilities over different benchmarks.",
            "score": 0.4607384271074223,
            "section_title": "Comparing Models",
            "char_start_offset": 9146,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 202,
                    "end": 391
                },
                {
                    "start": 393,
                    "end": 559
                },
                {
                    "start": 559,
                    "end": 693
                },
                {
                    "start": 693,
                    "end": 885
                },
                {
                    "start": 885,
                    "end": 1025
                },
                {
                    "start": 1027,
                    "end": 1182
                },
                {
                    "start": 1182,
                    "end": 1319
                },
                {
                    "start": 1319,
                    "end": 1517
                },
                {
                    "start": 1517,
                    "end": 1698
                }
            ],
            "ref_mentions": [
                {
                    "start": 1533,
                    "end": 1552,
                    "matchedPaperCorpusId": "266361876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92138671875
        },
        {
            "corpus_id": "271213241",
            "title": "Putting GPT-4o to the Sword: A Comprehensive Evaluation of Language, Vision, Speech, and Multimodal Proficiency",
            "text": "The ability to integrate and interpret information from multiple modalities is crucial for developing advanced intelligent systems.Multimodal capacity refers to the capability of a model to understand and synthesize information from various sources such as text, images, and audio.This enables the model to generate more comprehensive and contextually enriched responses.\n\nThe objective of assessing GPT-4o's multimodal capacity is to evaluate its performance across tasks that require the integration of different types of data.",
            "score": 0.4607217447920887,
            "section_title": "Multimodal Capacity of GPT-4o",
            "char_start_offset": 44590,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 281
                },
                {
                    "start": 281,
                    "end": 371
                },
                {
                    "start": 373,
                    "end": 529
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55078125
        },
        {
            "corpus_id": "277103675",
            "title": "Unified Autoregressive Visual Generation and Understanding with Continuous Tokens",
            "text": "Large Language Models (LLMs) have recently advanced from text-centric architectures, such as BERT [11] and GPT [35], toward multimodal systems capable of understanding and generating content across different modalities. GPT-3 [5] and PaLM [9] show that scaling language models leads to emergent capabilities, while Flamingo [2] further demonstrates that incorporating visual inputs facilitates unified multimodal reasoning. This trend toward unified vision-language model-using a single model for diverse tasks of visual understanding and generation-has promising potential for leveraging knowledge and reasoning abilities that transfer across different vision and language tasks, ultimately facilitating more robust and generalizable multimodal representation and modeling capabilities. \n\nMotivated by the advantages and strong scaling properties of autoregressive models, coupled with their simplicity, we investigate a pure autoregressive framework for unified visual generation and understanding, without the limitations introduced by vector quantization (VQ). In this paper, we introduce UniFluid, a unified framework that leverages continuous visual tokens within an autoregressive architecture to jointly handle vision-language generation and understanding tasks. Building upon pre-trained Gemma [48] on large-scale text corpus, UniFluid unlocks powerful visual generation and understanding capabilities through training with paired image-text data, and further allows these two tasks to mutually benefit each other within a single architecture. Specifically, UniFluid adopts a unified autoregressive framework where both text and continuous visual inputs are embedded as tokens in the same space, enabling seamless joint training of image generation and understanding tasks. UniFluid integrates a continuous tokenizer [12,21] for image generation and a pre-trained SigLIP [59] image encoder for understanding tasks, while textual inputs are processed using a standard SentencePiece tokenizer [20]. The resulting multimodal sequences are modeled autoregressively using Gemma [48] as the underlying transformer backbone. Task-specific prediction heads-a diffusion-based head for image generation and a cross-entropy head for text generation-ensure effective modality-specific training and inference, enabling UniFluid to efficiently learn shared representations that mutually enhance its generation and understanding capabilities.",
            "score": 0.4603593860514052,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2126
                },
                {
                    "start": 2127,
                    "end": 2436
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 102,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 226,
                    "end": 229,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 239,
                    "end": 242,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 324,
                    "end": 327,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1830,
                    "end": 1833,
                    "matchedPaperCorpusId": "270560593"
                },
                {
                    "start": 1880,
                    "end": 1884,
                    "matchedPaperCorpusId": "257767223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9794921875
        },
        {
            "corpus_id": "273532372",
            "title": "ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language Tuning",
            "text": "Typically, a projection layer is employed to align the vision feature space with the language token space. This projection layer can be a simple linear layer [5], [7], [33], [34], a resampling layer [35], [36], or other customized layers designed to facilitate effective fusion [37]- [41]. Additionally, some works use tokenized source images as input instead of extracted features [42], [43]. This design enables their capability for image generation. \n\nTo facilitate a better understanding of VL model performance, various benchmarks and toolkits have been introduced, such as MME [44], MMB [45], MMMU [46], MMT [47], and AVIBench [48]. These benchmarks have driven significant advancements in the architectures of VL models. \n\nAlthough existing multimodal architectures have achieved remarkable performance, inserting cross-attention modules at intermediate layers of LLMs introduces a substantial number of additional trainable parameters. On the other hand, concatenating vision features with language tokens extends the length of the input sequence, significantly increasing the computational resources needed for inference. Consequently, developing more efficient multimodal architectures is appealing.",
            "score": 0.46033120225160445,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 8205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1209
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 161,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 163,
                    "end": 166,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 199,
                    "end": 203,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 388,
                    "end": 392,
                    "matchedPaperCorpusId": "249848272"
                },
                {
                    "start": 604,
                    "end": 608,
                    "matchedPaperCorpusId": "265466525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8798828125
        },
        {
            "corpus_id": "258865501",
            "title": "MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal Image Generation",
            "text": "By fusing pre-trained model components, MULTIFUSION creates one cohesive system that requires less than 5% of the computational resources needed to train a comparable model from scratch. Our approach involves replacing the encoder of Stable Diffusion (SD) with a more advanced one built on a pre-trained LM. Fusing these components results in a downstream model with the ability to comprehend multilingual, interleaved multimodal inputs. The image generation component inherits this potential despite being trained solely on mono-modal data in a single language. The architecture of MULTIFUSION is illustrated in Fig. 1. The vast majority of pre-trained weights remain frozen, resulting in an efficient computational process overall. Subsequently, we outline how to effectively combine and align the involved modules for image generation. \n\nInput encoders. The CLIP encoder [35] used by SD is unsuited for interleaved multimodal inputs as it disregards context and yields disjoint encodings of text and images. Previous work has demonstrated that text encoders based on context-sensitive LMs improve the expressiveness of downstream image generation models [40,2]. Accordingly, we model the backbone of MULTIFUSION's encoder as an autoregressive transformer [10] using rotary position embeddings [46] trained on a multilingual corpus of various languages (step 1.1 in Fig. 1). We enable multimodality by adding an image prefix and dedicated adapters to the architecture (step 1.2). The image prefix maps images into sequences of token embeddings. During training, the language model parameters remain frozen following MAGMA [16,30,25]. As a result, the LM enables prompting with arbitrarily interleaved sequences of text and image tokens. \n\nSemantic embeddings. In order to use the LM as an encoder, we extract the model's hidden representations before the language modeling head. While these representations already capture semantic information from the model's pre-training, they need to be optimized and aligned further for usage in arbitrary downstream tasks (step 2.1 in Fig. 1).",
            "score": 0.46023937921880664,
            "section_title": "MULTIFUSION",
            "char_start_offset": 9097,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1738
                },
                {
                    "start": 1741,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2084
                }
            ],
            "ref_mentions": [
                {
                    "start": 874,
                    "end": 878,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1258,
                    "end": 1262,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1624,
                    "end": 1628,
                    "matchedPaperCorpusId": "245117852"
                },
                {
                    "start": 1628,
                    "end": 1631,
                    "matchedPaperCorpusId": "250301249"
                },
                {
                    "start": 1631,
                    "end": 1634,
                    "matchedPaperCorpusId": "59599816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94677734375
        },
        {
            "corpus_id": "265351653",
            "title": "Multimodal Large Language Models: A Survey",
            "text": "ITC involves constructing positive and negative sample pairs to align images and texts through contrastive learning. In addition, by leveraging MLM and MVM techniques, it can learn to infer the subtle connections between language and visual data by reconstructing masked linguistic tokens from a combination of linguistic knowledge and visual cues. In this way, it could improve its ability to comprehend and generate multimodal content. TM can be seen as a binary classification task that aims to predict whether an image and text pair match. In general, using different learning objectives in combination can enhance the performance of multimodal models. For instance, in the UNITER model, incorporating more learning objectives generally leads to better results. UNITER utilizes multiple learning objectives, such as MLM and ITC, and performs well across various specialized scenarios. However, using too many learning objectives may not always yield favorable results. This was validated in the experiment on the METER [30]. \n\nModel construction. Based on the different model structures, multimodal models can be categorized into encoder-only and encoder-decoder models. Encoder-only models utilize only the encoder part of the Transformer. The multimodal input is directly processed by the encoder to produce the output. Common examples of encoder-only models include CLIP [23] and ALBEF [32], which are suitable for tasks like imagetext retrieval but not ideal for tasks like image captioning. The encoder-decoder models incorporate both the encoder and decoder parts of the Transformer. The decoder receives the previously generated tokens and its own output to generate the output sequence auto-regressively. Encoder-decoder models, such as T5 [33] and SimVLM [34], leverage the decoder's capabilities and are suitable for generation tasks, but may not be as well-suited for tasks like image-text retrieval. \n\nInformation fusion. After encoding different modalities separately, it is necessary to design an encoder for multimodal encoding. Based on different fusion methods, multimodal models can be categorized into fusion encoder and dual encoder models [35]. The fusion encoder utilizes fusion methods to interact between modalities.",
            "score": 0.4601711398355276,
            "section_title": "III. PRACTICAL GUIDE FOR TECHNICAL POINTS",
            "char_start_offset": 12679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1915
                },
                {
                    "start": 1918,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2047
                },
                {
                    "start": 2048,
                    "end": 2169
                },
                {
                    "start": 2170,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 1023,
                    "end": 1027,
                    "matchedPaperCorpusId": "241033425"
                },
                {
                    "start": 1393,
                    "end": 1397,
                    "matchedPaperCorpusId": "236034189"
                },
                {
                    "start": 1752,
                    "end": 1756,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1768,
                    "end": 1772,
                    "matchedPaperCorpusId": "237291550"
                },
                {
                    "start": 2164,
                    "end": 2168,
                    "matchedPaperCorpusId": "245219186"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92529296875
        },
        {
            "corpus_id": "266573642",
            "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
            "text": "GPT4Tools [63] enables open-source language models to effectively use multimodal tools. It constructs a tool-related instructional dataset from advanced language models and utilizes Low-Rank Adaptation (LoRA) optimization to enhance the language models' tool-usage capabilities. Additionally, it proposes a benchmark to evaluate the accuracy of language models in using tools, demonstrating significant improvements in tool usage across various visual tasks. As shown in Figure 17, the GPT4Tools framework involves constructing a tool-related instruction dataset by prompting an advanced language model with various multimodal contexts. This dataset is then used to fine-tune open-source language models using Low-Rank Adaptation (LoRA) optimization, enabling them to effectively use tools for visual tasks such as comprehension and image generation. Additionally, the framework includes a benchmark to evaluate the language models' ability to use tools, showcasing significant improvements in tool usage accuracy. TextBind [72] enhances large language models with multi-turn interleaved multimodal instruction-following capabilities. It significantly reduces the need for high-quality exemplar data, making it more accessible and scalable for real-world tasks. The proposed model, MIM, trained on TextBind, outperforms recent baselines in open-world multimodal conversations, demonstrating remarkable performance in textual response generation, image generation, and overall multimodal instruction-following. As shown in Figure 18, MIM seamlessly integrates image encoder and decoder models to accommodate interleaved image-text inputs and outputs. It supplements large language models with visual input and output modules, enabling the model to process multi-turn interleaved multimodal instructions and generate coherent responses. The architecture is trained in two stages, focusing on aligning the feature spaces of vision and language models and further improving instructionfollowing capabilities.",
            "score": 0.4601428247175749,
            "section_title": "Image Generation",
            "char_start_offset": 47568,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 2004
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "271329267",
            "title": "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives",
            "text": "Multimodal generative models represent a significant advancement in AI. These models possess the capability to understand and create content by leveraging various data types, such as text, images, and audio [64], [65]. This integration of different data modalities enables these models to capture a more comprehensive understanding of concepts [66]. By utilizing information from these diverse sources, multimodal generative models aim to overcome the limitations inherent in traditional models that focus solely on a single data type [65]. Unimodal methods, traditional approaches that primarily focus on a single modality, such as text or images, have limitations in capturing the full complexity of real-world data [65]. For example, text-based models may lack the ability to incorporate visual or emotional context into their understanding, while image-based models might lack textual or semantic understanding [65]. Multimodal generative models address these limitations by integrating information from different modalities, such as text, images, and audio. This allows them to achieve a better understanding of the data and subsequently generate content that reflects the richness of human expression and experience. However, training multimodal models comes with its own set of challenges. These models can be computationally expensive to train and require large amounts of labeled data for each modality [65]. Additionally, finding effective techniques to seamlessly integrate information from different modalities remains an active area of research [67]. There are two main architectures used for multimodal learning: early fusion and late fusion [68]. Early fusion combines data from all modalities at the beginning of the model, while late fusion processes each modality independently before combining the results. The ability of multimodal generative models to understand and create content across different data types makes them invaluable for a wide range of tasks requiring a deep understanding of multimodal data [69]. Some real-world applications include generating realistic product descriptions with images for e-commerce platforms or creating personalized music recommendations based on a user's audio preferences and listening history. In addition to this, these models have demonstrated remarkable capabilities in various tasks, including medical imaging analysis, image captioning, text-to-image synthesis, video understanding, and audio-visual storytelling [69].",
            "score": 0.4599635915168978,
            "section_title": "G. Multimodal Generative Models",
            "char_start_offset": 28234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2256
                },
                {
                    "start": 2257,
                    "end": 2486
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 211,
                    "matchedPaperCorpusId": "3328096"
                },
                {
                    "start": 213,
                    "end": 217,
                    "matchedPaperCorpusId": "247038603"
                },
                {
                    "start": 344,
                    "end": 348,
                    "matchedPaperCorpusId": "202767602"
                },
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "247038603"
                },
                {
                    "start": 718,
                    "end": 722,
                    "matchedPaperCorpusId": "247038603"
                },
                {
                    "start": 915,
                    "end": 919,
                    "matchedPaperCorpusId": "247038603"
                },
                {
                    "start": 1412,
                    "end": 1416,
                    "matchedPaperCorpusId": "247038603"
                },
                {
                    "start": 1558,
                    "end": 1562,
                    "matchedPaperCorpusId": "352650"
                },
                {
                    "start": 1656,
                    "end": 1660,
                    "matchedPaperCorpusId": "10137425"
                },
                {
                    "start": 2029,
                    "end": 2033,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93701171875
        },
        {
            "corpus_id": "269043091",
            "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions",
            "text": "This enables enriched contextual understanding, showcasing a comprehensive approach to multimodal learning and generation.Pretrained on 12% of VTP, the model uniformly weighed losses in pretraining, with a tenfold emphasis on unaligned text loss during fine-tuning.Ablation studies underscore its ability to maintain content consistency and adapt to dynamic changes in video-audio sequence MiniGPT-4 [118]: MiniGPT-4 combines a frozen visual encoder (ViT + Q-Former from BLIP-2) with LLM using a single trainable projection layer.Pretrained on aligned image-text pairs and fine-tuned on detailed image descriptions, MiniGPT-4 exhibits GPT-4-like capabilities without training vision or language modules separately.The finetuning process enhances language outputs, demonstrating diverse skills like meme interpretation, recipe generation, and poem composition.The model's architecture involves a vision encoder, linear projection layer, and large language model.\n\nMiniGPT-v2 [14]: The model architecture of MiniGPT-v2 consists of a ViT visual backbone, a projection layer for dimension matching , and a large language model like LLaMA-2 [81] for the final generation .The ViT backbone is frozen during training, and four adjacent visual output tokens are concatenated and projected into LLaMA-2 space.Task-specific identifiers are incorporated during training using a three-stage strategy with weakly labeled image-text datasets and multi-modal instructional datasets.The model demonstrates superior performance in visual question-answering and visual grounding, outperforming other generalist models.The use of task identifier tokens enhances efficiency in multi-task learning, contributing to its state-of-the-art performance.Challenges include occasional hal-lucinations, emphasizing the need for more high-quality imagetext-aligned data.\n\nLLaVA-Plus [54]: LLaVA-Plus, is a general-purpose multimodal assistant designed to enhance LMMs through visual instruction tuning.The model maintains a skill repository with diverse vision and vision-language pre-trained models, activating relevant tools in response to user inputs for various tasks.",
            "score": 0.4598953795561939,
            "section_title": "GPT-4V",
            "char_start_offset": 25352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 122,
                    "end": 265
                },
                {
                    "start": 265,
                    "end": 530
                },
                {
                    "start": 530,
                    "end": 714
                },
                {
                    "start": 714,
                    "end": 859
                },
                {
                    "start": 859,
                    "end": 961
                },
                {
                    "start": 963,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1300
                },
                {
                    "start": 1300,
                    "end": 1467
                },
                {
                    "start": 1467,
                    "end": 1600
                },
                {
                    "start": 1600,
                    "end": 1727
                },
                {
                    "start": 1727,
                    "end": 1840
                },
                {
                    "start": 1842,
                    "end": 1972
                },
                {
                    "start": 1972,
                    "end": 2142
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82421875
        },
        {
            "corpus_id": "273653892",
            "title": "Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies",
            "text": "In the following days, U-PaLM [69], FlAN-PaLM [35] and PaLM-2 were released. \n\n3) Multimodal Large Language Models: a) MLLM: Background: Multimodal Large Language Model (MLLM), is an extension of LLM which adopts multimodal information as input such as text, sound, video, etc. to enable multiple dimensional reasoning and text generation. \n\nBefore the emergence of MLLM, significant research efforts were dedicated to multi-modality. These efforts can generally be categorized into representative and generative paradigms. An exemplary work in the representative paradigm is CLIP [70], which serves as a foundational contribution. \n\nThis process yields a visual encoder [71][72] and a text encoder, effectively establishing a bridge for downstream multimodal tasks. In contrast, generative frameworks [73][74] approach multimodal tasks by transforming them into sequenceto-sequence tasks. MLLM distinguishes itself from previous multimodal research in two key aspects. b) MLLM: Architecture: Figure 1 illustrates the mainstream architecture of multimodal large language models, typically composed of three modules: a multimodal encoder, an LLM, and a modal connector. \n\nMultimodal Encoder. This module incorporates non-text inputs, such as images or audio, and encoding the raw infor- Modal Connector. This module serves as a crucial bridge between different modalities, allowing efficient communication with the LLM. It accomplishes this by projecting information into a space that the LLM can readily comprehend. Through training the connector, the encoded multimodal tokens can be transformed to LLM prompt tokens that illustrate the content presented by the image, video, etc. Consequently, the LLM will generate the expected content based on the request and prompt. \n\nB. Optimization , Datasets, and Evaluation of Large Language Models 1) Instruction Tuning: Instruction tuning in large language models has undergone significant development, evolving from initial efforts in multi-task fine-tuning without explicit instruction prompts to sophisticated techniques leveraging diverse tasks and templates.",
            "score": 0.4589528145165079,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 29224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 79,
                    "end": 339
                },
                {
                    "start": 342,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1168
                },
                {
                    "start": 1171,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1771
                },
                {
                    "start": 1774,
                    "end": 2108
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 50,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 581,
                    "end": 585,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 671,
                    "end": 675,
                    "matchedPaperCorpusId": "254636568"
                },
                {
                    "start": 802,
                    "end": 806,
                    "matchedPaperCorpusId": "246634906"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88427734375
        },
        {
            "corpus_id": "273654137",
            "title": "GiVE: Guiding Visual Encoder to Perceive Overlooked Information",
            "text": "Multimodal Large Language Models (MLLMs) [1] have advanced general artificial intelligence with strong generation and inference capabilities in applications such as text-to-video generation [2], visual question answering [3], and embodied robotics [4]. A common architecture combines a visual encoder with a Large Language Model (LLM), embedding non-textual data into vectors interpretable by the LLM via a mapping mechanism. While research [5] highlights the effectiveness of this design, the quality of image embeddings from the visual encoder remains critical to MLLM performance. \n\nAn image encoder is a specialized visual encoder designed to map high-dimensional image data to a lowerdimensional feature space. These encoders can be broadly categorized based on their pre-training tasks into two main types: reconstruction-based and cross-modal contrastive learningbased encoders. Image encoding models trained with reconstruction tasks [6], [7] are proficient in capturing comprehensive image details. However, these models lack semantic alignment with text during training, which complicates the LLM's ability to interpret image embeddings [8], [9]. Consequently, such encoders are infrequently utilized in MLLMs. \n\nVision Transformer (ViT) models trained with image-text contrastive learning [10], [11] generally align effectively with LLMs but face an implicit \"ignore\" problem, limiting their expressive capability. This limitation arises because different modalities convey distinct types of information. For instance, an image may feature multiple objects with unique attributes, such as texture, color, spatial location, and potential interactions. In contrast, abstract text typically highlights only the most salient objects and provides limited descriptions of other visual elements. ViTs trained for image-text matching tend to focus on the salient regions of the image that correspond to the text, thereby overlooking secondary elements like the background. MLLMs using such visual encoders exhibit diminished response quality when users inquire about nonsalient objects.",
            "score": 0.45892898618861455,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1220
                },
                {
                    "start": 1223,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2089
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 44,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 190,
                    "end": 193,
                    "matchedPaperCorpusId": "258187553"
                },
                {
                    "start": 221,
                    "end": 224,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 248,
                    "end": 251,
                    "matchedPaperCorpusId": "276913118"
                },
                {
                    "start": 441,
                    "end": 444,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 1152,
                    "end": 1155,
                    "matchedPaperCorpusId": "264492080"
                },
                {
                    "start": 1300,
                    "end": 1304,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1306,
                    "end": 1310,
                    "matchedPaperCorpusId": "262179304"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94970703125
        },
        {
            "corpus_id": "267750413",
            "title": "MMMModal - Multi-Images Multi-Audio Multi-turn Multi-Modal",
            "text": "MMMModal aims to align visual and audio information from pretrained vision and audio encoders with an advanced large language model (LLM). We aim to bridge the gap between the visual and audio encoders and the LLM using a linear projection layer. To create an effective multimodal model, we followed a two-stage training approach exemplified by the works of [4,8,9] which have notably produced great results. In the initial stage, the model is pretrained on aligned image-text pairs and audio-text pairs to acquire knowledge of vision and audio language through the alignment projection layer. In the second stage, we fine-tune the pretrained model using a generated multiturn multiaudio images synthetic dataset, incorporating a designed conversational template to enable model comprehension on multi-images, multi-audio, and multi-images-multi-audio within a single multiturn session. \n\nThe visualization below provides an overview of the architecture for MMMModal,",
            "score": 0.4588020601064419,
            "section_title": "Finetuning Procedure",
            "char_start_offset": 20559,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 886
                },
                {
                    "start": 889,
                    "end": 967
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8359375
        },
        {
            "corpus_id": "278714664",
            "title": "Visual Planning: Let's Think Only with Images",
            "text": "Large Language Models (LLMs) [6,38,2] have demonstrated strong capabilities in language understanding and generation, as well as growing competence in complex reasoning, enabled by their chain-of-thought reasoning abilities [50]. Building on these advances, recent work extends LLMs to support multiple modalities, yielding so-called Multimodal Large Language Models (MLLMs) [42,22]: they incorporate visual embedded information at the input to tackle a broader spectrum of tasks, such as visual spatial reasoning [33,30] and navigation [15,29]. However, despite their multimodal inputs, these methods perform reasoning purely in the text format during inference, from captioning visual content [19] to generating verbal rationales [59]. \n\nBuilding on this observation, we argue that performing multimodal reasoning only in the text pathway may not always offer the most intuitive or effective strategy, particularly for tasks that depend heavily on visual information and/or are 'vision-first' by design. Indeed, recent results from multimodal benchmarks [43,30,7,8] offer growing evidence that purely language-based reasoning falls short in certain domains, particularly those involving spatial, geometric, or physical dynamics [56]. Such reliance on grounding visual information into text before reasoning introduces a modality gap that Go straight to the crossing. \n\nThen turn slightly left at the crossing. \n\nTo conclude: you should first go straight to ...... hinders the model's ability to capture visual features and state transitions. This highlights a potential shortcoming of current MLLMs: while they process image inputs, they do not naturally \"think\" in images. For instance, tasks such as planning a route through a maze, designing the layout of a room, or predicting the next state of a mechanical system are often better served by visual representations, as verbal descriptions may struggle to accurately capture complex spatial reasoning relationships. These examples suggest a broader question, which we aim to tackle in this work: can models plan in non-verbal modalities, such as images, without being mediated by text? \n\nCognitive science also offers compelling motivation for this question [36]. Dual Coding Theory [39] proposes that human cognition operates through both verbal and nonverbal channels, each capable of independent representational and inferential processes.",
            "score": 0.4587406173461257,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1368
                },
                {
                    "start": 1371,
                    "end": 1411
                },
                {
                    "start": 1414,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2140
                },
                {
                    "start": 2143,
                    "end": 2218
                },
                {
                    "start": 2219,
                    "end": 2397
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 32,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 32,
                    "end": 35,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 224,
                    "end": 228,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 514,
                    "end": 518,
                    "matchedPaperCorpusId": "248496506"
                },
                {
                    "start": 518,
                    "end": 521,
                    "matchedPaperCorpusId": "270226026"
                },
                {
                    "start": 537,
                    "end": 541,
                    "matchedPaperCorpusId": "247627890"
                },
                {
                    "start": 541,
                    "end": 544,
                    "matchedPaperCorpusId": "268733151"
                },
                {
                    "start": 732,
                    "end": 736,
                    "matchedPaperCorpusId": "256504063"
                },
                {
                    "start": 1060,
                    "end": 1063,
                    "matchedPaperCorpusId": "270226026"
                },
                {
                    "start": 1063,
                    "end": 1065,
                    "matchedPaperCorpusId": "270062772"
                },
                {
                    "start": 1065,
                    "end": 1067,
                    "matchedPaperCorpusId": "274789454"
                },
                {
                    "start": 2213,
                    "end": 2217,
                    "matchedPaperCorpusId": "6684396"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.794921875
        },
        {
            "corpus_id": "275336723",
            "title": "A Survey of State of the Art Large Vision Language Models: Alignment, Benchmark, Evaluations and Challenges",
            "text": "These models use contrastive learning to align image and text embeddings in a shared latent space, effectively capturing cross-modal relationships. However, newer models, such as LLaVA [142], often do not include a dedicated text encoder. Instead, they rely on large language models (LLMs) (e.g., LLaMA [212], Vicuna [186]) for text understanding, integrating visual inputs through projection layers or cross-attention mechanisms [137]. This shift shows a growing trend of using the capabilities of LLMs over vision components for more versatile and advanced multimodal reasoning and generation tasks. Text Decoder leverages LLMs as the primary text generator, using visual encoders to project image features [106]. GPT-4V [182], Flamingo [8], and Kosmos-2 [188] use this approach. These models typically use a minimal visual projection mechanism, allowing the powerful language decoder to generate contextually rich outputs. VisualBERT and Vil-BERT [127,153] provide the foundation to decoder architectures for multimodal pretraining. Training VLMs from scratch typically requires a separate text decoder, whereas using LLMs as the backbone often uses the original decoders from the LLM. (Figure 1). Cross-Attention Mechanisms enable visual-text interactions by allowing tokens from one modality (vision) to influence tokens from the other modality (text) [137]. These layers compute attention scores across modalities, but not all models use them. VisualBERT [153] and Flamingo [9] employ cross-attention, while CLIP [194] does not.",
            "score": 0.45848561650759634,
            "section_title": "Common Architecture Components",
            "char_start_offset": 5826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1534
                }
            ],
            "ref_mentions": [
                {
                    "start": 1480,
                    "end": 1483,
                    "matchedPaperCorpusId": "248476411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95458984375
        },
        {
            "corpus_id": "266573844",
            "title": "MIVC: Multiple Instance Visual Component for Visual-Language Models",
            "text": "With the continuous evolution of deep learning, an increasing number of research endeavors have shifted their focus towards the fusion of different modalities to address more complex scenarios in the real world.For instance, in Visual Question Answering (VQA), users may pose ques-tions to models based on a set of images, expecting answers.In image captioning, users provide a set of images, asking the model to generate descriptive text regarding the content of these images.In early works [2,11,38], images and text were separately transformed into features using ResNet and LSTM, followed by concatenation before being fed into a prediction layer for inference.\n\nIn recent years, with the emergence of Transformers [32], there has been a revolutionary shift in the universal network architecture in the field of Natural Language Processing (NLP).The utilization of the global attention mechanism within Transformers has become increasingly prevalent.Subsequently, exploration of Transformers in the field of Computer Vision (CV) has also taken flight.Vision Transformers (ViT) [9], for example, employ the same Transformer architecture as in NLP but divide images into several patches to be treated as vision tokens as input.With extensive pre-training on large scale data, ViT has demonstrated superior performance to ResNet.Through Transformers, CV and NLP have achieved structural unification which enables multimodal model development.For example, phrase grounding [12,34] aligns the visual signals with arbitrary caption words semantically, which extends the object detection task beyond the fixed list of categories in the label set.\n\nMore recently, with the rise of generative models [3,27,31], the application of multimodal capabilities to generative tasks has become an open question.DALL-E [28], for instance, embarked on a pretraining task where images were tokenized, enabled text-to-image generation.Subsequently, various vision-language models (VLMs) have been proposed to enhance the fusion of text and images.For example, BLIP2 [18] introduced the use of a Q-Former to align images more effectively with the input space of text.TCL [35] employed triplet contrastive learning to simultaneously learn from text and images.",
            "score": 0.4584091673774095,
            "section_title": "SOTA Visual-Language Models",
            "char_start_offset": 5117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 341
                },
                {
                    "start": 341,
                    "end": 477
                },
                {
                    "start": 477,
                    "end": 665
                },
                {
                    "start": 667,
                    "end": 850
                },
                {
                    "start": 850,
                    "end": 954
                },
                {
                    "start": 954,
                    "end": 1055
                },
                {
                    "start": 1055,
                    "end": 1229
                },
                {
                    "start": 1229,
                    "end": 1330
                },
                {
                    "start": 1330,
                    "end": 1443
                },
                {
                    "start": 1443,
                    "end": 1643
                },
                {
                    "start": 1645,
                    "end": 1797
                },
                {
                    "start": 1797,
                    "end": 1917
                },
                {
                    "start": 1917,
                    "end": 2029
                },
                {
                    "start": 2029,
                    "end": 2148
                },
                {
                    "start": 2148,
                    "end": 2240
                }
            ],
            "ref_mentions": [
                {
                    "start": 492,
                    "end": 495,
                    "matchedPaperCorpusId": "3180429"
                },
                {
                    "start": 495,
                    "end": 498,
                    "matchedPaperCorpusId": "8081284"
                },
                {
                    "start": 498,
                    "end": 501,
                    "matchedPaperCorpusId": "6733279"
                },
                {
                    "start": 1473,
                    "end": 1477,
                    "matchedPaperCorpusId": "219721362"
                },
                {
                    "start": 1698,
                    "end": 1701,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1804,
                    "end": 1808,
                    "matchedPaperCorpusId": "232035663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "273185450",
            "title": "Human-in-the-loop Reasoning For Traffic Sign Detection: Collaborative Approach Yolo With Video-llava",
            "text": "Large language models (LLMs) employ transformer-based neural network architecture and are trained on vast amounts of text data to identify patterns and relationships in natural language. These models have demonstrated the ability to both interpret and produce text like human language. However, LLMs are primarily focused on processing text inputs and outputs, lacking the ability to directly perceive and reason about visual information like images and videos. This limitation has motivated research into extending LLMs to handle multi-modal inputs beyond just text. Vision-language models (VLMs), also known as large vision-language models (LVLMs), aim to bridge this gap by enabling LLMs to understand and generate responses based on both textual and visual inputs. These models typically involve integrating computer vision components like convolutional neural networks (CNNs) or vision transformers (ViTs) [11] to extract visual features, which are then aligned and fused with the text representations within the LLM architecture. Approaches like pre-training on large image-text or video-text datasets and instruction tuning [17] have shown promising results in grounding LLMs in visual contexts for tasks like visual question answering, image/video captioning, and multimodal reasoning. Recent visual-language models like Video-LLaVA [4] take this a step further by introducing a unified approach to visual-language modeling, integrating both image and video data into a single, coherent representation for large language models. Video-LLaVA is based on an auto-regressive language model, which is fine-tuned on multimodal instruction-following data generated by Llava1.5 [10] and VideoChat [12]. \n\nAs shown in Figure 2 the structure of Video-LLaVA is designed to effectively integrate visual and textual data into a unified representation for enhanced multi-modal reasoning. Central to its structure are the LanguageBind encoders [18], which extract features from both images and videos and map these visual signals into a shared textual feature space, facilitating a unified visual representation. These encoders are initialized from OpenCLIP [14], ensuring that the visual features are aligned with the language features, thereby creating a unified visual representation.",
            "score": 0.4583866382144247,
            "section_title": "Large Vision-Language Model",
            "char_start_offset": 6479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2281
                }
            ],
            "ref_mentions": [
                {
                    "start": 1131,
                    "end": 1135,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92919921875
        },
        {
            "corpus_id": "262059822",
            "title": "Brain-Inspired Remote Sensing Foundation Models and Open Problems: A Comprehensive Survey",
            "text": "In the current landscape, FMs have evolved to encompass more than just deep models; they now focus on utilizing vast amounts of data and computational power to tackle diverse problems. The goal is to use a unified model capable of addressing multiple modalities and tasks. Consequently, there is a growing emphasis on training vision-language models [123]. Table II summarizes some of the representative vision-language FMs. CLIP [10] is a prominent example that leverages a large-scale collection of image-text pairs from the Internet for contrastive learning, enabling the creation of a unified representation of multimodal data. Inspired by CLIP, various multimodal image FMs have been proposed, differing in their model structures [30], [124], feature representations [125], [126], multimodal feature fusion approaches [27], feature alignment loss functions [28], pretraining methods [29], and more. These advancements have significantly improved the performance of multimodal FMs. \n\nIn addition, DALL-E [127] combines the diffusion model with multimodal FMs to generate images from text. SAM [31] introduces a promptable model with training strategies that enable the segmentation of objects using text, points, and lines as prompts. GPT-4 [128] exhibits superhuman capabilities on  various professional and academic datasets. mPLUG-2 [130] introduces a multimodule composition network, including text, image, and video. Metatransformer [131] proposed a unified framework performing learning across 12 modalities with unpaired data (e.g., natural language, 2-D images, 3-D point clouds, audio, video, time series, and tabular data). \n\nThese models are all built upon large-scale training data and self-supervised methods, harnessing the potential of unlabeled multimodal data encompassing both vision and language to train the FMs. As a result, these models can effectively perform a wide range of tasks that involve both vision and language processing.",
            "score": 0.45828397262020615,
            "section_title": "C. Vision-Language FM",
            "char_start_offset": 39763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1637
                },
                {
                    "start": 1640,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1958
                }
            ],
            "ref_mentions": [
                {
                    "start": 430,
                    "end": 434,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 735,
                    "end": 739,
                    "matchedPaperCorpusId": "260068316"
                },
                {
                    "start": 741,
                    "end": 746,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 823,
                    "end": 827,
                    "matchedPaperCorpusId": "236034189"
                },
                {
                    "start": 862,
                    "end": 866,
                    "matchedPaperCorpusId": "248512473"
                },
                {
                    "start": 888,
                    "end": 892,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1097,
                    "end": 1101,
                    "matchedPaperCorpusId": "257952310"
                },
                {
                    "start": 1340,
                    "end": 1345,
                    "matchedPaperCorpusId": "256459873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95166015625
        },
        {
            "corpus_id": "276618123",
            "title": "LiGT: Layout-infused Generative Transformer for Visual Question Answering on Vietnamese Receipts",
            "text": "We choose the Text-to-Text Transfer Transformer (T5 [27]) as the language model backbone for our architecture. T5 is a widely used model to tackle VQA tasks, easily adapting to process additional modalities. Based on the T5 architecture, many multimodal extensions have proposed and achieved state-of-the-art performance not only in document VQA [5,19,21,23] but also in scene-text VQA [34][35][36], and many other multimodal studies. Empirical studies show that T5 architecture has a certain level of resistance against OCR errors with its capability to generate texts by using the model's built-in vocabulary. Applying the T5 model to our Language Model component is projected to enhance 2D spatial processing ability and equip our LiGT model the flexibility in producing answers. To effectively handle the Vietnamese language, we made use of the Vietnamese version of T5, named ViT5 [37], pretrained on a large resource of high-quality and diverse Vietnamese texts.",
            "score": 0.45783596993663445,
            "section_title": "Language Model",
            "char_start_offset": 29831,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 968
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 56,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 346,
                    "end": 349,
                    "matchedPaperCorpusId": "231719058"
                },
                {
                    "start": 349,
                    "end": 352,
                    "matchedPaperCorpusId": "231951453"
                },
                {
                    "start": 352,
                    "end": 355,
                    "matchedPaperCorpusId": "254275326"
                },
                {
                    "start": 386,
                    "end": 390,
                    "matchedPaperCorpusId": "245425054"
                },
                {
                    "start": 390,
                    "end": 394,
                    "matchedPaperCorpusId": "261395684"
                },
                {
                    "start": 394,
                    "end": 398,
                    "matchedPaperCorpusId": "252211798"
                },
                {
                    "start": 886,
                    "end": 890,
                    "matchedPaperCorpusId": "248798402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82568359375
        },
        {
            "corpus_id": "268385444",
            "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
            "text": "Multimodal Language Models Recent Multimodal Language Models have made remarkable advances in various domains, including vision and language understanding (Li et al., 2022;2023b;Liu et al., 2023;Huang et al., 2023c;Peng et al., 2023;Zhu et al., 2023), interleaved image and text understanding (Alayrac et al., 2022), interleaved image and text generation (Dong et al., 2023).Some more unified models can perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio (Wu et al., 2023;Lu et al., 2023).However, none of these models can perceive 3D inputs or output actions according to 3D input.\n\nVision-Language-Action Models Previous vision-language models with action output have predominantly leveraged 2D features, thereby lacking the capability of 3D spatial understanding (Driess et al., 2023b;Brohan et al., 2022;2023).In contrast, our model is guided by 3D features, which are predicted in alignment with goal objectives in our general world model.We are the first to leverage 3D features such as point clouds for action token generation, significantly improving action planning accuracy.Additionally, this pipeline possesses the potential to be extended for applications in real-world scenarios.\n\n3D Foundation Models Our paper is closely related to the 3D foundation models that integrate 3D features in MLLMs (Hong et al., 2023;Chen et al., 2023b;Qi et al., 2023;Xu et al., 2023;Huang et al., 2023a;Zhou et al., 2023;Guo et al., 2023;Li et al., 2024).These studies have successfully stepped forward to leverage foundation models to comprehend 3D features.However, they primarily focus on analyzing and reasoning in the current observable state of the 3D scenes, thereby revealing a limitation in predicting future features that extend beyond immediate perception.Contrasting with them, we aim to not only understand the perceivable scenes but also predict imperceptible multimodal features guided by specific goals.This capability enables our model to further generate action tokens to interact with the 3D world.",
            "score": 0.4574871191264999,
            "section_title": "Related Works",
            "char_start_offset": 5047,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 375
                },
                {
                    "start": 375,
                    "end": 536
                },
                {
                    "start": 536,
                    "end": 629
                },
                {
                    "start": 631,
                    "end": 861
                },
                {
                    "start": 861,
                    "end": 991
                },
                {
                    "start": 991,
                    "end": 1131
                },
                {
                    "start": 1131,
                    "end": 1239
                },
                {
                    "start": 1241,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1601
                },
                {
                    "start": 1601,
                    "end": 1809
                },
                {
                    "start": 1809,
                    "end": 1961
                },
                {
                    "start": 1961,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 172,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 293,
                    "end": 315,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 519,
                    "end": 535,
                    "matchedPaperCorpusId": "249848272"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8623046875
        },
        {
            "corpus_id": "272987668",
            "title": "From Linguistic Giants to Sensory Maestros: A Survey on Cross-Modal Reasoning with Large Language Models",
            "text": "Multimodal pre-training is a methodology used to train models to acquire knowledge from various data sources such as images, text, and audio. The goal is to develop a comprehensive understanding of the world by leveraging extensive datasets. These models effectively grasp the complex interplay between different modalities and generate rich representations that capture key semantic information. \n\nFlamingo [121], a cutting-edge Visual Language Model (VLM), adeptly amalgamates textual and visual information to swiftly adapt to cross-modal reasoning tasks with limited data. Utilizing pre-trained models, the Perceiver Resampler converts features from the Vision Encoder into tokens, conditioning the language model through cross-attention layers. This innovative approach enables accurate prediction of subsequent tokens, generating coherent, contextually relevant responses. Throughout the training process, Flamingo selectively fine-tunes the Perceiver Resampler and language modules, while keeping the Vision Encoder parameters fixed. \n\nCheetor [122] endeavors to leverage the Visual Prompt Generator (VPG) for improved performance in CMR tasks, with a particular focus on VQA. Currently, available VPG approaches, such as linear projection, Resampler, and Qformer, extract visual prompts from densely encoded image features obtained through vision backbones, exemplified by ViT [84]. \n\nMAGMA [123], by converting image features into language embeddings, adapts language transformers to visual data without complete retraining. It comprises four elements: a Visual Encoder that generates feature vectors from images, an Image Prefix module that translates these vectors into embeddings, an autoregressive Language Model integrating these embeddings, and Adapter layers tuned in situ within the transformer language model. Throughout the training, the Language Model's architecture remains static, utilizing pre-existing GPT-J weights [124]. Simultaneously, the Visual Encoder draws on pre-trained CLIP weights, with the Image Prefix and Adapters undergoing training from the ground up. \n\nPrismer [125], an efficient encoder-decoder model, leverages a pre-trained expert library for handling multimodal inputs and generating text. It employs a vision encoder for RGB images and labels, alongside an autoregressive text decoder.",
            "score": 0.45736695665636273,
            "section_title": "Multimodal Pre-training Foundation Models",
            "char_start_offset": 37372,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1390
                },
                {
                    "start": 1393,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2091
                },
                {
                    "start": 2094,
                    "end": 2235
                },
                {
                    "start": 2236,
                    "end": 2332
                }
            ],
            "ref_mentions": [
                {
                    "start": 408,
                    "end": 413,
                    "matchedPaperCorpusId": "248476411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94580078125
        },
        {
            "corpus_id": "268385552",
            "title": "Veagle: Advancements in Multimodal Representation Learning",
            "text": "In recent years, the surge of interest in Large Language Models (LLMs) has reshaped the landscape of natural language under-standing, a significant surge in the examination and application of Multimodal Large Language Models (MLLMs) has been ob-served. \n\nAllowing models to harness various modalities such as text, images, videos, and voice, MLLMs have become vi-tal in the creation of adaptable all-purpose assistants. Despite their impressive generalization abilities across a wide spec-trum of tasks and the development of Vision Language Mod-els (VLMs) which incorporate LLMs with visual understanding competence, contemporary models encounter challenges in in-terpreting embedded text within images. This limitation is the focal point of this research as images inclusive of text are preva-lent in our everyday lives and comprehending such content is imperative for human visual perception. \n\nOur research presents a new way of doing things by combining learned query embeddings with additional visual assistance. This method uses encoded patch embeddings to deal with the limitations of information that language models typically get from images. As a result, it enhances how well a model can understand and perceive the relationship between text and images. Our model, called Veagle, starts by using a pre-trained vision encoder and language model. We train it in two stages to avoid forgetting what it already knows and make training less complicated, ultimately making the model more effective. We tested the model using standard Visual Question-Answering (VQA) benchmarks and protocols for evaluating images with a lot of text. Our Veagle model significantly improves the un-derstanding and perception of the relationship between text and images, outperforming traditional benchmarks in addressing the challenges of comprehending embedded text within images. \n\nIn this research, we present Veagle, an innovative model that represents a significant leap forward in the field of multimodal learning and interpretation. At the heart of Veagle is the incorporation of an enhanced version of the BLIVA [1] architecture, where cutting-edge components synergize to amplify its capabilities. Notably, we integrate a superior vision abstrac-tor sourced from mPlugOwl [2], enhancing the model's visual processing capabilities.",
            "score": 0.4572977999130498,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 255,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1868
                },
                {
                    "start": 1871,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2193
                },
                {
                    "start": 2194,
                    "end": 2326
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8935546875
        },
        {
            "corpus_id": "273850262",
            "title": "REEDIT: Multimodal Exemplar-Based Image Editing",
            "text": "Diffusion Models. Prior to diffusion models, GANs [13,59,63] were the de-facto generative models used for (conditional) image synthesis and editing. However, training GAN networks is prone to instability and mode collapse, among many issues. Recently, large-scale text-to-image generative models [8, 9, 39-41, 43, 58] have benefitted from superior model architectures [50] and large-scale training data available on the internet. Of particular interest is diffusion models [17,35,37,40,41,43,46], that are trained to denoise random gaussian noise resulting in high-fidelity and highly diverse images. These models are typically trained on millions of text-image pairs. In this work, we use a pretrained Stable Diffusion [41] model which operates in the latent space instead of the image pixel space. \n\nMultimodal Vision-Language Models (VLMs). Multimodal VLMs [25, 27-29, 38, 45] have the remarkable capability to understand and process both texts and images. Two particularly useful works fall in the scope of this paper: CLIP [38] and LLaVA [27][28][29]. CLIP represents both images and texts in a shared embedding space. It was trained on 400M image-text pairs in a contrastive manner -maximizing the similarity between related image-text embeddings, while minimizing the similarity between unrelated image-text embeddings. LLaVA combines a visual encoder with Vicuna [5] to provide powerful language and visual comprehension capabilities. It has impressive capacity to follow user instructions based on visual cues. \n\nText-based Image Editing. Diffusion models, with their impressive generative capabilites, have also been adapted for image editing [7,10,19,23,24,33,36,42,49,56,60,62]. Multimodal models like CLIP [38], and cross-attention mechanisms [50] have enabled conditioning a diffusion model to directly edit an image with a text input [2,35]. SDEdit [32] takes an image as input along with a user guide, and subsequently denoises it using SDE prior to increase its realism.",
            "score": 0.45704100838204775,
            "section_title": "Related Work",
            "char_start_offset": 3691,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 18,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1519
                },
                {
                    "start": 1522,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1987
                }
            ],
            "ref_mentions": [
                {
                    "start": 54,
                    "end": 57,
                    "matchedPaperCorpusId": "1277217"
                },
                {
                    "start": 57,
                    "end": 60,
                    "matchedPaperCorpusId": "14924561"
                },
                {
                    "start": 483,
                    "end": 486,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 486,
                    "end": 489,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 720,
                    "end": 724,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1028,
                    "end": 1032,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1043,
                    "end": 1047,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 1051,
                    "end": 1055,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1662,
                    "end": 1665,
                    "matchedPaperCorpusId": "244909410"
                },
                {
                    "start": 1668,
                    "end": 1671,
                    "matchedPaperCorpusId": "253581838"
                },
                {
                    "start": 1671,
                    "end": 1674,
                    "matchedPaperCorpusId": "256616002"
                },
                {
                    "start": 1674,
                    "end": 1677,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 1677,
                    "end": 1680,
                    "matchedPaperCorpusId": "253801961"
                },
                {
                    "start": 1683,
                    "end": 1686,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 1686,
                    "end": 1689,
                    "matchedPaperCorpusId": "254408758"
                },
                {
                    "start": 1719,
                    "end": 1723,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1849,
                    "end": 1852,
                    "matchedPaperCorpusId": "244714366"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91259765625
        },
        {
            "corpus_id": "273655206",
            "title": "Vulnerability of LLMs to Vertically Aligned Text Manipulations",
            "text": "\u2022 GPT-3.5: A robust large language model developed by OpenAI, capable of generating text based on instructions, and highly effective across diverse natural language processing tasks. \n\n\u2022 GPT-4: An advanced multi-modal language model from OpenAI that accepts both image and text inputs for text generation, achieving near-human performance on various benchmarks. \n\n\u2022 GPT-4O-MINI: A cost-efficient multimodal model, released by OpenAI on July 18, 2024, is a distilled version of GPT-4O, offering low latency and cost while supporting a wide range of tasks.",
            "score": 0.45682650084994814,
            "section_title": "A Large Language Models",
            "char_start_offset": 25659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 361
                },
                {
                    "start": 364,
                    "end": 554
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "250698723",
            "title": "NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis",
            "text": "Autoregressive Methods DALL\u2022E [19] tokenizes each image into discrete visual tokens and trains an autoregressive model to generate visual tokens from the corresponding text. The output image is reconstructed by the VQVAE decoder, which takes the visual tokens generated by the autoregressive model as inputs. Parti [30] follows the same architecture of DALL\u2022E, but uses ViT-VQGAN [29] to discretize and reconstruct images, which is an improved version of VQGAN from both architecture and codebook learning aspects. NUWA [28] is the first autoregressive visual synthesis pre-trained model to support both image and video generation tasks. Compared to these previous works, NUWA-Infinity introduces the autoregressive over autoregressive mechanism into the generation procedure, which enables the capability of generating variable-size images and videos. Diffusion Methods DALL\u2022E 2 [18] generates image embedding from an input text based on either an autoregressive or a diffusion model, and uses a diffusion model to produce the output image. Imagen [20] uses a frozen large-scale pre-trained language model T5-XXL [17] to encode each input text, and uses two diffusion models to generate high-resolution images based on the text embeddings. Both of these two diffusion-based text-to-image generation methods cannot support arbitrarily-sized image generation, as the size of the output images is pre-defined before training and inference.",
            "score": 0.45672757629566485,
            "section_title": "Related Work",
            "char_start_offset": 4524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1437
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 34,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 380,
                    "end": 384,
                    "matchedPaperCorpusId": "238582653"
                },
                {
                    "start": 520,
                    "end": 524,
                    "matchedPaperCorpusId": "244527261"
                },
                {
                    "start": 1114,
                    "end": 1118,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85693359375
        },
        {
            "corpus_id": "267200040",
            "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion",
            "text": "Multimodal Large Language Models (MLLMs) have significantly broadened the capabilities of language models to process various modalities (Liu et al., 2023a;Li et al., 2021Li et al., , 2022a;;Wang et al., 2023;Driess et al., 2023). These models inherently facilitate interleaved vision-language input, effectively handling multiple images. Models such as GILL (Koh et al., 2023), Emu (Sun et al., 2023), and DreamLLM (Dong et al., 2023) specialize in interleaved vision-language generation by aligning the output space of MLLMs with the diffusion image decoder. However, these methods primarily align at a semantic level and may struggle with detailed, subject-driven image generation. BLIP-Diffusion (Li et al., 2023a) synthesizes images by composing subjects with random backgrounds, endowing it with zero-shot, subject-driven text-toimage generation capabilities. However, its specific input template and training process limit scalability for multiple entities. KOSMOS-G (Pan et al., 2023), a model closely related to our work, leverages a MLLM to encode interleaved text-visual inputs, and the U-Net of Stable Diffusion (SD) v1.5 as the image decoder. The key component of KOSMOS-G is an AlignerNet, which is trained solely on textual data to align the output embedding space of the frozen SDv1.5 U-Net with the MLLM. In contrast, our approach centers on training the U-Net model end-to-end specifically for multimodal diffusion, significantly enhancing both the faithfulness and relevance of generated images in multimodal contexts. Differing from alignmentbased approaches, our two-stage training strategy markedly improves the model's proficiency in following multimodal instructions, particularly in complex multi-entity scenarios.",
            "score": 0.4567096511985207,
            "section_title": "Generating with Multi-modal Language Models",
            "char_start_offset": 14730,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1738
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 170,
                    "matchedPaperCorpusId": "229924402"
                },
                {
                    "start": 170,
                    "end": 190,
                    "matchedPaperCorpusId": "247519008"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96435546875
        },
        {
            "corpus_id": "264145994",
            "title": "VLIS: Unimodal Language Models Guide Multimodal Language Generation",
            "text": "Multimodal language generation, which leverages the synergy of language and vision, is a rapidly expanding field. However, existing vision-language models face challenges in tasks that require complex linguistic understanding. To address this issue, we introduce Visual-Language models as Importance Sampling weights (VLIS), a novel framework that combines the visual conditioning capability of vision-language models with the language understanding of unimodal text-only language models without further training. It extracts pointwise mutual information of each image and text from a visual-language model and uses the value as an importance sampling weight to adjust the token likelihood from a text-only model. VLIS improves vision-language models on diverse tasks, including commonsense understanding (WHOOPS, OK-VQA, and ScienceQA) and complex text generation (Concadia, Image Paragraph Captioning, and ROCStories). Our results suggest that VLIS represents a promising new direction for multimodal language generation.",
            "score": 0.45649484343019153,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89111328125
        },
        {
            "corpus_id": "277452239",
            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
            "text": "The LLaVA model, schematically represented in Fig. 1, consists of these components: \n\n\u2022 Vision Encoder: Processes image data using a pre-trained Vision Transformer (ViT) model with multiple transformer layers, such as CLIP (Radford et al., 2021) or SigLIP (Zhai et al., 2023). It extracts visual features from input images; \u2022 Language Model: Handles text processing, typically an LLM such as Qwen (Bai et al., 2023), capable of understanding, generating, and reasoning with natural language; \u2022 Projector: Since the vision encoder and language model operate in different feature spaces, the projector transforms visual embeddings into a format compatible with the language model input space. This enables effective integration of visual and textual information. LLaVA employs a twolayer Multi-Layer Perceptron (MLP) trained to align the modalities, ensuring that the visual embeddings can be seamlessly used by the language model; \u2022 Multimodal Fusion Layer: It aligns and fuses the visual features from the vision encoder with the text embeddings from the language model, enabling the model to process both modalities jointly through self-attention mechanisms. \n\nThe model processes multimodal image-text inputs as follows: \n\n1. The input text (instruction or query) is tokenized into numerical tokens using a predefined vocabulary. 2. The input image is divided into patches and passed through the vision encoder, which extracts key features such as objects, colors, textures, and spatial relationships. The projector then converts these visual representations into language-compatible embeddings; 3. The multimodal fusion layer integrates the visual embeddings into the input sequence of the language model, allowing it to process both visual and textual data jointly. \n\nThe model then generates an output response based on the given task, such as answering questions about the image (VQA) or generating a descriptive caption (image captioning).",
            "score": 0.4564039786571711,
            "section_title": "Model architecture",
            "char_start_offset": 12503,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 86,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1222
                },
                {
                    "start": 1225,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1769
                },
                {
                    "start": 1772,
                    "end": 1946
                }
            ],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 245,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.837890625
        },
        {
            "corpus_id": "271213320",
            "title": "ICCV23 Visual-Dialog Emotion Explanation Challenge: SEU_309 Team Technical Report",
            "text": "To bridge this gap, the research and development of Large Vision Language Models (LVLMs) [9,2,10,15,14] have gained momentum. LVLMs are designed to perceive and understand both textual and visual information, thereby broadening the scope of applications for AI models. This integration allows for a more holistic understanding of content, enabling models to perform tasks that involve both text and images, such as image captioning, visual question answering, and visual grounding. \n\nLVLMs are built upon the foundation of LLMs, incorporating additional components that allow them to process and understand visual data. These components typically include visual receptors for image processing, input-output interfaces that enable the model to handle multimodal data, and training pipelines that are tailored to accommodate the complexities of learning from both text and images. Furthermore, these models are often trained on multilingual multimodal cleaned corpora, enhancing their ability to function across different languages and modalities. \n\nThe development of LVLMs represents a significant step forward in the field of AI, offering new possibilities for applications that require an understanding of the world that extends beyond text. By combining the capabilities of LLMs with the ability to process visual information, LVLMs are paving the way for more advanced and versatile AI systems. \n\nThese models have set new benchmarks in various visualcentric tasks, demonstrating their potential in enhancing the performance and applicability of AI across a broad range of domains.",
            "score": 0.4558425490719533,
            "section_title": "Related Work",
            "char_start_offset": 4643,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1045
                },
                {
                    "start": 1048,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1398
                },
                {
                    "start": 1401,
                    "end": 1585
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9658203125
        },
        {
            "corpus_id": "270703735",
            "title": "MR-MLLM: Mutual Reinforcement of Multimodal Comprehension and Vision Perception",
            "text": "The proposed Mutually Reinforced Multimodal Large Language Model (MR-MLLM) framework, shown in Figure 4, represents a groundbreaking approach to multimodal learning.It integrates the strengths of visual perception and language comprehension, pioneering a shared query mechanism that synergistically enhances both tasks of multimodal comprehension and vision perception.This framework leverages the rich perceptual information from vision perception models and the contextual insights of large language models, harmonizing them to elevate multimodal comprehension.\n\nCentral to MR-MLLM is the dual-stream architecture, composed of a visual perception stream and a linguistic processing stream.The visual stream, powered by pre-trained visual encoders, f V , transforms raw images I into visual feature spaces V f :\n\nIn parallel, the linguistic stream, realized through a Large Language Model, f L , processes textual input T into rich contextual embeddings L e , adept at capturing the nuanced interdependencies within the text:\n\nWe introduce a novel shared query structure, Q shared , which is formulated by integrating the outputs from two pre-trained visual encoders, which are perception forward and visual forward.This integration, achieved through a sophisticated fusion function F, leverages multi-knowledge interaction to enhance mutual understanding:\n\nFurthermore, MR-MLLM innovatively incorporates perception outputs as a new modality, enriching multimodal interactions.This is realized by embedding these perception outputs, P o , into the existing multimodal framework G:\n\nAdditionally, MR-MLLM enhances traditional prompt generation by integrating perception-enhanced prompts into the Large Language Model.This integration leads to a more informed and contextually aware language model, significantly enhancing its response generation capabilities.The enhanced prompt generation manner is represented as E prompt , which refines the model's ability to generate relevant and contextually rich responses.This is mathematically articulated as:\n\nIn this equation, P represents the function responsible for encoding the enhanced prompts, P o denotes the perception outputs integrated into the prompt, and T orig is the original textual prompt.",
            "score": 0.45553432592787413,
            "section_title": "Overall pipeline of MR-MLLM",
            "char_start_offset": 10749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 165,
                    "end": 369
                },
                {
                    "start": 369,
                    "end": 563
                },
                {
                    "start": 565,
                    "end": 691
                },
                {
                    "start": 691,
                    "end": 812
                },
                {
                    "start": 814,
                    "end": 1026
                },
                {
                    "start": 1028,
                    "end": 1217
                },
                {
                    "start": 1217,
                    "end": 1357
                },
                {
                    "start": 1359,
                    "end": 1478
                },
                {
                    "start": 1478,
                    "end": 1581
                },
                {
                    "start": 1583,
                    "end": 1717
                },
                {
                    "start": 1717,
                    "end": 1859
                },
                {
                    "start": 1859,
                    "end": 2013
                },
                {
                    "start": 2013,
                    "end": 2051
                },
                {
                    "start": 2053,
                    "end": 2249
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92138671875
        },
        {
            "corpus_id": "274776316",
            "title": "Optimizing Vision-Language Interactions Through Decoder-Only Models",
            "text": "Vision-Language Models (VLMs) have become pivotal in advancing tasks requiring multimodal understanding, such as visual question answering (VQA), image captioning, and multimodal dialogue systems. Traditional VLMs predominantly employ a two-stage framework: a visual encoder extracts image features, which are subsequently processed by a language model (LM) to generate outputs. While effective, this encoderdecoder approach introduces several challenges that hinder scalability, efficiency, and cross-modal alignment [1]. \n\nOne significant limitation of encoder-based VLMs is their reliance on separate visual encoders. This architectural choice imposes constraints on input resolution and aspect ratios, as the encoder must be pre-trained to handle specific image distributions. Moreover, the dependence on a visual encoder increases computational overhead during both training and inference, complicating deployment in resource-constrained environments. Additionally, the outputs from visual encoders often lack fine-grained alignment with the language model's latent space, leading to suboptimal cross-modal reasoning and integration [1]. \n\nMotivated by these challenges, researchers have explored encoder-free VLMs that eliminate the need for dedicated visual encoders, processing raw visual inputs directly within a unified architecture. This paradigm shift offers several advantages: (1) the ability to handle images of arbitrary resolutions and aspect ratios; (2) reduced computational complexity and improved inference efficiency; and (3) enhanced flexibility for aligning multimodal representations [1]. However, existing encoder-free approaches still face critical limitations, such as difficulty in balancing visual and textual information and reduced interpretability in handling complex multimodal tasks. \n\nTo address these limitations, we propose a novel framework, MUDAIF (Multimodal Unified Decoder with Adaptive Input Fusion), which introduces a purely decoder-based architecture for VLMs. Unlike traditional methods, MUDAIF integrates visual and textual modalities through a Vision-Token Adapter (VTA) layer that dynamically converts image data into pseudo-text tokens, enabling seamless processing by the language model.",
            "score": 0.4554696438451046,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1142
                },
                {
                    "start": 1145,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1818
                },
                {
                    "start": 1821,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2240
                }
            ],
            "ref_mentions": [
                {
                    "start": 518,
                    "end": 521,
                    "matchedPaperCorpusId": "270559398"
                },
                {
                    "start": 1138,
                    "end": 1141,
                    "matchedPaperCorpusId": "270559398"
                },
                {
                    "start": 1391,
                    "end": 1394,
                    "matchedPaperCorpusId": "270559398"
                },
                {
                    "start": 1609,
                    "end": 1612,
                    "matchedPaperCorpusId": "270559398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "272694523",
            "title": "OmniGen: Unified Image Generation",
            "text": "Generative Foundation Models. The GPT series [46,53] have demonstrated that language models can learn numerous 2 https://huggingface.co/openai/clip-vit-large-patch14-336 tasks via training on a large-scale dataset. Beyond language, multi-modal large language models [11,37] have been proposed to integrate vision and language capabilities. However, they lack the capability to generate images. Some works propose integrating LLMs with diffusion models to equip LLMs with image generation capability [18,60,61,69]. Others use discrete tokens to support both image and text generation simultaneously [41,62,71]. They focus on multi-modal generation with limited image-generation capability. Concurrent works such as TransFusion [79] and Show-O [71] unify diffusion and autoregressive methods into a single model, generating text autoregressively and images through diffusion. Nonetheless, like existing diffusion models, these works focus on a limited range of image generation tasks, primarily text-to-image generation, and cannot cover more complex and various visual generation tasks. The construction of a universal foundation model for image generation remains unclear and has not been fully explored. \n\nDiffusion Model. Recent advancements in diffusion models have been remarkable, with notable contributions from the Stable Diffusion (SD) series [12,51,55], DALL-E [54], and Imagen [25]. These models are predominantly designed for text-to-image generation tasks. Many efforts have been made to extend the capabilities of diffusion models, such as ControlNet [75], T2I-Adapter [43], StyleShot [15]. Instruct-Pix2Pix [3], InstructSD [48] and EMU-edit [58] explore performing general image editing tasks through instructions. However, these methods are task-specific, extending the capabilities of SD by modifying the model architecture. In contrast, OmniGen is a model that natively supports various image-generation tasks, and no longer requires any preprocessing steps or assistance from other models. There is some work exploring the unification of computer vision (CV) tasks [2,14,19,65].",
            "score": 0.45505783494140983,
            "section_title": "Related Work",
            "char_start_offset": 26526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1204
                },
                {
                    "start": 1207,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2096
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 52,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 266,
                    "end": 270,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 270,
                    "end": 273,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 503,
                    "end": 506,
                    "matchedPaperCorpusId": "266374640"
                },
                {
                    "start": 506,
                    "end": 509,
                    "matchedPaperCorpusId": "265506621"
                },
                {
                    "start": 509,
                    "end": 512,
                    "matchedPaperCorpusId": "261696650"
                },
                {
                    "start": 598,
                    "end": 602,
                    "matchedPaperCorpusId": "266573555"
                },
                {
                    "start": 1351,
                    "end": 1355,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 1358,
                    "end": 1361,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1564,
                    "end": 1568,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 1621,
                    "end": 1624,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 1655,
                    "end": 1659,
                    "matchedPaperCorpusId": "265221391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85009765625
        },
        {
            "corpus_id": "252917726",
            "title": "LAION-5B: An open large-scale dataset for training next generation image-text models",
            "text": "Vision-Language Models. Radford et al. [58] made a large step forward in multimodal learning for image-text data with their CLIP (Contrastive Language-Image Pre-training) model. The authors proposed a contrastive learning scheme to embed both images and text into a shared representation space, which enabled unparalleled performance in zero-shot image classification. Moreover, CLIP made large progress on multiple challenging distribution shifts [78,84]. \n\nAfter CLIP's initial success, ALIGN and BASIC improved contrastive multimodal learning by increasing the training set size and the batch size used for training [28,54]. LiT also increased training scale and experimented with a combination of pre-trained image representations and contrastive fine-tuning to connect frozen image representations to text [94]. Flamingo introduced the first large vision-language model with in-context learning [2]. Other papers have combined contrastive losses with image captioning to further improve performance [43,89]. Beyond image classification and retrieval, the community later adapted CLIP to further vision tasks such as object navigation and visual question answering [17,32,50,72]. \n\nAnother direction that has recently seen large progress in multimodal learning is text-guided image generation [47,62,95]. Specifically, DALL-E demonstrated diverse image generation capabilities for text prompts combining multiple concepts [59]. GLIDE, DALL-E 2, Imagen, Parti, and Stable Diffusion then improved visual fidelity and text-prompt correspondence [52,60,64,66,90]. \n\nImage-Text Datasets. Earlier dataset creation efforts such as MS-COCO and Visual Genome curated image and region labels through human annotation [36,44]. While this resulted in highquality labels, it also limited the scale of the datasets to only 330K and 5M examples, respectively. The web-harvested YFCC-100M dataset is substantially larger with about 99 million images and one million videos from Flickr, but only contains the user-generated metadata without additional annotations collected specifically for training computer vision models [79]. As a result, the text associated with an image sometimes has little to no correspondence with the actual image content.",
            "score": 0.4544715515742831,
            "section_title": "Related Work",
            "char_start_offset": 5040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1183
                },
                {
                    "start": 1186,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1563
                },
                {
                    "start": 1566,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2235
                }
            ],
            "ref_mentions": [
                {
                    "start": 39,
                    "end": 43,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "220280805"
                },
                {
                    "start": 1301,
                    "end": 1304,
                    "matchedPaperCorpusId": "1563370"
                },
                {
                    "start": 1304,
                    "end": 1307,
                    "matchedPaperCorpusId": "1277217"
                },
                {
                    "start": 1711,
                    "end": 1715,
                    "matchedPaperCorpusId": "4492210"
                },
                {
                    "start": 1715,
                    "end": 1718,
                    "matchedPaperCorpusId": "14113767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9033203125
        },
        {
            "corpus_id": "277226223",
            "title": "DE-CLIP: Unsupervised Dense Counting Method Based on Multimodal Deep Sharing Prompts and Cross-Modal Alignment Ranking",
            "text": "This study proposes a method for optimizing the visual-language model, with the overall framework consisting of two phases: training and testing. Our approach enhances the model's numerical perception by constructing ordered text prompts and guiding the image encoder to learn ordered embedded representations through cross-modal alignment losses. Through the hierarchical recursive injection of visual information, the text encoder achieves layer-by-layer fusion of textual and visual prompts, improving the representation capability of multimodal information. The image encoder interacts deeply with the text prompts at each layer of the transformer, enhancing the synergy between visual features and text information. The multimodal collaborative fusion module enables bidirectional interaction between textual and visual information through self-attention and cross-modal attention mechanisms, thereby improving the model's ability to understand and process complex scenes. Finally, the density estimation result is generated by matching the image and text. The overall architecture of the model is shown in Figure 2. \n\nand cross-modal attention mechanisms, thereby improving the model's ability to understand and process complex scenes. Finally, the density estimation result is generated by matching the image and text. The overall architecture of the model is shown in Figure 2.",
            "score": 0.45376926563434966,
            "section_title": "General Architecture",
            "char_start_offset": 12520,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1121
                },
                {
                    "start": 1124,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1385
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84716796875
        },
        {
            "corpus_id": "257632157",
            "title": "Retrieving Multimodal Information for Augmented Generation: A Survey",
            "text": "Recent advances on pretrained models shed light on general image-text multi-modal models (Ramesh et al., 2021a;Alayrac et al., 2022b;Aghajanyan et al., 2022;Yu et al., 2022;Dou et al., 2022;Li et al., 2023a). However, these models require huge computational resources for pretraining and large amounts of model parameters -as they need to memorize vast world knowledge. (Karpukhin et al., 2020a) to fetch text documents relevant to the target image in the database. Finally, each retrieved document is concatenated with the initial question to generate the final prediction, similar to RAG (Lewis et al., 2020). Besides external documents, PICa (Yang et al., 2022b) and KAT (Gui et al., 2022)  Text generation For general text generation tasks, image retrieval can also help expand contexts. Yang et al. (2022a) augments a text model's \"imagination \" by retrieving existing images and synthesizing newly generated images. As a result, fueling language models with imagination can improve performances in many downstream natural language tasks. Similarly, Zhu et al. (2023) compares \"imagination\" augmentation with synthetic and retrieved images and argues that machine-generated images could provide better guidance due to better consideration of the contexts. Moreover, Fang and Feng (2022) shows that machine translation can be significantly improved by retrieving visual information at the phrase level, especially when the textual context is limited. Image RAG can also help low-resource tasks such as medical report generation (Wu et al., 2022a) and architectural description generation (Mille et al., 2020). \n\nBeyond retrieving images before generating text, Re-Imagen (Chen et al., 2022c) leverages a multimodal knowledge base to retrieve image-text pairs to facilitate image generation. RA-CM3 (Yasunaga et al., 2022) can generate mixtures of images and text. It shows that retrieval-augmented image generation performs much better on knowledge-intensive generation tasks and opens up new capabilities such as multimodal in-context learning.",
            "score": 0.4537515025757334,
            "section_title": "Image",
            "char_start_offset": 7588,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1613
                },
                {
                    "start": 1616,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 2049
                }
            ],
            "ref_mentions": [
                {
                    "start": 89,
                    "end": 111,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 590,
                    "end": 610,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 645,
                    "end": 665,
                    "matchedPaperCorpusId": "237485500"
                },
                {
                    "start": 674,
                    "end": 692,
                    "matchedPaperCorpusId": "245219268"
                },
                {
                    "start": 792,
                    "end": 811,
                    "matchedPaperCorpusId": "253098783"
                },
                {
                    "start": 1055,
                    "end": 1072,
                    "matchedPaperCorpusId": "252781022"
                },
                {
                    "start": 1532,
                    "end": 1550,
                    "matchedPaperCorpusId": "252819241"
                },
                {
                    "start": 1592,
                    "end": 1612,
                    "matchedPaperCorpusId": "230094661"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80029296875
        },
        {
            "corpus_id": "264492302",
            "title": "Invariant Meets Specific: A Scalable Harmful Memes Detection Framework",
            "text": "Multimodal models aim to improve the performance of downstream vision and language tasks by pretraining the model on large-scale image-text pairs and have received tremendous success on various multimodal tasks.Existing multimodal models could be divided into two types (i.e., single-stream and dual-stream) from the architectural perspective.The single-stream models utilize a single transformer [43] encoder to model both image and text representations in a unified semantic space.The semantic interaction of inter-modal and intra-modal could be simultaneously captured by the self-attention mechanism.The representative models include VisualBERT [20], VL-BERT [38] and UNITER [5].However, it is unfavorable to exchange cross-modal heterogeneous information by treating two types of information equally.The dual-stream models encode images and texts with the individual image encoder and text encoder.The semantic interaction of inter-modal is captured by the cross-attention mechanism and the semantic interaction of intra-modal is mainly accomplished through the linear transformation layer.The representative models include ViLBERT [29], X-LXMERT [6] and ERNIE-VIL [50].However, the inter-modal interaction is insufficient compared to the single-stream models.This paper mainly focuses on the dual-stream models for harmful memes detection and relies on knowledge from off-the-shelf pre-trained dual-stream models (e.g., CLIP [35], ALBEF [19] and BLIP [18]) to discriminate memes.To solve the problem of insufficient interaction between modalities, we utilize the powerful ability of graph neural networks to capture node information interaction to complement the dual-stream models.",
            "score": 0.4537482703932148,
            "section_title": "Multimodal Models",
            "char_start_offset": 8714,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 483
                },
                {
                    "start": 483,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 683
                },
                {
                    "start": 683,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 903
                },
                {
                    "start": 903,
                    "end": 1095
                },
                {
                    "start": 1095,
                    "end": 1175
                },
                {
                    "start": 1175,
                    "end": 1265
                },
                {
                    "start": 1265,
                    "end": 1485
                },
                {
                    "start": 1485,
                    "end": 1688
                }
            ],
            "ref_mentions": [
                {
                    "start": 397,
                    "end": 401,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 663,
                    "end": 667,
                    "matchedPaperCorpusId": "201317624"
                },
                {
                    "start": 679,
                    "end": 682,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 1137,
                    "end": 1141,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 1152,
                    "end": 1155,
                    "matchedPaperCorpusId": "219964325"
                },
                {
                    "start": 1170,
                    "end": 1174,
                    "matchedPaperCorpusId": "220265934"
                },
                {
                    "start": 1431,
                    "end": 1435,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1443,
                    "end": 1447,
                    "matchedPaperCorpusId": "236034189"
                },
                {
                    "start": 1457,
                    "end": 1461,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8076171875
        },
        {
            "corpus_id": "261076491",
            "title": "Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages",
            "text": "Multimodal learning can be formulated as modeling the relationship between images, denoted as x, and text, denoted as y, in a target language l t . In this context, the image-to-text generation, which can be roughly summarized as generating description for input images, is to learn the conditional distribution p \u03b8 (y lt |x) parameterized by \u03b8; the text-to-image generation, which is to synthesize relevant images given input text prompts, is to learn p \u03d5 (x|y lt ) parameterized by \u03d5. \n\nIn the vanilla setting, these conditional distributions are typically trained using image-text pairs D t = {(x i , y lt i )} N i=1 in the target language l t (Radford et al., 2021;Yang et al., 2022). However, high-quality image-text pairs are extremely scarce in most languages. To mitigate the dependency on native image-text pairs, we introduce the pivot language l p , which contains abundant multimodal pairs \n\n, where M \u226b N . Imitating the human learning mechanism that can naturally align visual concepts with various learned languages, MPM aims to transfer visual concepts learned in the pivot language to the target language. \n\nMPM divides the multimodal learning process in target language l t into two consecutive stages: multilingual alignment and multimodal alignment. For the multilingual alignment, MPM aims to establish the cross-lingual alignment for l t and l p . This is achieved by directly leveraging a pretrained multilingual LLM, denoted as f \u03c3 , which can provide close hidden representations for text pair y lt and y lp with similar semantics, i.e., f \u03c3 (y lt ) \u2248 f \u03c3 (y lp ). For the multimodal alignment, MPM utilize the sufficient multimodal resource D p in the pivot language and optimize the image-to-text objective p \u03b8 (y lp |x) and text-to-image objective p \u03d5 (x|y lp ). In the following sections, we introduce the training process of multimodal alignment stage.",
            "score": 0.45370482306138943,
            "section_title": "PROBLEM FORMULATION AND OVERVIEW",
            "char_start_offset": 8262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 486
                },
                {
                    "start": 489,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1122
                },
                {
                    "start": 1125,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1882
                }
            ],
            "ref_mentions": [
                {
                    "start": 647,
                    "end": 669,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94873046875
        },
        {
            "corpus_id": "248525064",
            "title": "Language Models Can See: Plugging Visual Controls in Text Generation",
            "text": "In this work, we present MAGIC, a novel decoding scheme that plugs visual controls into the generation of a language model. MAGIC is a training-free framework that enables the LM to address challenging multimodal tasks in a zero-shot manner without sacrificing the decoding speed. To verify the versatility and extensibility of MAGIC, we comprehensively evaluate our approach on two imagegrounded text generation tasks: (i) image captioning and (ii) visually grounded story generation. \n\nExperimental results demonstrate that our approach notably outperforms previous state-of-the-art methods in both automatic and human evaluations. \n\nFuture Work. While our focus in this study is zero-shot image grounded text generation using a language model, we would like to note that MAGIC Search is a model architecture agnostic decoding scheme. In other words, it can naturally fit into any existing multimodal generative model which takes both the image and text as input. However, it is out of the scope of this paper and we will leave it to future work. \n\nMoreover, in theory, MAGIC is a generic framework that can be extended to modalities beyond text and image. Controls in any form, of any modalities, can be plugged into the language model as long as a certain similarity metric can be found to measure the relevance between the control and the generated text. In future work, we would like to explore the possibility of adapting MAGIC to other modalities beyond images (e.g., audios and videos) therefore enabling the language model to generate text grounded on multimodal intelligence.",
            "score": 0.4534841780533203,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 26784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 633
                },
                {
                    "start": 636,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1048
                },
                {
                    "start": 1051,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1586
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93603515625
        },
        {
            "corpus_id": "266902630",
            "title": "MISS: A Generative Pretraining and Finetuning Approach for Med-VQA",
            "text": "The architecture of MISS is demonstrated in Figure 1 a, our model adopts the encoderdecoder structure in its entirety. Unlike most dual-tower image-text VLP models in the past, our model's encoder is divided into only two parts -the image encoder and the JTM encoder. For the image encoder, we borrow from the settings adopted by most recent works, which utilize the vision transformer(ViT) [7] as the image feature extractor. For an image input I, it's firstly reshaped into flattened 2D patches and then encoded into a embedding sequence {x <cls> , x 1 , ..., x n }. After that, a 12-layer transformer encoder will extract its high-dimensional features. \n\nThe JTM encoder replaces the text and multi-modal encoders used in recent works and it performs representation learning of text and multi-modal features simultaneously. As shown in Figure 1(a), each JTM encoder is composed of 12 transformer-based layers, with each layer containing a bidirectional self-attention layer, a cross-attention layer, and a feed-forward layer. For each text input T , it's first pre-processed by the tokenizer into a token sequence. Then, we feed it into the JTM encoder for multi-layer representation learning, where it interacts with the image features through cross-attention. Specifically, we define the text representation as {w <cls> , w 1 , ..., w n }, and the image embeddings are defined as {v <cls> , v 1 , ..., v n }. Both of these representations fuse and compute multimodal representations through \n\nwhere text representation {w <cls> , w 1 , ..., w n } generates query vectors Q, and the image representation {v <cls> , v 1 , ..., v n } generates key vectors K and value vectors V . The decoder part of the model includes a text decoder, which aims to decode the multimodal feature representation obtained by the JTM encoder into an output text representation.",
            "score": 0.4529185854465375,
            "section_title": "Model Architecture",
            "char_start_offset": 4506,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 655
                },
                {
                    "start": 658,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1495
                },
                {
                    "start": 1498,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1859
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80615234375
        },
        {
            "corpus_id": "258546776",
            "title": "Building Multimodal AI Chatbots",
            "text": "For response generators that take in both text and image inputs and return text output, the model architecture used as response generators is VisionEncoderDecoder from Huggingface [27]. VisionEncoderDecoder is a general class for an image encoder and a text decoder.\n\nSimilarly to VisionTextEncoder for implementing image retriever, VisionEncoderDecoder is selected because any previously released image encoders and text decoders can be plugged into this class, allowing a lot of freedom in choice of models from each modality.\n\nPrior to using VisionEncoderDecoder, preliminary experiments were conducted using public multimodal encoder-decoder architectures including OFA [26] and BLIP [14]. However, their performance on response generation was too low due to the relatively small size of text decoders around 110M parameters. This size was suitable for the target tasks of OFA and BLIP, which include image-text retrieval, image captioning, and visual question answering that require generation of only a few tokens such as captions and short answers (e.g., \"How many dogs are in this photo?\" with the answer \"1\"). On the other hand, the response generator in my chatbot system requires a sufficiently large text decoder capable of generating longer sequences. Thus, VisionEncoderDecoder was a better option as it allows the use of any pretrained text decoders regardless of their size.\n\nVisionEncoderDecoder consists of two main architectural components: an image encoder and a text decoder. For the task of response generation, the image encoder takes in either the currently retrieved or the most recently retrieved image and encodes it into an image representation. Then, the text decoder takes in this image representation and combines it with representations of dialogue history and current response tokens to generate the next response token.\n\nThe family of models used as image encoders in multimodal response generators is ViT [5], which is also used as image encoders in image retrievers. In ViT, a sequence of image patches is encoded into an image representation using self attention and feedforward layers.\n\nThe architecture of ViT is explained in Section 4.1.1 in further detail.\n\nTwo variants of ViT",
            "score": 0.4526204614337529,
            "section_title": "Multimodal Model Architecture",
            "char_start_offset": 23298,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 184,
                    "matchedPaperCorpusId": "269498086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86083984375
        },
        {
            "corpus_id": "271571532",
            "title": "SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving",
            "text": "VLMs represent a significant advancement in the field of artificial intelligence, aiming to bridge the gap between visual and textual data. The architecture of VLMs typically involves two main components: a visual encoder and a language decoder. The visual encoder processes the image to extract features, which are then combined with text representations generated by the language decoder. This integrated approach enables the model to perform complex reasoning tasks that require both visual and linguistic understanding. These models combine the strengths of vision models, such as convolutional neural networks (CNNs) [15,16] and ViT [17][18][19][20], with the capabilities of language models, allowing for more comprehensive understanding and interaction with multimodal inputs. \n\nEfforts in VLMs focused on tasks like image captioning and VQA. Image captioning involves generating descriptive textual annotations for images, while VQA requires the model to answer questions about the content of an image. Nowadays, more and more VLMs have demonstrated impressive capabilities in these areas by effectively integrating visual and textual information. Flamingo [21] utilizes visual and language inputs as prompts, demonstrating exceptional few-shot performance in visual question answering. GPT-4, [9] the LLaVA series [6,7] introduces visual instruction tuning to enhance the instruction-following capabilities of VLMs. Concurrently, models like KOSMOS-2 [22], and Qwen-VL [23] have advanced VLMs by incorporating visual grounding capabilities, facilitating tasks such as region description and localization. Furthermore, PaLM-E [24] and EmbodiedGPT [25] represent significant strides in adapting VLMs for embodied applications, vastly expanding their potential use cases. These advancements illustrate the rapid progress in the development and application of VLMs, showcasing their ability to handle increasingly complex multimodal tasks. As the integration of vision and language continues to evolve, VLMs are poised to play a pivotal role in various AI-driven domains, including autonomous driving, healthcare, and interactive robotics.",
            "score": 0.4525692939634977,
            "section_title": "Vision-language Models",
            "char_start_offset": 4829,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2144
                }
            ],
            "ref_mentions": [
                {
                    "start": 622,
                    "end": 626,
                    "matchedPaperCorpusId": "253446956"
                },
                {
                    "start": 626,
                    "end": 629,
                    "matchedPaperCorpusId": "8485068"
                },
                {
                    "start": 638,
                    "end": 642,
                    "matchedPaperCorpusId": "235652212"
                },
                {
                    "start": 642,
                    "end": 646,
                    "matchedPaperCorpusId": "235247946"
                },
                {
                    "start": 646,
                    "end": 650,
                    "matchedPaperCorpusId": "253510587"
                },
                {
                    "start": 650,
                    "end": 654,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 1165,
                    "end": 1169,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1326,
                    "end": 1328,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1655,
                    "end": 1659,
                    "matchedPaperCorpusId": "258865718"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "277621130",
            "title": "Lumina-OmniLV: A Unified Multimodal Framework for General Low-Level Vision",
            "text": "Given an input image x, our goal is to generate the target image using both textual instructions and in-context visual exemplars. We explore two different encoding strategies: (1) Separate encoding, where text prompts are processed using a large language model (LLM), while visual exemplars are encoded independently. (2) Unified encoding, where both text and visual inputs are fused within a multimodal language model (MLLM). Fig. 2 illustrates the architectural differences between these two approaches. While unified encoding benefits from parameter efficiency   and leverages cross-modal correlations, we observe that it introduces critical limitations especially when applied to dense prediction tasks. Specifically, multimodal encoders often misinterpret task instructions, leading to inconsistencies in generated outputs. To better understand this issue, we visualize the encoded feature distributions in Fig. 3. Our findings indicate that mixing text and image prompts within a single encoder leads to severe task ambiguity. Since visual tokens dominate the shared feature space, text-based instructions often get overshadowed, leading to misalignment and incorrect outputs, as shown in Fig. 10. \n\nBased on these observations, we adopt a separate encoding strategy: text instructions are processed via an LLM, while image exemplars are encoded using a visual VAE. This ensures clearer task separation, preventing interference between textual and visual guidance, and improves task accuracy across a vast number of low-level vision tasks.",
            "score": 0.45245503502809437,
            "section_title": "Encoding Multimodal Information",
            "char_start_offset": 9798,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1545
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85888671875
        },
        {
            "corpus_id": "271924334",
            "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation",
            "text": "This motivates a research question: can one single transformer handle both multimodal understanding and generation? \n\nVery recently, Chameleon (Team, 2024) demonstrated this is possible. Specifically, Chameleon enables an early fusion of different modalities to generate both text and image tokens through the same manner of autoregressive modeling. While it is reasonable to model text tokens autoregressively (Touvron et al., 2023;Liu et al., 2024c), it is less clear whether it is better to model im- age/video patches (or pixels) autoregressively as well. An apparent and significant bottleneck of autoregressively predicting an image is the large number of sampling steps required due to its causal attention, particularly when dealing with images/videos in higher resolution. Further, (continuous) diffusion models (Podell et al., 2023;Esser et al., 2024) have exhibited superior capabilities in visual generation than autoregressive ones and are in full attention. This motivates us to ponder: can such one single transformer involve both autoregressive and diffusion modeling? Here we envision a new paradigm that text is represented as discrete tokens and modeled autoregressively same with large language models (LLMs), and continuous image pixels are modeled using denoising diffusion. However, it is non-trivial to integrate these two distinct techniques into one single network due to the significant differences between discrete text tokens and continuous image representations. Another challenge lies in the fact that existing state-of-the-art diffusion models typically rely on two distinct models, i.e., a text encoder to encode text conditional information and a denoising network to predict noise. \n\nTo this end, we present a novel unified model, i.e., Show-o, capable of addressing both multimodal understanding and generation tasks simultaneously with mixed autoregressive and diffusion modeling (as shown in Fig. 2). Specifically, Show-o is built upon a pre-trained LLM and inherits the autoregressive modeling capability for text-based reasoning.",
            "score": 0.45244033105667414,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 118,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2069
                }
            ],
            "ref_mentions": [
                {
                    "start": 842,
                    "end": 861,
                    "matchedPaperCorpusId": "268247980"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95166015625
        },
        {
            "corpus_id": "267750101",
            "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling",
            "text": "To overcome these challenges, we introduce AnyGPT, an any-to-any multimodal language model that employs discrete representations for unified processing. AnyGPT is equipped with multimodal tokenizers that compress raw multimodal data, such as images and audio, into a sequence of discrete semantic tokens. These discrete representations enable the core LLM to unify tasks such as perception, understanding, reasoning, and generation in an autoregressive manner at the semantic level. Subsequently, de-tokenizers convert the discrete representations back into the original modal representations at the perceptual level. Thanks to discrete representation, which filters out high-frequency, modality-specific perceptual information while preserving essential low-frequency semantic information (Ge et al., 2023a;Borsos et al., 2023a;Rombach et al., 2022), we can train our model stably without any alterations to the existing LLM architecture or training paradigms. Instead, our approach relies solely on data-level preprocessing. This allows for the seamless integration of new modalities into LLMs, akin to the addition of new languages, and permits the direct application of existing LLM tools, which enhances the efficiency of both the training and inference stages. \n\nFurthermore, to mitigate the scarcity of multimodal alignment data encompassing all modalities, we build a text-centric multimodal alignment dataset for pre-training. Our goal is to use text as a bridge, by aligning other modalities with text, to achieve mutual alignment among all modalities, since natural language is the most refined modality of semantic representation and is present in the majority of multimodal alignment datasets. To endow the model with the capability to comprehend and generate content interwoven with multiple modalities, we employ advanced generative models to synthesize a multimodal instruction dataset, AnyInstruct-108k. This dataset, comprising 108k samples of multi-turn conversations, enables AnyGPT to handle arbitrary combinations of multimodal inputs and outputs. \n\nExperimental results demonstrate that AnyGPT achieves zero-shot performance comparable to that of specialized models across various modalities.",
            "score": 0.4524062566811426,
            "section_title": "Introduction",
            "char_start_offset": 2062,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1266
                },
                {
                    "start": 1269,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2069
                },
                {
                    "start": 2072,
                    "end": 2215
                }
            ],
            "ref_mentions": [
                {
                    "start": 808,
                    "end": 829,
                    "matchedPaperCorpusId": "252111134"
                },
                {
                    "start": 829,
                    "end": 850,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90966796875
        },
        {
            "corpus_id": "274981694",
            "title": "SilVar: Speech Driven Multimodal Model for Reasoning Visual Question Answering and Object Localization",
            "text": "Visual Language Models (VLMs) have gained significant attention due to their capacity to bridge the gap between visual and textual modalities, facilitating more intuitive interactions between humans and machines. These models are valuable in tasks like visual question answering (VQA), which may involve yes/no answers, multiple-choice questions, or even generating image descriptions. With advances in deep learning, VLMs can now effectively handle visual scenes and provide meaningful textual outputs that explain or describe those scenes in natural language. \n\nRecent advancements in VLMs, such as CLIP [43], have enabled Large Language Models (LLMs) to process images and text simultaneously [3,6,45]. For example, Flamingo is a VLM that can handle both modalities and excels at tasks like image captioning and VQA [3]. Similarly, BLIP-2 connects a visual encoder to LLMs using a querying transformer, creating a more efficient multimodal model [29]. There are several VLM models, such as LLava [33], LocVLM [46], and LISA [27], that further improve model predictions through reasoning and prompting techniques. \n\nDespite the significant advancements in VLMs, most of them currently support only text-based interactions, limiting their application to scenarios where text input is inconvenient or unavailable. In addition, while reasoning and prompting techniques for LLMs have been explored, these techniques for speech-based instruction remain largely underexplored. Recently, models like GPT-4o [42] have enabled interaction with LLMs through speech, significantly enhancing the user experience compared to traditional text-based interactions. However, there is still a lack of exploration in the open-source community on building such speech interaction models based on foundation models. \n\nTo enable speech interaction with LLMs, speech instruction models such as Qwen2-Audio [14], SALMONN [50], and Llama-Omni [19] have been developed to process speech instead of text-based instructions. Although these models enable speech instruction, they are not capable of understanding both images and text simultaneously. Inspired by VLMs and ARS models, we propose SilVar, a multimodal model that can understand both images and audio or images and text, at the same time.",
            "score": 0.4521767030155613,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 561
                },
                {
                    "start": 564,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1796
                },
                {
                    "start": 1799,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2273
                }
            ],
            "ref_mentions": [
                {
                    "start": 606,
                    "end": 610,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 701,
                    "end": 704,
                    "matchedPaperCorpusId": "259991075"
                },
                {
                    "start": 819,
                    "end": 822,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 949,
                    "end": 953,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 999,
                    "end": 1003,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1012,
                    "end": 1016,
                    "matchedPaperCorpusId": "269043025"
                },
                {
                    "start": 1027,
                    "end": 1031,
                    "matchedPaperCorpusId": "260351258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95166015625
        },
        {
            "corpus_id": "260887336",
            "title": "Large Language Models and Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges",
            "text": "The new Gemini 1.53 developed by Google delivers dramatically enhanced performance with a more efficient architecture compared to its previous version (Team et al., 2023b). \n\nHowever, a majority of FMs are typically engineered for either understanding-based or generation-based tasks. To address this issue, BLIP (Li et al., 2022), a new Vision-Language Pre-training (VLP) framework, is introduced for both vision-language understanding and generation tasks. To effectively utilize the noisy web data by bootstrapping the captions, a captioner is employed to generate synthetic captions, subsequently, a filter is adopted to remove the noisy ones. BLIP achieves state-of-the-art results across various vision-language tasks, such as image-text retrieval, image captioning, and VQA. Similarly, OpenFlamingo (Alayrac et al., 2022) is trained on large-scale multimodal web corpora with arbitrarily interleaved text and images for a variety of image and video tasks via few-shot learning, such as image captioning and multiple-choice VQA (Alayrac et al., 2022). KOSMOS-1 (Huang et al., 2023), a Multimodal Large Language Model (MLLM), is subsequently introduced to process multimodal inputs, adhere to instructions, and perform in-context learning for language tasks and multimodal tasks, such as multimodal dialogue, image captioning, VQA, and image recognition with descriptions. Impressively, KOSMOS-1 demonstrates exemplary performance across various settings, such as zero-shot, few-shot, and multimodal chain-of-thought prompting, on various tasks without necessitating any gradient updates or fine-tuning. Built on previous versions, GPT-4 (OpenAI, 2023), a largescale and multimodal model is introduced by OpenAI to accept both image and text inputs and produce text outputs. \n\nThe future of MFMs is set to be exciting, with further advancements anticipated in several areas.",
            "score": 0.4519512402420408,
            "section_title": "Multimodal foundation models (MFMs)",
            "char_start_offset": 25234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 175,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1779
                },
                {
                    "start": 1782,
                    "end": 1879
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 330,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.919921875
        },
        {
            "corpus_id": "272689854",
            "title": "TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings",
            "text": "Visual language models primarily consist of a visual encoder and a large language model, representing prominent architectures in the multimodal domain. Researchers have proposed numerous architectures (Li et al. 2023a;Zhu et al. 2024;Chen et al. 2023c) for integrating visual features into advanced LLM inference pipelines. Llama-Adapter (Zhang et al. 2023) proposes to generate language answer with taking the image input as condition. Flamingo (Alayrac et al. 2022) and LLaVA (Liu et al. 2024c)  Inspired by the compact structure and outstanding performance of LLaVA-1.5 (Liu et al. 2024a), we use LLaVA-1.5 as our baseline and incorporate a text-guided approach, similar to other LLaVA-based methods. Unlike most of these methods, which create additional datasets to enhance performance, our improvements focus entirely on the model architecture itself. This approach can further enhance the performance of methods that rely on extra datasets. The results in fifth and sixth lines of Table 1 verified this point.",
            "score": 0.45167820752489374,
            "section_title": "Related Work Vision Language Models",
            "char_start_offset": 5135,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1015
                }
            ],
            "ref_mentions": [
                {
                    "start": 446,
                    "end": 467,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 478,
                    "end": 496,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79296875
        },
        {
            "corpus_id": "273969525",
            "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
            "text": "Visual Generation with Flow-based Generative Models. Recent years have witnessed remarkable progress in visual generation through diffusion models [32,83], leading to impressive models like [67,73,[76][77][78][79]. Building on these advances, flow-based generative models [3,55,61] emerged as a simplified alternative framework. These approaches have recently enabled advanced visual generation models [23,36] that achieve superior empirical performance with faster sampling. Our work demonstrates that rectified flow [60][61][62] can be effectively integrated into LLMs, creating unified models that excel in both understanding and generation tasks. \n\nUnified Models For Understanding and Generation. The development of multimodal large language models (MLLMs) has enabled effective integration of text and visual information. Building upon powerful LLMs [7,91,92], recent MLLMs [2,15,49,56,58,64] have demonstrated exceptional multimodal understanding capabilities. Current research increasingly focuses on architectures that can simultaneously handle visual understanding and generation tasks. One approach extends MLLMs with pre-trained diffusion models [19, 25-27, 87, 101]. However, these systems essentially utilize diffusion models as external tools, where the MLLM generates conditions for image generation without possessing direct generative capabilities. This separation often results in suboptimal performance compared to standalone diffusion models [25,87]. Another line of work [88,97,99,100,108] aim to train a single LLM for both tasks. Many of these methods employ vector-quantization [22,86] to convert images into discrete tokens, enabling unified autoregressive processing [88,97]. While straightforward to implement, these approaches are inherently limited by their image tokenization quality. \n\nOur work focuses on developing unified models that combine autoregressive capabilities with flow/diffusion models, leveraging their proven effectiveness in visual generation.",
            "score": 0.45156124054951113,
            "section_title": "Related Work",
            "char_start_offset": 2700,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1815
                },
                {
                    "start": 1818,
                    "end": 1992
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 151,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 194,
                    "end": 197,
                    "matchedPaperCorpusId": "259341735"
                },
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "254854449"
                },
                {
                    "start": 209,
                    "end": 213,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 272,
                    "end": 275,
                    "matchedPaperCorpusId": "252668615"
                },
                {
                    "start": 275,
                    "end": 278,
                    "matchedPaperCorpusId": "252734897"
                },
                {
                    "start": 278,
                    "end": 281,
                    "matchedPaperCorpusId": "252111177"
                },
                {
                    "start": 402,
                    "end": 406,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 522,
                    "end": 526,
                    "matchedPaperCorpusId": "252111177"
                },
                {
                    "start": 526,
                    "end": 530,
                    "matchedPaperCorpusId": "261697392"
                },
                {
                    "start": 880,
                    "end": 883,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 883,
                    "end": 886,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 886,
                    "end": 889,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 889,
                    "end": 892,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 892,
                    "end": 895,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1467,
                    "end": 1470,
                    "matchedPaperCorpusId": "259765944"
                },
                {
                    "start": 1603,
                    "end": 1607,
                    "matchedPaperCorpusId": "229297973"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "273098704",
            "title": "Visual Prompting in LLMs for Enhancing Emotion Recognition",
            "text": "LLMs such as LLaMA (Touvron et al., 2023), ChatGPT-3 (Brown et al., 2020), ChatGPT-4 (Achiam et al., 2023), and PaLM (Chowdhery et al., 2023) have demonstrated remarkable zeroshot transfer capabilities in natural language processing. Recently, VLLMs, which leverage imagetext data pairs from the web, have gained prominence in the computer vision domain. MiniGPT-4 (Zhu et al., 2023), a model that combines a visual encoder with an advanced language model, can enable multi-modal capabilities such as generating detailed image descriptions and designing websites from sketches. Video-LLaVA (Zhang et al., 2023a) is a multi-modal framework that enhances Large Language Models with the ability to understand and generate responses based on both visual and auditory content in videos. LLaVA (Liu et al., 2023) is a newly developed, end-to-end trained, large multimodal model that combines a vision encoder with a language model, demonstrating promising abilities in multimodal chat. Although VLLMs exhibit remarkable capabilities in vision-based tasks such as image segmentation and object detection, they typically require fine-tuning of the vision and text encoders using existing open vocabulary methods when applied to specific tasks. In contrast, this paper proposes a zero-shot architecture for emotion recognition, overcoming the need for task-specific fine-tuning.",
            "score": 0.4515321188692629,
            "section_title": "Vision Large Language models",
            "char_start_offset": 5460,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1369
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 141,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83447265625
        },
        {
            "corpus_id": "273993727",
            "title": "Multimodal Instruction Tuning with Hybrid State Space Models",
            "text": "Multimodal Large Language Models. Most current multimodal large language models (MLLMs) integrate large language models (LLMs) with vision encoders, such as ViT (Dosovitskiy, 2020) and CLIP (Radford et al., 2021), by incorporating image representations into the LLMs (Liu et al., 2024d;b;c). The LLM then performs various vision-language (VL) tasks in an autoregressive manner. Within this framework, MLLMs primarily differ in their approaches to combining text and image representations. Most works employ a modular architecture that utilizes an intermediate network to map visual features into the text token embedding space of the LLM. Prominent examples, such as LLaVA (Liu et al., 2024d), often use a multi-layer perceptron (MLP) layer to link the visual encoder with the LLM. Alternatively, other approaches use sampler-based modules to bridge the visual encoder and the LLM (Li et al., 2023c;Dai et al., 2023). While these sampler-based modules effectively reduce the number of visual tokens, they typically require large-scale pre-training to achieve satisfactory performance. \n\nMLLMs with High-Resolution Images. MLLMs often use pre-trained visual encoders for visiondependent tasks. However, these encoders typically rely on low resolutions, such as 224 \u00d7 224 or 336 \u00d7 336 pixels, which limits their ability to perceive small or blurry objects. This limitation can lead to failures in tasks that require clear and recognizable details for fine distinctions between objects, such as OCR and document understanding (Tong et al., 2024). Recently, various methods have been introduced to incorporate high-resolution inputs to enhance the capabilities of MLLMs by providing more fine-grained visual features (Luo et al., 2023a;Hong et al., 2024;Li et al., 2023a;Liu et al., 2024c;Tong et al., 2024;Wang et al., 2023).",
            "score": 0.45151869215014795,
            "section_title": "RELATED WORKS",
            "char_start_offset": 4328,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1822
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 212,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 267,
                    "end": 286,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 673,
                    "end": 692,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 881,
                    "end": 899,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 899,
                    "end": 916,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 1523,
                    "end": 1542,
                    "matchedPaperCorpusId": "266976992"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92724609375
        },
        {
            "corpus_id": "273233293",
            "title": "Towards Interpreting Visual Information Processing in Vision-Language Models",
            "text": "Building upon this foundation, Radford et al. (2021) introduced Contrastive Language-Image Pretraining (CLIP), which jointly trains a vision transformer and a text trasnsformer to produce similar embeddings for matching image-text pairs. CLIP's image encoder is currently being used for stateof-the-art performance on many downstream tasks like zero-shot image classification and visionlanguage models like LLaVA. \n\nConnecting Text and Image Models. Tsimpoukelli et al. (2021) pioneered the concept of training an image encoder such that its output can be used by a frozen language model for multimodal fewshot reasoning. Merullo et al. (2022) advanced this approach by freezing both the pre-trained image encoder and pre-trained language model, training only a linear map that converts image features into inputs for the text model. This introduced the \"adapter style\" approach, utilizing pre-trained components for both modalities. \n\nBuilding on this foundation, Liu et al. (2023b) introduced LLaVA, which combined a CLIP ViT-L/14 image encoder with a Vicuna-13B language model, connected by a trained linear projection, or an MLP for improved performance (Liu et al., 2023a). Crucially, LLaVA's training process focuses solely on fine-tuning for multimodal conversation, without any next-token pretraining on image-text pairs. This approach allows LLaVA to leverage the strong pre-trained capabilities of both the vision and language models while efficiently adapting to multimodal tasks.",
            "score": 0.45086127403585174,
            "section_title": "LM",
            "char_start_offset": 7725,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 933
                },
                {
                    "start": 936,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1491
                }
            ],
            "ref_mentions": [
                {
                    "start": 450,
                    "end": 476,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 622,
                    "end": 643,
                    "matchedPaperCorpusId": "252668479"
                },
                {
                    "start": 965,
                    "end": 983,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9443359375
        },
        {
            "corpus_id": "248887528",
            "title": "Overcoming Language Disparity in Online Content Classification with Multimodal Learning",
            "text": "We implement a multimodal classifier (Ngiam et al. 2011) that fuses the latent representations of individual modalities (text and image) to perform classification based on the joint modeling of both input modalities. We feed the concatenation of fine-tuned text and image representations (i.e., T \u2295 I) to the multimodal classifier, which is essentially a series of fully connected layers with ReLU activation (Agarap 2018). The architecture of the multimodal classifier comprises an input layer (1024 neurons), 3 hidden layers (512, 128, 32 neurons), and an output layer (neurons = number of classes in the dataset). We train a multimodal classifier for each language in each task. Similar to image-only and text-only classification models discussed above, for each training instance, we use Adam optimizer (Kingma and Ba 2014) with a learning rate initialized at 10 \u22124 . We use early stopping based on the validation set loss to stop the training and avoid overfitting on the train set. \n\nWe use the same evaluation metrics to evaluate the imageonly and multimodal classifiers as we did for the text-only ones, and report the average of 10 different runs in Table 4. Additionally, in Figures 4 and 5 we present the root-meansquared deviation (RMSD en ) values of F 1 scores of non-English languages with respect to that of the English language for text-only and multimodal classifiers.",
            "score": 0.45084507554340847,
            "section_title": "Multimodal classification model:",
            "char_start_offset": 26795,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1386
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 55,
                    "matchedPaperCorpusId": "212628707"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82275390625
        },
        {
            "paperId": "b678fa886360b9ce2df7cf4cc7bb574dea2f1dba",
            "corpusId": 278394763,
            "title": "Multimodal Benchmarking and Recommendation of Text-to-Image Generation Models",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 12,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.04650, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "100693087",
                    "name": "Kapil Wanaskar"
                },
                {
                    "authorId": "2359634616",
                    "name": "Gaytri Jena"
                },
                {
                    "authorId": "2377003",
                    "name": "Magdalini Eirinaki"
                }
            ],
            "abstract": "This work presents an open-source unified benchmarking and evaluation framework for text-to-image generation models, with a particular focus on the impact of metadata augmented prompts. Leveraging the DeepFashion-MultiModal dataset, we assess generated outputs through a comprehensive set of quantitative metrics, including Weighted Score, CLIP (Contrastive Language Image Pre-training)-based similarity, LPIPS (Learned Perceptual Image Patch Similarity), FID (Frechet Inception Distance), and retrieval-based measures, as well as qualitative analysis. Our results demonstrate that structured metadata enrichments greatly enhance visual realism, semantic fidelity, and model robustness across diverse text-to-image architectures. While not a traditional recommender system, our framework enables task-specific recommendations for model selection and prompt design based on evaluation metrics.",
            "corpus_id": "278394763",
            "text": "This work presents an open-source unified benchmarking and evaluation framework for text-to-image generation models, with a particular focus on the impact of metadata augmented prompts. Leveraging the DeepFashion-MultiModal dataset, we assess generated outputs through a comprehensive set of quantitative metrics, including Weighted Score, CLIP (Contrastive Language Image Pre-training)-based similarity, LPIPS (Learned Perceptual Image Patch Similarity), FID (Frechet Inception Distance), and retrieval-based measures, as well as qualitative analysis. Our results demonstrate that structured metadata enrichments greatly enhance visual realism, semantic fidelity, and model robustness across diverse text-to-image architectures. While not a traditional recommender system, our framework enables task-specific recommendations for model selection and prompt design based on evaluation metrics.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.845703125
        },
        {
            "paperId": "0a98925fc24153a6b1efc9cb7b722befe4020c5e",
            "corpusId": 274408992,
            "title": "Semantic Image Synthesis from Text: Current Trends and Future Horizons in Text-to-Image Generation",
            "venue": "EAI Endorsed Transactions on Internet of Things",
            "year": 2024,
            "referenceCount": 11,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.4108/eetiot.5336",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.4108/eetiot.5336?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.4108/eetiot.5336, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333145140",
                    "name": "Lakshmanan Sudha"
                },
                {
                    "authorId": "2333145526",
                    "name": "Kari Balakrishnan Aruna"
                },
                {
                    "authorId": "2305836649",
                    "name": "V. Sureka"
                },
                {
                    "authorId": "2333142670",
                    "name": "Mathavan Niveditha"
                },
                {
                    "authorId": "2305840406",
                    "name": "S. Prema"
                }
            ],
            "abstract": "Text-to-image generation, a captivating intersection of natural language processing and computer vision, has undergone a remarkable evolution in recent years. This research paper provides a comprehensive review of the state-of-the-art in text-to-image generation techniques, highlighting key advancements and emerging trends. We begin by surveying the foundational models, with a focus on Generative Adversarial Networks (GANs) and their pivotal role in generating realistic and diverse images from textual descriptions. We delve into the intricacies of training data, model architectures, and evaluation metrics, offering insights into the challenges and opportunities in this field. Furthermore, this paper explores the synergistic relationship between natural language processing and computer vision, showcasing multimodal models like DALL-E and CLIP. These models not only generate images from text but also understand the contextual relationships between textual descriptions and images, opening avenues for content recommendation, search engines, and visual storytelling. The paper discusses applications spanning art, design, e-commerce, healthcare, and education, where text-to-image generation has made significant inroads. We highlight the potential of this technology in automating content creation, aiding in diagnostics, and transforming the fashion and e-commerce industries. However, the journey of text-to-image generation is not without its challenges. We address ethical considerations, emphasizing responsible AI and the mitigation of biases in generated content. We also explore interpretability and model transparency, critical for ensuring trust and accountability.",
            "corpus_id": "274408992",
            "text": "Text-to-image generation, a captivating intersection of natural language processing and computer vision, has undergone a remarkable evolution in recent years. This research paper provides a comprehensive review of the state-of-the-art in text-to-image generation techniques, highlighting key advancements and emerging trends. We begin by surveying the foundational models, with a focus on Generative Adversarial Networks (GANs) and their pivotal role in generating realistic and diverse images from textual descriptions. We delve into the intricacies of training data, model architectures, and evaluation metrics, offering insights into the challenges and opportunities in this field. Furthermore, this paper explores the synergistic relationship between natural language processing and computer vision, showcasing multimodal models like DALL-E and CLIP. These models not only generate images from text but also understand the contextual relationships between textual descriptions and images, opening avenues for content recommendation, search engines, and visual storytelling. The paper discusses applications spanning art, design, e-commerce, healthcare, and education, where text-to-image generation has made significant inroads. We highlight the potential of this technology in automating content creation, aiding in diagnostics, and transforming the fashion and e-commerce industries. However, the journey of text-to-image generation is not without its challenges. We address ethical considerations, emphasizing responsible AI and the mitigation of biases in generated content. We also explore interpretability and model transparency, critical for ensuring trust and accountability.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.92138671875
        },
        {
            "paperId": "0d1b6ec85a7dee7e4483da3420484a3fd7d689a8",
            "corpusId": 279275564,
            "title": "A survey on pre-training and transfer learning for multimodal Vision-Language Models",
            "venue": "Advances in Engineering Innovation",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54254/2977-3903/2025.23982?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54254/2977-3903/2025.23982, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2367554787",
                    "name": "Zhongren Liang"
                }
            ],
            "abstract": "In recent years, Vision-Language Models (VLMs) have emerged as a significant breakthrough in multimodal learning, demonstrating remarkable progress in tasks such as image-text alignment, image generation, and semantic reasoning. This paper systematically reviews current VLM pretraining methodologies, including contrastive learning and generative paradigms, while providing an in-depth analysis of efficient transfer learning strategies such as prompt tuning, LoRA, and adapter modules. Through representative models like CLIP, BLIP, and GIT, we examine their practical applications in visual grounding, image-text retrieval, visual question answering, affective computing, and embodied AI. Furthermore, we identify persistent challenges in fine-grained semantic modeling, cross-modal reasoning, and cross-lingual transfer. Finally, we envision future trends in unified architectures, multimodal reinforcement learning, and domain adaptation, aiming to provide systematic reference and technical insights for subsequent research.",
            "corpus_id": "279275564",
            "text": "In recent years, Vision-Language Models (VLMs) have emerged as a significant breakthrough in multimodal learning, demonstrating remarkable progress in tasks such as image-text alignment, image generation, and semantic reasoning. This paper systematically reviews current VLM pretraining methodologies, including contrastive learning and generative paradigms, while providing an in-depth analysis of efficient transfer learning strategies such as prompt tuning, LoRA, and adapter modules. Through representative models like CLIP, BLIP, and GIT, we examine their practical applications in visual grounding, image-text retrieval, visual question answering, affective computing, and embodied AI. Furthermore, we identify persistent challenges in fine-grained semantic modeling, cross-modal reasoning, and cross-lingual transfer. Finally, we envision future trends in unified architectures, multimodal reinforcement learning, and domain adaptation, aiming to provide systematic reference and technical insights for subsequent research.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.88916015625
        },
        {
            "paperId": "7f71875f8214dffa4f3276da123c4990a6d437cc",
            "corpusId": 247447159,
            "title": "Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation",
            "venue": "Findings",
            "year": 2022,
            "referenceCount": 60,
            "citationCount": 94,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2203.06386",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.06386, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47653392",
                    "name": "Wenliang Dai"
                },
                {
                    "authorId": "48557122",
                    "name": "Lu Hou"
                },
                {
                    "authorId": "50812138",
                    "name": "Lifeng Shang"
                },
                {
                    "authorId": "2110310493",
                    "name": "Xin Jiang"
                },
                {
                    "authorId": "30738758",
                    "name": "Qun Liu"
                },
                {
                    "authorId": "2057151752",
                    "name": "Pascale Fung"
                }
            ],
            "abstract": "The recent large-scale vision-language pre-training (VLP) of dual-stream architectures (e.g., CLIP) with a tremendous amount of image-text pair data, has shown its superiority on various multimodal alignment tasks. Despite its success, the resulting models are not capable of multimodal generative tasks due to the weak text encoder. To tackle this problem, we propose to augment the dual-stream VLP model with a textual pre-trained language model (PLM) via vision-language knowledge distillation (VLKD), enabling the capability for multimodal generation. VLKD is pretty data- and computation-efficient compared to the pre-training from scratch. Experimental results show that the resulting model has strong zero-shot performance on multimodal generation tasks, such as open-ended visual question answering and image captioning. For example, it achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous state-of-the-art zero-shot model with 7\\times fewer parameters. Furthermore, the original textual language understanding and generation ability of the PLM is maintained after VLKD, which makes our model versatile for both multimodal and unimodal tasks.",
            "corpus_id": "247447159",
            "text": "The recent large-scale vision-language pre-training (VLP) of dual-stream architectures (e.g., CLIP) with a tremendous amount of image-text pair data, has shown its superiority on various multimodal alignment tasks. Despite its success, the resulting models are not capable of multimodal generative tasks due to the weak text encoder. To tackle this problem, we propose to augment the dual-stream VLP model with a textual pre-trained language model (PLM) via vision-language knowledge distillation (VLKD), enabling the capability for multimodal generation. VLKD is pretty data- and computation-efficient compared to the pre-training from scratch. Experimental results show that the resulting model has strong zero-shot performance on multimodal generation tasks, such as open-ended visual question answering and image captioning. For example, it achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous state-of-the-art zero-shot model with 7\\times fewer parameters. Furthermore, the original textual language understanding and generation ability of the PLM is maintained after VLKD, which makes our model versatile for both multimodal and unimodal tasks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.93994140625
        },
        {
            "paperId": "8e27e73e61d772686c7ff30f8bcafc167058b231",
            "corpusId": 273406244,
            "title": "Survey of Different Large Language Model Architectures: Trends, Benchmarks, and Challenges",
            "venue": "IEEE Access",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 21,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3482107",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.03220, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284694862",
                    "name": "Minghao Shao"
                },
                {
                    "authorId": "2289835770",
                    "name": "Abdul Basit"
                },
                {
                    "authorId": "2285311151",
                    "name": "Ramesh Karri"
                },
                {
                    "authorId": "2238832044",
                    "name": "Muhammad Shafique"
                }
            ],
            "abstract": "Large Language Models (LLMs) represent a class of deep learning models adept at understanding natural language and generating coherent responses to various prompts or queries. These models far exceed the complexity of conventional neural networks, often encompassing dozens of neural network layers and containing billions to trillions of parameters. They are typically trained on vast datasets, utilizing architectures based on transformer blocks. Present-day LLMs are multi-functional, capable of performing a range of tasks from text generation and language translation to question answering, as well as code generation and analysis. An advanced subset of these models, known as Multimodal Large Language Models (MLLMs), extends LLM capabilities to process and interpret multiple data modalities, including images, audio, and video. This enhancement empowers MLLMs with capabilities like video editing, image comprehension, and captioning for visual content. This survey provides a comprehensive overview of the recent advancements in LLMs. We begin by tracing the evolution of LLMs and subsequently delve into the advent and nuances of MLLMs. We analyze emerging state-of-the-art MLLMs, exploring their technical features, strengths, and limitations. Additionally, we present a comparative analysis of these models and discuss their challenges, potential limitations, and prospects for future development.",
            "corpus_id": "273406244",
            "text": "Large Language Models (LLMs) represent a class of deep learning models adept at understanding natural language and generating coherent responses to various prompts or queries. These models far exceed the complexity of conventional neural networks, often encompassing dozens of neural network layers and containing billions to trillions of parameters. They are typically trained on vast datasets, utilizing architectures based on transformer blocks. Present-day LLMs are multi-functional, capable of performing a range of tasks from text generation and language translation to question answering, as well as code generation and analysis. An advanced subset of these models, known as Multimodal Large Language Models (MLLMs), extends LLM capabilities to process and interpret multiple data modalities, including images, audio, and video. This enhancement empowers MLLMs with capabilities like video editing, image comprehension, and captioning for visual content. This survey provides a comprehensive overview of the recent advancements in LLMs. We begin by tracing the evolution of LLMs and subsequently delve into the advent and nuances of MLLMs. We analyze emerging state-of-the-art MLLMs, exploring their technical features, strengths, and limitations. Additionally, we present a comparative analysis of these models and discuss their challenges, potential limitations, and prospects for future development.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9638671875
        },
        {
            "paperId": "8a41b3f1dc9e9d81a3cb266dfa6c2fb0fa1a4875",
            "corpusId": 278171331,
            "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 70,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.20996, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306250779",
                    "name": "Sicheng Mo"
                },
                {
                    "authorId": "2279870002",
                    "name": "Thao Nguyen"
                },
                {
                    "authorId": "2358313855",
                    "name": "Xun Huang"
                },
                {
                    "authorId": "2317302544",
                    "name": "Siddharth Srinivasan Iyer"
                },
                {
                    "authorId": "2273697246",
                    "name": "Yijun Li"
                },
                {
                    "authorId": "2358093278",
                    "name": "Yuchen Liu"
                },
                {
                    "authorId": "2358039089",
                    "name": "Abhishek Tandon"
                },
                {
                    "authorId": "2257003591",
                    "name": "Eli Shechtman"
                },
                {
                    "authorId": "2265586470",
                    "name": "Krishna Kumar Singh"
                },
                {
                    "authorId": "2272231489",
                    "name": "Yong Jae Lee"
                },
                {
                    "authorId": "2274216889",
                    "name": "Bolei Zhou"
                },
                {
                    "authorId": "2351822426",
                    "name": "Yuheng Li"
                }
            ],
            "abstract": "We propose X-Fusion, a framework that extends pretrained Large Language Models (LLMs) for multimodal tasks while preserving their language capabilities. X-Fusion employs a dual-tower design with modality-specific weights, keeping the LLM's parameters frozen while integrating vision-specific information for both understanding and generation. Our experiments demonstrate that X-Fusion consistently outperforms alternative architectures on both image-to-text and text-to-image tasks. We find that incorporating understanding-focused data improves generation quality, reducing image data noise enhances overall performance, and feature alignment accelerates convergence for smaller models but has minimal impact on larger ones. Our findings provide valuable insights into building efficient unified multimodal models.",
            "corpus_id": "278171331",
            "text": "We propose X-Fusion, a framework that extends pretrained Large Language Models (LLMs) for multimodal tasks while preserving their language capabilities. X-Fusion employs a dual-tower design with modality-specific weights, keeping the LLM's parameters frozen while integrating vision-specific information for both understanding and generation. Our experiments demonstrate that X-Fusion consistently outperforms alternative architectures on both image-to-text and text-to-image tasks. We find that incorporating understanding-focused data improves generation quality, reducing image data noise enhances overall performance, and feature alignment accelerates convergence for smaller models but has minimal impact on larger ones. Our findings provide valuable insights into building efficient unified multimodal models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.98291015625
        },
        {
            "paperId": "648ff891c766cc1f6ba07ff8ce47f3cf2b34faa9",
            "corpusId": 269575746,
            "title": "Towards Retrieval-Augmented Architectures for Image Captioning",
            "venue": "ACM Trans. Multim. Comput. Commun. Appl.",
            "year": 2024,
            "referenceCount": 97,
            "citationCount": 12,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3663667",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.13127, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2179325162",
                    "name": "Sara Sarto"
                },
                {
                    "authorId": "3468983",
                    "name": "Marcella Cornia"
                },
                {
                    "authorId": "1843795",
                    "name": "L. Baraldi"
                },
                {
                    "authorId": "2299854427",
                    "name": "Alessandro Nicolosi"
                },
                {
                    "authorId": "1741922",
                    "name": "R. Cucchiara"
                }
            ],
            "abstract": "The objective of image captioning models is to bridge the gap between the visual and linguistic modalities by generating natural language descriptions that accurately reflect the content of input images. In recent years, researchers have leveraged deep learning-based models and made advances in the extraction of visual features and the design of multimodal connections to tackle this task. This work presents a novel approach toward developing image captioning models that utilize an external kNN memory to improve the generation process. Specifically, we propose two model variants that incorporate a knowledge retriever component that is based on visual similarities, a differentiable encoder to represent input images, and a kNN-augmented language model to predict tokens based on contextual cues and text retrieved from the external memory. We experimentally validate our approach on COCO and nocaps datasets and demonstrate that incorporating an explicit external memory can significantly enhance the quality of captions, especially with a larger retrieval corpus. This work provides valuable insights into retrieval-augmented captioning models and opens up new avenues for improving image captioning at a larger scale.",
            "corpus_id": "269575746",
            "text": "The objective of image captioning models is to bridge the gap between the visual and linguistic modalities by generating natural language descriptions that accurately reflect the content of input images. In recent years, researchers have leveraged deep learning-based models and made advances in the extraction of visual features and the design of multimodal connections to tackle this task. This work presents a novel approach toward developing image captioning models that utilize an external kNN memory to improve the generation process. Specifically, we propose two model variants that incorporate a knowledge retriever component that is based on visual similarities, a differentiable encoder to represent input images, and a kNN-augmented language model to predict tokens based on contextual cues and text retrieved from the external memory. We experimentally validate our approach on COCO and nocaps datasets and demonstrate that incorporating an explicit external memory can significantly enhance the quality of captions, especially with a larger retrieval corpus. This work provides valuable insights into retrieval-augmented captioning models and opens up new avenues for improving image captioning at a larger scale.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.771484375
        },
        {
            "paperId": "d92c9d8af6c12090421ef31f7b24f8845ad03413",
            "corpusId": 263230901,
            "title": "From Templates to Transformers: A survey of Multimodal Image Captioning Decoders",
            "venue": "2023 International Conference on Computer, Electronics & Electrical Engineering & their Applications (IC2E3)",
            "year": 2023,
            "referenceCount": 32,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IC2E357697.2023.10262494?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IC2E357697.2023.10262494, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "152599892",
                    "name": "Himanshu Sharma"
                },
                {
                    "authorId": "1876867",
                    "name": "Devanand Padha"
                }
            ],
            "abstract": "Automated text-to-image (image synthesis) and image-to-text (image captioning) generation are two of the most challenging and cutting-edge fields of study in Computer Vision (CV) in conjunction with Natural Language Processing (NLP). The image-to-text synthesis, also known as Image Captioning (IC), has numerous applications in visual assistance, machine vision, remote security, healthcare, and remote sensing, among others. Since their origin, the IC frameworks have been constructed utilizing a two-subcomponent pipeline consisting of the visual feature extraction and natural language modeling subcomponents. The recent surge of interest in the applications of sequential deep learning models in machine translation has resulted in the development of efficient language modeling architectures. Deep sequential modeling architectures, such as Recurrent Neural Networks (RNN), Long Short Term Memory (LSTM), and Gated Recurrent Units (GRU), tackle the intricacies of the multi-modular space far better than existing machine learning-based translation approaches. The tremendous growth of deep language modeling architectures in IC necessitates a comprehensive review of its literature. In this survey, we conduct an exhaustive and analytical analysis of the language modeling architectures used in IC frameworks for caption generation, along with their training corpus datasets. We also identify a list of open research issues and potential research areas for future work.",
            "corpus_id": "263230901",
            "text": "Automated text-to-image (image synthesis) and image-to-text (image captioning) generation are two of the most challenging and cutting-edge fields of study in Computer Vision (CV) in conjunction with Natural Language Processing (NLP). The image-to-text synthesis, also known as Image Captioning (IC), has numerous applications in visual assistance, machine vision, remote security, healthcare, and remote sensing, among others. Since their origin, the IC frameworks have been constructed utilizing a two-subcomponent pipeline consisting of the visual feature extraction and natural language modeling subcomponents. The recent surge of interest in the applications of sequential deep learning models in machine translation has resulted in the development of efficient language modeling architectures. Deep sequential modeling architectures, such as Recurrent Neural Networks (RNN), Long Short Term Memory (LSTM), and Gated Recurrent Units (GRU), tackle the intricacies of the multi-modular space far better than existing machine learning-based translation approaches. The tremendous growth of deep language modeling architectures in IC necessitates a comprehensive review of its literature. In this survey, we conduct an exhaustive and analytical analysis of the language modeling architectures used in IC frameworks for caption generation, along with their training corpus datasets. We also identify a list of open research issues and potential research areas for future work.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8515625
        },
        {
            "paperId": "0dfa0598fddd03b4e1a1445f9c9f824c93512054",
            "corpusId": 276394919,
            "title": "From No to Know: Taxonomy, Challenges, and Opportunities for Negation Understanding in Multimodal Foundation Models",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 42,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.09645, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2250562057",
                    "name": "M. Vatsa"
                },
                {
                    "authorId": "5014060",
                    "name": "Aparna Bharati"
                },
                {
                    "authorId": "2073395889",
                    "name": "S. Mittal"
                },
                {
                    "authorId": "2041134713",
                    "name": "Richa Singh"
                }
            ],
            "abstract": "Negation, a linguistic construct conveying absence, denial, or contradiction, poses significant challenges for multilingual multimodal foundation models. These models excel in tasks like machine translation, text-guided generation, image captioning, audio interactions, and video processing but often struggle to accurately interpret negation across diverse languages and cultural contexts. In this perspective paper, we propose a comprehensive taxonomy of negation constructs, illustrating how structural, semantic, and cultural factors influence multimodal foundation models. We present open research questions and highlight key challenges, emphasizing the importance of addressing these issues to achieve robust negation handling. Finally, we advocate for specialized benchmarks, language-specific tokenization, fine-grained attention mechanisms, and advanced multimodal architectures. These strategies can foster more adaptable and semantically precise multimodal foundation models, better equipped to navigate and accurately interpret the complexities of negation in multilingual, multimodal environments.",
            "corpus_id": "276394919",
            "text": "Negation, a linguistic construct conveying absence, denial, or contradiction, poses significant challenges for multilingual multimodal foundation models. These models excel in tasks like machine translation, text-guided generation, image captioning, audio interactions, and video processing but often struggle to accurately interpret negation across diverse languages and cultural contexts. In this perspective paper, we propose a comprehensive taxonomy of negation constructs, illustrating how structural, semantic, and cultural factors influence multimodal foundation models. We present open research questions and highlight key challenges, emphasizing the importance of addressing these issues to achieve robust negation handling. Finally, we advocate for specialized benchmarks, language-specific tokenization, fine-grained attention mechanisms, and advanced multimodal architectures. These strategies can foster more adaptable and semantically precise multimodal foundation models, better equipped to navigate and accurately interpret the complexities of negation in multilingual, multimodal environments.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.36279296875
        },
        {
            "paperId": "1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa",
            "corpusId": 246634906,
            "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework",
            "venue": "International Conference on Machine Learning",
            "year": 2022,
            "referenceCount": 123,
            "citationCount": 880,
            "influentialCitationCount": 161,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.03052, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2155302144",
                    "name": "Peng Wang"
                },
                {
                    "authorId": null,
                    "name": "An Yang"
                },
                {
                    "authorId": "47447639",
                    "name": "Rui Men"
                },
                {
                    "authorId": "35996608",
                    "name": "Junyang Lin"
                },
                {
                    "authorId": "3768186",
                    "name": "Shuai Bai"
                },
                {
                    "authorId": "2109657047",
                    "name": "Zhikang Li"
                },
                {
                    "authorId": "47793076",
                    "name": "Jianxin Ma"
                },
                {
                    "authorId": "144161025",
                    "name": "Chang Zhou"
                },
                {
                    "authorId": "1709595",
                    "name": "Jingren Zhou"
                },
                {
                    "authorId": "38385080",
                    "name": "Hongxia Yang"
                }
            ],
            "abstract": "In this work, we pursue a unified paradigm for multimodal pretraining to break the scaffolds of complex task/modality-specific customization. We propose OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task Comprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks, including image generation, visual grounding, image captioning, image classification, language modeling, etc., in a simple sequence-to-sequence learning framework. OFA follows the instruction-based learning in both pretraining and finetuning stages, requiring no extra task-specific layers for downstream tasks. In comparison with the recent state-of-the-art vision&language models that rely on extremely large cross-modal datasets, OFA is pretrained on only 20M publicly available image-text pairs. Despite its simplicity and relatively small-scale training data, OFA achieves new SOTAs in a series of cross-modal tasks while attaining highly competitive performances on uni-modal tasks. Our further analysis indicates that OFA can also effectively transfer to unseen tasks and unseen domains. Our code and models are publicly available at https://github.com/OFA-Sys/OFA.",
            "corpus_id": "246634906",
            "text": "In this work, we pursue a unified paradigm for multimodal pretraining to break the scaffolds of complex task/modality-specific customization. We propose OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task Comprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks, including image generation, visual grounding, image captioning, image classification, language modeling, etc., in a simple sequence-to-sequence learning framework. OFA follows the instruction-based learning in both pretraining and finetuning stages, requiring no extra task-specific layers for downstream tasks. In comparison with the recent state-of-the-art vision&language models that rely on extremely large cross-modal datasets, OFA is pretrained on only 20M publicly available image-text pairs. Despite its simplicity and relatively small-scale training data, OFA achieves new SOTAs in a series of cross-modal tasks while attaining highly competitive performances on uni-modal tasks. Our further analysis indicates that OFA can also effectively transfer to unseen tasks and unseen domains. Our code and models are publicly available at https://github.com/OFA-Sys/OFA.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.775390625
        },
        {
            "paperId": "c4a7fcf07d921c07f98c5629082b2adc4fb06536",
            "corpusId": 277272508,
            "title": "Image-to-Text for Medical Reports Using Adaptive Co-Attention and Triple-LSTM Module",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 43,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.18297, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2337913126",
                    "name": "Yishen Liu"
                },
                {
                    "authorId": "2352056113",
                    "name": "Shengda Liu"
                },
                {
                    "authorId": "2283841398",
                    "name": "Hudan Pan"
                }
            ],
            "abstract": "Medical report generation requires specialized expertise that general large models often fail to accurately capture. Moreover, the inherent repetition and similarity in medical data make it difficult for models to extract meaningful features, resulting in a tendency to overfit. So in this paper, we propose a multimodal model, Co-Attention Triple-LSTM Network (CA-TriNet), a deep learning model that combines transformer architectures with a Multi-LSTM network. Its Co-Attention module synergistically links a vision transformer with a text transformer to better differentiate medical images with similarities, augmented by an adaptive weight operator to catch and amplify image labels with minor similarities. Furthermore, its Triple-LSTM module refines generated sentences using targeted image objects. Extensive evaluations over three public datasets have demonstrated that CA-TriNet outperforms state-of-the-art models in terms of comprehensive ability, even pre-trained large language models on some metrics.",
            "corpus_id": "277272508",
            "text": "Medical report generation requires specialized expertise that general large models often fail to accurately capture. Moreover, the inherent repetition and similarity in medical data make it difficult for models to extract meaningful features, resulting in a tendency to overfit. So in this paper, we propose a multimodal model, Co-Attention Triple-LSTM Network (CA-TriNet), a deep learning model that combines transformer architectures with a Multi-LSTM network. Its Co-Attention module synergistically links a vision transformer with a text transformer to better differentiate medical images with similarities, augmented by an adaptive weight operator to catch and amplify image labels with minor similarities. Furthermore, its Triple-LSTM module refines generated sentences using targeted image objects. Extensive evaluations over three public datasets have demonstrated that CA-TriNet outperforms state-of-the-art models in terms of comprehensive ability, even pre-trained large language models on some metrics.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.94091796875
        },
        {
            "paperId": "558c0e253617a235ebaed7f46a586347af7b9d76",
            "corpusId": 278838898,
            "title": "Image Caption Generation Using Deep Learning",
            "venue": "Journal of Information Systems Engineering & Management",
            "year": 2025,
            "referenceCount": 16,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.52783/jisem.v10i47s.9258?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.52783/jisem.v10i47s.9258, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2363162312",
                    "name": "D. Prannav"
                },
                {
                    "authorId": "2363162032",
                    "name": "Adnan Anwar"
                },
                {
                    "authorId": "2363162448",
                    "name": "Chandrashekar Patil"
                }
            ],
            "abstract": "Image caption generation, a primary application domain in computer vision and natural language processing, produces text captions of images from deep learning models. The current paper suggests a CNN-LSTM-based system for automatic captioning, where pre-trained convolutional neural networks (CNNs) are employed for image feature extraction and long short-term memory (LSTM) networks for sequential text generation. Inspired by the Flickr8k dataset, the paper emphasizes primary challenges such as vocabulary sparsity, overfitting, and computational complexity. Experimental results achieve BLEU scores of 0.66 or more, exhibiting coherent caption generation and qualitative analysis discloses captioning inefficiencies for complex scenes. The paper also discusses future enhancements such as transformer-based architectures and attention mechanisms to improve caption accuracy and accessibility. The work contributes to improving large-scale human-computer interaction through multimodal AI systems. Caption generation is an important area at the intersection of computer vision and natural language processing, including the generation of descriptive text captions describing images using advanced deep-learning methodologies. Current paper suggests a new approach through a hybrid CNN-LSTM-based system for automatic captioning. This state-of-the-art model employs pre-trained convolutional neural networks (CNNs) for robust image feature extraction to identify and interpret relevant features in an image. These identified features are then fed to long short-term memory (LSTM) networks adept at generating coherent and relevant sequential text based on the visual input. The experimental results revealed excellent BLEU scores of 0.66 or higher, which reflects the model's capacity to generate captions not only accurate but also linguistically sound. Qualitative analysis of the generated captions does call out inefficiencies in handling complicated scenes with more than one element or activity, and it suggests where there is potential for improvement in the future. In the future, the paper foresees potential enhancements, such as the application of transformer-based models and attention, which would significantly improve caption accuracy and user experience for accessibility. Overall, this work contributes to advancing the state of large-scale human-computer interaction by developing sophisticated multimodal AI systems for interpreting and generating human-like text from visual inputs.",
            "corpus_id": "278838898",
            "text": "Image caption generation, a primary application domain in computer vision and natural language processing, produces text captions of images from deep learning models. The current paper suggests a CNN-LSTM-based system for automatic captioning, where pre-trained convolutional neural networks (CNNs) are employed for image feature extraction and long short-term memory (LSTM) networks for sequential text generation. Inspired by the Flickr8k dataset, the paper emphasizes primary challenges such as vocabulary sparsity, overfitting, and computational complexity. Experimental results achieve BLEU scores of 0.66 or more, exhibiting coherent caption generation and qualitative analysis discloses captioning inefficiencies for complex scenes. The paper also discusses future enhancements such as transformer-based architectures and attention mechanisms to improve caption accuracy and accessibility. The work contributes to improving large-scale human-computer interaction through multimodal AI systems. Caption generation is an important area at the intersection of computer vision and natural language processing, including the generation of descriptive text captions describing images using advanced deep-learning methodologies. Current paper suggests a new approach through a hybrid CNN-LSTM-based system for automatic captioning. This state-of-the-art model employs pre-trained convolutional neural networks (CNNs) for robust image feature extraction to identify and interpret relevant features in an image. These identified features are then fed to long short-term memory (LSTM) networks adept at generating coherent and relevant sequential text based on the visual input. The experimental results revealed excellent BLEU scores of 0.66 or higher, which reflects the model's capacity to generate captions not only accurate but also linguistically sound. Qualitative analysis of the generated captions does call out inefficiencies in handling complicated scenes with more than one element or activity, and it suggests where there is potential for improvement in the future. In the future, the paper foresees potential enhancements, such as the application of transformer-based models and attention, which would significantly improve caption accuracy and user experience for accessibility. Overall, this work contributes to advancing the state of large-scale human-computer interaction by developing sophisticated multimodal AI systems for interpreting and generating human-like text from visual inputs.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.74951171875
        },
        {
            "paperId": "fbb635034baeb45f330426ed43de127421aaf15b",
            "corpusId": 275342967,
            "title": "Visual question answering: from early developments to recent advances - a survey",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.03939, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2155649080",
                    "name": "Ngoc Dung Huynh"
                },
                {
                    "authorId": "3080469",
                    "name": "Mohamed Reda Bouadjenek"
                },
                {
                    "authorId": "2256860681",
                    "name": "Sunil Aryal"
                },
                {
                    "authorId": "2240180680",
                    "name": "Imran Razzak"
                },
                {
                    "authorId": "2296782788",
                    "name": "Hakim Hacid"
                }
            ],
            "abstract": "Visual Question Answering (VQA) is an evolving research field aimed at enabling machines to answer questions about visual content by integrating image and language processing techniques such as feature extraction, object detection, text embedding, natural language understanding, and language generation. With the growth of multimodal data research, VQA has gained significant attention due to its broad applications, including interactive educational tools, medical image diagnosis, customer service, entertainment, and social media captioning. Additionally, VQA plays a vital role in assisting visually impaired individuals by generating descriptive content from images. This survey introduces a taxonomy of VQA architectures, categorizing them based on design choices and key components to facilitate comparative analysis and evaluation. We review major VQA approaches, focusing on deep learning-based methods, and explore the emerging field of Large Visual Language Models (LVLMs) that have demonstrated success in multimodal tasks like VQA. The paper further examines available datasets and evaluation metrics essential for measuring VQA system performance, followed by an exploration of real-world VQA applications. Finally, we highlight ongoing challenges and future directions in VQA research, presenting open questions and potential areas for further development. This survey serves as a comprehensive resource for researchers and practitioners interested in the latest advancements and future",
            "corpus_id": "275342967",
            "text": "Visual Question Answering (VQA) is an evolving research field aimed at enabling machines to answer questions about visual content by integrating image and language processing techniques such as feature extraction, object detection, text embedding, natural language understanding, and language generation. With the growth of multimodal data research, VQA has gained significant attention due to its broad applications, including interactive educational tools, medical image diagnosis, customer service, entertainment, and social media captioning. Additionally, VQA plays a vital role in assisting visually impaired individuals by generating descriptive content from images. This survey introduces a taxonomy of VQA architectures, categorizing them based on design choices and key components to facilitate comparative analysis and evaluation. We review major VQA approaches, focusing on deep learning-based methods, and explore the emerging field of Large Visual Language Models (LVLMs) that have demonstrated success in multimodal tasks like VQA. The paper further examines available datasets and evaluation metrics essential for measuring VQA system performance, followed by an exploration of real-world VQA applications. Finally, we highlight ongoing challenges and future directions in VQA research, presenting open questions and potential areas for further development. This survey serves as a comprehensive resource for researchers and practitioners interested in the latest advancements and future",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.84375
        },
        {
            "paperId": "0ff873d70990e16f1aaf8420a9a5fdccee701f14",
            "corpusId": 279071015,
            "title": "D2AF: A Dual-Driven Annotation and Filtering Framework for Visual Grounding",
            "venue": "",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.24372, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2364650786",
                    "name": "Yichi Zhang"
                },
                {
                    "authorId": "2364693318",
                    "name": "Gongwei Chen"
                },
                {
                    "authorId": "2365448811",
                    "name": "Jun Zhu"
                },
                {
                    "authorId": "2365405925",
                    "name": "Jia Wan"
                }
            ],
            "abstract": "Visual Grounding is a task that aims to localize a target region in an image based on a free-form natural language description. With the rise of Transformer architectures, there is an increasing need for larger datasets to boost performance. However, the high cost of manual annotation poses a challenge, hindering the scale of data and the ability of large models to enhance their effectiveness. Previous pseudo label generation methods heavily rely on human-labeled captions of the original dataset, limiting scalability and diversity. To address this, we propose D2AF, a robust annotation framework for visual grounding using only input images. This approach overcomes dataset size limitations and enriches both the quantity and diversity of referring expressions. Our approach leverages multimodal large models and object detection models. By implementing dual-driven annotation strategies, we effectively generate detailed region-text pairs using both closed-set and open-set approaches. We further conduct an in-depth analysis of data quantity and data distribution. Our findings demonstrate that increasing data volume enhances model performance. However, the degree of improvement depends on how well the pseudo labels broaden the original data distribution. Based on these insights, we propose a consistency and distribution aware filtering method to further improve data quality by effectively removing erroneous and redundant data. This approach effectively eliminates noisy data, leading to improved performance. Experiments on three visual grounding tasks demonstrate that our method significantly improves the performance of existing models and achieves state-of-the-art results.",
            "corpus_id": "279071015",
            "text": "Visual Grounding is a task that aims to localize a target region in an image based on a free-form natural language description. With the rise of Transformer architectures, there is an increasing need for larger datasets to boost performance. However, the high cost of manual annotation poses a challenge, hindering the scale of data and the ability of large models to enhance their effectiveness. Previous pseudo label generation methods heavily rely on human-labeled captions of the original dataset, limiting scalability and diversity. To address this, we propose D2AF, a robust annotation framework for visual grounding using only input images. This approach overcomes dataset size limitations and enriches both the quantity and diversity of referring expressions. Our approach leverages multimodal large models and object detection models. By implementing dual-driven annotation strategies, we effectively generate detailed region-text pairs using both closed-set and open-set approaches. We further conduct an in-depth analysis of data quantity and data distribution. Our findings demonstrate that increasing data volume enhances model performance. However, the degree of improvement depends on how well the pseudo labels broaden the original data distribution. Based on these insights, we propose a consistency and distribution aware filtering method to further improve data quality by effectively removing erroneous and redundant data. This approach effectively eliminates noisy data, leading to improved performance. Experiments on three visual grounding tasks demonstrate that our method significantly improves the performance of existing models and achieves state-of-the-art results.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.263671875
        },
        {
            "paperId": "f7f71bad41e6b76c455dd7edf333c3831e766bcf",
            "corpusId": 269302577,
            "title": "Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 37,
            "citationCount": 10,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.15100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2297920756",
                    "name": "Xun Wu"
                },
                {
                    "authorId": "3110003",
                    "name": "Shaohan Huang"
                },
                {
                    "authorId": "2253471545",
                    "name": "Furu Wei"
                }
            ],
            "abstract": "Recent studies have demonstrated the exceptional potentials of leveraging human preference datasets to refine text-to-image generative models, enhancing the alignment between generated images and textual prompts. Despite these advances, current human preference datasets are either prohibitively expensive to construct or suffer from a lack of diversity in preference dimensions, resulting in limited applicability for instruction tuning in open-source text-to-image generative models and hinder further exploration. To address these challenges and promote the alignment of generative models through instruction tuning, we leverage multimodal large language models to create VisionPrefer, a high-quality and fine-grained preference dataset that captures multiple preference aspects. We aggregate feedback from AI annotators across four aspects: prompt-following, aesthetic, fidelity, and harmlessness to construct VisionPrefer. To validate the effectiveness of VisionPrefer, we train a reward model VP-Score over VisionPrefer to guide the training of text-to-image generative models and the preference prediction accuracy of VP-Score is comparable to human annotators. Furthermore, we use two reinforcement learning methods to supervised fine-tune generative models to evaluate the performance of VisionPrefer, and extensive experimental results demonstrate that VisionPrefer significantly improves text-image alignment in compositional image generation across diverse aspects, e.g., aesthetic, and generalizes better than previous human-preference metrics across various image distributions. Moreover, VisionPrefer indicates that the integration of AI-generated synthetic data as a supervisory signal is a promising avenue for achieving improved alignment with human preferences in vision generative models.",
            "corpus_id": "269302577",
            "text": "Recent studies have demonstrated the exceptional potentials of leveraging human preference datasets to refine text-to-image generative models, enhancing the alignment between generated images and textual prompts. Despite these advances, current human preference datasets are either prohibitively expensive to construct or suffer from a lack of diversity in preference dimensions, resulting in limited applicability for instruction tuning in open-source text-to-image generative models and hinder further exploration. To address these challenges and promote the alignment of generative models through instruction tuning, we leverage multimodal large language models to create VisionPrefer, a high-quality and fine-grained preference dataset that captures multiple preference aspects. We aggregate feedback from AI annotators across four aspects: prompt-following, aesthetic, fidelity, and harmlessness to construct VisionPrefer. To validate the effectiveness of VisionPrefer, we train a reward model VP-Score over VisionPrefer to guide the training of text-to-image generative models and the preference prediction accuracy of VP-Score is comparable to human annotators. Furthermore, we use two reinforcement learning methods to supervised fine-tune generative models to evaluate the performance of VisionPrefer, and extensive experimental results demonstrate that VisionPrefer significantly improves text-image alignment in compositional image generation across diverse aspects, e.g., aesthetic, and generalizes better than previous human-preference metrics across various image distributions. Moreover, VisionPrefer indicates that the integration of AI-generated synthetic data as a supervisory signal is a promising avenue for achieving improved alignment with human preferences in vision generative models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.77490234375
        },
        {
            "paperId": "952a2ecdc3e8938e5bac718206f6355fda029a79",
            "corpusId": 268249158,
            "title": "Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 48,
            "citationCount": 17,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.02677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108438929",
                    "name": "Weizhi Wang"
                },
                {
                    "authorId": "2267338166",
                    "name": "Khalil Mrini"
                },
                {
                    "authorId": "2269846471",
                    "name": "Linjie Yang"
                },
                {
                    "authorId": "2271185974",
                    "name": "Sateesh Kumar"
                },
                {
                    "authorId": "2270211545",
                    "name": "Yu Tian"
                },
                {
                    "authorId": "2243564882",
                    "name": "Xifeng Yan"
                },
                {
                    "authorId": "2265388083",
                    "name": "Heng Wang"
                }
            ],
            "abstract": "We propose a novel framework for filtering image-text data by leveraging fine-tuned Multimodal Language Models (MLMs). Our approach outperforms predominant filtering methods (e.g., CLIPScore) via integrating the recent advances in MLMs. We design four distinct yet complementary metrics to holistically measure the quality of image-text data. A new pipeline is established to construct high-quality instruction data for fine-tuning MLMs as data filters. Comparing with CLIPScore, our MLM filters produce more precise and comprehensive scores that directly improve the quality of filtered data and boost the performance of pre-trained models. We achieve significant improvements over CLIPScore on popular foundation models (i.e., CLIP and BLIP2) and various downstream tasks. Our MLM filter can generalize to different models and tasks, and be used as a drop-in replacement for CLIPScore. An additional ablation study is provided to verify our design choices for the MLM filter.",
            "corpus_id": "268249158",
            "text": "We propose a novel framework for filtering image-text data by leveraging fine-tuned Multimodal Language Models (MLMs). Our approach outperforms predominant filtering methods (e.g., CLIPScore) via integrating the recent advances in MLMs. We design four distinct yet complementary metrics to holistically measure the quality of image-text data. A new pipeline is established to construct high-quality instruction data for fine-tuning MLMs as data filters. Comparing with CLIPScore, our MLM filters produce more precise and comprehensive scores that directly improve the quality of filtered data and boost the performance of pre-trained models. We achieve significant improvements over CLIPScore on popular foundation models (i.e., CLIP and BLIP2) and various downstream tasks. Our MLM filter can generalize to different models and tasks, and be used as a drop-in replacement for CLIPScore. An additional ablation study is provided to verify our design choices for the MLM filter.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.83837890625
        }
    ],
    "quotes": {
        "cost": 0.2339969999999999,
        "quotes": [
            {
                "idx": 0,
                "key": "[253802096 | Yasunaga et al. | 2023 | Citations: 108]",
                "snippets": "Recent multimodal models such as DALL-E and CM3 have achieved remarkable progress in text-to-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the model parameters, requiring increasingly larger models and training data to capture more knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Recent multimodal models such as DALL-E and CM3 have achieved remarkable progress in text-to-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the model parameters, requiring increasingly larger models and training data to capture more knowledge.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[258959284 | Koh et al. | 2023 | Citations: 259]",
                "snippets": "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 802,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[262898422 | Aiello et al. | 2023 | Citations: 31]",
                "snippets": "In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 463,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[265498814 | Li et al. | 2023 | Citations: 69]",
                "snippets": "Recent work [11,17,18,34,54,60] further empower LLMs with the ability of generating images beyond texts (acting like a combination of GPT-4V [1] and DALL-E 3 [5]), since they contend that the premise for the emergence of multimodal capabilities is that text and image can be represented and processed interchangeably in a unified autoregressive Transformer.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 428,
                        "end": 785,
                        "sentence_offsets": [
                            {
                                "start": 428,
                                "end": 785
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recent work [11,17,18,34,54,60] further empower LLMs with the ability of generating images beyond texts (acting like a combination of GPT-4V [1] and DALL-E 3 [5]), since they contend that the premise for the emergence of multimodal capabilities is that text and image can be represented and processed interchangeably in a unified autoregressive Transformer."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[266191741 | Zhu et al. | 2023 | Citations: 26]",
                "snippets": "\u2022 LLaVA (Liu et al., 2024): LlaVA (Language-Image LLaMA) is a multimodal model combining language and image data. It extends the LLaMA architecture to handle both modalities, enabling tasks like image captioning, visual question answering, and imagebased text generation.\n\n\u2022 Qwen-VL series (Bai et al., 2023): Qwen-VL (Qwen Large Vision Language Model) is the multimodal version of the large model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-VL accepts image, text, and bounding box as inputs, outputs text, and bounding box.\n\n\u2022 InternLM-XComposer2-VL (Dong et al., 2024): InternLM-XComposer2 is a cuttingedge vision-language model excelling in free-form text-image composition and comprehension, crafting content from diverse inputs like outlines, detailed specs, and reference images. Using a Partial LoRA (PLoRA) approach, it balances vision understanding and text composition.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258179774 | Liu et al. | 2023 | Citations: 4921]": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.",
                    "[263625818 | Srivastava et al. | 2022 | Citations: 1776]": "Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit\"breakthrough\"behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting."
                },
                "metadata": [
                    {
                        "section_title": "Conclusion and Discussion",
                        "pdf_hash": "",
                        "start": 372,
                        "end": 1277,
                        "sentence_offsets": [
                            {
                                "start": 127,
                                "end": 469
                            },
                            {
                                "start": 472,
                                "end": 585
                            },
                            {
                                "start": 586,
                                "end": 743
                            },
                            {
                                "start": 746,
                                "end": 890
                            },
                            {
                                "start": 891,
                                "end": 934
                            },
                            {
                                "start": 935,
                                "end": 1023
                            },
                            {
                                "start": 1026,
                                "end": 1285
                            }
                        ],
                        "ref_mentions": [
                            "258179774",
                            "263625818"
                        ],
                        "quote": "\u2022 LLaVA (Liu et al., 2024): LlaVA (Language-Image LLaMA) is a multimodal model combining language and image data. It extends the LLaMA architecture to handle both modalities, enabling tasks like image captioning, visual question answering, and imagebased text generation.\n\n\u2022 Qwen-VL series (Bai et al., 2023): Qwen-VL (Qwen Large Vision Language Model) is the multimodal version of the large model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-VL accepts image, text, and bounding box as inputs, outputs text, and bounding box.\n\n\u2022 InternLM-XComposer2-VL (Dong et al., 2024): InternLM-XComposer2 is a cuttingedge vision-language model excelling in free-form text-image composition and comprehension, crafting content from diverse inputs like outlines, detailed specs, and reference images. Using a Partial LoRA (PLoRA) approach, it balances vision understanding and text composition."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[266210376 | Zhu et al. | 2023 | Citations: 37]",
                "snippets": "To expand the capabilities of generating image in a multimodal context, certain efforts, e.g., Visual ChatGPT [47], attempt to connect LLMs with image generation tools in a cascaded pipeline by transferring text messages, which inevitably introduce instability and noise. Alternatively, another line of research achieves it by optimizing models in an end-to-end manner [9,18,23,30]48]. By aligning the output space with the image diffusion models, VL models can not only perceive but also generate images and text.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 698,
                        "end": 1212,
                        "sentence_offsets": [
                            {
                                "start": 698,
                                "end": 969
                            },
                            {
                                "start": 970,
                                "end": 1083
                            },
                            {
                                "start": 1084,
                                "end": 1212
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To expand the capabilities of generating image in a multimodal context, certain efforts, e.g., Visual ChatGPT [47], attempt to connect LLMs with image generation tools in a cascaded pipeline by transferring text messages, which inevitably introduce instability and noise. Alternatively, another line of research achieves it by optimizing models in an end-to-end manner [9,18,23,30]48]. By aligning the output space with the image diffusion models, VL models can not only perceive but also generate images and text."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[266573642 | Huang et al. | 2023 | Citations: 23]",
                "snippets": "GPT4Tools [63] enables open-source language models to effectively use multimodal tools. It constructs a tool-related instructional dataset from advanced language models and utilizes Low-Rank Adaptation (LoRA) optimization to enhance the language models' tool-usage capabilities. Additionally, it proposes a benchmark to evaluate the accuracy of language models in using tools, demonstrating significant improvements in tool usage across various visual tasks. As shown in Figure 17, the GPT4Tools framework involves constructing a tool-related instruction dataset by prompting an advanced language model with various multimodal contexts. This dataset is then used to fine-tune open-source language models using Low-Rank Adaptation (LoRA) optimization, enabling them to effectively use tools for visual tasks such as comprehension and image generation.\n\nTextBind [72] enhances large language models with multi-turn interleaved multimodal instruction-following capabilities. It significantly reduces the need for high-quality exemplar data, making it more accessible and scalable for real-world tasks. The proposed model, MIM, trained on TextBind, outperforms recent baselines in open-world multimodal conversations, demonstrating remarkable performance in textual response generation, image generation, and overall multimodal instruction-following. As shown in Figure 18, MIM seamlessly integrates image encoder and decoder models to accommodate interleaved image-text inputs and outputs. It supplements large language models with visual input and output modules, enabling the model to process multi-turn interleaved multimodal instructions and generate coherent responses.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "GPT4Tools [63] enables open-source language models to effectively use multimodal tools. It constructs a tool-related instructional dataset from advanced language models and utilizes Low-Rank Adaptation (LoRA) optimization to enhance the language models' tool-usage capabilities. Additionally, it proposes a benchmark to evaluate the accuracy of language models in using tools, demonstrating significant improvements in tool usage across various visual tasks. As shown in Figure 17, the GPT4Tools framework involves constructing a tool-related instruction dataset by prompting an advanced language model with various multimodal contexts. This dataset is then used to fine-tune open-source language models using Low-Rank Adaptation (LoRA) optimization, enabling them to effectively use tools for visual tasks such as comprehension and image generation.\n\nTextBind [72] enhances large language models with multi-turn interleaved multimodal instruction-following capabilities. It significantly reduces the need for high-quality exemplar data, making it more accessible and scalable for real-world tasks. The proposed model, MIM, trained on TextBind, outperforms recent baselines in open-world multimodal conversations, demonstrating remarkable performance in textual response generation, image generation, and overall multimodal instruction-following. As shown in Figure 18, MIM seamlessly integrates image encoder and decoder models to accommodate interleaved image-text inputs and outputs. It supplements large language models with visual input and output modules, enabling the model to process multi-turn interleaved multimodal instructions and generate coherent responses.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[267200040 | Li et al. | 2024 | Citations: 8]",
                "snippets": "Models such as GILL (Koh et al., 2023), Emu (Sun et al., 2023), and DreamLLM (Dong et al., 2023) specialize in interleaved vision-language generation by aligning the output space of MLLMs with the diffusion image decoder. However, these methods primarily align at a semantic level and may struggle with detailed, subject-driven image generation. BLIP-Diffusion (Li et al., 2023a) synthesizes images by composing subjects with random backgrounds, endowing it with zero-shot, subject-driven text-toimage generation capabilities. However, its specific input template and training process limit scalability for multiple entities. KOSMOS-G (Pan et al., 2023), a model closely related to our work, leverages a MLLM to encode interleaved text-visual inputs, and the U-Net of Stable Diffusion (SD) v1.5 as the image decoder.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Generating with Multi-modal Language Models",
                        "pdf_hash": "",
                        "start": 338,
                        "end": 1154,
                        "sentence_offsets": [
                            {
                                "start": 338,
                                "end": 559
                            },
                            {
                                "start": 560,
                                "end": 683
                            },
                            {
                                "start": 684,
                                "end": 864
                            },
                            {
                                "start": 865,
                                "end": 963
                            },
                            {
                                "start": 964,
                                "end": 1154
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Models such as GILL (Koh et al., 2023), Emu (Sun et al., 2023), and DreamLLM (Dong et al., 2023) specialize in interleaved vision-language generation by aligning the output space of MLLMs with the diffusion image decoder. However, these methods primarily align at a semantic level and may struggle with detailed, subject-driven image generation. BLIP-Diffusion (Li et al., 2023a) synthesizes images by composing subjects with random backgrounds, endowing it with zero-shot, subject-driven text-toimage generation capabilities. However, its specific input template and training process limit scalability for multiple entities. KOSMOS-G (Pan et al., 2023), a model closely related to our work, leverages a MLLM to encode interleaved text-visual inputs, and the U-Net of Stable Diffusion (SD) v1.5 as the image decoder."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[267897956 | Younesi et al. | 2024 | Citations: 47]",
                "snippets": "One strategy involves employing a dual-stream architecture with distinct CNN and transformer encoders interacting via coattentional transformer layers [97]. The CNN extracts visual features from images, providing contextual information that can guide language generation and comprehension. The transformer architecture models the semantics and syntax of text. Their interaction enables the generation of captions based on image content or the retrieval of pertinent images for textual queries. Alternative methods directly incorporate CNNs within the transformer architecture as visual token encoders that operate with text token encoders [98]. The CNN projections of image patches are appended to text token embeddings as inputs to the transformer layers. This unified architecture allows for end-to-end optimization of parameters for both vision and language tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "H. Evolution of Language Models and Multimodal LLMs",
                        "pdf_hash": "",
                        "start": 860,
                        "end": 1727,
                        "sentence_offsets": [
                            {
                                "start": 860,
                                "end": 1016
                            },
                            {
                                "start": 1017,
                                "end": 1149
                            },
                            {
                                "start": 1150,
                                "end": 1219
                            },
                            {
                                "start": 1220,
                                "end": 1353
                            },
                            {
                                "start": 1354,
                                "end": 1504
                            },
                            {
                                "start": 1505,
                                "end": 1616
                            },
                            {
                                "start": 1617,
                                "end": 1727
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "One strategy involves employing a dual-stream architecture with distinct CNN and transformer encoders interacting via coattentional transformer layers [97]. The CNN extracts visual features from images, providing contextual information that can guide language generation and comprehension. The transformer architecture models the semantics and syntax of text. Their interaction enables the generation of captions based on image content or the retrieval of pertinent images for textual queries. Alternative methods directly incorporate CNNs within the transformer architecture as visual token encoders that operate with text token encoders [98]. The CNN projections of image patches are appended to text token embeddings as inputs to the transformer layers. This unified architecture allows for end-to-end optimization of parameters for both vision and language tasks."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[269791516 | Team et al. | 2024 | Citations: 339]",
                "snippets": "We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 825,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[270391394 | Yang et al. | 2024 | Citations: 13]",
                "snippets": "Notable examples include Gemini [111], GPT-4V, and ImageBind (Girdhar et al., 2023).These models showcase the ability to process text, images, audio, and video, enhancing functionalities such as translation, image recognition, and more...BLIP-2 [112] introduces a novel approach to integrate a frozen pre-trained visual encoder with a frozen large language model using a Q-former module.This module employs learnable input queries that interact with image features and the LLM, allowing for effective cross-modal learning.This setup helps maintain the versatility of the LLM while incorporating visual information effectively.LLava [113] is a represent large multimodal model integrating a pre-trained CLIP [116] visual encoder (ViT-L/14), the Vicuna [115] language model, and a simple linear projection layer.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258564264 | Girdhar et al. | 2023 | Citations: 941]": "We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications \u2018out-of-the-box\u2019 including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks."
                },
                "metadata": [
                    {
                        "section_title": "Large Multimodal Models",
                        "pdf_hash": "",
                        "start": 1111,
                        "end": 1329,
                        "sentence_offsets": [
                            {
                                "start": 1111,
                                "end": 1178
                            },
                            {
                                "start": 1178,
                                "end": 1330
                            }
                        ],
                        "ref_mentions": [
                            "258564264"
                        ],
                        "quote": "Notable examples include Gemini [111], GPT-4V, and ImageBind (Girdhar et al., 2023).These models showcase the ability to process text, images, audio, and video, enhancing functionalities such as translation, image recognition, and more"
                    },
                    {
                        "section_title": "Large Multimodal Models",
                        "pdf_hash": "",
                        "start": 1429,
                        "end": 2001,
                        "sentence_offsets": [
                            {
                                "start": 1330,
                                "end": 1578
                            },
                            {
                                "start": 1578,
                                "end": 1713
                            },
                            {
                                "start": 1713,
                                "end": 1817
                            },
                            {
                                "start": 1817,
                                "end": 2001
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "BLIP-2 [112] introduces a novel approach to integrate a frozen pre-trained visual encoder with a frozen large language model using a Q-former module.This module employs learnable input queries that interact with image features and the LLM, allowing for effective cross-modal learning.This setup helps maintain the versatility of the LLM while incorporating visual information effectively.LLava [113] is a represent large multimodal model integrating a pre-trained CLIP [116] visual encoder (ViT-L/14), the Vicuna [115] language model, and a simple linear projection layer."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[270688372 | Liu et al. | 2024 | Citations: 11]",
                "snippets": "Recent works have begun to address this gap, with the LMMs extended with diffusion models, exploring the generation of mixed text and image outputs (Koh et al., 2023)Sun et al., 2023b;(Dong et al., 2023)Tian et al., 2024;(Zhan et al., 2024)Chen et al., 2023). These models leverage advanced architectures and training techniques to enhance their ability to produce coherent and contextually relevant interleaved content.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258959284 | Koh et al. | 2023 | Citations: 259]": "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.",
                    "[261975252 | Dong et al. | 2023 | Citations: 201]": "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.",
                    "[267750101 | Zhan et al. | 2024 | Citations: 136]": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/"
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 676,
                        "end": 1094,
                        "sentence_offsets": [
                            {
                                "start": 676,
                                "end": 933
                            },
                            {
                                "start": 934,
                                "end": 1094
                            }
                        ],
                        "ref_mentions": [
                            "258959284",
                            "261975252",
                            "267750101"
                        ],
                        "quote": "Recent works have begun to address this gap, with the LMMs extended with diffusion models, exploring the generation of mixed text and image outputs (Koh et al., 2023)Sun et al., 2023b;(Dong et al., 2023)Tian et al., 2024;(Zhan et al., 2024)Chen et al., 2023). These models leverage advanced architectures and training techniques to enhance their ability to produce coherent and contextually relevant interleaved content."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[271213320 | Yuan et al. | 2024 | Citations: 0]",
                "snippets": "LVLMs are built upon the foundation of LLMs, incorporating additional components that allow them to process and understand visual data. These components typically include visual receptors for image processing, input-output interfaces that enable the model to handle multimodal data, and training pipelines that are tailored to accommodate the complexities of learning from both text and images.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 484,
                        "end": 878,
                        "sentence_offsets": [
                            {
                                "start": 484,
                                "end": 619
                            },
                            {
                                "start": 620,
                                "end": 878
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "LVLMs are built upon the foundation of LLMs, incorporating additional components that allow them to process and understand visual data. These components typically include visual receptors for image processing, input-output interfaces that enable the model to handle multimodal data, and training pipelines that are tailored to accommodate the complexities of learning from both text and images."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[271693411 | Wang et al. | 2024 | Citations: 36]",
                "snippets": "Multimodal Output Decoder: Lastly, the multimodal output decoder reconverts the fused, integrated multimodal information back into a usable form tailored to specific tasks, such as Image captioning, the decoder might generate descriptive text based on visual inputs. Video understanding tasks, it could produce annotations or summaries combining both visual and auditory data. Each decoder is meticulously designed to optimize accuracy and quality, ensuring that the output precisely reflects the combined insights gained from the integrated modalities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Main Components of Multimodal Large Language Models",
                        "pdf_hash": "",
                        "start": 227,
                        "end": 780,
                        "sentence_offsets": [
                            {
                                "start": 227,
                                "end": 493
                            },
                            {
                                "start": 494,
                                "end": 603
                            },
                            {
                                "start": 604,
                                "end": 780
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Multimodal Output Decoder: Lastly, the multimodal output decoder reconverts the fused, integrated multimodal information back into a usable form tailored to specific tasks, such as Image captioning, the decoder might generate descriptive text based on visual inputs. Video understanding tasks, it could produce annotations or summaries combining both visual and auditory data. Each decoder is meticulously designed to optimize accuracy and quality, ensuring that the output precisely reflects the combined insights gained from the integrated modalities."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[271709823 | Liu et al. | 2024 | Citations: 59]",
                "snippets": "Another line of research (Koh et al., 2023)(Dong et al., 2023)Sun et al., 2023;(Wu et al., 2023) has proposed augmenting MLLMs with the ability to generate images, videos, and audio from text instructions. These approaches introduce additional visual tokens for generation and align these generative tokens as conditional information with a pretrained generator, such as Stable Diffusion (Esser et al., 2024;Podell et al., 2023) for text-to-image generation. Consequently, the generation capabilities heavily rely on the external expert generator rather than MLLMs themselves, resulting in inconsistent and inferior generation results. To combine the strength of both approaches, our model aims to learn both understanding and generation of images using an MLLM with native multimodal capabilities, drawing inspiration from Chameleon (Team, 2024a), a mixed-modal early-fusion foundation model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258959284 | Koh et al. | 2023 | Citations: 259]": "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.",
                    "[261696650 | Wu et al. | 2023 | Citations: 506]": "While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/",
                    "[261975252 | Dong et al. | 2023 | Citations: 201]": "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."
                },
                "metadata": [
                    {
                        "section_title": "B. Related Work",
                        "pdf_hash": "",
                        "start": 896,
                        "end": 1781,
                        "sentence_offsets": [
                            {
                                "start": 896,
                                "end": 1093
                            },
                            {
                                "start": 1094,
                                "end": 1346
                            },
                            {
                                "start": 1347,
                                "end": 1523
                            },
                            {
                                "start": 1524,
                                "end": 1781
                            }
                        ],
                        "ref_mentions": [
                            "258959284",
                            "261975252",
                            "261696650"
                        ],
                        "quote": "Another line of research (Koh et al., 2023)(Dong et al., 2023)Sun et al., 2023;(Wu et al., 2023) has proposed augmenting MLLMs with the ability to generate images, videos, and audio from text instructions. These approaches introduce additional visual tokens for generation and align these generative tokens as conditional information with a pretrained generator, such as Stable Diffusion (Esser et al., 2024;Podell et al., 2023) for text-to-image generation. Consequently, the generation capabilities heavily rely on the external expert generator rather than MLLMs themselves, resulting in inconsistent and inferior generation results. To combine the strength of both approaches, our model aims to learn both understanding and generation of images using an MLLM with native multimodal capabilities, drawing inspiration from Chameleon (Team, 2024a), a mixed-modal early-fusion foundation model."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[271924334 | Xie et al. | 2024 | Citations: 229]",
                "snippets": "In recent years, an increasing number of studies (Ge et al., 2024;Wu et al., 2023b;(Tang et al., 2023)Ye et al., 2024a;(Dong et al., 2023)Aiello et al., 2024) have focused on unified multimodal language models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;Sun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive modeling to generate images. SEED-X (Ge et al., 2024) proposes a unified and versatile foundation system capable of handling both multimodal understanding and generation tasks. In this approach, continuous image representations from CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model (LLM) to perform next-word prediction and image representation regression. DreamLLM (Dong et al., 2023)) also explores the potential of enabling multimodal comprehension and creation based on LLMs. Chameleon (Team, 2024) introduces a family of token-based mixed-modal models capable of both comprehending and generating images. This approach represents all modalities as discrete tokens and utilizes a unified transformer-based architecture and trains the model from scratch in an end-to-end manner.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[231591445 | Radford et al. | 2021 | Citations: 29867]": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
                    "[258822817 | Tang et al. | 2023 | Citations: 188]": "We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io",
                    "[261975252 | Dong et al. | 2023 | Citations: 201]": "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."
                },
                "metadata": [
                    {
                        "section_title": "UNIFIED VISION-LANGUAGE FOUNDATION MODEL",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1223,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 253
                            },
                            {
                                "start": 254,
                                "end": 414
                            },
                            {
                                "start": 415,
                                "end": 562
                            },
                            {
                                "start": 563,
                                "end": 799
                            },
                            {
                                "start": 800,
                                "end": 921
                            },
                            {
                                "start": 922,
                                "end": 1051
                            },
                            {
                                "start": 1052,
                                "end": 1223
                            }
                        ],
                        "ref_mentions": [
                            "258822817",
                            "261975252",
                            "231591445",
                            "261975252"
                        ],
                        "quote": "In recent years, an increasing number of studies (Ge et al., 2024;Wu et al., 2023b;(Tang et al., 2023)Ye et al., 2024a;(Dong et al., 2023)Aiello et al., 2024) have focused on unified multimodal language models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;Sun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive modeling to generate images. SEED-X (Ge et al., 2024) proposes a unified and versatile foundation system capable of handling both multimodal understanding and generation tasks. In this approach, continuous image representations from CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model (LLM) to perform next-word prediction and image representation regression. DreamLLM (Dong et al., 2023)) also explores the potential of enabling multimodal comprehension and creation based on LLMs. Chameleon (Team, 2024) introduces a family of token-based mixed-modal models capable of both comprehending and generating images. This approach represents all modalities as discrete tokens and utilizes a unified transformer-based architecture and trains the model from scratch in an end-to-end manner."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[273022981 | Khaldi et al. | 2024 | Citations: 1]",
                "snippets": "LVLMs have emerged as a natural evolution of integrating LLMs with LVMs, enabling seamless multimodal interaction. By utilizing machine-generated instruction-following data during training, these models facilitate more dynamic and interactive communication between humans and machines. LVLMs are capable of processing both visual and textual inputs, generating coherent and contextually relevant responses in natural language...Notable LVLMs include BLIP (Li et al., 2022), one of the earliest unified vision-language encoder-decoder architectures; LLaVA (Large Language and Vision Assistant) (Liu et al. 2023), an end-to-end multimodal model that connects a vision encoder to a language model for versatile visual-language understanding; and GPT-4 (Achiam et al. 2023), a proprietary foundation model capable of multimodal reasoning, accessible exclusively via APIs such as ChatGPT.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[246411402 | Li et al. | 2022 | Citations: 4413]": "Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP."
                },
                "metadata": [
                    {
                        "section_title": "Large Vision Language Models:",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 425,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 114
                            },
                            {
                                "start": 115,
                                "end": 285
                            },
                            {
                                "start": 286,
                                "end": 426
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "LVLMs have emerged as a natural evolution of integrating LLMs with LVMs, enabling seamless multimodal interaction. By utilizing machine-generated instruction-following data during training, these models facilitate more dynamic and interactive communication between humans and machines. LVLMs are capable of processing both visual and textual inputs, generating coherent and contextually relevant responses in natural language"
                    },
                    {
                        "section_title": "Large Vision Language Models:",
                        "pdf_hash": "",
                        "start": 673,
                        "end": 1128,
                        "sentence_offsets": [
                            {
                                "start": 429,
                                "end": 1127
                            }
                        ],
                        "ref_mentions": [
                            "246411402"
                        ],
                        "quote": "Notable LVLMs include BLIP (Li et al., 2022), one of the earliest unified vision-language encoder-decoder architectures; LLaVA (Large Language and Vision Assistant) (Liu et al. 2023), an end-to-end multimodal model that connects a vision encoder to a language model for versatile visual-language understanding; and GPT-4 (Achiam et al. 2023), a proprietary foundation model capable of multimodal reasoning, accessible exclusively via APIs such as ChatGPT."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[273233771 | Liu et al. | 2024 | Citations: 0]",
                "snippets": "Multimodal large language models (MLLMs) demonstrate strong performance across visual tasks, but their efficiency is hindered by significant computational and memory demands from processing long contexts in multimodal inputs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Multimodal large language models (MLLMs) demonstrate strong performance across visual tasks, but their efficiency is hindered by significant computational and memory demands from processing long contexts in multimodal inputs.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[273350685 | Cao et al. | 2024 | Citations: 8]",
                "snippets": "Multimodal Large Language Models (MLLMs) integrate visual representations from images with linguistic embeddings from text, thereby enhancing the models' capabilities in comprehending and generating language descriptions of visual content. Most open-source MLLMs employ architectures that include a pre-trained vision encoder, an LLM, and a crossmodal connector. Early models, such as the BLIP series (Dai et al., 2023), (Li et al., 2023), utilized the Q-Former module to align text and images, thus improving multimodal capabilities. Flamingo (Alayrac et al., 2022) employed a gated cross-attention mechanism to integrate images and text. LLaVA-1.5 (Liu et al., 2023) adopted an MLP projector to connect the pre-trained vision encoder with the LLM. InternVL (Chen et al., 2023), [6] employed a dynamic resolution strategy, segmenting images into tiles and encoding both the tiles and the thumbnail view together. It then uses a pixel shuffle operation to reduce the number of visual tokens before integrating these features with the LLM through an MLP projector.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[248476411 | Alayrac et al. | 2022 | Citations: 3607]": "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",
                    "[256390509 | Li et al. | 2023 | Citations: 4653]": "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",
                    "[258615266 | Dai et al. | 2023 | Citations: 2096]": "Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at https://github.com/salesforce/LAVIS/tree/main/projects/instructblip.",
                    "[263672058 | Liu et al. | 2023 | Citations: 2824]": "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available.",
                    "[266521410 | Chen et al. | 2023 | Citations: 1215]": "The exponential growth of large language models (LLMs) has opened up numerous possibilities for multi-modal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foun-dation model (Intern VL), which scales up the vision foun-dation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, vision-language tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the development of multi-modal large models."
                },
                "metadata": [
                    {
                        "section_title": "II. RELATED WORK A. Multimodal Large Language Model",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 986,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 239
                            },
                            {
                                "start": 240,
                                "end": 362
                            },
                            {
                                "start": 363,
                                "end": 506
                            },
                            {
                                "start": 507,
                                "end": 593
                            },
                            {
                                "start": 594,
                                "end": 688
                            },
                            {
                                "start": 689,
                                "end": 836
                            },
                            {
                                "start": 837,
                                "end": 986
                            }
                        ],
                        "ref_mentions": [
                            "258615266",
                            "256390509",
                            "248476411",
                            "263672058",
                            "266521410"
                        ],
                        "quote": "Multimodal Large Language Models (MLLMs) integrate visual representations from images with linguistic embeddings from text, thereby enhancing the models' capabilities in comprehending and generating language descriptions of visual content. Most open-source MLLMs employ architectures that include a pre-trained vision encoder, an LLM, and a crossmodal connector. Early models, such as the BLIP series (Dai et al., 2023), (Li et al., 2023), utilized the Q-Former module to align text and images, thus improving multimodal capabilities. Flamingo (Alayrac et al., 2022) employed a gated cross-attention mechanism to integrate images and text. LLaVA-1.5 (Liu et al., 2023) adopted an MLP projector to connect the pre-trained vision encoder with the LLM. InternVL (Chen et al., 2023), [6] employed a dynamic resolution strategy, segmenting images into tiles and encoding both the tiles and the thumbnail view together. It then uses a pixel shuffle operation to reduce the number of visual tokens before integrating these features with the LLM through an MLP projector."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[273403986 | Fang et al. | 2024 | Citations: 15]",
                "snippets": "Recent studies (Sun et al., 2023;Ge et al., 2024b) have explored MLLM's potential for visual generation, beyond the previously well-explored visual understanding and reasoning with MLLMs. These approaches enable MLLMs to process image-text inputs and produce either textual outputs or semantic-level visual tokens. In the case of image generation, these visual tokens are subsequently transformed into pixel-space images using diffusion-based decoders. Such unified frameworks empower MLLMs to perform a wide spectrum of tasks within a single framework, ranging from detailed visual analysis to creative image synthesis.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 573,
                        "end": 1193,
                        "sentence_offsets": [
                            {
                                "start": 573,
                                "end": 760
                            },
                            {
                                "start": 761,
                                "end": 887
                            },
                            {
                                "start": 888,
                                "end": 1025
                            },
                            {
                                "start": 1026,
                                "end": 1193
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recent studies (Sun et al., 2023;Ge et al., 2024b) have explored MLLM's potential for visual generation, beyond the previously well-explored visual understanding and reasoning with MLLMs. These approaches enable MLLMs to process image-text inputs and produce either textual outputs or semantic-level visual tokens. In the case of image generation, these visual tokens are subsequently transformed into pixel-space images using diffusion-based decoders. Such unified frameworks empower MLLMs to perform a wide spectrum of tasks within a single framework, ranging from detailed visual analysis to creative image synthesis."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[273969525 | Ma et al. | 2024 | Citations: 39]",
                "snippets": "One prominent direction involves utilizing pre-trained text-to-image models for high-quality generation while training LLMs to generate conditions for these models (Dong et al., 2023)[25][26][27](Sun et al., 2023). However, this approach introduces architectural complexity and potentially constrains the model's capabilities through maintaining separate LLM and generative components. Alternative approaches [88,97,99,100,108] propose training a single LLM for both tasks, typically incorporating either diffusion models (Ho et al., 2020)[83] or vector-quantized autoregressive models (Esser et al., 2020)[86]...Another approach extends MLLMs with pre-trained diffusion models [19, 25-27, 87, 101]. However, these systems essentially utilize diffusion models as external tools, where the MLLM generates conditions for image generation without possessing direct generative capabilities. This separation often results in suboptimal performance compared to standalone diffusion models [25,87]. Another line of work [88,97,99,100,108] aim to train a single LLM for both tasks. Many of these methods employ vector-quantization [22,86] to convert images into discrete tokens, enabling unified autoregressive processing [88]97].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[219955663 | Ho et al. | 2020 | Citations: 18352]": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL",
                    "[229297973 | Esser et al. | 2020 | Citations: 3004]": "Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers. Project page at https://git.io/JLlvY.",
                    "[259765944 | Sun et al. | 2023 | Citations: 138]": "We present Emu, a Transformer-based multimodal foundation model, which can seamlessly generate images and texts in multimodal context. This omnivore model can take in any single-modality or multimodal data input indiscriminately (e.g., interleaved image, text and video) through a one-model-for-all autoregressive training process. First, visual signals are encoded into embeddings, and together with text tokens form an interleaved input sequence. Emu is then end-to-end trained with a unified objective of classifying the next text token or regressing the next visual embedding in the multimodal sequence. This versatile multimodality empowers the exploration of diverse pretraining data sources at scale, such as videos with interleaved frames and text, webpages with interleaved images and text, as well as web-scale image-text pairs and video-text pairs. Emu can serve as a generalist multimodal interface for both image-to-text and text-to-image tasks, and supports in-context image and text generation. Across a broad range of zero-shot/few-shot tasks including image captioning, visual question answering, video question answering and text-to-image generation, Emu demonstrates superb performance compared to state-of-the-art large multimodal models. Extended capabilities such as multimodal assistants via instruction tuning are also demonstrated with impressive performance.",
                    "[261975252 | Dong et al. | 2023 | Citations: 201]": "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 439,
                        "end": 988,
                        "sentence_offsets": [
                            {
                                "start": 439,
                                "end": 623
                            },
                            {
                                "start": 624,
                                "end": 794
                            },
                            {
                                "start": 795,
                                "end": 989
                            }
                        ],
                        "ref_mentions": [
                            "261975252",
                            "259765944",
                            "219955663",
                            "229297973"
                        ],
                        "quote": "One prominent direction involves utilizing pre-trained text-to-image models for high-quality generation while training LLMs to generate conditions for these models (Dong et al., 2023)[25][26][27](Sun et al., 2023). However, this approach introduces architectural complexity and potentially constrains the model's capabilities through maintaining separate LLM and generative components. Alternative approaches [88,97,99,100,108] propose training a single LLM for both tasks, typically incorporating either diffusion models (Ho et al., 2020)[83] or vector-quantized autoregressive models (Esser et al., 2020)[86]"
                    },
                    {
                        "quote": "Another approach extends MLLMs with pre-trained diffusion models [19, 25-27, 87, 101]. However, these systems essentially utilize diffusion models as external tools, where the MLLM generates conditions for image generation without possessing direct generative capabilities. This separation often results in suboptimal performance compared to standalone diffusion models [25,87]. Another line of work [88,97,99,100,108] aim to train a single LLM for both tasks. Many of these methods employ vector-quantization [22,86] to convert images into discrete tokens, enabling unified autoregressive processing [88]97].",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[274305669 | Kang et al. | 2024 | Citations: 0]",
                "snippets": "More advanced vision-language tasks require a tighter integration between the visual and linguistic modalities. This has led to a surge of interest in unified multimodal representation models that can process both images and text within a single framework (Baltru\u0161aitis et al., 2017)...Recently, pretrained language models, such as BERT [11] and GPT (Brown et al., 2020), have significantly influenced the vision-language domain. Researchers have extended these models to handle multimodal inputs, leveraging their contextual understanding of text...Instruction-based VLMs like GPT-4V [33], LLaVA [28], and InstructBLIP [10] are a new type of large vision-language model that are trained to follow natural language instructions and prompts to perform various tasks. A key advantage these models have over traditional fine-tuning is they can adapt to new tasks without needing gradient updates or lots of specific training data. The instruction format allows rapid adaptation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10137425 | Baltrusaitis et al. | 2017 | Citations: 2938]": "Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.",
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
                },
                "metadata": [
                    {
                        "section_title": "Vision-Lauguage Model",
                        "pdf_hash": "",
                        "start": 343,
                        "end": 602,
                        "sentence_offsets": [
                            {
                                "start": 343,
                                "end": 454
                            },
                            {
                                "start": 455,
                                "end": 603
                            }
                        ],
                        "ref_mentions": [
                            "10137425"
                        ],
                        "quote": "More advanced vision-language tasks require a tighter integration between the visual and linguistic modalities. This has led to a surge of interest in unified multimodal representation models that can process both images and text within a single framework (Baltru\u0161aitis et al., 2017)"
                    },
                    {
                        "section_title": "Vision-Lauguage Model",
                        "pdf_hash": "",
                        "start": 830,
                        "end": 1075,
                        "sentence_offsets": [
                            {
                                "start": 830,
                                "end": 956
                            },
                            {
                                "start": 957,
                                "end": 1075
                            }
                        ],
                        "ref_mentions": [
                            "218971783"
                        ],
                        "quote": "Recently, pretrained language models, such as BERT [11] and GPT (Brown et al., 2020), have significantly influenced the vision-language domain. Researchers have extended these models to handle multimodal inputs, leveraging their contextual understanding of text"
                    },
                    {
                        "section_title": "Vision-Lauguage Model",
                        "pdf_hash": "",
                        "start": 1875,
                        "end": 2301,
                        "sentence_offsets": [
                            {
                                "start": 1875,
                                "end": 2090
                            },
                            {
                                "start": 2091,
                                "end": 2252
                            },
                            {
                                "start": 2253,
                                "end": 2300
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Instruction-based VLMs like GPT-4V [33], LLaVA [28], and InstructBLIP [10] are a new type of large vision-language model that are trained to follow natural language instructions and prompts to perform various tasks. A key advantage these models have over traditional fine-tuning is they can adapt to new tasks without needing gradient updates or lots of specific training data. The instruction format allows rapid adaptation."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[274597781 | Serra et al. | 2024 | Citations: 0]",
                "snippets": "Recently, the availability of large open datasets [14,15] and improved techniques to align text and image embeddings (Ge et al., 2023) have also enabled the creation of multimodal models that can both understand and generate visual content within a single architecture (Tang et al., 2023)[18](Zhan et al., 2024)[20].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258822817 | Tang et al. | 2023 | Citations: 188]": "We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io",
                    "[263605855 | Ge et al. | 2023 | Citations: 137]": "The great success of Large Language Models (LLMs) has expanded the potential of multimodality, contributing to the gradual evolution of General Artificial Intelligence (AGI). A true AGI agent should not only possess the capability to perform predefined multi-tasks but also exhibit emergent abilities in an open-world context. However, despite the considerable advancements made by recent multimodal LLMs, they still fall short in effectively unifying comprehension and generation tasks, let alone open-world emergent abilities. We contend that the key to overcoming the present impasse lies in enabling text and images to be represented and processed interchangeably within a unified autoregressive Transformer. To this end, we introduce SEED, an elaborate image tokenizer that empowers LLMs with the ability to SEE and Draw at the same time. We identify two crucial design principles: (1) Image tokens should be independent of 2D physical patch positions and instead be produced with a 1D causal dependency, exhibiting intrinsic interdependence that aligns with the left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens should capture high-level semantics consistent with the degree of semantic abstraction in words, and be optimized for both discriminativeness and reconstruction during the tokenizer training phase. With SEED tokens, LLM is able to perform scalable multimodal autoregression under its original training recipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by large-scale pretraining and instruction tuning on the interleaved textual and visual data, demonstrating impressive performance on a broad range of multimodal comprehension and generation tasks. More importantly, SEED-LLaMA has exhibited compositional emergent abilities such as multi-turn in-context multimodal generation, acting like your AI assistant.",
                    "[267750101 | Zhan et al. | 2024 | Citations: 136]": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/"
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 426,
                        "end": 699,
                        "sentence_offsets": [
                            {
                                "start": 426,
                                "end": 699
                            }
                        ],
                        "ref_mentions": [
                            "263605855",
                            "258822817",
                            "267750101"
                        ],
                        "quote": "Recently, the availability of large open datasets [14,15] and improved techniques to align text and image embeddings (Ge et al., 2023) have also enabled the creation of multimodal models that can both understand and generate visual content within a single architecture (Tang et al., 2023)[18](Zhan et al., 2024)[20]."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[274860024 | Shi et al. | 2024 | Citations: 28]",
                "snippets": "Over the past few years, we have seen significant progress in multimodal generative models capable of understanding and generating interleaved text and images in arbitrary sequences (Dong et al., 2023;(Koh et al., 2023)Lin et al., 2024b). Models like Transfusion (Zhou et al., 2024), Chameleon (Team, 2024b), and Unified-IO (Lu et al., 2022)(Lu et al., 2023) demonstrate the potential of unified architectures that seamlessly handle both image and text modalities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258959284 | Koh et al. | 2023 | Citations: 259]": "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.",
                    "[249848272 | Lu et al. | 2022 | Citations: 413]": "We propose Unified-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region captioning and referring expression, to natural language processing tasks such as question answering and paraphrasing. Developing a single unified model for such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs pertaining to each task, including RGB images, per-pixel maps, binary masks, bounding boxes, and language. We achieve this unification by homogenizing every supported input and output into a sequence of discrete vocabulary tokens. This common representation across all tasks allows us to train a single transformer-based architecture, jointly on over 90 diverse datasets in the vision and language fields. Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning. Code and demos for Unified-IO are available at: https://unified-io.allenai.org.",
                    "[266573555 | Lu et al. | 2023 | Citations: 174]": "We present Unified-IO 2,the. first autoregressive multi-modal model that is capable of understanding and generating image, text, audio, and action. To unify different modalities, we tokenize inputs and outputs - images, text, audio, action, bounding boxes etc., into a shared semantic space and then process them with a single encoder-decoder transformer model. Since training with such diverse modalities is challenging, we propose various architectural improvements to stabilize model training. We train our model from scratch on a large multimodal pre-training corpus from diverse sources with a multimodal mixture of denoisers objective. To learn an expansive set of skills, such as following multimodal instructions, we construct and. finetune on an ensemble of 120 datasets with prompts and augmentations. With a single unified model, Unified-io 2 achieves state-of-the-art performance on the GRIT benchmark and strong results in more than 35 benchmarks, including image generation and understanding, natural language understanding, video and audio understanding, and robotic manipulation. We release all our models to the research community."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 466,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 237
                            },
                            {
                                "start": 238,
                                "end": 466
                            }
                        ],
                        "ref_mentions": [
                            "258959284",
                            "249848272",
                            "266573555"
                        ],
                        "quote": "Over the past few years, we have seen significant progress in multimodal generative models capable of understanding and generating interleaved text and images in arbitrary sequences (Dong et al., 2023;(Koh et al., 2023)Lin et al., 2024b). Models like Transfusion (Zhou et al., 2024), Chameleon (Team, 2024b), and Unified-IO (Lu et al., 2022)(Lu et al., 2023) demonstrate the potential of unified architectures that seamlessly handle both image and text modalities."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[275337159 | Franklin et al. | 2025 | Citations: 0]",
                "snippets": "Recent works have expanded their applications and enhanced their architectures to achieve better performance, efficiency, and scalability [21]- (Zhou et al., 2022). \n\nA primary focus in the development of LVLMs has been the design of architectures that effectively unify language and vision modalities. Recent models have proposed end-to-end frameworks that leverage shared embeddings for both text and images, enabling them to excel at tasks requiring fine-grained multimodal reasoning (Zhang et al., 2024). Additionally, techniques such as mixture of experts and relational reasoning mechanisms have been introduced to improve scalability and enhance the relational reasoning capabilities of LVLMs (Lin et al., 2024), [26].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249712198 | Zhou et al. | 2022 | Citations: 43]": "A ranker plays an indispensable role in the de facto 'retrieval&rerank' pipeline, but its training still lags behind -- learning from moderate negatives or/and serving as an auxiliary module for a retriever. In this work, we first identify two major barriers to a robust ranker, i.e., inherent label noises caused by a well-trained retriever and non-ideal negatives sampled for a high-capable ranker. Thereby, we propose multiple retrievers as negative generators improve the ranker's robustness, where i) involving extensive out-of-distribution label noises renders the ranker against each noise distribution, and ii) diverse hard negatives from a joint distribution are relatively close to the ranker's negative distribution, leading to more challenging thus effective training. To evaluate our robust ranker (dubbed R$^2$anker), we conduct experiments in various settings on the popular passage retrieval benchmark, including BM25-reranking, full-ranking, retriever distillation, etc. The empirical results verify the new state-of-the-art effectiveness of our model.",
                    "[267311517 | Lin et al. | 2024 | Citations: 178]": "Recent advances demonstrate that scaling Large Vision-Language Models (LVLMs) effectively improves downstream task performances. However, existing scaling methods enable all model parameters to be active for each token in the calculation, which brings massive training and inferring costs. In this work, we propose a simple yet effective training strategy MoE-Tuning for LVLMs. This strategy innovatively addresses the common issue of performance degradation in multi-modal sparsity learning, consequently constructing a sparse model with an outrageous number of parameters but a constant computational cost. Furthermore, we present the MoE-LLaVA, a MoE-based sparse LVLM architecture, which uniquely activates only the top-k experts through routers during deployment, keeping the remaining experts inactive. Extensive experiments show the significant performance of MoE-LLaVA in a variety of visual understanding and object hallucination benchmarks. Remarkably, with only approximately 3B sparsely activated parameters, MoE-LLaVA demonstrates performance comparable to the LLaVA-1.5-7B on various visual understanding datasets and even surpasses the LLaVA-1.5-13B in object hallucination benchmark. Through MoE-LLaVA, we aim to establish a baseline for sparse LVLMs and provide valuable insights for future research in developing more efficient and effective multi-modal learning systems. Code is released at https://github.com/PKU-YuanGroup/MoE-LLaVA.",
                    "[270923961 | Zhang et al. | 2024 | Citations: 117]": "We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision language model that supports long-contextual input and output. IXC-2.5 excels in various text-image comprehension and composition applications, achieving GPT-4V level capabilities with merely 7B LLM backend. Trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in tasks requiring extensive input and output contexts. Compared to its previous 2.0 version, InternLM-XComposer-2.5 features three major upgrades in vision-language comprehension: (1) Ultra-High Resolution Understanding, (2) Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In addition to comprehension, IXC-2.5 extends to two compelling applications using extra LoRA parameters for text-image composition: (1) Crafting Webpages and (2) Composing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28 benchmarks, outperforming existing open-source state-of-the-art models on 16 benchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on 16 key tasks. The InternLM-XComposer-2.5 is publicly available at https://github.com/InternLM/InternLM-XComposer."
                },
                "metadata": [
                    {
                        "section_title": "B. Large Vision-Language Models",
                        "pdf_hash": "",
                        "start": 344,
                        "end": 1024,
                        "sentence_offsets": [
                            {
                                "start": 344,
                                "end": 493
                            },
                            {
                                "start": 496,
                                "end": 631
                            },
                            {
                                "start": 632,
                                "end": 821
                            },
                            {
                                "start": 822,
                                "end": 1024
                            }
                        ],
                        "ref_mentions": [
                            "249712198",
                            "270923961",
                            "267311517"
                        ],
                        "quote": "Recent works have expanded their applications and enhanced their architectures to achieve better performance, efficiency, and scalability [21]- (Zhou et al., 2022). \n\nA primary focus in the development of LVLMs has been the design of architectures that effectively unify language and vision modalities. Recent models have proposed end-to-end frameworks that leverage shared embeddings for both text and images, enabling them to excel at tasks requiring fine-grained multimodal reasoning (Zhang et al., 2024). Additionally, techniques such as mixture of experts and relational reasoning mechanisms have been introduced to improve scalability and enhance the relational reasoning capabilities of LVLMs (Lin et al., 2024), [26]."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[276422421 | Fang et al. | 2025 | Citations: 2]",
                "snippets": "To address this limitation, DreamLLM [6] integrates an LLM backbone with a diffusion model to enable image generation as a multimodal output. Emu2 [31] scales this architecture to 37B parameters, demonstrating strong multimodal in-context learning and the ability to handle complex tasks requiring real-time reasoning, such as visual prompting and object-grounded generation. Chameleon [33] proposes a stable training strategy from the ground up, featuring an alignment process and architectural parameterization tailored to early-fusion, token-based, mixed-modal settings.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "To address this limitation, DreamLLM [6] integrates an LLM backbone with a diffusion model to enable image generation as a multimodal output. Emu2 [31] scales this architecture to 37B parameters, demonstrating strong multimodal in-context learning and the ability to handle complex tasks requiring real-time reasoning, such as visual prompting and object-grounded generation. Chameleon [33] proposes a stable training strategy from the ground up, featuring an alignment process and architectural parameterization tailored to early-fusion, token-based, mixed-modal settings.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[276782523 | Zhao et al. | 2025 | Citations: 1]",
                "snippets": "Unified multimodal generative models aim to bridge the gap between understanding and generation tasks, and integrate vision and language into a single framework, enabling the model to learn shared representations across modalities [1,(Aiello et al., 2023)(Dong et al., 2023)19,(Sun et al., 2023)62,71,72,74,76,77,79,85]. SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM (Dong et al., 2023) extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[262898422 | Aiello et al. | 2023 | Citations: 31]": "In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.",
                    "[231591445 | Radford et al. | 2021 | Citations: 29867]": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
                    "[261975252 | Dong et al. | 2023 | Citations: 201]": "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.",
                    "[266374640 | Sun et al. | 2023 | Citations: 290]": "The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research."
                },
                "metadata": [
                    {
                        "section_title": "Unified Multimodal Generative Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1273,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 270
                            },
                            {
                                "start": 271,
                                "end": 507
                            },
                            {
                                "start": 508,
                                "end": 631
                            },
                            {
                                "start": 632,
                                "end": 831
                            },
                            {
                                "start": 832,
                                "end": 1109
                            },
                            {
                                "start": 1110,
                                "end": 1273
                            }
                        ],
                        "ref_mentions": [
                            "262898422",
                            "261975252",
                            "266374640",
                            "231591445",
                            "261975252"
                        ],
                        "quote": "Unified multimodal generative models aim to bridge the gap between understanding and generation tasks, and integrate vision and language into a single framework, enabling the model to learn shared representations across modalities [1,(Aiello et al., 2023)(Dong et al., 2023)19,(Sun et al., 2023)62,71,72,74,76,77,79,85]. SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM (Dong et al., 2023) extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[276928288 | Zou et al. | 2025 | Citations: 1]",
                "snippets": "Recent studies have emerged that seek to integrate multimodal understanding with visual generation, aiming to develop unified systems capable of handling both tasks simultaneously. Such designs hold the potential to foster mutual enhancement between generation and understanding, offering a promising pathway toward truly unifying all modalities. Numerous studies have sought to preserve the text generation paradigm of LLMs while exploring the impact [46,64,66,67] of integrating diverse visual generation paradigms, such as diffusion models [24], flow-based generative models (Esser et al., 2024)[40], and vector-quantized autoregressive models [56].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[268247980 | Esser et al. | 2024 | Citations: 1401]": "Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 389,
                        "end": 1024,
                        "sentence_offsets": [
                            {
                                "start": 389,
                                "end": 569
                            },
                            {
                                "start": 570,
                                "end": 735
                            },
                            {
                                "start": 736,
                                "end": 1024
                            }
                        ],
                        "ref_mentions": [
                            "268247980"
                        ],
                        "quote": "Recent studies have emerged that seek to integrate multimodal understanding with visual generation, aiming to develop unified systems capable of handling both tasks simultaneously. Such designs hold the potential to foster mutual enhancement between generation and understanding, offering a promising pathway toward truly unifying all modalities. Numerous studies have sought to preserve the text generation paradigm of LLMs while exploring the impact [46,64,66,67] of integrating diverse visual generation paradigms, such as diffusion models [24], flow-based generative models (Esser et al., 2024)[40], and vector-quantized autoregressive models [56]."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[276929205 | Niu et al. | 2025 | Citations: 26]",
                "snippets": "Unified multimodal models aim to construct general-purpose models capable of processing both textual and visual inputs, and performing cross-modal generation and understanding. These models [4,8,16,17,21,25,35,38,39,41,[43][44][45][48] are typically built upon powerful large language models (LLMs) [54] and extend next-token prediction [2] to image generation: the LLM generates visual tokens, and a VQ-VAE [42] or Diffusion model serves as a detokenizer. Moreover, Transfusion [55] and Show-O [49] demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework. D-DiT [22] achieves both Text-to-Image (T2I) and Imageto-Text (I2T) tasks using an end-to-end diffusion model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Unified Multimodal Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 735,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 176
                            },
                            {
                                "start": 177,
                                "end": 455
                            },
                            {
                                "start": 456,
                                "end": 624
                            },
                            {
                                "start": 625,
                                "end": 735
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Unified multimodal models aim to construct general-purpose models capable of processing both textual and visual inputs, and performing cross-modal generation and understanding. These models [4,8,16,17,21,25,35,38,39,41,[43][44][45][48] are typically built upon powerful large language models (LLMs) [54] and extend next-token prediction [2] to image generation: the LLM generates visual tokens, and a VQ-VAE [42] or Diffusion model serves as a detokenizer. Moreover, Transfusion [55] and Show-O [49] demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework. D-DiT [22] achieves both Text-to-Image (T2I) and Imageto-Text (I2T) tasks using an end-to-end diffusion model."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[276929408 | Fang et al. | 2025 | Citations: 1]",
                "snippets": "Recent works [1,11,26,(Sun et al., 2023)[40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs (Brooks et al., 2022)13,(Mokady et al., 2023) offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[253581213 | Brooks et al. | 2022 | Citations: 1833]": "We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models\u2014a language model (GPT-3) and a text-to-image model (Stable Diffusion)\u2014to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.",
                    "[266374640 | Sun et al. | 2023 | Citations: 290]": "The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research."
                },
                "metadata": [
                    {
                        "section_title": "MLLM for Image Generation and Editing",
                        "pdf_hash": "",
                        "start": 561,
                        "end": 1202,
                        "sentence_offsets": [
                            {
                                "start": 561,
                                "end": 757
                            },
                            {
                                "start": 758,
                                "end": 941
                            },
                            {
                                "start": 942,
                                "end": 1202
                            }
                        ],
                        "ref_mentions": [
                            "266374640",
                            "253581213",
                            "253581838"
                        ],
                        "quote": "Recent works [1,11,26,(Sun et al., 2023)[40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs (Brooks et al., 2022)13,(Mokady et al., 2023) offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[277065986 | Bates et al. | 2025 | Citations: 0]",
                "snippets": "Researchers developed MM-LLMs that incorporate visual, auditory, and textual data into unified frameworks to address these limitations. Models like CLIP (Radford et al., 2021) align visual embeddings with textual embeddings, enabling cross-modal tasks such as visual question answering (VQA) and zero-shot image classification. For example, CLIP (Contrastive Language-Image Pretraining) learns shared visual and textual semantics from large-scale datasets of image-text pairs, effectively bridging linguistic context and visual perception. MM-LLMs leverage these multimodal capabilities to tackle tasks that traditional LLMs cannot address, such as interpreting UML diagrams and generating corresponding executable code. MM-LLMs align diagram structures with text-based outputs by combining visual encoders with language models, enhancing accuracy and flexibility for applications like UML-to-code automation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Addressing the Problem",
                        "pdf_hash": "",
                        "start": 520,
                        "end": 1429,
                        "sentence_offsets": [
                            {
                                "start": 520,
                                "end": 655
                            },
                            {
                                "start": 656,
                                "end": 847
                            },
                            {
                                "start": 848,
                                "end": 1059
                            },
                            {
                                "start": 1060,
                                "end": 1240
                            },
                            {
                                "start": 1241,
                                "end": 1429
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Researchers developed MM-LLMs that incorporate visual, auditory, and textual data into unified frameworks to address these limitations. Models like CLIP (Radford et al., 2021) align visual embeddings with textual embeddings, enabling cross-modal tasks such as visual question answering (VQA) and zero-shot image classification. For example, CLIP (Contrastive Language-Image Pretraining) learns shared visual and textual semantics from large-scale datasets of image-text pairs, effectively bridging linguistic context and visual perception. MM-LLMs leverage these multimodal capabilities to tackle tasks that traditional LLMs cannot address, such as interpreting UML diagrams and generating corresponding executable code. MM-LLMs align diagram structures with text-based outputs by combining visual encoders with language models, enhancing accuracy and flexibility for applications like UML-to-code automation."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[277103675 | Fan et al. | 2025 | Citations: 8]",
                "snippets": "We present UniFluid, a unified autoregressive framework for joint visual generation and understanding leveraging continuous visual tokens. Our unified autoregressive architecture processes multimodal image and text inputs, generating discrete tokens for text and continuous tokens for image...UniFluid, a unified framework that leverages continuous visual tokens within an autoregressive architecture to jointly handle vision-language generation and understanding tasks. Building upon pre-trained Gemma [48] on large-scale text corpus, UniFluid unlocks powerful visual generation and understanding capabilities through training with paired image-text data, and further allows these two tasks to mutually benefit each other within a single architecture. Specifically, UniFluid adopts a unified autoregressive framework where both text and continuous visual inputs are embedded as tokens in the same space, enabling seamless joint training of image generation and understanding tasks. UniFluid integrates a continuous tokenizer [12,(Li et al., 2024) for image generation and a pre-trained SigLIP (Zhai et al., 2023) image encoder for understanding tasks, while textual inputs are processed using a standard SentencePiece tokenizer [20]. The resulting multimodal sequences are modeled autoregressively using Gemma [48] as the underlying transformer backbone. Task-specific prediction heads-a diffusion-based head for image generation and a cross-entropy head for text generation-ensure effective modality-specific training and inference, enabling UniFluid to efficiently learn shared representations that mutually enhance its generation and understanding capabilities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[257767223 | Zhai et al. | 2023 | Citations: 1204]": "We propose a simple pairwise sigmoid loss for imagetext pre-training. Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. With only four TPUv4 chips, we can train a Base CLIP model at 4k batch size and a Large LiT model at 20k batch size, the latter achieves 84.5% ImageNet zero-shot accuracy in two days. This disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We hope our research motivates further explorations in improving the quality and efficiency of language-image pre-training.",
                    "[270560593 | Li et al. | 2024 | Citations: 238]": "Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: https://github.com/LTH14/mar."
                },
                "metadata": [
                    {
                        "quote": "We present UniFluid, a unified autoregressive framework for joint visual generation and understanding leveraging continuous visual tokens. Our unified autoregressive architecture processes multimodal image and text inputs, generating discrete tokens for text and continuous tokens for image",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1093,
                        "end": 2437,
                        "sentence_offsets": [
                            {
                                "start": 1065,
                                "end": 1270
                            },
                            {
                                "start": 1271,
                                "end": 1552
                            },
                            {
                                "start": 1553,
                                "end": 1782
                            },
                            {
                                "start": 1783,
                                "end": 2005
                            },
                            {
                                "start": 2006,
                                "end": 2126
                            },
                            {
                                "start": 2127,
                                "end": 2436
                            }
                        ],
                        "ref_mentions": [
                            "270560593",
                            "257767223"
                        ],
                        "quote": "UniFluid, a unified framework that leverages continuous visual tokens within an autoregressive architecture to jointly handle vision-language generation and understanding tasks. Building upon pre-trained Gemma [48] on large-scale text corpus, UniFluid unlocks powerful visual generation and understanding capabilities through training with paired image-text data, and further allows these two tasks to mutually benefit each other within a single architecture. Specifically, UniFluid adopts a unified autoregressive framework where both text and continuous visual inputs are embedded as tokens in the same space, enabling seamless joint training of image generation and understanding tasks. UniFluid integrates a continuous tokenizer [12,(Li et al., 2024) for image generation and a pre-trained SigLIP (Zhai et al., 2023) image encoder for understanding tasks, while textual inputs are processed using a standard SentencePiece tokenizer [20]. The resulting multimodal sequences are modeled autoregressively using Gemma [48] as the underlying transformer backbone. Task-specific prediction heads-a diffusion-based head for image generation and a cross-entropy head for text generation-ensure effective modality-specific training and inference, enabling UniFluid to efficiently learn shared representations that mutually enhance its generation and understanding capabilities."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[277741397 | Lim et al. | 2025 | Citations: 0]",
                "snippets": "Recent progress has led to the emergence of multimodal large language models (MLLMs) (Liu et al., 2023), (Liu et al., 2023), which have become a prominent paradigm for addressing multimodal tasks. MLLMs typically integrate a pre-trained language model with a vision encoder within a unified architecture. Instruction-following capability is considered essential for handling various downstream tasks, and therefore instructiontuned LLMs (Chung et al., 2022), (Zheng et al., 2023) are frequently adopted as the language backbone in these models. In parallel, visual encoders (Radford et al., 2021), (Zhai et al., 2023) are utilized to extract semantically rich representations from visual data. To enable seamless interaction between visual and textual modalities, a projector module is commonly used to map visual features into the language embedding space. This configuration allows the language model to jointly process and reason over multimodal inputs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[231591445 | Radford et al. | 2021 | Citations: 29867]": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
                    "[253018554 | Chung et al. | 2022 | Citations: 3154]": "Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.",
                    "[257767223 | Zhai et al. | 2023 | Citations: 1204]": "We propose a simple pairwise sigmoid loss for imagetext pre-training. Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. With only four TPUv4 chips, we can train a Base CLIP model at 4k batch size and a Large LiT model at 20k batch size, the latter achieves 84.5% ImageNet zero-shot accuracy in two days. This disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We hope our research motivates further explorations in improving the quality and efficiency of language-image pre-training.",
                    "[258179774 | Liu et al. | 2023 | Citations: 4921]": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.",
                    "[259129398 | Zheng et al. | 2023 | Citations: 4439]": "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.",
                    "[263672058 | Liu et al. | 2023 | Citations: 2824]": "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available."
                },
                "metadata": [
                    {
                        "section_title": "B. Multimodal Large Language Models",
                        "pdf_hash": "",
                        "start": 512,
                        "end": 1375,
                        "sentence_offsets": [
                            {
                                "start": 512,
                                "end": 680
                            },
                            {
                                "start": 681,
                                "end": 788
                            },
                            {
                                "start": 789,
                                "end": 996
                            },
                            {
                                "start": 997,
                                "end": 1112
                            },
                            {
                                "start": 1113,
                                "end": 1276
                            },
                            {
                                "start": 1277,
                                "end": 1375
                            }
                        ],
                        "ref_mentions": [
                            "258179774",
                            "263672058",
                            "253018554",
                            "259129398",
                            "231591445",
                            "257767223"
                        ],
                        "quote": "Recent progress has led to the emergence of multimodal large language models (MLLMs) (Liu et al., 2023), (Liu et al., 2023), which have become a prominent paradigm for addressing multimodal tasks. MLLMs typically integrate a pre-trained language model with a vision encoder within a unified architecture. Instruction-following capability is considered essential for handling various downstream tasks, and therefore instructiontuned LLMs (Chung et al., 2022), (Zheng et al., 2023) are frequently adopted as the language backbone in these models. In parallel, visual encoders (Radford et al., 2021), (Zhai et al., 2023) are utilized to extract semantically rich representations from visual data. To enable seamless interaction between visual and textual modalities, a projector module is commonly used to map visual features into the language embedding space. This configuration allows the language model to jointly process and reason over multimodal inputs."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[278327487 | Zhang et al. | 2025 | Citations: 1]",
                "snippets": "In this section, we primarily focus on unified multimodal models that support vision-language understanding and generation, i.e., models that take both image and text as input and produce either text or image as output. As shown in Fig. 5, existing unified models can be broadly categorized into three main types: diffusion models, autoregressive models, and fused AR + diffusion models. For autoregressive models, we further classify them based on their modality encoding methods into four subcategories: pixel-based encoding, semantic-based encoding, learnable query-based encoding, and hybrid encoding. Each of these encoding strategies represents different ways of handling visual and textual data, leading to varying levels of integration and flexibility in the multimodal representations. Fused AR + diffusion models are divided into two subcategories based on modality encoding: pixel-based encoding and hybrid encoding. These models combine aspects of both autoregressive and diffusion techniques, offering a promising approach to more unified and efficient multimodal generation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "UNIFIED MULTIMODAL MODELS FOR UNDER-STANDING AND GENERATION",
                        "pdf_hash": "",
                        "start": 717,
                        "end": 1805,
                        "sentence_offsets": [
                            {
                                "start": 717,
                                "end": 936
                            },
                            {
                                "start": 937,
                                "end": 1104
                            },
                            {
                                "start": 1105,
                                "end": 1322
                            },
                            {
                                "start": 1323,
                                "end": 1511
                            },
                            {
                                "start": 1512,
                                "end": 1644
                            },
                            {
                                "start": 1645,
                                "end": 1805
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this section, we primarily focus on unified multimodal models that support vision-language understanding and generation, i.e., models that take both image and text as input and produce either text or image as output. As shown in Fig. 5, existing unified models can be broadly categorized into three main types: diffusion models, autoregressive models, and fused AR + diffusion models. For autoregressive models, we further classify them based on their modality encoding methods into four subcategories: pixel-based encoding, semantic-based encoding, learnable query-based encoding, and hybrid encoding. Each of these encoding strategies represents different ways of handling visual and textual data, leading to varying levels of integration and flexibility in the multimodal representations. Fused AR + diffusion models are divided into two subcategories based on modality encoding: pixel-based encoding and hybrid encoding. These models combine aspects of both autoregressive and diffusion techniques, offering a promising approach to more unified and efficient multimodal generation."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[278339632 | Jiang et al. | 2025 | Citations: 1]",
                "snippets": "LMMs are capable of simultaneously processing and understanding data from different modalities, such as vision, language, haptic and auditory. These models achieve comprehensive processing and reasoning of multimodal information by integrating the features of various modalities in a unified high-dimensional space. They utilize advanced neural network architectures, such as transformers and diffusion models, to extract features from each modality and optimize their representations through techniques such as contrastive learning and self-supervised learning...these LMMs can also perform cross-modal reasoning and prediction, such as generating images or audio from text.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Multimodal data",
                        "pdf_hash": "",
                        "start": 27,
                        "end": 588,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 169
                            },
                            {
                                "start": 170,
                                "end": 342
                            },
                            {
                                "start": 343,
                                "end": 589
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "LMMs are capable of simultaneously processing and understanding data from different modalities, such as vision, language, haptic and auditory. These models achieve comprehensive processing and reasoning of multimodal information by integrating the features of various modalities in a unified high-dimensional space. They utilize advanced neural network architectures, such as transformers and diffusion models, to extract features from each modality and optimize their representations through techniques such as contrastive learning and self-supervised learning"
                    },
                    {
                        "section_title": "Multimodal data",
                        "pdf_hash": "",
                        "start": 2271,
                        "end": 2383,
                        "sentence_offsets": [
                            {
                                "start": 2258,
                                "end": 2382
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "these LMMs can also perform cross-modal reasoning and prediction, such as generating images or audio from text."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[278714599 | Magay et al. | 2025 | Citations: 0]",
                "snippets": "Recent advancements have introduced multimodal models that integrate capabilities across multiple modalities, further enhancing their utility. GPT-4 [24], for instance, extends the capabilities of its predecessors by incorporating more sophisticated multimodal inputs, allowing it to process and generate text, images, and other forms of data simultaneously. This capability significantly enhances its application in fields requiring complex reasoning and contextual understanding across different types of information. LLaVA (Large Language and Vision Assistant) (Liu et al., 2023) is another recent model that exemplifies the integration of visual and linguistic processing. LLaVA is designed to handle tasks that require understanding and generating coherent responses based on both visual and textual inputs, such as detailed scene descriptions and interactive question-answering involving visual context.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258179774 | Liu et al. | 2023 | Citations: 4921]": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available."
                },
                "metadata": [
                    {
                        "section_title": "Foundation Models",
                        "pdf_hash": "",
                        "start": 859,
                        "end": 1754,
                        "sentence_offsets": [
                            {
                                "start": 859,
                                "end": 1001
                            },
                            {
                                "start": 1002,
                                "end": 1217
                            },
                            {
                                "start": 1218,
                                "end": 1378
                            },
                            {
                                "start": 1379,
                                "end": 1521
                            },
                            {
                                "start": 1522,
                                "end": 1754
                            }
                        ],
                        "ref_mentions": [
                            "258179774"
                        ],
                        "quote": "Recent advancements have introduced multimodal models that integrate capabilities across multiple modalities, further enhancing their utility. GPT-4 [24], for instance, extends the capabilities of its predecessors by incorporating more sophisticated multimodal inputs, allowing it to process and generate text, images, and other forms of data simultaneously. This capability significantly enhances its application in fields requiring complex reasoning and contextual understanding across different types of information. LLaVA (Large Language and Vision Assistant) (Liu et al., 2023) is another recent model that exemplifies the integration of visual and linguistic processing. LLaVA is designed to handle tasks that require understanding and generating coherent responses based on both visual and textual inputs, such as detailed scene descriptions and interactive question-answering involving visual context."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.033168,
        "cot": "To address the user query about advanced multimodal language models capable of generating both text and images and their architectures, I'll organize the quotes into meaningful dimensions that directly answer the query.\n\nFirst, I need an introduction section to provide background on multimodal language models and their evolution. This will help establish the context for the more specific details that follow.\n\nThen, I should create dimensions that specifically address:\n1. Notable advanced multimodal models that can generate both text and images - This should be a list format to clearly enumerate the most significant models mentioned in the quotes.\n2. Architectural approaches for multimodal generation - This should be a synthesis to explain the different architectural paradigms used to enable multimodal generation capabilities.\n3. Integration mechanisms between modalities - This should be a synthesis to explore how these models connect the text and image components.\n4. Training and optimization methods - This should be a synthesis to discuss how these models are trained to enable both text and image generation.\n5. Capabilities and applications - This should be a list to show what these multimodal generative models can actually do.\n\nThese dimensions will provide a comprehensive overview of the current state of multimodal generative models while directly answering the user's question about both the models themselves and the architectures that enable their capabilities.",
        "plan": {
            "Introduction to Multimodal Language Models (synthesis)": [],
            "Notable Advanced Multimodal Models with Text-Image Generation Capabilities (list)": [
                0,
                4,
                7,
                9,
                10,
                11,
                14,
                15,
                16,
                19,
                20,
                23,
                25,
                26,
                28,
                31,
                35
            ],
            "Architectural Approaches for Multimodal Generation (synthesis)": [
                1,
                2,
                3,
                5,
                8,
                12,
                13,
                17,
                18,
                20,
                21,
                22,
                24,
                27,
                28,
                29,
                33,
                34
            ],
            "Integration Mechanisms Between Language and Vision Modalities (synthesis)": [
                1,
                6,
                8,
                10,
                14,
                15,
                18,
                20,
                26,
                27,
                30,
                31,
                32,
                33
            ],
            "Training and Optimization Methods (synthesis)": [
                6,
                9,
                14,
                15,
                21,
                31
            ],
            "Capabilities and Applications of Multimodal Generative Models (list)": [
                1,
                5,
                6,
                16,
                19,
                23,
                26,
                29,
                30,
                31,
                34
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Multimodal Language Models",
                "tldr": "Multimodal language models (MLMs) represent the evolution of AI systems capable of processing and generating content across different modalities including text, images, audio, and video. The most advanced models integrate separate encoders and decoders for different modalities, with sophisticated alignment mechanisms that enable cross-modal generation capabilities. (LLM Memory)",
                "text": "\nMultimodal language models (MLMs) extend traditional language models by incorporating the ability to understand and generate content across multiple modalities, such as text, images, audio, and video. These models represent a significant advancement in AI capabilities, moving beyond text-only processing to comprehend and create rich, multi-format content that more closely mimics human communication patterns. The most advanced MLMs capable of text-to-image and image-to-text generation typically employ specialized neural architectures that can encode and decode different types of data representations. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nAt their core, these models must solve the fundamental challenge of aligning different representational spaces - the semantic space of language and the visual space of images. This alignment enables the models to translate concepts from one modality to another, allowing for capabilities like generating images from textual descriptions or providing textual explanations of visual content. The architectural designs that facilitate this cross-modal generation typically involve either end-to-end trained unified models or modular approaches that combine specialized components for each modality. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nRecent advances in transformer-based architectures have significantly accelerated the development of these multimodal systems, allowing them to handle increasingly complex tasks that require simultaneous reasoning across different types of information. The progression from unimodal to multimodal models reflects the broader trend in AI research toward creating more versatile systems that can interact with the world in ways that more closely resemble human cognitive capabilities. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Notable Advanced Multimodal Models with Text-Image Generation Capabilities",
                "tldr": "The landscape of multimodal language models has evolved rapidly with several notable systems now capable of both understanding and generating content across text and image modalities. Key models like DreamLLM, Chameleon, LLaVA, and Emu employ different architectural approaches including diffusion models, discrete tokenization, and early fusion techniques to achieve cross-modal generation capabilities. (20 sources)",
                "text": "\n## DALL-E and CM3\nEarly pioneering multimodal models like DALL-E and CM3 demonstrated remarkable progress in text-to-image and image-to-text generation, but primarily stored all learned knowledge within model parameters, requiring increasingly larger models and training datasets to capture more knowledge <Paper corpusId=\"253802096\" paperTitle=\"(Yasunaga et al., 2023)\" isShortName></Paper>.\n\n## Vision-Language Models\nSeveral notable vision-language models have advanced the field with multimodal capabilities:\n\n- **LLaVA (Language-Image LLaMA)**: Extends the LLaMA architecture to handle both language and vision modalities, enabling tasks like image captioning, visual question answering, and image-based text generation <Paper corpusId=\"266191741\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258179774\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. LLaVA was the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data <Paper corpusId=\"273022981\" paperTitle=\"(Khaldi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258179774\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\n- **BLIP and BLIP-2**: Among the earliest unified vision-language encoder-decoder architectures, introducing a novel approach to integrate a frozen pre-trained visual encoder with a frozen large language model using a Q-former module <Paper corpusId=\"273022981\" paperTitle=\"(Khaldi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246411402\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper> <Paper corpusId=\"270391394\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>.\n\n- **Qwen-VL**: A multimodal version of the Qwen large model series that accepts image, text, and bounding box as inputs while outputting text and bounding boxes <Paper corpusId=\"266191741\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper>.\n\n- **InternLM-XComposer2-VL**: A cutting-edge vision-language model excelling in free-form text-image composition and comprehension, crafting content from diverse inputs using a Partial LoRA approach <Paper corpusId=\"266191741\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper>.\n\n## Text-Image Generation Models\nRecent advances have focused on models with bidirectional generation capabilities:\n\n- **GILL, Emu, and DreamLLM**: These models specialize in interleaved vision-language generation by aligning the output space of multimodal language models with diffusion image decoders <Paper corpusId=\"267200040\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. DreamLLM specifically focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space, fostering the generation of interleaved documents with both text and image content <Paper corpusId=\"276422421\" paperTitle=\"(Fang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"261975252\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>.\n\n- **Emu2**: A 37B-parameter generative multimodal model demonstrating strong multimodal in-context learning and the ability to handle complex tasks requiring real-time reasoning <Paper corpusId=\"276422421\" paperTitle=\"(Fang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"266374640\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>.\n\n- **KOSMOS-G**: Leverages a multimodal large language model to encode interleaved text-visual inputs, using the U-Net of Stable Diffusion v1.5 as the image decoder <Paper corpusId=\"267200040\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\n- **NExT-GPT**: An end-to-end general-purpose any-to-any multimodal system connecting an LLM with multimodal adaptors and different diffusion decoders, enabling perception of inputs and generation of outputs in arbitrary combinations of text, images, videos, and audio <Paper corpusId=\"271709823\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"261696650\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>.\n\n## Unified Multimodal Architectures\nSome models use novel architectural approaches for unified multimodal processing:\n\n- **Chameleon**: A family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence <Paper corpusId=\"269791516\" paperTitle=\"(Team et al., 2024)\" isShortName></Paper>. This approach represents all modalities as discrete tokens and utilizes a unified transformer-based architecture, training the model from scratch in an end-to-end manner <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\n- **SEED-X**: A unified and versatile foundation system where continuous image representations from a CLIP ViT encoder are combined with text tokens and fed into a large language model to perform next-word prediction and image representation regression <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"276782523\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>.\n\n- **Transfusion and Show-O**: These models demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework <Paper corpusId=\"276929205\" paperTitle=\"(Niu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"276782523\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>.\n\n- **UniFluid**: A unified autoregressive framework that leverages continuous visual tokens within an autoregressive architecture to jointly handle vision-language generation and understanding tasks, built upon pre-trained Gemma <Paper corpusId=\"277103675\" paperTitle=\"(Fan et al., 2025)\" isShortName></Paper>.\n\n- **CoDi (Composable Diffusion)**: A novel generative model capable of generating any combination of output modalities from any combination of input modalities, employing a novel composable generation strategy <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258822817\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>.\n\n- **AnyGPT**: A model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music <Paper corpusId=\"270688372\" paperTitle=\"(Liu et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"267750101\" paperTitle=\"(Zhan et al., 2024)\" isShortName></Paper>.\n\nThe evolution of these models shows a clear trend toward unified architectures that can seamlessly handle multiple modalities while performing both understanding and generation tasks within a single framework.",
                "citations": [
                    {
                        "id": "(Yasunaga et al., 2023)",
                        "snippets": [
                            "Recent multimodal models such as DALL-E and CM3 have achieved remarkable progress in text-to-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the model parameters, requiring increasingly larger models and training data to capture more knowledge."
                        ],
                        "paper": {
                            "corpus_id": 253802096,
                            "title": "Retrieval-Augmented Multimodal Language Modeling",
                            "authors": [
                                {
                                    "authorId": "19168196",
                                    "name": "Michihiro Yasunaga"
                                },
                                {
                                    "authorId": "2201435",
                                    "name": "Armen Aghajanyan"
                                },
                                {
                                    "authorId": "3040379",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "2191899140",
                                    "name": "Rich James"
                                },
                                {
                                    "authorId": "1702139",
                                    "name": "J. Leskovec"
                                },
                                {
                                    "authorId": "145419642",
                                    "name": "Percy Liang"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "2072801764",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 108
                        },
                        "score": 0.9912109375
                    },
                    {
                        "id": "(Zhu et al., 2023)",
                        "snippets": [
                            "\u2022 LLaVA (Liu et al., 2024): LlaVA (Language-Image LLaMA) is a multimodal model combining language and image data. It extends the LLaMA architecture to handle both modalities, enabling tasks like image captioning, visual question answering, and imagebased text generation.\n\n\u2022 Qwen-VL series (Bai et al., 2023): Qwen-VL (Qwen Large Vision Language Model) is the multimodal version of the large model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-VL accepts image, text, and bounding box as inputs, outputs text, and bounding box.\n\n\u2022 InternLM-XComposer2-VL (Dong et al., 2024): InternLM-XComposer2 is a cuttingedge vision-language model excelling in free-form text-image composition and comprehension, crafting content from diverse inputs like outlines, detailed specs, and reference images. Using a Partial LoRA (PLoRA) approach, it balances vision understanding and text composition."
                        ],
                        "paper": {
                            "corpus_id": 266191741,
                            "title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2543684",
                                    "name": "Kaijie Zhu"
                                },
                                {
                                    "authorId": "2261935625",
                                    "name": "Qinlin Zhao"
                                },
                                {
                                    "authorId": "2261741520",
                                    "name": "Hao Chen"
                                },
                                {
                                    "authorId": "2145270616",
                                    "name": "Jindong Wang"
                                },
                                {
                                    "authorId": "2249681654",
                                    "name": "Xing Xie"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 26
                        },
                        "score": 0.96044921875
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available."
                        ],
                        "paper": {
                            "corpus_id": 258179774,
                            "title": "Visual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2143856368",
                                    "name": "Haotian Liu"
                                },
                                {
                                    "authorId": "2109737569",
                                    "name": "Chunyuan Li"
                                },
                                {
                                    "authorId": "31060482",
                                    "name": "Qingyang Wu"
                                },
                                {
                                    "authorId": "144756076",
                                    "name": "Yong Jae Lee"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 4921
                        },
                        "score": 0
                    },
                    {
                        "id": "(Khaldi et al., 2024)",
                        "snippets": [
                            "LVLMs have emerged as a natural evolution of integrating LLMs with LVMs, enabling seamless multimodal interaction. By utilizing machine-generated instruction-following data during training, these models facilitate more dynamic and interactive communication between humans and machines. LVLMs are capable of processing both visual and textual inputs, generating coherent and contextually relevant responses in natural language",
                            "Notable LVLMs include BLIP (Li et al., 2022), one of the earliest unified vision-language encoder-decoder architectures; LLaVA (Large Language and Vision Assistant) (Liu et al. 2023), an end-to-end multimodal model that connects a vision encoder to a language model for versatile visual-language understanding; and GPT-4 (Achiam et al. 2023), a proprietary foundation model capable of multimodal reasoning, accessible exclusively via APIs such as ChatGPT."
                        ],
                        "paper": {
                            "corpus_id": 273022981,
                            "title": "Exploring Social Media Image Categorization Using Large Models with Different Adaptation Methods: A Case Study on Cultural Nature's Contributions to People",
                            "authors": [
                                {
                                    "authorId": "35729419",
                                    "name": "Rohaifa Khaldi"
                                },
                                {
                                    "authorId": "2273276043",
                                    "name": "Domingo Alcaraz-Segura"
                                },
                                {
                                    "authorId": "2323737781",
                                    "name": "Ignacio S'anchez-Herrera"
                                },
                                {
                                    "authorId": "2271941340",
                                    "name": "Javier Mart\u00ednez-L\u00f3pez"
                                },
                                {
                                    "authorId": "2323747675",
                                    "name": "Carlos Javier Navarro"
                                },
                                {
                                    "authorId": "3030006",
                                    "name": "S. Tabik"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.9697265625
                    },
                    {
                        "id": "(Li et al., 2022)",
                        "snippets": [
                            "Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP."
                        ],
                        "paper": {
                            "corpus_id": 246411402,
                            "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation",
                            "authors": [
                                {
                                    "authorId": "49299019",
                                    "name": "Junnan Li"
                                },
                                {
                                    "authorId": "2981509",
                                    "name": "Dongxu Li"
                                },
                                {
                                    "authorId": "2054594326",
                                    "name": "Caiming Xiong"
                                },
                                {
                                    "authorId": "1741126",
                                    "name": "S. Hoi"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 4413
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "Notable examples include Gemini [111], GPT-4V, and ImageBind (Girdhar et al., 2023).These models showcase the ability to process text, images, audio, and video, enhancing functionalities such as translation, image recognition, and more",
                            "BLIP-2 [112] introduces a novel approach to integrate a frozen pre-trained visual encoder with a frozen large language model using a Q-former module.This module employs learnable input queries that interact with image features and the LLM, allowing for effective cross-modal learning.This setup helps maintain the versatility of the LLM while incorporating visual information effectively.LLava [113] is a represent large multimodal model integrating a pre-trained CLIP [116] visual encoder (ViT-L/14), the Vicuna [115] language model, and a simple linear projection layer."
                        ],
                        "paper": {
                            "corpus_id": 270391394,
                            "title": "Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey",
                            "authors": [
                                {
                                    "authorId": "2261895888",
                                    "name": "Hao Yang"
                                },
                                {
                                    "authorId": "49339265",
                                    "name": "Yanyan Zhao"
                                },
                                {
                                    "authorId": "143792101",
                                    "name": "Yang Wu"
                                },
                                {
                                    "authorId": "2214828656",
                                    "name": "Shilong Wang"
                                },
                                {
                                    "authorId": "2261492452",
                                    "name": "Tian Zheng"
                                },
                                {
                                    "authorId": "2261686105",
                                    "name": "Hongbo Zhang"
                                },
                                {
                                    "authorId": "2279751409",
                                    "name": "Wanxiang Che"
                                },
                                {
                                    "authorId": "2203961541",
                                    "name": "Bing Qin"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 13
                        },
                        "score": 0.9580078125
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Models such as GILL (Koh et al., 2023), Emu (Sun et al., 2023), and DreamLLM (Dong et al., 2023) specialize in interleaved vision-language generation by aligning the output space of MLLMs with the diffusion image decoder. However, these methods primarily align at a semantic level and may struggle with detailed, subject-driven image generation. BLIP-Diffusion (Li et al., 2023a) synthesizes images by composing subjects with random backgrounds, endowing it with zero-shot, subject-driven text-toimage generation capabilities. However, its specific input template and training process limit scalability for multiple entities. KOSMOS-G (Pan et al., 2023), a model closely related to our work, leverages a MLLM to encode interleaved text-visual inputs, and the U-Net of Stable Diffusion (SD) v1.5 as the image decoder."
                        ],
                        "paper": {
                            "corpus_id": 267200040,
                            "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion",
                            "authors": [
                                {
                                    "authorId": "2289660121",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2118734325",
                                    "name": "Xue Xu"
                                },
                                {
                                    "authorId": "2288740225",
                                    "name": "Jiachen Liu"
                                },
                                {
                                    "authorId": "2288393805",
                                    "name": "Xinyan Xiao"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 8
                        },
                        "score": 0.96435546875
                    },
                    {
                        "id": "(Fang et al., 2025)",
                        "snippets": [
                            "To address this limitation, DreamLLM [6] integrates an LLM backbone with a diffusion model to enable image generation as a multimodal output. Emu2 [31] scales this architecture to 37B parameters, demonstrating strong multimodal in-context learning and the ability to handle complex tasks requiring real-time reasoning, such as visual prompting and object-grounded generation. Chameleon [33] proposes a stable training strategy from the ground up, featuring an alignment process and architectural parameterization tailored to early-fusion, token-based, mixed-modal settings."
                        ],
                        "paper": {
                            "corpus_id": 276422421,
                            "title": "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs",
                            "authors": [
                                {
                                    "authorId": "2306162276",
                                    "name": "Yi Fang"
                                },
                                {
                                    "authorId": "2057050247",
                                    "name": "Bowen Jin"
                                },
                                {
                                    "authorId": "2346106510",
                                    "name": "Jiacheng Shen"
                                },
                                {
                                    "authorId": "2307734628",
                                    "name": "Sirui Ding"
                                },
                                {
                                    "authorId": "2282981308",
                                    "name": "Qiaoyu Tan"
                                },
                                {
                                    "authorId": "2257136881",
                                    "name": "Jiawei Han"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.97900390625
                    },
                    {
                        "id": "(Dong et al., 2023)",
                        "snippets": [
                            "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."
                        ],
                        "paper": {
                            "corpus_id": 261975252,
                            "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
                            "authors": [
                                {
                                    "authorId": "2056965063",
                                    "name": "Runpei Dong"
                                },
                                {
                                    "authorId": "2118643247",
                                    "name": "Chunrui Han"
                                },
                                {
                                    "authorId": "2211415443",
                                    "name": "Yuang Peng"
                                },
                                {
                                    "authorId": "3424017",
                                    "name": "Zekun Qi"
                                },
                                {
                                    "authorId": "2242581956",
                                    "name": "Zheng Ge"
                                },
                                {
                                    "authorId": "2161319190",
                                    "name": "Jinrong Yang"
                                },
                                {
                                    "authorId": "48096671",
                                    "name": "Liang Zhao"
                                },
                                {
                                    "authorId": "26913717",
                                    "name": "Jian\u2010Yuan Sun"
                                },
                                {
                                    "authorId": "2157473950",
                                    "name": "Hongyu Zhou"
                                },
                                {
                                    "authorId": "134085586",
                                    "name": "Hao-Ran Wei"
                                },
                                {
                                    "authorId": "2069029697",
                                    "name": "Xiangwen Kong"
                                },
                                {
                                    "authorId": "2185865433",
                                    "name": "Xiangyu Zhang"
                                },
                                {
                                    "authorId": "2244131405",
                                    "name": "Kaisheng Ma"
                                },
                                {
                                    "authorId": "2242612318",
                                    "name": "Li Yi"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 201
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sun et al., 2023)",
                        "snippets": [
                            "The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research."
                        ],
                        "paper": {
                            "corpus_id": 266374640,
                            "title": "Generative Multimodal Models are In-Context Learners",
                            "authors": [
                                {
                                    "authorId": "2264040642",
                                    "name": "Quan Sun"
                                },
                                {
                                    "authorId": "2263702215",
                                    "name": "Yufeng Cui"
                                },
                                {
                                    "authorId": "2264589602",
                                    "name": "Xiaosong Zhang"
                                },
                                {
                                    "authorId": "2264274388",
                                    "name": "Fan Zhang"
                                },
                                {
                                    "authorId": "23716915",
                                    "name": "Qiying Yu"
                                },
                                {
                                    "authorId": "2275808712",
                                    "name": "Zhengxiong Luo"
                                },
                                {
                                    "authorId": "2217456303",
                                    "name": "Yueze Wang"
                                },
                                {
                                    "authorId": "2275170369",
                                    "name": "Yongming Rao"
                                },
                                {
                                    "authorId": "2222717281",
                                    "name": "Jingjing Liu"
                                },
                                {
                                    "authorId": "2257137554",
                                    "name": "Tiejun Huang"
                                },
                                {
                                    "authorId": "2263959521",
                                    "name": "Xinlong Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 290
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "Another line of research (Koh et al., 2023)(Dong et al., 2023)Sun et al., 2023;(Wu et al., 2023) has proposed augmenting MLLMs with the ability to generate images, videos, and audio from text instructions. These approaches introduce additional visual tokens for generation and align these generative tokens as conditional information with a pretrained generator, such as Stable Diffusion (Esser et al., 2024;Podell et al., 2023) for text-to-image generation. Consequently, the generation capabilities heavily rely on the external expert generator rather than MLLMs themselves, resulting in inconsistent and inferior generation results. To combine the strength of both approaches, our model aims to learn both understanding and generation of images using an MLLM with native multimodal capabilities, drawing inspiration from Chameleon (Team, 2024a), a mixed-modal early-fusion foundation model."
                        ],
                        "paper": {
                            "corpus_id": 271709823,
                            "title": "Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining",
                            "authors": [
                                {
                                    "authorId": "2308683426",
                                    "name": "Dongyang Liu"
                                },
                                {
                                    "authorId": "2283419424",
                                    "name": "Shitian Zhao"
                                },
                                {
                                    "authorId": "2300371391",
                                    "name": "Le Zhuo"
                                },
                                {
                                    "authorId": "2284068796",
                                    "name": "Weifeng Lin"
                                },
                                {
                                    "authorId": "2312346585",
                                    "name": "Yu Qiao"
                                },
                                {
                                    "authorId": "2266421952",
                                    "name": "Hongsheng Li"
                                },
                                {
                                    "authorId": "2310909023",
                                    "name": "Peng Gao"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 59
                        },
                        "score": 0.96044921875
                    },
                    {
                        "id": "(Wu et al., 2023)",
                        "snippets": [
                            "While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/"
                        ],
                        "paper": {
                            "corpus_id": 261696650,
                            "title": "NExT-GPT: Any-to-Any Multimodal LLM",
                            "authors": [
                                {
                                    "authorId": "1957924118",
                                    "name": "Shengqiong Wu"
                                },
                                {
                                    "authorId": "46959445",
                                    "name": "Hao Fei"
                                },
                                {
                                    "authorId": "1990265392",
                                    "name": "Leigang Qu"
                                },
                                {
                                    "authorId": "144540018",
                                    "name": "Wei Ji"
                                },
                                {
                                    "authorId": "144078686",
                                    "name": "Tat-Seng Chua"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 506
                        },
                        "score": 0
                    },
                    {
                        "id": "(Team et al., 2024)",
                        "snippets": [
                            "We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model."
                        ],
                        "paper": {
                            "corpus_id": 269791516,
                            "title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models",
                            "authors": [
                                {
                                    "authorId": "2301457346",
                                    "name": "Chameleon Team"
                                },
                                {
                                    "authorId": "39960571",
                                    "name": "Jacob Kahn"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 339
                        },
                        "score": 0.98291015625
                    },
                    {
                        "id": "(Xie et al., 2024)",
                        "snippets": [
                            "In recent years, an increasing number of studies (Ge et al., 2024;Wu et al., 2023b;(Tang et al., 2023)Ye et al., 2024a;(Dong et al., 2023)Aiello et al., 2024) have focused on unified multimodal language models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;Sun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive modeling to generate images. SEED-X (Ge et al., 2024) proposes a unified and versatile foundation system capable of handling both multimodal understanding and generation tasks. In this approach, continuous image representations from CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model (LLM) to perform next-word prediction and image representation regression. DreamLLM (Dong et al., 2023)) also explores the potential of enabling multimodal comprehension and creation based on LLMs. Chameleon (Team, 2024) introduces a family of token-based mixed-modal models capable of both comprehending and generating images. This approach represents all modalities as discrete tokens and utilizes a unified transformer-based architecture and trains the model from scratch in an end-to-end manner."
                        ],
                        "paper": {
                            "corpus_id": 271924334,
                            "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation",
                            "authors": [
                                {
                                    "authorId": "2306603347",
                                    "name": "Jinheng Xie"
                                },
                                {
                                    "authorId": "2258958963",
                                    "name": "Weijia Mao"
                                },
                                {
                                    "authorId": "2237427303",
                                    "name": "Zechen Bai"
                                },
                                {
                                    "authorId": "2248388634",
                                    "name": "David Junhao Zhang"
                                },
                                {
                                    "authorId": "2316668014",
                                    "name": "Weihao Wang"
                                },
                                {
                                    "authorId": "2298569956",
                                    "name": "Kevin Qinghong Lin"
                                },
                                {
                                    "authorId": "2248678348",
                                    "name": "Yuchao Gu"
                                },
                                {
                                    "authorId": "2316662510",
                                    "name": "Zhijie Chen"
                                },
                                {
                                    "authorId": "2309246065",
                                    "name": "Zhenheng Yang"
                                },
                                {
                                    "authorId": "2244623976",
                                    "name": "Mike Zheng Shou"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 229
                        },
                        "score": 0.9619140625
                    },
                    {
                        "id": "(Zhao et al., 2025)",
                        "snippets": [
                            "Unified multimodal generative models aim to bridge the gap between understanding and generation tasks, and integrate vision and language into a single framework, enabling the model to learn shared representations across modalities [1,(Aiello et al., 2023)(Dong et al., 2023)19,(Sun et al., 2023)62,71,72,74,76,77,79,85]. SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM (Dong et al., 2023) extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities."
                        ],
                        "paper": {
                            "corpus_id": 276782523,
                            "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
                            "authors": [
                                {
                                    "authorId": "2248946985",
                                    "name": "Rui Zhao"
                                },
                                {
                                    "authorId": "2258958963",
                                    "name": "Weijia Mao"
                                },
                                {
                                    "authorId": "2344762475",
                                    "name": "Mike Zheng Shou"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9658203125
                    },
                    {
                        "id": "(Niu et al., 2025)",
                        "snippets": [
                            "Unified multimodal models aim to construct general-purpose models capable of processing both textual and visual inputs, and performing cross-modal generation and understanding. These models [4,8,16,17,21,25,35,38,39,41,[43][44][45][48] are typically built upon powerful large language models (LLMs) [54] and extend next-token prediction [2] to image generation: the LLM generates visual tokens, and a VQ-VAE [42] or Diffusion model serves as a detokenizer. Moreover, Transfusion [55] and Show-O [49] demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework. D-DiT [22] achieves both Text-to-Image (T2I) and Imageto-Text (I2T) tasks using an end-to-end diffusion model."
                        ],
                        "paper": {
                            "corpus_id": 276929205,
                            "title": "WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation",
                            "authors": [
                                {
                                    "authorId": "2350309448",
                                    "name": "Yuwei Niu"
                                },
                                {
                                    "authorId": "2253434320",
                                    "name": "Munan Ning"
                                },
                                {
                                    "authorId": "2349744842",
                                    "name": "Mengren Zheng"
                                },
                                {
                                    "authorId": "2333420566",
                                    "name": "Bin Lin"
                                },
                                {
                                    "authorId": "2328406001",
                                    "name": "Peng Jin"
                                },
                                {
                                    "authorId": "2315613899",
                                    "name": "Jiaqi Liao"
                                },
                                {
                                    "authorId": "2253467098",
                                    "name": "Kun-Peng Ning"
                                },
                                {
                                    "authorId": "2256419313",
                                    "name": "Bin Zhu"
                                },
                                {
                                    "authorId": "2280992738",
                                    "name": "Li Yuan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 26
                        },
                        "score": 0.96630859375
                    },
                    {
                        "id": "(Fan et al., 2025)",
                        "snippets": [
                            "We present UniFluid, a unified autoregressive framework for joint visual generation and understanding leveraging continuous visual tokens. Our unified autoregressive architecture processes multimodal image and text inputs, generating discrete tokens for text and continuous tokens for image",
                            "UniFluid, a unified framework that leverages continuous visual tokens within an autoregressive architecture to jointly handle vision-language generation and understanding tasks. Building upon pre-trained Gemma [48] on large-scale text corpus, UniFluid unlocks powerful visual generation and understanding capabilities through training with paired image-text data, and further allows these two tasks to mutually benefit each other within a single architecture. Specifically, UniFluid adopts a unified autoregressive framework where both text and continuous visual inputs are embedded as tokens in the same space, enabling seamless joint training of image generation and understanding tasks. UniFluid integrates a continuous tokenizer [12,(Li et al., 2024) for image generation and a pre-trained SigLIP (Zhai et al., 2023) image encoder for understanding tasks, while textual inputs are processed using a standard SentencePiece tokenizer [20]. The resulting multimodal sequences are modeled autoregressively using Gemma [48] as the underlying transformer backbone. Task-specific prediction heads-a diffusion-based head for image generation and a cross-entropy head for text generation-ensure effective modality-specific training and inference, enabling UniFluid to efficiently learn shared representations that mutually enhance its generation and understanding capabilities."
                        ],
                        "paper": {
                            "corpus_id": 277103675,
                            "title": "Unified Autoregressive Visual Generation and Understanding with Continuous Tokens",
                            "authors": [
                                {
                                    "authorId": "2347484699",
                                    "name": "Lijie Fan"
                                },
                                {
                                    "authorId": "34689393",
                                    "name": "Luming Tang"
                                },
                                {
                                    "authorId": "2333872078",
                                    "name": "Siyang Qin"
                                },
                                {
                                    "authorId": "2307269819",
                                    "name": "Tianhong Li"
                                },
                                {
                                    "authorId": "2350843695",
                                    "name": "Xuan Yang"
                                },
                                {
                                    "authorId": "2266238535",
                                    "name": "Siyuan Qiao"
                                },
                                {
                                    "authorId": "2350755056",
                                    "name": "Andreas Steiner"
                                },
                                {
                                    "authorId": "2333512682",
                                    "name": "Chen Sun"
                                },
                                {
                                    "authorId": "2281034398",
                                    "name": "Yuanzhen Li"
                                },
                                {
                                    "authorId": "2351408307",
                                    "name": "Tao Zhu"
                                },
                                {
                                    "authorId": "2325950285",
                                    "name": "Michael Rubinstein"
                                },
                                {
                                    "authorId": "3063676",
                                    "name": "Michalis Raptis"
                                },
                                {
                                    "authorId": "2262516617",
                                    "name": "Deqing Sun"
                                },
                                {
                                    "authorId": "1737285",
                                    "name": "Radu Soricut"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.9794921875
                    },
                    {
                        "id": "(Tang et al., 2023)",
                        "snippets": [
                            "We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io"
                        ],
                        "paper": {
                            "corpus_id": 258822817,
                            "title": "Any-to-Any Generation via Composable Diffusion",
                            "authors": [
                                {
                                    "authorId": "151270642",
                                    "name": "Zineng Tang"
                                },
                                {
                                    "authorId": "2155459391",
                                    "name": "Ziyi Yang"
                                },
                                {
                                    "authorId": "8652308",
                                    "name": "Chenguang Zhu"
                                },
                                {
                                    "authorId": "48262024",
                                    "name": "Michael Zeng"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 188
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al._1, 2024)",
                        "snippets": [
                            "Recent works have begun to address this gap, with the LMMs extended with diffusion models, exploring the generation of mixed text and image outputs (Koh et al., 2023)Sun et al., 2023b;(Dong et al., 2023)Tian et al., 2024;(Zhan et al., 2024)Chen et al., 2023). These models leverage advanced architectures and training techniques to enhance their ability to produce coherent and contextually relevant interleaved content."
                        ],
                        "paper": {
                            "corpus_id": 270688372,
                            "title": "Holistic Evaluation for Interleaved Text-and-Image Generation",
                            "authors": [
                                {
                                    "authorId": "2123130842",
                                    "name": "Minqian Liu"
                                },
                                {
                                    "authorId": "2136442661",
                                    "name": "Zhiyang Xu"
                                },
                                {
                                    "authorId": "2254334449",
                                    "name": "Zihao Lin"
                                },
                                {
                                    "authorId": "2243992517",
                                    "name": "Trevor Ashby"
                                },
                                {
                                    "authorId": "2218905547",
                                    "name": "Joy Rimchala"
                                },
                                {
                                    "authorId": "2307975528",
                                    "name": "Jiaxin Zhang"
                                },
                                {
                                    "authorId": "2238885968",
                                    "name": "Lifu Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 11
                        },
                        "score": 0.97021484375
                    },
                    {
                        "id": "(Zhan et al., 2024)",
                        "snippets": [
                            "We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/"
                        ],
                        "paper": {
                            "corpus_id": 267750101,
                            "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling",
                            "authors": [
                                {
                                    "authorId": "2217531907",
                                    "name": "Jun Zhan"
                                },
                                {
                                    "authorId": "2087363104",
                                    "name": "Junqi Dai"
                                },
                                {
                                    "authorId": "2153258452",
                                    "name": "Jiasheng Ye"
                                },
                                {
                                    "authorId": "2118117212",
                                    "name": "Yunhua Zhou"
                                },
                                {
                                    "authorId": "2109797247",
                                    "name": "Dong Zhang"
                                },
                                {
                                    "authorId": "2284732560",
                                    "name": "Zhigeng Liu"
                                },
                                {
                                    "authorId": "2278615698",
                                    "name": "Xin Zhang"
                                },
                                {
                                    "authorId": "2032236274",
                                    "name": "Ruibin Yuan"
                                },
                                {
                                    "authorId": "2143853895",
                                    "name": "Ge Zhang"
                                },
                                {
                                    "authorId": "2107897400",
                                    "name": "Linyang Li"
                                },
                                {
                                    "authorId": "146948229",
                                    "name": "Hang Yan"
                                },
                                {
                                    "authorId": "2276508494",
                                    "name": "Jie Fu"
                                },
                                {
                                    "authorId": "2067331064",
                                    "name": "Tao Gui"
                                },
                                {
                                    "authorId": "153345698",
                                    "name": "Tianxiang Sun"
                                },
                                {
                                    "authorId": "2284724292",
                                    "name": "Yugang Jiang"
                                },
                                {
                                    "authorId": "1767521",
                                    "name": "Xipeng Qiu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 136
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Architectural Approaches for Multimodal Generation",
                "tldr": "Multimodal generation models employ diverse architectural approaches including unified transformer-based designs, dual-stream architectures, and hybrid systems that combine large language models with specialized visual components. Key design patterns include early fusion methods that process tokens from all modalities uniformly, adapter-based approaches that connect pre-trained components, and end-to-end architectures that handle multiple modalities within a single framework. (24 sources)",
                "text": "\n## Unified Transformer Architectures\nOne major approach to multimodal generation involves unified transformer architectures that process both text and image tokens within a single framework. These models represent both modalities as discrete tokens that can be handled by the same transformer layers. The Chameleon family of models pioneered this early-fusion token-based approach, capable of understanding and generating images and text in any sequence by representing all modalities as discrete tokens within a unified architecture <Paper corpusId=\"265498814\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. Similarly, AnyGPT utilizes discrete representations for unified processing of various modalities, including speech, text, images, and music, without requiring alterations to standard LLM architectures <Paper corpusId=\"274597781\" paperTitle=\"(Serra et al., 2024)\" isShortName></Paper> <Paper corpusId=\"267750101\" paperTitle=\"(Zhan et al., 2024)\" isShortName></Paper>.\n\nModels like UniFluid demonstrate that a unified autoregressive framework can leverage continuous visual tokens to jointly handle vision-language generation and understanding tasks <Paper corpusId=\"278327487\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>. These approaches typically employ either vector-quantized autoregressive models or diffusion-based methods to create consistent token representations across modalities <Paper corpusId=\"276929205\" paperTitle=\"(Niu et al., 2025)\" isShortName></Paper>.\n\n## Connector and Adapter Approaches\nAnother prevalent approach involves connecting pre-trained vision and language models through specialized connectors or adapters. For example, Emu proposes an efficient mapping network that translates text representations from an LLM into the embedding space of visual models, enabling leverage of strong text representations for visual outputs <Paper corpusId=\"258959284\" paperTitle=\"(Koh et al., 2023)\" isShortName></Paper>. Similarly, the Joint Autoregressive Mixture (JAM) framework systematically fuses existing text and image generation models in a modular approach <Paper corpusId=\"262898422\" paperTitle=\"(Aiello et al., 2023)\" isShortName></Paper>.\n\nMany open-source Multimodal Large Language Models (MLLMs) employ architectures with a pre-trained vision encoder, an LLM, and a crossmodal connector. For instance, the BLIP series utilized a Q-Former module to align text and images <Paper corpusId=\"273350685\" paperTitle=\"(Cao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"256390509\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper>, while Flamingo employed a gated cross-attention mechanism to integrate images and text <Paper corpusId=\"273350685\" paperTitle=\"(Cao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"248476411\" paperTitle=\"(Alayrac et al., 2022)\" isShortName></Paper>. LLaVA-1.5 adopted an MLP projector to connect the pre-trained vision encoder with the LLM <Paper corpusId=\"273350685\" paperTitle=\"(Cao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>.\n\n## Dual-Stream Architectures\nDual-stream architectures employ separate encoders for different modalities with interaction mechanisms between them. One strategy involves a distinct CNN encoder for visual features and a transformer encoder for text, interacting via co-attentional transformer layers <Paper corpusId=\"267897956\" paperTitle=\"(Younesi et al., 2024)\" isShortName></Paper>. The CNN extracts visual features providing contextual information to guide language generation, while the transformer handles text semantics. Alternative methods directly incorporate CNNs within the transformer architecture as visual token encoders that operate alongside text token encoders <Paper corpusId=\"267897956\" paperTitle=\"(Younesi et al., 2024)\" isShortName></Paper>.\n\n## Diffusion-Based Approaches\nDiffusion models have become central to many multimodal generation architectures. Recent advancements integrate diffusion-based generation with language models in various ways. One approach involves utilizing pre-trained text-to-image diffusion models while training LLMs to generate conditions for these models <Paper corpusId=\"273969525\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. Models like DreamLLM employ diffusion models for the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space <Paper corpusId=\"273969525\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper> <Paper corpusId=\"261975252\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>.\n\nTransfusion and Show-O have demonstrated that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework <Paper corpusId=\"276929205\" paperTitle=\"(Niu et al., 2025)\" isShortName></Paper>. Similarly, research by Zou et al. explores integrating various visual generation paradigms, such as diffusion models, flow-based generative models, and vector-quantized autoregressive models with LLMs <Paper corpusId=\"276928288\" paperTitle=\"(Zou et al., 2025)\" isShortName></Paper> <Paper corpusId=\"268247980\" paperTitle=\"(Esser et al., 2024)\" isShortName></Paper>.\n\n## End-to-End vs. Cascaded Systems\nArchitectural approaches can also be categorized by their integration style. Some systems, like Visual ChatGPT, connect LLMs with image generation tools in a cascaded pipeline by transferring text messages, which can introduce instability and noise <Paper corpusId=\"266210376\" paperTitle=\"(Zhu et al._1, 2023)\" isShortName></Paper>. In contrast, end-to-end optimized models align the output space with image diffusion models, enabling both perception and generation of images and text within a unified framework <Paper corpusId=\"266210376\" paperTitle=\"(Zhu et al._1, 2023)\" isShortName></Paper>.\n\nThe CoDi (Composable Diffusion) model exemplifies an end-to-end approach as a novel generative model capable of generating any combination of output modalities from any combination of input modalities, employing a composable generation strategy <Paper corpusId=\"274597781\" paperTitle=\"(Serra et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258822817\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>.\n\n## Multimodal Output Decoders\nA critical component in multimodal architectures is the output decoder, which reconverts fused, integrated multimodal information back into usable forms tailored to specific tasks. For image captioning, the decoder might generate descriptive text based on visual inputs, while for video understanding tasks, it could produce annotations or summaries combining both visual and auditory data <Paper corpusId=\"271693411\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n## Efficiency Considerations\nAs multimodal models grow in complexity, efficiency becomes increasingly important. MLLMs face significant computational and memory demands when processing long contexts in multimodal inputs <Paper corpusId=\"273233771\" paperTitle=\"(Liu et al._2, 2024)\" isShortName></Paper>. Recent models like InternLM-XComposer-2.5 address this by supporting long-contextual input and output, enabling the model to handle extensive multimodal contexts efficiently <Paper corpusId=\"275337159\" paperTitle=\"(Franklin et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270923961\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. Other approaches like mixture-of-experts have been introduced to improve scalability while enhancing relational reasoning capabilities <Paper corpusId=\"275337159\" paperTitle=\"(Franklin et al., 2025)\" isShortName></Paper> <Paper corpusId=\"267311517\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper>.\n\nThese architectural approaches collectively represent the evolving landscape of multimodal generation systems, with each design offering different trade-offs in terms of integration depth, efficiency, and generative capabilities <Paper corpusId=\"278339632\" paperTitle=\"(Jiang et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "Recent work [11,17,18,34,54,60] further empower LLMs with the ability of generating images beyond texts (acting like a combination of GPT-4V [1] and DALL-E 3 [5]), since they contend that the premise for the emergence of multimodal capabilities is that text and image can be represented and processed interchangeably in a unified autoregressive Transformer."
                        ],
                        "paper": {
                            "corpus_id": 265498814,
                            "title": "SEED-Bench-2: Benchmarking Multimodal Large Language Models",
                            "authors": [
                                {
                                    "authorId": "1491798621",
                                    "name": "Bohao Li"
                                },
                                {
                                    "authorId": "51123495",
                                    "name": "Yuying Ge"
                                },
                                {
                                    "authorId": "152988335",
                                    "name": "Yixiao Ge"
                                },
                                {
                                    "authorId": "2243452799",
                                    "name": "Guangzhi Wang"
                                },
                                {
                                    "authorId": "2151036422",
                                    "name": "Rui Wang"
                                },
                                {
                                    "authorId": "2268726753",
                                    "name": "Ruimao Zhang"
                                },
                                {
                                    "authorId": "2265579883",
                                    "name": "Ying Shan"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 69
                        },
                        "score": 0.96826171875
                    },
                    {
                        "id": "(Serra et al., 2024)",
                        "snippets": [
                            "Recently, the availability of large open datasets [14,15] and improved techniques to align text and image embeddings (Ge et al., 2023) have also enabled the creation of multimodal models that can both understand and generate visual content within a single architecture (Tang et al., 2023)[18](Zhan et al., 2024)[20]."
                        ],
                        "paper": {
                            "corpus_id": 274597781,
                            "title": "The Narrow Gate: Localized Image-Text Communication in Vision-Language Models",
                            "authors": [
                                {
                                    "authorId": "2319817038",
                                    "name": "Alessandro Serra"
                                },
                                {
                                    "authorId": "2284691756",
                                    "name": "Francesco Ortu"
                                },
                                {
                                    "authorId": "2334479581",
                                    "name": "Emanuele Panizon"
                                },
                                {
                                    "authorId": "108193025",
                                    "name": "L. Valeriani"
                                },
                                {
                                    "authorId": "2308039508",
                                    "name": "Lorenzo Basile"
                                },
                                {
                                    "authorId": "5366502",
                                    "name": "A. Ansuini"
                                },
                                {
                                    "authorId": "104350259",
                                    "name": "Diego Doimo"
                                },
                                {
                                    "authorId": "2269294135",
                                    "name": "Alberto Cazzaniga"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.97509765625
                    },
                    {
                        "id": "(Zhan et al., 2024)",
                        "snippets": [
                            "We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/"
                        ],
                        "paper": {
                            "corpus_id": 267750101,
                            "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling",
                            "authors": [
                                {
                                    "authorId": "2217531907",
                                    "name": "Jun Zhan"
                                },
                                {
                                    "authorId": "2087363104",
                                    "name": "Junqi Dai"
                                },
                                {
                                    "authorId": "2153258452",
                                    "name": "Jiasheng Ye"
                                },
                                {
                                    "authorId": "2118117212",
                                    "name": "Yunhua Zhou"
                                },
                                {
                                    "authorId": "2109797247",
                                    "name": "Dong Zhang"
                                },
                                {
                                    "authorId": "2284732560",
                                    "name": "Zhigeng Liu"
                                },
                                {
                                    "authorId": "2278615698",
                                    "name": "Xin Zhang"
                                },
                                {
                                    "authorId": "2032236274",
                                    "name": "Ruibin Yuan"
                                },
                                {
                                    "authorId": "2143853895",
                                    "name": "Ge Zhang"
                                },
                                {
                                    "authorId": "2107897400",
                                    "name": "Linyang Li"
                                },
                                {
                                    "authorId": "146948229",
                                    "name": "Hang Yan"
                                },
                                {
                                    "authorId": "2276508494",
                                    "name": "Jie Fu"
                                },
                                {
                                    "authorId": "2067331064",
                                    "name": "Tao Gui"
                                },
                                {
                                    "authorId": "153345698",
                                    "name": "Tianxiang Sun"
                                },
                                {
                                    "authorId": "2284724292",
                                    "name": "Yugang Jiang"
                                },
                                {
                                    "authorId": "1767521",
                                    "name": "Xipeng Qiu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 136
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2025)",
                        "snippets": [
                            "In this section, we primarily focus on unified multimodal models that support vision-language understanding and generation, i.e., models that take both image and text as input and produce either text or image as output. As shown in Fig. 5, existing unified models can be broadly categorized into three main types: diffusion models, autoregressive models, and fused AR + diffusion models. For autoregressive models, we further classify them based on their modality encoding methods into four subcategories: pixel-based encoding, semantic-based encoding, learnable query-based encoding, and hybrid encoding. Each of these encoding strategies represents different ways of handling visual and textual data, leading to varying levels of integration and flexibility in the multimodal representations. Fused AR + diffusion models are divided into two subcategories based on modality encoding: pixel-based encoding and hybrid encoding. These models combine aspects of both autoregressive and diffusion techniques, offering a promising approach to more unified and efficient multimodal generation."
                        ],
                        "paper": {
                            "corpus_id": 278327487,
                            "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
                            "authors": [
                                {
                                    "authorId": "2359211182",
                                    "name": "Xinjie Zhang"
                                },
                                {
                                    "authorId": "2359201363",
                                    "name": "Jintao Guo"
                                },
                                {
                                    "authorId": "2337775059",
                                    "name": "Shanshan Zhao"
                                },
                                {
                                    "authorId": "2345947101",
                                    "name": "Minghao Fu"
                                },
                                {
                                    "authorId": "2275153684",
                                    "name": "Lunhao Duan"
                                },
                                {
                                    "authorId": "2339237238",
                                    "name": "Guo-Hua Wang"
                                },
                                {
                                    "authorId": "2304395071",
                                    "name": "Qing-Guo Chen"
                                },
                                {
                                    "authorId": "2304361003",
                                    "name": "Zhao Xu"
                                },
                                {
                                    "authorId": "2305289815",
                                    "name": "Weihua Luo"
                                },
                                {
                                    "authorId": "2304530663",
                                    "name": "Kaifu Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.97998046875
                    },
                    {
                        "id": "(Niu et al., 2025)",
                        "snippets": [
                            "Unified multimodal models aim to construct general-purpose models capable of processing both textual and visual inputs, and performing cross-modal generation and understanding. These models [4,8,16,17,21,25,35,38,39,41,[43][44][45][48] are typically built upon powerful large language models (LLMs) [54] and extend next-token prediction [2] to image generation: the LLM generates visual tokens, and a VQ-VAE [42] or Diffusion model serves as a detokenizer. Moreover, Transfusion [55] and Show-O [49] demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework. D-DiT [22] achieves both Text-to-Image (T2I) and Imageto-Text (I2T) tasks using an end-to-end diffusion model."
                        ],
                        "paper": {
                            "corpus_id": 276929205,
                            "title": "WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation",
                            "authors": [
                                {
                                    "authorId": "2350309448",
                                    "name": "Yuwei Niu"
                                },
                                {
                                    "authorId": "2253434320",
                                    "name": "Munan Ning"
                                },
                                {
                                    "authorId": "2349744842",
                                    "name": "Mengren Zheng"
                                },
                                {
                                    "authorId": "2333420566",
                                    "name": "Bin Lin"
                                },
                                {
                                    "authorId": "2328406001",
                                    "name": "Peng Jin"
                                },
                                {
                                    "authorId": "2315613899",
                                    "name": "Jiaqi Liao"
                                },
                                {
                                    "authorId": "2253467098",
                                    "name": "Kun-Peng Ning"
                                },
                                {
                                    "authorId": "2256419313",
                                    "name": "Bin Zhu"
                                },
                                {
                                    "authorId": "2280992738",
                                    "name": "Li Yuan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 26
                        },
                        "score": 0.96630859375
                    },
                    {
                        "id": "(Koh et al., 2023)",
                        "snippets": [
                            "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs."
                        ],
                        "paper": {
                            "corpus_id": 258959284,
                            "title": "Generating Images with Multimodal Language Models",
                            "authors": [
                                {
                                    "authorId": "23978705",
                                    "name": "Jing Yu Koh"
                                },
                                {
                                    "authorId": "47070750",
                                    "name": "Daniel Fried"
                                },
                                {
                                    "authorId": "145124475",
                                    "name": "R. Salakhutdinov"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 259
                        },
                        "score": 0.98779296875
                    },
                    {
                        "id": "(Aiello et al., 2023)",
                        "snippets": [
                            "In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models."
                        ],
                        "paper": {
                            "corpus_id": 262898422,
                            "title": "Jointly Training Large Autoregressive Multimodal Models",
                            "authors": [
                                {
                                    "authorId": "2185505465",
                                    "name": "Emanuele Aiello"
                                },
                                {
                                    "authorId": "49297123",
                                    "name": "L. Yu"
                                },
                                {
                                    "authorId": "2247227174",
                                    "name": "Yixin Nie"
                                },
                                {
                                    "authorId": "2201435",
                                    "name": "Armen Aghajanyan"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 31
                        },
                        "score": 0.986328125
                    },
                    {
                        "id": "(Cao et al., 2024)",
                        "snippets": [
                            "Multimodal Large Language Models (MLLMs) integrate visual representations from images with linguistic embeddings from text, thereby enhancing the models' capabilities in comprehending and generating language descriptions of visual content. Most open-source MLLMs employ architectures that include a pre-trained vision encoder, an LLM, and a crossmodal connector. Early models, such as the BLIP series (Dai et al., 2023), (Li et al., 2023), utilized the Q-Former module to align text and images, thus improving multimodal capabilities. Flamingo (Alayrac et al., 2022) employed a gated cross-attention mechanism to integrate images and text. LLaVA-1.5 (Liu et al., 2023) adopted an MLP projector to connect the pre-trained vision encoder with the LLM. InternVL (Chen et al., 2023), [6] employed a dynamic resolution strategy, segmenting images into tiles and encoding both the tiles and the thumbnail view together. It then uses a pixel shuffle operation to reduce the number of visual tokens before integrating these features with the LLM through an MLP projector."
                        ],
                        "paper": {
                            "corpus_id": 273350685,
                            "title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding",
                            "authors": [
                                {
                                    "authorId": "2312369775",
                                    "name": "Yue Cao"
                                },
                                {
                                    "authorId": "2312345209",
                                    "name": "Yangzhou Liu"
                                },
                                {
                                    "authorId": "2305731793",
                                    "name": "Zhe Chen"
                                },
                                {
                                    "authorId": "2146870036",
                                    "name": "Guangchen Shi"
                                },
                                {
                                    "authorId": "2257133501",
                                    "name": "Wenhai Wang"
                                },
                                {
                                    "authorId": "2326454048",
                                    "name": "Danhuai Zhao"
                                },
                                {
                                    "authorId": "2276323159",
                                    "name": "Tong Lu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.9599609375
                    },
                    {
                        "id": "(Li et al._1, 2023)",
                        "snippets": [
                            "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions."
                        ],
                        "paper": {
                            "corpus_id": 256390509,
                            "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                            "authors": [
                                {
                                    "authorId": "49299019",
                                    "name": "Junnan Li"
                                },
                                {
                                    "authorId": "2981509",
                                    "name": "Dongxu Li"
                                },
                                {
                                    "authorId": "1702137",
                                    "name": "S. Savarese"
                                },
                                {
                                    "authorId": "2184854289",
                                    "name": "Steven C. H. Hoi"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 4653
                        },
                        "score": 0
                    },
                    {
                        "id": "(Alayrac et al., 2022)",
                        "snippets": [
                            "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data."
                        ],
                        "paper": {
                            "corpus_id": 248476411,
                            "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
                            "authors": [
                                {
                                    "authorId": "2285263",
                                    "name": "Jean-Baptiste Alayrac"
                                },
                                {
                                    "authorId": "7408951",
                                    "name": "Jeff Donahue"
                                },
                                {
                                    "authorId": "152831141",
                                    "name": "Pauline Luc"
                                },
                                {
                                    "authorId": "19200186",
                                    "name": "Antoine Miech"
                                },
                                {
                                    "authorId": "2159207795",
                                    "name": "Iain Barr"
                                },
                                {
                                    "authorId": "66535271",
                                    "name": "Yana Hasson"
                                },
                                {
                                    "authorId": "3257286",
                                    "name": "Karel Lenc"
                                },
                                {
                                    "authorId": "1697879",
                                    "name": "A. Mensch"
                                },
                                {
                                    "authorId": "2143434227",
                                    "name": "Katie Millican"
                                },
                                {
                                    "authorId": "47447264",
                                    "name": "Malcolm Reynolds"
                                },
                                {
                                    "authorId": "81387328",
                                    "name": "Roman Ring"
                                },
                                {
                                    "authorId": "2143538252",
                                    "name": "Eliza Rutherford"
                                },
                                {
                                    "authorId": "12159303",
                                    "name": "Serkan Cabi"
                                },
                                {
                                    "authorId": "22237490",
                                    "name": "Tengda Han"
                                },
                                {
                                    "authorId": "48398849",
                                    "name": "Zhitao Gong"
                                },
                                {
                                    "authorId": "2412073",
                                    "name": "Sina Samangooei"
                                },
                                {
                                    "authorId": "49601928",
                                    "name": "Marianne Monteiro"
                                },
                                {
                                    "authorId": "10698483",
                                    "name": "Jacob Menick"
                                },
                                {
                                    "authorId": "148016269",
                                    "name": "Sebastian Borgeaud"
                                },
                                {
                                    "authorId": "2065040422",
                                    "name": "Andy Brock"
                                },
                                {
                                    "authorId": "3208081",
                                    "name": "Aida Nematzadeh"
                                },
                                {
                                    "authorId": "7782886",
                                    "name": "Sahand Sharifzadeh"
                                },
                                {
                                    "authorId": "9961753",
                                    "name": "Mikolaj Binkowski"
                                },
                                {
                                    "authorId": "2026369796",
                                    "name": "Ricardo Barreira"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "1688869",
                                    "name": "Andrew Zisserman"
                                },
                                {
                                    "authorId": "34838386",
                                    "name": "K. Simonyan"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 3607
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al._1, 2023)",
                        "snippets": [
                            "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available."
                        ],
                        "paper": {
                            "corpus_id": 263672058,
                            "title": "Improved Baselines with Visual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2143856368",
                                    "name": "Haotian Liu"
                                },
                                {
                                    "authorId": "2243126534",
                                    "name": "Chunyuan Li"
                                },
                                {
                                    "authorId": "1527091339",
                                    "name": "Yuheng Li"
                                },
                                {
                                    "authorId": "2256122200",
                                    "name": "Yong Jae Lee"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 2824
                        },
                        "score": 0
                    },
                    {
                        "id": "(Younesi et al., 2024)",
                        "snippets": [
                            "One strategy involves employing a dual-stream architecture with distinct CNN and transformer encoders interacting via coattentional transformer layers [97]. The CNN extracts visual features from images, providing contextual information that can guide language generation and comprehension. The transformer architecture models the semantics and syntax of text. Their interaction enables the generation of captions based on image content or the retrieval of pertinent images for textual queries. Alternative methods directly incorporate CNNs within the transformer architecture as visual token encoders that operate with text token encoders [98]. The CNN projections of image patches are appended to text token embeddings as inputs to the transformer layers. This unified architecture allows for end-to-end optimization of parameters for both vision and language tasks."
                        ],
                        "paper": {
                            "corpus_id": 267897956,
                            "title": "A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends",
                            "authors": [
                                {
                                    "authorId": "2260382962",
                                    "name": "Abolfazl Younesi"
                                },
                                {
                                    "authorId": "153631156",
                                    "name": "Mohsen Ansari"
                                },
                                {
                                    "authorId": "2260384993",
                                    "name": "Mohammadamin Fazli"
                                },
                                {
                                    "authorId": "2490667",
                                    "name": "A. Ejlali"
                                },
                                {
                                    "authorId": "2238832044",
                                    "name": "Muhammad Shafique"
                                },
                                {
                                    "authorId": "2247781719",
                                    "name": "Joerg Henkel"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 47
                        },
                        "score": 0.96630859375
                    },
                    {
                        "id": "(Ma et al., 2024)",
                        "snippets": [
                            "One prominent direction involves utilizing pre-trained text-to-image models for high-quality generation while training LLMs to generate conditions for these models (Dong et al., 2023)[25][26][27](Sun et al., 2023). However, this approach introduces architectural complexity and potentially constrains the model's capabilities through maintaining separate LLM and generative components. Alternative approaches [88,97,99,100,108] propose training a single LLM for both tasks, typically incorporating either diffusion models (Ho et al., 2020)[83] or vector-quantized autoregressive models (Esser et al., 2020)[86]",
                            "Another approach extends MLLMs with pre-trained diffusion models [19, 25-27, 87, 101]. However, these systems essentially utilize diffusion models as external tools, where the MLLM generates conditions for image generation without possessing direct generative capabilities. This separation often results in suboptimal performance compared to standalone diffusion models [25,87]. Another line of work [88,97,99,100,108] aim to train a single LLM for both tasks. Many of these methods employ vector-quantization [22,86] to convert images into discrete tokens, enabling unified autoregressive processing [88]97]."
                        ],
                        "paper": {
                            "corpus_id": 273969525,
                            "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
                            "authors": [
                                {
                                    "authorId": "2326805499",
                                    "name": "Yiyang Ma"
                                },
                                {
                                    "authorId": "2326998908",
                                    "name": "Xingchao Liu"
                                },
                                {
                                    "authorId": "2326444644",
                                    "name": "Xi-aokang Chen"
                                },
                                {
                                    "authorId": "2326440587",
                                    "name": "Wen Liu"
                                },
                                {
                                    "authorId": "2330475459",
                                    "name": "Chengyue Wu"
                                },
                                {
                                    "authorId": "2326514370",
                                    "name": "Zhiyu Wu"
                                },
                                {
                                    "authorId": "2327000291",
                                    "name": "Zizheng Pan"
                                },
                                {
                                    "authorId": "2279107352",
                                    "name": "Zhenda Xie"
                                },
                                {
                                    "authorId": "2315951020",
                                    "name": "Haowei Zhang"
                                },
                                {
                                    "authorId": "2279159169",
                                    "name": "Xingkai Yu"
                                },
                                {
                                    "authorId": "2330538675",
                                    "name": "Liang Zhao"
                                },
                                {
                                    "authorId": "2330376936",
                                    "name": "Yisong Wang"
                                },
                                {
                                    "authorId": "2330291409",
                                    "name": "Jiaying Liu"
                                },
                                {
                                    "authorId": "2278217940",
                                    "name": "C. Ruan"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 39
                        },
                        "score": 0.9814453125
                    },
                    {
                        "id": "(Dong et al., 2023)",
                        "snippets": [
                            "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."
                        ],
                        "paper": {
                            "corpus_id": 261975252,
                            "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
                            "authors": [
                                {
                                    "authorId": "2056965063",
                                    "name": "Runpei Dong"
                                },
                                {
                                    "authorId": "2118643247",
                                    "name": "Chunrui Han"
                                },
                                {
                                    "authorId": "2211415443",
                                    "name": "Yuang Peng"
                                },
                                {
                                    "authorId": "3424017",
                                    "name": "Zekun Qi"
                                },
                                {
                                    "authorId": "2242581956",
                                    "name": "Zheng Ge"
                                },
                                {
                                    "authorId": "2161319190",
                                    "name": "Jinrong Yang"
                                },
                                {
                                    "authorId": "48096671",
                                    "name": "Liang Zhao"
                                },
                                {
                                    "authorId": "26913717",
                                    "name": "Jian\u2010Yuan Sun"
                                },
                                {
                                    "authorId": "2157473950",
                                    "name": "Hongyu Zhou"
                                },
                                {
                                    "authorId": "134085586",
                                    "name": "Hao-Ran Wei"
                                },
                                {
                                    "authorId": "2069029697",
                                    "name": "Xiangwen Kong"
                                },
                                {
                                    "authorId": "2185865433",
                                    "name": "Xiangyu Zhang"
                                },
                                {
                                    "authorId": "2244131405",
                                    "name": "Kaisheng Ma"
                                },
                                {
                                    "authorId": "2242612318",
                                    "name": "Li Yi"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 201
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zou et al., 2025)",
                        "snippets": [
                            "Recent studies have emerged that seek to integrate multimodal understanding with visual generation, aiming to develop unified systems capable of handling both tasks simultaneously. Such designs hold the potential to foster mutual enhancement between generation and understanding, offering a promising pathway toward truly unifying all modalities. Numerous studies have sought to preserve the text generation paradigm of LLMs while exploring the impact [46,64,66,67] of integrating diverse visual generation paradigms, such as diffusion models [24], flow-based generative models (Esser et al., 2024)[40], and vector-quantized autoregressive models [56]."
                        ],
                        "paper": {
                            "corpus_id": 276928288,
                            "title": "OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models",
                            "authors": [
                                {
                                    "authorId": "2261949071",
                                    "name": "Jialv Zou"
                                },
                                {
                                    "authorId": "2060439659",
                                    "name": "Bencheng Liao"
                                },
                                {
                                    "authorId": "2261816376",
                                    "name": "Qian Zhang"
                                },
                                {
                                    "authorId": "2257432695",
                                    "name": "Wenyu Liu"
                                },
                                {
                                    "authorId": "2266175736",
                                    "name": "Xinggang Wang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.96435546875
                    },
                    {
                        "id": "(Esser et al., 2024)",
                        "snippets": [
                            "Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available."
                        ],
                        "paper": {
                            "corpus_id": 268247980,
                            "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
                            "authors": [
                                {
                                    "authorId": "35175531",
                                    "name": "Patrick Esser"
                                },
                                {
                                    "authorId": "3411322",
                                    "name": "Sumith Kulal"
                                },
                                {
                                    "authorId": "119843260",
                                    "name": "A. Blattmann"
                                },
                                {
                                    "authorId": "2316859494",
                                    "name": "Rahim Entezari"
                                },
                                {
                                    "authorId": "2188737195",
                                    "name": "Jonas Muller"
                                },
                                {
                                    "authorId": "2289994508",
                                    "name": "Harry Saini"
                                },
                                {
                                    "authorId": "2290013499",
                                    "name": "Yam Levi"
                                },
                                {
                                    "authorId": "2053482699",
                                    "name": "Dominik Lorenz"
                                },
                                {
                                    "authorId": "40562186",
                                    "name": "Axel Sauer"
                                },
                                {
                                    "authorId": "2290014125",
                                    "name": "Frederic Boesel"
                                },
                                {
                                    "authorId": "2221125727",
                                    "name": "Dustin Podell"
                                },
                                {
                                    "authorId": "102541178",
                                    "name": "Tim Dockhorn"
                                },
                                {
                                    "authorId": "2221127565",
                                    "name": "Zion English"
                                },
                                {
                                    "authorId": "2221126982",
                                    "name": "Kyle Lacey"
                                },
                                {
                                    "authorId": "2290014122",
                                    "name": "Alex Goodwin"
                                },
                                {
                                    "authorId": "2290014387",
                                    "name": "Yannik Marek"
                                },
                                {
                                    "authorId": "1660819540",
                                    "name": "Robin Rombach"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1401
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhu et al._1, 2023)",
                        "snippets": [
                            "To expand the capabilities of generating image in a multimodal context, certain efforts, e.g., Visual ChatGPT [47], attempt to connect LLMs with image generation tools in a cascaded pipeline by transferring text messages, which inevitably introduce instability and noise. Alternatively, another line of research achieves it by optimizing models in an end-to-end manner [9,18,23,30]48]. By aligning the output space with the image diffusion models, VL models can not only perceive but also generate images and text."
                        ],
                        "paper": {
                            "corpus_id": 266210376,
                            "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
                            "authors": [
                                {
                                    "authorId": "2274151094",
                                    "name": "Jinguo Zhu"
                                },
                                {
                                    "authorId": "2258784456",
                                    "name": "Xiaohan Ding"
                                },
                                {
                                    "authorId": "152988335",
                                    "name": "Yixiao Ge"
                                },
                                {
                                    "authorId": "51123495",
                                    "name": "Yuying Ge"
                                },
                                {
                                    "authorId": "2254048096",
                                    "name": "Sijie Zhao"
                                },
                                {
                                    "authorId": "2310758544",
                                    "name": "Hengshuang Zhao"
                                },
                                {
                                    "authorId": "2274490604",
                                    "name": "Xiaohua Wang"
                                },
                                {
                                    "authorId": "2265579883",
                                    "name": "Ying Shan"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 37
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Tang et al., 2023)",
                        "snippets": [
                            "We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io"
                        ],
                        "paper": {
                            "corpus_id": 258822817,
                            "title": "Any-to-Any Generation via Composable Diffusion",
                            "authors": [
                                {
                                    "authorId": "151270642",
                                    "name": "Zineng Tang"
                                },
                                {
                                    "authorId": "2155459391",
                                    "name": "Ziyi Yang"
                                },
                                {
                                    "authorId": "8652308",
                                    "name": "Chenguang Zhu"
                                },
                                {
                                    "authorId": "48262024",
                                    "name": "Michael Zeng"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 188
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Multimodal Output Decoder: Lastly, the multimodal output decoder reconverts the fused, integrated multimodal information back into a usable form tailored to specific tasks, such as Image captioning, the decoder might generate descriptive text based on visual inputs. Video understanding tasks, it could produce annotations or summaries combining both visual and auditory data. Each decoder is meticulously designed to optimize accuracy and quality, ensuring that the output precisely reflects the combined insights gained from the integrated modalities."
                        ],
                        "paper": {
                            "corpus_id": 271693411,
                            "title": "A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks",
                            "authors": [
                                {
                                    "authorId": "2136025369",
                                    "name": "Jiaqi Wang"
                                },
                                {
                                    "authorId": "2273631049",
                                    "name": "Hanqi Jiang"
                                },
                                {
                                    "authorId": "2116426849",
                                    "name": "Yi-Hsueh Liu"
                                },
                                {
                                    "authorId": "120688117",
                                    "name": "Chong-Yi Ma"
                                },
                                {
                                    "authorId": "2273584640",
                                    "name": "Xu Zhang"
                                },
                                {
                                    "authorId": "2221032216",
                                    "name": "Yi Pan"
                                },
                                {
                                    "authorId": "2210636248",
                                    "name": "Mengyuan Liu"
                                },
                                {
                                    "authorId": "2314692435",
                                    "name": "Peiran Gu"
                                },
                                {
                                    "authorId": "2314692233",
                                    "name": "Sichen Xia"
                                },
                                {
                                    "authorId": "2284031962",
                                    "name": "Wenjun Li"
                                },
                                {
                                    "authorId": "2257095790",
                                    "name": "Yutong Zhang"
                                },
                                {
                                    "authorId": "2238905102",
                                    "name": "Zihao Wu"
                                },
                                {
                                    "authorId": "2145977326",
                                    "name": "Zheng Liu"
                                },
                                {
                                    "authorId": "2215167446",
                                    "name": "Tianyang Zhong"
                                },
                                {
                                    "authorId": "2257302793",
                                    "name": "Bao Ge"
                                },
                                {
                                    "authorId": "2269508672",
                                    "name": "Tuo Zhang"
                                },
                                {
                                    "authorId": "2251076040",
                                    "name": "Ning Qiang"
                                },
                                {
                                    "authorId": "1742535",
                                    "name": "Xintao Hu"
                                },
                                {
                                    "authorId": "2300922460",
                                    "name": "Xi Jiang"
                                },
                                {
                                    "authorId": "2290203999",
                                    "name": "Xin Zhang"
                                },
                                {
                                    "authorId": "2157449203",
                                    "name": "Wei Zhang"
                                },
                                {
                                    "authorId": "2272672300",
                                    "name": "Dinggang Shen"
                                },
                                {
                                    "authorId": "2301126629",
                                    "name": "Tianming Liu"
                                },
                                {
                                    "authorId": "2277750447",
                                    "name": "Shu Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 36
                        },
                        "score": 0.9619140625
                    },
                    {
                        "id": "(Liu et al._2, 2024)",
                        "snippets": [
                            "Multimodal large language models (MLLMs) demonstrate strong performance across visual tasks, but their efficiency is hindered by significant computational and memory demands from processing long contexts in multimodal inputs."
                        ],
                        "paper": {
                            "corpus_id": 273233771,
                            "title": "PAR: Prompt-Aware Token Reduction Method for Efficient Large Multimodal Models",
                            "authors": [
                                {
                                    "authorId": "2325197953",
                                    "name": "Yingen Liu"
                                },
                                {
                                    "authorId": "2257429787",
                                    "name": "Fan Wu"
                                },
                                {
                                    "authorId": "2262008831",
                                    "name": "Ruihui Li"
                                },
                                {
                                    "authorId": "2242362590",
                                    "name": "Zhuo Tang"
                                },
                                {
                                    "authorId": "2278395518",
                                    "name": "Kenli Li"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Franklin et al., 2025)",
                        "snippets": [
                            "Recent works have expanded their applications and enhanced their architectures to achieve better performance, efficiency, and scalability [21]- (Zhou et al., 2022). \n\nA primary focus in the development of LVLMs has been the design of architectures that effectively unify language and vision modalities. Recent models have proposed end-to-end frameworks that leverage shared embeddings for both text and images, enabling them to excel at tasks requiring fine-grained multimodal reasoning (Zhang et al., 2024). Additionally, techniques such as mixture of experts and relational reasoning mechanisms have been introduced to improve scalability and enhance the relational reasoning capabilities of LVLMs (Lin et al., 2024), [26]."
                        ],
                        "paper": {
                            "corpus_id": 275337159,
                            "title": "Vision-Driven Prompt Optimization for Large Language Models in Multimodal Generative Tasks",
                            "authors": [
                                {
                                    "authorId": "2338834916",
                                    "name": "Leo Franklin"
                                },
                                {
                                    "authorId": "2324583118",
                                    "name": "Apiradee Boonmee"
                                },
                                {
                                    "authorId": "2324586634",
                                    "name": "Kritsada Wongsuwan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.97509765625
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision language model that supports long-contextual input and output. IXC-2.5 excels in various text-image comprehension and composition applications, achieving GPT-4V level capabilities with merely 7B LLM backend. Trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in tasks requiring extensive input and output contexts. Compared to its previous 2.0 version, InternLM-XComposer-2.5 features three major upgrades in vision-language comprehension: (1) Ultra-High Resolution Understanding, (2) Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In addition to comprehension, IXC-2.5 extends to two compelling applications using extra LoRA parameters for text-image composition: (1) Crafting Webpages and (2) Composing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28 benchmarks, outperforming existing open-source state-of-the-art models on 16 benchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on 16 key tasks. The InternLM-XComposer-2.5 is publicly available at https://github.com/InternLM/InternLM-XComposer."
                        ],
                        "paper": {
                            "corpus_id": 270923961,
                            "title": "InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output",
                            "authors": [
                                {
                                    "authorId": "2213750400",
                                    "name": "Pan Zhang"
                                },
                                {
                                    "authorId": "2118187561",
                                    "name": "Xiao-wen Dong"
                                },
                                {
                                    "authorId": "12862495",
                                    "name": "Yuhang Zang"
                                },
                                {
                                    "authorId": "50206929",
                                    "name": "Yuhang Cao"
                                },
                                {
                                    "authorId": "2274346959",
                                    "name": "Rui Qian"
                                },
                                {
                                    "authorId": "2307935043",
                                    "name": "Lin Chen"
                                },
                                {
                                    "authorId": "2303800191",
                                    "name": "Qipeng Guo"
                                },
                                {
                                    "authorId": "31463937",
                                    "name": "Haodong Duan"
                                },
                                {
                                    "authorId": "2256857728",
                                    "name": "Bin Wang"
                                },
                                {
                                    "authorId": "2161162356",
                                    "name": "Linke Ouyang"
                                },
                                {
                                    "authorId": "2266356137",
                                    "name": "Songyang Zhang"
                                },
                                {
                                    "authorId": "2266359401",
                                    "name": "Wenwei Zhang"
                                },
                                {
                                    "authorId": "47002704",
                                    "name": "Yining Li"
                                },
                                {
                                    "authorId": "2287681221",
                                    "name": "Yang Gao"
                                },
                                {
                                    "authorId": "2075416290",
                                    "name": "Peng Sun"
                                },
                                {
                                    "authorId": "2268020798",
                                    "name": "Xinyue Zhang"
                                },
                                {
                                    "authorId": "2268754066",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2281785148",
                                    "name": "Jingwen Li"
                                },
                                {
                                    "authorId": "2257133501",
                                    "name": "Wenhai Wang"
                                },
                                {
                                    "authorId": "2282113542",
                                    "name": "Hang Yan"
                                },
                                {
                                    "authorId": "2267889334",
                                    "name": "Conghui He"
                                },
                                {
                                    "authorId": "2298585927",
                                    "name": "Xingcheng Zhang"
                                },
                                {
                                    "authorId": "2250666679",
                                    "name": "Kai Chen"
                                },
                                {
                                    "authorId": "2292283383",
                                    "name": "Jifeng Dai"
                                },
                                {
                                    "authorId": "2284394767",
                                    "name": "Yu Qiao"
                                },
                                {
                                    "authorId": "2269713956",
                                    "name": "Dahua Lin"
                                },
                                {
                                    "authorId": "2267494294",
                                    "name": "Jiaqi Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 117
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lin et al., 2024)",
                        "snippets": [
                            "Recent advances demonstrate that scaling Large Vision-Language Models (LVLMs) effectively improves downstream task performances. However, existing scaling methods enable all model parameters to be active for each token in the calculation, which brings massive training and inferring costs. In this work, we propose a simple yet effective training strategy MoE-Tuning for LVLMs. This strategy innovatively addresses the common issue of performance degradation in multi-modal sparsity learning, consequently constructing a sparse model with an outrageous number of parameters but a constant computational cost. Furthermore, we present the MoE-LLaVA, a MoE-based sparse LVLM architecture, which uniquely activates only the top-k experts through routers during deployment, keeping the remaining experts inactive. Extensive experiments show the significant performance of MoE-LLaVA in a variety of visual understanding and object hallucination benchmarks. Remarkably, with only approximately 3B sparsely activated parameters, MoE-LLaVA demonstrates performance comparable to the LLaVA-1.5-7B on various visual understanding datasets and even surpasses the LLaVA-1.5-13B in object hallucination benchmark. Through MoE-LLaVA, we aim to establish a baseline for sparse LVLMs and provide valuable insights for future research in developing more efficient and effective multi-modal learning systems. Code is released at https://github.com/PKU-YuanGroup/MoE-LLaVA."
                        ],
                        "paper": {
                            "corpus_id": 267311517,
                            "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models",
                            "authors": [
                                {
                                    "authorId": "2254329478",
                                    "name": "Bin Lin"
                                },
                                {
                                    "authorId": "2275126715",
                                    "name": "Zhenyu Tang"
                                },
                                {
                                    "authorId": "2267264117",
                                    "name": "Yang Ye"
                                },
                                {
                                    "authorId": "2220852923",
                                    "name": "Jiaxi Cui"
                                },
                                {
                                    "authorId": "2256419313",
                                    "name": "Bin Zhu"
                                },
                                {
                                    "authorId": "2185571736",
                                    "name": "Peng Jin"
                                },
                                {
                                    "authorId": "2000011573",
                                    "name": "Jinfa Huang"
                                },
                                {
                                    "authorId": "2253845661",
                                    "name": "Junwu Zhang"
                                },
                                {
                                    "authorId": "2253434320",
                                    "name": "Munan Ning"
                                },
                                {
                                    "authorId": "2266507381",
                                    "name": "Li Yuan"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 178
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jiang et al., 2025)",
                        "snippets": [
                            "LMMs are capable of simultaneously processing and understanding data from different modalities, such as vision, language, haptic and auditory. These models achieve comprehensive processing and reasoning of multimodal information by integrating the features of various modalities in a unified high-dimensional space. They utilize advanced neural network architectures, such as transformers and diffusion models, to extract features from each modality and optimize their representations through techniques such as contrastive learning and self-supervised learning",
                            "these LMMs can also perform cross-modal reasoning and prediction, such as generating images or audio from text."
                        ],
                        "paper": {
                            "corpus_id": 278339632,
                            "title": "A Comprehensive Survey of Large AI Models for Future Communications: Foundations, Applications and Challenges",
                            "authors": [
                                {
                                    "authorId": "40540111",
                                    "name": "Feibo Jiang"
                                },
                                {
                                    "authorId": "2290889674",
                                    "name": "Cunhua Pan"
                                },
                                {
                                    "authorId": "2152288497",
                                    "name": "Li Dong"
                                },
                                {
                                    "authorId": "2244014700",
                                    "name": "Kezhi Wang"
                                },
                                {
                                    "authorId": "145118318",
                                    "name": "M. Debbah"
                                },
                                {
                                    "authorId": "2340230621",
                                    "name": "Dusit Niyato"
                                },
                                {
                                    "authorId": "2267902891",
                                    "name": "Zhu Han"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9619140625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Integration Mechanisms Between Language and Vision Modalities",
                "tldr": "Multimodal language models employ various integration mechanisms to align language and vision modalities, including projection layers, cross-attention mechanisms, and unified token spaces. These approaches range from adapter-based methods that connect pre-trained components to early fusion techniques that process all modalities uniformly within a shared architecture. (16 sources)",
                "text": "\n## Cross-Modal Alignment Approaches\n\nA fundamental challenge in multimodal systems is effectively aligning information between different modality spaces. Several approaches have emerged to address this challenge. One common method employs projection layers that map between visual and language embedding spaces. For example, Emu uses an efficient mapping network that translates text representations from an LLM into the embedding space of visual models, enabling leverage of strong text representations for visual outputs <Paper corpusId=\"258959284\" paperTitle=\"(Koh et al., 2023)\" isShortName></Paper>. Similarly, LLaVA adopts a simple MLP projector to connect a pre-trained CLIP visual encoder with a language model <Paper corpusId=\"270391394\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>.\n\nAnother prominent approach involves cross-attention mechanisms that enable different modalities to attend to each other. The Flamingo model pioneered the use of gated cross-attention to integrate images and text <Paper corpusId=\"273350685\" paperTitle=\"(Cao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"248476411\" paperTitle=\"(Alayrac et al., 2022)\" isShortName></Paper>. This allows the model to selectively focus on relevant aspects of each modality during processing, creating dynamic relationships between visual and textual elements.\n\n## Specialized Connector Modules\n\nBeyond simple projection layers, more sophisticated connector modules have been developed to bridge modalities. The BLIP series introduced the Q-Former module, which employs learnable input queries that interact with image features and the language model <Paper corpusId=\"273350685\" paperTitle=\"(Cao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"256390509\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper>. This approach allows for effective cross-modal learning while maintaining the versatility of the underlying language model.\n\nImageBind represents another important advancement in cross-modal integration, learning a joint embedding space across six different modalities (images, text, audio, depth, thermal, and IMU data) <Paper corpusId=\"270391394\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258564264\" paperTitle=\"(Girdhar et al., 2023)\" isShortName></Paper>. What makes ImageBind particularly notable is that it demonstrates that all combinations of paired data are not necessary to train such a joint embedding, and that image-paired data alone is sufficient to bind multiple modalities together.\n\n## Dual-Stream vs. Early Fusion Architectures\n\nTwo major architectural paradigms have emerged for integrating language and vision: dual-stream and early fusion approaches. Dual-stream architectures employ separate encoders for different modalities with interaction mechanisms between them. One implementation involves a distinct CNN encoder for visual features and a transformer encoder for text, interacting via co-attentional transformer layers <Paper corpusId=\"267897956\" paperTitle=\"(Younesi et al., 2024)\" isShortName></Paper>. The CNN extracts visual features providing contextual information to guide language generation, while the transformer handles text semantics.\n\nIn contrast, early fusion approaches represent all modalities within a unified token space. Chameleon exemplifies this approach as a family of token-based mixed-modal models capable of processing images and text in any arbitrary sequence by representing all modalities as discrete tokens <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>. AnyGPT similarly utilizes discrete representations for unified processing of various modalities, including speech, text, images, and music <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\nMore recent advancements like UniFluid demonstrate that continuous visual tokens can be effectively used within an autoregressive architecture to jointly handle vision-language generation and understanding tasks <Paper corpusId=\"277103675\" paperTitle=\"(Fan et al., 2025)\" isShortName></Paper>. This approach embeds both text and continuous visual inputs as tokens in the same space, enabling seamless joint training.\n\n## Low-Rank Adaptation for Efficient Integration\n\nLow-Rank Adaptation (LoRA) has emerged as an efficient technique for enhancing multimodal capabilities. The GPT4Tools framework utilizes LoRA optimization to enhance language models' ability to use multimodal tools <Paper corpusId=\"266573642\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. This approach enables fine-tuning with minimal parameter updates, making it more accessible and computationally efficient to integrate visual capabilities into existing language models.\n\n## Integration for Bidirectional Generation\n\nFor models capable of both text-to-image and image-to-text generation, specialized integration mechanisms have been developed. One approach involves integrating diffusion-based generation with language models by training LLMs to generate conditions for pre-trained text-to-image diffusion models <Paper corpusId=\"273969525\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. DreamLLM exemplifies this approach through generative modeling of both language and image posteriors via direct sampling in the multimodal space <Paper corpusId=\"273969525\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper> <Paper corpusId=\"261975252\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>.\n\nAlternative integration methods for bidirectional generation include combining bidirectional image diffusion with autoregressive text prediction within a unified framework, as demonstrated by Transfusion and Show-O <Paper corpusId=\"276782523\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>. Similarly, the SEED-X model combines continuous image representations from a CLIP ViT encoder with text tokens in a shared architecture <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\nRecent work has also explored integrating various visual generation paradigms, such as diffusion models, flow-based generative models, and vector-quantized autoregressive models with LLMs <Paper corpusId=\"276928288\" paperTitle=\"(Zou et al., 2025)\" isShortName></Paper>. This represents a convergence toward more flexible integration mechanisms that can accommodate multiple generative approaches within a unified framework.\n\nThe diversity of integration mechanisms continues to evolve, with research increasingly focused on approaches that allow for more seamless interaction between modalities while enabling both understanding and generation tasks within a unified model architecture <Paper corpusId=\"278327487\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Koh et al., 2023)",
                        "snippets": [
                            "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs."
                        ],
                        "paper": {
                            "corpus_id": 258959284,
                            "title": "Generating Images with Multimodal Language Models",
                            "authors": [
                                {
                                    "authorId": "23978705",
                                    "name": "Jing Yu Koh"
                                },
                                {
                                    "authorId": "47070750",
                                    "name": "Daniel Fried"
                                },
                                {
                                    "authorId": "145124475",
                                    "name": "R. Salakhutdinov"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 259
                        },
                        "score": 0.98779296875
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "Notable examples include Gemini [111], GPT-4V, and ImageBind (Girdhar et al., 2023).These models showcase the ability to process text, images, audio, and video, enhancing functionalities such as translation, image recognition, and more",
                            "BLIP-2 [112] introduces a novel approach to integrate a frozen pre-trained visual encoder with a frozen large language model using a Q-former module.This module employs learnable input queries that interact with image features and the LLM, allowing for effective cross-modal learning.This setup helps maintain the versatility of the LLM while incorporating visual information effectively.LLava [113] is a represent large multimodal model integrating a pre-trained CLIP [116] visual encoder (ViT-L/14), the Vicuna [115] language model, and a simple linear projection layer."
                        ],
                        "paper": {
                            "corpus_id": 270391394,
                            "title": "Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey",
                            "authors": [
                                {
                                    "authorId": "2261895888",
                                    "name": "Hao Yang"
                                },
                                {
                                    "authorId": "49339265",
                                    "name": "Yanyan Zhao"
                                },
                                {
                                    "authorId": "143792101",
                                    "name": "Yang Wu"
                                },
                                {
                                    "authorId": "2214828656",
                                    "name": "Shilong Wang"
                                },
                                {
                                    "authorId": "2261492452",
                                    "name": "Tian Zheng"
                                },
                                {
                                    "authorId": "2261686105",
                                    "name": "Hongbo Zhang"
                                },
                                {
                                    "authorId": "2279751409",
                                    "name": "Wanxiang Che"
                                },
                                {
                                    "authorId": "2203961541",
                                    "name": "Bing Qin"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 13
                        },
                        "score": 0.9580078125
                    },
                    {
                        "id": "(Liu et al._1, 2023)",
                        "snippets": [
                            "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available."
                        ],
                        "paper": {
                            "corpus_id": 263672058,
                            "title": "Improved Baselines with Visual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2143856368",
                                    "name": "Haotian Liu"
                                },
                                {
                                    "authorId": "2243126534",
                                    "name": "Chunyuan Li"
                                },
                                {
                                    "authorId": "1527091339",
                                    "name": "Yuheng Li"
                                },
                                {
                                    "authorId": "2256122200",
                                    "name": "Yong Jae Lee"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 2824
                        },
                        "score": 0
                    },
                    {
                        "id": "(Cao et al., 2024)",
                        "snippets": [
                            "Multimodal Large Language Models (MLLMs) integrate visual representations from images with linguistic embeddings from text, thereby enhancing the models' capabilities in comprehending and generating language descriptions of visual content. Most open-source MLLMs employ architectures that include a pre-trained vision encoder, an LLM, and a crossmodal connector. Early models, such as the BLIP series (Dai et al., 2023), (Li et al., 2023), utilized the Q-Former module to align text and images, thus improving multimodal capabilities. Flamingo (Alayrac et al., 2022) employed a gated cross-attention mechanism to integrate images and text. LLaVA-1.5 (Liu et al., 2023) adopted an MLP projector to connect the pre-trained vision encoder with the LLM. InternVL (Chen et al., 2023), [6] employed a dynamic resolution strategy, segmenting images into tiles and encoding both the tiles and the thumbnail view together. It then uses a pixel shuffle operation to reduce the number of visual tokens before integrating these features with the LLM through an MLP projector."
                        ],
                        "paper": {
                            "corpus_id": 273350685,
                            "title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding",
                            "authors": [
                                {
                                    "authorId": "2312369775",
                                    "name": "Yue Cao"
                                },
                                {
                                    "authorId": "2312345209",
                                    "name": "Yangzhou Liu"
                                },
                                {
                                    "authorId": "2305731793",
                                    "name": "Zhe Chen"
                                },
                                {
                                    "authorId": "2146870036",
                                    "name": "Guangchen Shi"
                                },
                                {
                                    "authorId": "2257133501",
                                    "name": "Wenhai Wang"
                                },
                                {
                                    "authorId": "2326454048",
                                    "name": "Danhuai Zhao"
                                },
                                {
                                    "authorId": "2276323159",
                                    "name": "Tong Lu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.9599609375
                    },
                    {
                        "id": "(Alayrac et al., 2022)",
                        "snippets": [
                            "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data."
                        ],
                        "paper": {
                            "corpus_id": 248476411,
                            "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
                            "authors": [
                                {
                                    "authorId": "2285263",
                                    "name": "Jean-Baptiste Alayrac"
                                },
                                {
                                    "authorId": "7408951",
                                    "name": "Jeff Donahue"
                                },
                                {
                                    "authorId": "152831141",
                                    "name": "Pauline Luc"
                                },
                                {
                                    "authorId": "19200186",
                                    "name": "Antoine Miech"
                                },
                                {
                                    "authorId": "2159207795",
                                    "name": "Iain Barr"
                                },
                                {
                                    "authorId": "66535271",
                                    "name": "Yana Hasson"
                                },
                                {
                                    "authorId": "3257286",
                                    "name": "Karel Lenc"
                                },
                                {
                                    "authorId": "1697879",
                                    "name": "A. Mensch"
                                },
                                {
                                    "authorId": "2143434227",
                                    "name": "Katie Millican"
                                },
                                {
                                    "authorId": "47447264",
                                    "name": "Malcolm Reynolds"
                                },
                                {
                                    "authorId": "81387328",
                                    "name": "Roman Ring"
                                },
                                {
                                    "authorId": "2143538252",
                                    "name": "Eliza Rutherford"
                                },
                                {
                                    "authorId": "12159303",
                                    "name": "Serkan Cabi"
                                },
                                {
                                    "authorId": "22237490",
                                    "name": "Tengda Han"
                                },
                                {
                                    "authorId": "48398849",
                                    "name": "Zhitao Gong"
                                },
                                {
                                    "authorId": "2412073",
                                    "name": "Sina Samangooei"
                                },
                                {
                                    "authorId": "49601928",
                                    "name": "Marianne Monteiro"
                                },
                                {
                                    "authorId": "10698483",
                                    "name": "Jacob Menick"
                                },
                                {
                                    "authorId": "148016269",
                                    "name": "Sebastian Borgeaud"
                                },
                                {
                                    "authorId": "2065040422",
                                    "name": "Andy Brock"
                                },
                                {
                                    "authorId": "3208081",
                                    "name": "Aida Nematzadeh"
                                },
                                {
                                    "authorId": "7782886",
                                    "name": "Sahand Sharifzadeh"
                                },
                                {
                                    "authorId": "9961753",
                                    "name": "Mikolaj Binkowski"
                                },
                                {
                                    "authorId": "2026369796",
                                    "name": "Ricardo Barreira"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "1688869",
                                    "name": "Andrew Zisserman"
                                },
                                {
                                    "authorId": "34838386",
                                    "name": "K. Simonyan"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 3607
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al._1, 2023)",
                        "snippets": [
                            "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions."
                        ],
                        "paper": {
                            "corpus_id": 256390509,
                            "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                            "authors": [
                                {
                                    "authorId": "49299019",
                                    "name": "Junnan Li"
                                },
                                {
                                    "authorId": "2981509",
                                    "name": "Dongxu Li"
                                },
                                {
                                    "authorId": "1702137",
                                    "name": "S. Savarese"
                                },
                                {
                                    "authorId": "2184854289",
                                    "name": "Steven C. H. Hoi"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 4653
                        },
                        "score": 0
                    },
                    {
                        "id": "(Girdhar et al., 2023)",
                        "snippets": [
                            "We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications \u2018out-of-the-box\u2019 including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks."
                        ],
                        "paper": {
                            "corpus_id": 258564264,
                            "title": "ImageBind One Embedding Space to Bind Them All",
                            "authors": [
                                {
                                    "authorId": "3102850",
                                    "name": "Rohit Girdhar"
                                },
                                {
                                    "authorId": "1388811741",
                                    "name": "Alaaeldin El-Nouby"
                                },
                                {
                                    "authorId": "2109168016",
                                    "name": "Zhuang Liu"
                                },
                                {
                                    "authorId": "152964870",
                                    "name": "Mannat Singh"
                                },
                                {
                                    "authorId": "3085301",
                                    "name": "Kalyan Vasudev Alwala"
                                },
                                {
                                    "authorId": "2319608",
                                    "name": "Armand Joulin"
                                },
                                {
                                    "authorId": "1806773",
                                    "name": "Ishan Misra"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 941
                        },
                        "score": 0
                    },
                    {
                        "id": "(Younesi et al., 2024)",
                        "snippets": [
                            "One strategy involves employing a dual-stream architecture with distinct CNN and transformer encoders interacting via coattentional transformer layers [97]. The CNN extracts visual features from images, providing contextual information that can guide language generation and comprehension. The transformer architecture models the semantics and syntax of text. Their interaction enables the generation of captions based on image content or the retrieval of pertinent images for textual queries. Alternative methods directly incorporate CNNs within the transformer architecture as visual token encoders that operate with text token encoders [98]. The CNN projections of image patches are appended to text token embeddings as inputs to the transformer layers. This unified architecture allows for end-to-end optimization of parameters for both vision and language tasks."
                        ],
                        "paper": {
                            "corpus_id": 267897956,
                            "title": "A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends",
                            "authors": [
                                {
                                    "authorId": "2260382962",
                                    "name": "Abolfazl Younesi"
                                },
                                {
                                    "authorId": "153631156",
                                    "name": "Mohsen Ansari"
                                },
                                {
                                    "authorId": "2260384993",
                                    "name": "Mohammadamin Fazli"
                                },
                                {
                                    "authorId": "2490667",
                                    "name": "A. Ejlali"
                                },
                                {
                                    "authorId": "2238832044",
                                    "name": "Muhammad Shafique"
                                },
                                {
                                    "authorId": "2247781719",
                                    "name": "Joerg Henkel"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 47
                        },
                        "score": 0.96630859375
                    },
                    {
                        "id": "(Xie et al., 2024)",
                        "snippets": [
                            "In recent years, an increasing number of studies (Ge et al., 2024;Wu et al., 2023b;(Tang et al., 2023)Ye et al., 2024a;(Dong et al., 2023)Aiello et al., 2024) have focused on unified multimodal language models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;Sun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive modeling to generate images. SEED-X (Ge et al., 2024) proposes a unified and versatile foundation system capable of handling both multimodal understanding and generation tasks. In this approach, continuous image representations from CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model (LLM) to perform next-word prediction and image representation regression. DreamLLM (Dong et al., 2023)) also explores the potential of enabling multimodal comprehension and creation based on LLMs. Chameleon (Team, 2024) introduces a family of token-based mixed-modal models capable of both comprehending and generating images. This approach represents all modalities as discrete tokens and utilizes a unified transformer-based architecture and trains the model from scratch in an end-to-end manner."
                        ],
                        "paper": {
                            "corpus_id": 271924334,
                            "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation",
                            "authors": [
                                {
                                    "authorId": "2306603347",
                                    "name": "Jinheng Xie"
                                },
                                {
                                    "authorId": "2258958963",
                                    "name": "Weijia Mao"
                                },
                                {
                                    "authorId": "2237427303",
                                    "name": "Zechen Bai"
                                },
                                {
                                    "authorId": "2248388634",
                                    "name": "David Junhao Zhang"
                                },
                                {
                                    "authorId": "2316668014",
                                    "name": "Weihao Wang"
                                },
                                {
                                    "authorId": "2298569956",
                                    "name": "Kevin Qinghong Lin"
                                },
                                {
                                    "authorId": "2248678348",
                                    "name": "Yuchao Gu"
                                },
                                {
                                    "authorId": "2316662510",
                                    "name": "Zhijie Chen"
                                },
                                {
                                    "authorId": "2309246065",
                                    "name": "Zhenheng Yang"
                                },
                                {
                                    "authorId": "2244623976",
                                    "name": "Mike Zheng Shou"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 229
                        },
                        "score": 0.9619140625
                    },
                    {
                        "id": "(Fan et al., 2025)",
                        "snippets": [
                            "We present UniFluid, a unified autoregressive framework for joint visual generation and understanding leveraging continuous visual tokens. Our unified autoregressive architecture processes multimodal image and text inputs, generating discrete tokens for text and continuous tokens for image",
                            "UniFluid, a unified framework that leverages continuous visual tokens within an autoregressive architecture to jointly handle vision-language generation and understanding tasks. Building upon pre-trained Gemma [48] on large-scale text corpus, UniFluid unlocks powerful visual generation and understanding capabilities through training with paired image-text data, and further allows these two tasks to mutually benefit each other within a single architecture. Specifically, UniFluid adopts a unified autoregressive framework where both text and continuous visual inputs are embedded as tokens in the same space, enabling seamless joint training of image generation and understanding tasks. UniFluid integrates a continuous tokenizer [12,(Li et al., 2024) for image generation and a pre-trained SigLIP (Zhai et al., 2023) image encoder for understanding tasks, while textual inputs are processed using a standard SentencePiece tokenizer [20]. The resulting multimodal sequences are modeled autoregressively using Gemma [48] as the underlying transformer backbone. Task-specific prediction heads-a diffusion-based head for image generation and a cross-entropy head for text generation-ensure effective modality-specific training and inference, enabling UniFluid to efficiently learn shared representations that mutually enhance its generation and understanding capabilities."
                        ],
                        "paper": {
                            "corpus_id": 277103675,
                            "title": "Unified Autoregressive Visual Generation and Understanding with Continuous Tokens",
                            "authors": [
                                {
                                    "authorId": "2347484699",
                                    "name": "Lijie Fan"
                                },
                                {
                                    "authorId": "34689393",
                                    "name": "Luming Tang"
                                },
                                {
                                    "authorId": "2333872078",
                                    "name": "Siyang Qin"
                                },
                                {
                                    "authorId": "2307269819",
                                    "name": "Tianhong Li"
                                },
                                {
                                    "authorId": "2350843695",
                                    "name": "Xuan Yang"
                                },
                                {
                                    "authorId": "2266238535",
                                    "name": "Siyuan Qiao"
                                },
                                {
                                    "authorId": "2350755056",
                                    "name": "Andreas Steiner"
                                },
                                {
                                    "authorId": "2333512682",
                                    "name": "Chen Sun"
                                },
                                {
                                    "authorId": "2281034398",
                                    "name": "Yuanzhen Li"
                                },
                                {
                                    "authorId": "2351408307",
                                    "name": "Tao Zhu"
                                },
                                {
                                    "authorId": "2325950285",
                                    "name": "Michael Rubinstein"
                                },
                                {
                                    "authorId": "3063676",
                                    "name": "Michalis Raptis"
                                },
                                {
                                    "authorId": "2262516617",
                                    "name": "Deqing Sun"
                                },
                                {
                                    "authorId": "1737285",
                                    "name": "Radu Soricut"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.9794921875
                    },
                    {
                        "id": "(Huang et al., 2023)",
                        "snippets": [
                            "GPT4Tools [63] enables open-source language models to effectively use multimodal tools. It constructs a tool-related instructional dataset from advanced language models and utilizes Low-Rank Adaptation (LoRA) optimization to enhance the language models' tool-usage capabilities. Additionally, it proposes a benchmark to evaluate the accuracy of language models in using tools, demonstrating significant improvements in tool usage across various visual tasks. As shown in Figure 17, the GPT4Tools framework involves constructing a tool-related instruction dataset by prompting an advanced language model with various multimodal contexts. This dataset is then used to fine-tune open-source language models using Low-Rank Adaptation (LoRA) optimization, enabling them to effectively use tools for visual tasks such as comprehension and image generation.\n\nTextBind [72] enhances large language models with multi-turn interleaved multimodal instruction-following capabilities. It significantly reduces the need for high-quality exemplar data, making it more accessible and scalable for real-world tasks. The proposed model, MIM, trained on TextBind, outperforms recent baselines in open-world multimodal conversations, demonstrating remarkable performance in textual response generation, image generation, and overall multimodal instruction-following. As shown in Figure 18, MIM seamlessly integrates image encoder and decoder models to accommodate interleaved image-text inputs and outputs. It supplements large language models with visual input and output modules, enabling the model to process multi-turn interleaved multimodal instructions and generate coherent responses."
                        ],
                        "paper": {
                            "corpus_id": 266573642,
                            "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
                            "authors": [
                                {
                                    "authorId": "2115941903",
                                    "name": "Jiaxing Huang"
                                },
                                {
                                    "authorId": "2276743977",
                                    "name": "Jingyi Zhang"
                                },
                                {
                                    "authorId": "2276609830",
                                    "name": "Kai Jiang"
                                },
                                {
                                    "authorId": "49660254",
                                    "name": "Han Qiu"
                                },
                                {
                                    "authorId": "2237947102",
                                    "name": "Shijian Lu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 23
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Ma et al., 2024)",
                        "snippets": [
                            "One prominent direction involves utilizing pre-trained text-to-image models for high-quality generation while training LLMs to generate conditions for these models (Dong et al., 2023)[25][26][27](Sun et al., 2023). However, this approach introduces architectural complexity and potentially constrains the model's capabilities through maintaining separate LLM and generative components. Alternative approaches [88,97,99,100,108] propose training a single LLM for both tasks, typically incorporating either diffusion models (Ho et al., 2020)[83] or vector-quantized autoregressive models (Esser et al., 2020)[86]",
                            "Another approach extends MLLMs with pre-trained diffusion models [19, 25-27, 87, 101]. However, these systems essentially utilize diffusion models as external tools, where the MLLM generates conditions for image generation without possessing direct generative capabilities. This separation often results in suboptimal performance compared to standalone diffusion models [25,87]. Another line of work [88,97,99,100,108] aim to train a single LLM for both tasks. Many of these methods employ vector-quantization [22,86] to convert images into discrete tokens, enabling unified autoregressive processing [88]97]."
                        ],
                        "paper": {
                            "corpus_id": 273969525,
                            "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
                            "authors": [
                                {
                                    "authorId": "2326805499",
                                    "name": "Yiyang Ma"
                                },
                                {
                                    "authorId": "2326998908",
                                    "name": "Xingchao Liu"
                                },
                                {
                                    "authorId": "2326444644",
                                    "name": "Xi-aokang Chen"
                                },
                                {
                                    "authorId": "2326440587",
                                    "name": "Wen Liu"
                                },
                                {
                                    "authorId": "2330475459",
                                    "name": "Chengyue Wu"
                                },
                                {
                                    "authorId": "2326514370",
                                    "name": "Zhiyu Wu"
                                },
                                {
                                    "authorId": "2327000291",
                                    "name": "Zizheng Pan"
                                },
                                {
                                    "authorId": "2279107352",
                                    "name": "Zhenda Xie"
                                },
                                {
                                    "authorId": "2315951020",
                                    "name": "Haowei Zhang"
                                },
                                {
                                    "authorId": "2279159169",
                                    "name": "Xingkai Yu"
                                },
                                {
                                    "authorId": "2330538675",
                                    "name": "Liang Zhao"
                                },
                                {
                                    "authorId": "2330376936",
                                    "name": "Yisong Wang"
                                },
                                {
                                    "authorId": "2330291409",
                                    "name": "Jiaying Liu"
                                },
                                {
                                    "authorId": "2278217940",
                                    "name": "C. Ruan"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 39
                        },
                        "score": 0.9814453125
                    },
                    {
                        "id": "(Dong et al., 2023)",
                        "snippets": [
                            "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."
                        ],
                        "paper": {
                            "corpus_id": 261975252,
                            "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
                            "authors": [
                                {
                                    "authorId": "2056965063",
                                    "name": "Runpei Dong"
                                },
                                {
                                    "authorId": "2118643247",
                                    "name": "Chunrui Han"
                                },
                                {
                                    "authorId": "2211415443",
                                    "name": "Yuang Peng"
                                },
                                {
                                    "authorId": "3424017",
                                    "name": "Zekun Qi"
                                },
                                {
                                    "authorId": "2242581956",
                                    "name": "Zheng Ge"
                                },
                                {
                                    "authorId": "2161319190",
                                    "name": "Jinrong Yang"
                                },
                                {
                                    "authorId": "48096671",
                                    "name": "Liang Zhao"
                                },
                                {
                                    "authorId": "26913717",
                                    "name": "Jian\u2010Yuan Sun"
                                },
                                {
                                    "authorId": "2157473950",
                                    "name": "Hongyu Zhou"
                                },
                                {
                                    "authorId": "134085586",
                                    "name": "Hao-Ran Wei"
                                },
                                {
                                    "authorId": "2069029697",
                                    "name": "Xiangwen Kong"
                                },
                                {
                                    "authorId": "2185865433",
                                    "name": "Xiangyu Zhang"
                                },
                                {
                                    "authorId": "2244131405",
                                    "name": "Kaisheng Ma"
                                },
                                {
                                    "authorId": "2242612318",
                                    "name": "Li Yi"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 201
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhao et al., 2025)",
                        "snippets": [
                            "Unified multimodal generative models aim to bridge the gap between understanding and generation tasks, and integrate vision and language into a single framework, enabling the model to learn shared representations across modalities [1,(Aiello et al., 2023)(Dong et al., 2023)19,(Sun et al., 2023)62,71,72,74,76,77,79,85]. SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM (Dong et al., 2023) extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities."
                        ],
                        "paper": {
                            "corpus_id": 276782523,
                            "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
                            "authors": [
                                {
                                    "authorId": "2248946985",
                                    "name": "Rui Zhao"
                                },
                                {
                                    "authorId": "2258958963",
                                    "name": "Weijia Mao"
                                },
                                {
                                    "authorId": "2344762475",
                                    "name": "Mike Zheng Shou"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9658203125
                    },
                    {
                        "id": "(Zou et al., 2025)",
                        "snippets": [
                            "Recent studies have emerged that seek to integrate multimodal understanding with visual generation, aiming to develop unified systems capable of handling both tasks simultaneously. Such designs hold the potential to foster mutual enhancement between generation and understanding, offering a promising pathway toward truly unifying all modalities. Numerous studies have sought to preserve the text generation paradigm of LLMs while exploring the impact [46,64,66,67] of integrating diverse visual generation paradigms, such as diffusion models [24], flow-based generative models (Esser et al., 2024)[40], and vector-quantized autoregressive models [56]."
                        ],
                        "paper": {
                            "corpus_id": 276928288,
                            "title": "OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models",
                            "authors": [
                                {
                                    "authorId": "2261949071",
                                    "name": "Jialv Zou"
                                },
                                {
                                    "authorId": "2060439659",
                                    "name": "Bencheng Liao"
                                },
                                {
                                    "authorId": "2261816376",
                                    "name": "Qian Zhang"
                                },
                                {
                                    "authorId": "2257432695",
                                    "name": "Wenyu Liu"
                                },
                                {
                                    "authorId": "2266175736",
                                    "name": "Xinggang Wang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.96435546875
                    },
                    {
                        "id": "(Zhang et al., 2025)",
                        "snippets": [
                            "In this section, we primarily focus on unified multimodal models that support vision-language understanding and generation, i.e., models that take both image and text as input and produce either text or image as output. As shown in Fig. 5, existing unified models can be broadly categorized into three main types: diffusion models, autoregressive models, and fused AR + diffusion models. For autoregressive models, we further classify them based on their modality encoding methods into four subcategories: pixel-based encoding, semantic-based encoding, learnable query-based encoding, and hybrid encoding. Each of these encoding strategies represents different ways of handling visual and textual data, leading to varying levels of integration and flexibility in the multimodal representations. Fused AR + diffusion models are divided into two subcategories based on modality encoding: pixel-based encoding and hybrid encoding. These models combine aspects of both autoregressive and diffusion techniques, offering a promising approach to more unified and efficient multimodal generation."
                        ],
                        "paper": {
                            "corpus_id": 278327487,
                            "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
                            "authors": [
                                {
                                    "authorId": "2359211182",
                                    "name": "Xinjie Zhang"
                                },
                                {
                                    "authorId": "2359201363",
                                    "name": "Jintao Guo"
                                },
                                {
                                    "authorId": "2337775059",
                                    "name": "Shanshan Zhao"
                                },
                                {
                                    "authorId": "2345947101",
                                    "name": "Minghao Fu"
                                },
                                {
                                    "authorId": "2275153684",
                                    "name": "Lunhao Duan"
                                },
                                {
                                    "authorId": "2339237238",
                                    "name": "Guo-Hua Wang"
                                },
                                {
                                    "authorId": "2304395071",
                                    "name": "Qing-Guo Chen"
                                },
                                {
                                    "authorId": "2304361003",
                                    "name": "Zhao Xu"
                                },
                                {
                                    "authorId": "2305289815",
                                    "name": "Weihua Luo"
                                },
                                {
                                    "authorId": "2304530663",
                                    "name": "Kaifu Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.97998046875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Training and Optimization Methods",
                "tldr": "Training multimodal language models requires specialized approaches to handle diverse data types and architectures, including alignment strategies between modalities, innovative fine-tuning techniques, and efficient parameter optimization methods like Low-Rank Adaptation (LoRA). (12 sources)",
                "text": "\n## Unified Training Approaches\n\nTraining multimodal language models capable of both understanding and generating content across modalities presents unique challenges that have prompted various specialized approaches. The Chameleon family of models demonstrates an important advancement through its stable training approach \"from inception,\" meaning it trains a unified model from scratch rather than combining pre-trained components. This approach employs an architectural parameterization specifically designed for early-fusion, token-based, mixed-modal settings, allowing the model to learn joint representations of text and images from the beginning <Paper corpusId=\"269791516\" paperTitle=\"(Team et al., 2024)\" isShortName></Paper>.\n\nA growing trend in multimodal model training is the end-to-end approach where all modalities are processed within a unified framework. Rather than treating different modalities as separate components that need to be bridged, models like UniFluid embed both text and continuous visual inputs as tokens in the same representational space. This enables seamless joint training of image generation and understanding tasks within a single architecture. UniFluid specifically builds upon pre-trained Gemma on large-scale text corpora and unlocks visual capabilities through training with paired image-text data <Paper corpusId=\"277103675\" paperTitle=\"(Fan et al., 2025)\" isShortName></Paper>.\n\n## Parameter-Efficient Fine-Tuning\n\nAs multimodal models grow in complexity, parameter-efficient fine-tuning methods have become increasingly important. Low-Rank Adaptation (LoRA) has emerged as a particularly effective technique for enhancing multimodal capabilities while minimizing computational costs. The GPT4Tools framework exemplifies this approach by utilizing LoRA optimization to enhance language models' ability to use multimodal tools. This method involves constructing a tool-related instructional dataset from advanced language models and then fine-tuning open-source language models using LoRA, enabling them to effectively use tools for visual tasks such as comprehension and image generation <Paper corpusId=\"266573642\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\nLoRA's effectiveness stems from its ability to adapt models with minimal parameter updates, typically modifying only about 1% of parameters in certain projection layers. This not only benefits low-cost training but also facilitates convenient expansion to additional modalities, as demonstrated in frameworks like TextBind, which enhances large language models with multi-turn interleaved multimodal instruction-following capabilities <Paper corpusId=\"266573642\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\n## Instruction-Based Training\n\nInstruction-based training has become a crucial approach for developing versatile multimodal models. This method involves training models to follow natural language instructions and prompts to perform various tasks, allowing them to adapt to new tasks without requiring gradient updates or large amounts of task-specific training data. Models like GPT-4V, LLaVA, and InstructBLIP exemplify this approach, demonstrating that the instruction format enables rapid adaptation to new tasks and domains <Paper corpusId=\"274305669\" paperTitle=\"(Kang et al., 2024)\" isShortName></Paper>.\n\nTextBind further enhances this approach by reducing the need for high-quality exemplar data, making instruction-based training more accessible and scalable for real-world tasks. The resulting MIM model outperforms recent baselines in open-world multimodal conversations, demonstrating strong performance in textual response generation, image generation, and overall multimodal instruction-following <Paper corpusId=\"266573642\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\n## Cross-Modal Alignment Strategies\n\nA critical aspect of training multimodal models is developing effective strategies for aligning different modalities. One approach involves creating a shared multimodal space by bridging alignment in the diffusion process, as demonstrated by Composable Diffusion (CoDi). This enables synchronized generation of intertwined modalities, such as temporally aligned video and audio <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258822817\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>.\n\nAnother alignment strategy involves the use of continuous representations interleaved with text tokens for autoregressive modeling to generate images. SEED-X exemplifies this approach by combining continuous image representations from a CLIP ViT encoder with text tokens and feeding them into a large language model to perform next-word prediction and image representation regression <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"231591445\" paperTitle=\"(Radford et al., 2021)\" isShortName></Paper>.\n\nDreamLLM adopts a distinctive approach by focusing on generative modeling of both language and image posteriors through direct sampling in the raw multimodal space. This method avoids the limitations and information loss inherent to external feature extractors like CLIP, potentially enabling more thorough multimodal understanding <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"261975252\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>.\n\n## Modality-Specific Training Components\n\nTraining effective multimodal generative models often requires specialized components for different modalities. UniFluid, for instance, employs task-specific prediction heads\u2014a diffusion-based head for image generation and a cross-entropy head for text generation\u2014to ensure effective modality-specific training and inference. This approach enables the model to efficiently learn shared representations that mutually enhance its generation and understanding capabilities <Paper corpusId=\"277103675\" paperTitle=\"(Fan et al., 2025)\" isShortName></Paper>.\n\nSimilarly, approaches that augment multimodal language models with image generation capabilities typically introduce additional visual tokens for generation and align these generative tokens as conditional information with a pretrained generator, such as Stable Diffusion for text-to-image generation <Paper corpusId=\"271709823\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. These approaches, exemplified by models like Emu, use an efficient mapping network to translate text representations from an LLM into the embedding space of visual models, enabling leverage of strong text representations for visual outputs <Paper corpusId=\"271709823\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258959284\" paperTitle=\"(Koh et al., 2023)\" isShortName></Paper>.\n\n## Novel Loss Functions and Training Objectives\n\nThe development of specialized loss functions has been crucial for training multimodal models. Instead of relying solely on traditional categorical cross-entropy loss, some approaches like continuous tokenization for image generation define a Diffusion Loss function to model per-token probability distributions. This allows for autoregressive modeling in a continuous-valued space without requiring discrete-valued tokenizers <Paper corpusId=\"277103675\" paperTitle=\"(Fan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270560593\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>.\n\nAdditionally, some models employ a pairwise sigmoid loss for image-text pre-training, which operates solely on image-text pairs without requiring a global view of pairwise similarities for normalization. This approach allows for scaling up batch sizes while also performing better at smaller batch sizes, offering flexibility in training configurations <Paper corpusId=\"277103675\" paperTitle=\"(Fan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"257767223\" paperTitle=\"(Zhai et al., 2023)\" isShortName></Paper>.\n\nThese diverse training and optimization methods collectively enable the development of increasingly sophisticated multimodal language models capable of both understanding and generating content across different modalities, pushing the boundaries of what's possible in AI-generated multimedia content.",
                "citations": [
                    {
                        "id": "(Team et al., 2024)",
                        "snippets": [
                            "We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model."
                        ],
                        "paper": {
                            "corpus_id": 269791516,
                            "title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models",
                            "authors": [
                                {
                                    "authorId": "2301457346",
                                    "name": "Chameleon Team"
                                },
                                {
                                    "authorId": "39960571",
                                    "name": "Jacob Kahn"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 339
                        },
                        "score": 0.98291015625
                    },
                    {
                        "id": "(Fan et al., 2025)",
                        "snippets": [
                            "We present UniFluid, a unified autoregressive framework for joint visual generation and understanding leveraging continuous visual tokens. Our unified autoregressive architecture processes multimodal image and text inputs, generating discrete tokens for text and continuous tokens for image",
                            "UniFluid, a unified framework that leverages continuous visual tokens within an autoregressive architecture to jointly handle vision-language generation and understanding tasks. Building upon pre-trained Gemma [48] on large-scale text corpus, UniFluid unlocks powerful visual generation and understanding capabilities through training with paired image-text data, and further allows these two tasks to mutually benefit each other within a single architecture. Specifically, UniFluid adopts a unified autoregressive framework where both text and continuous visual inputs are embedded as tokens in the same space, enabling seamless joint training of image generation and understanding tasks. UniFluid integrates a continuous tokenizer [12,(Li et al., 2024) for image generation and a pre-trained SigLIP (Zhai et al., 2023) image encoder for understanding tasks, while textual inputs are processed using a standard SentencePiece tokenizer [20]. The resulting multimodal sequences are modeled autoregressively using Gemma [48] as the underlying transformer backbone. Task-specific prediction heads-a diffusion-based head for image generation and a cross-entropy head for text generation-ensure effective modality-specific training and inference, enabling UniFluid to efficiently learn shared representations that mutually enhance its generation and understanding capabilities."
                        ],
                        "paper": {
                            "corpus_id": 277103675,
                            "title": "Unified Autoregressive Visual Generation and Understanding with Continuous Tokens",
                            "authors": [
                                {
                                    "authorId": "2347484699",
                                    "name": "Lijie Fan"
                                },
                                {
                                    "authorId": "34689393",
                                    "name": "Luming Tang"
                                },
                                {
                                    "authorId": "2333872078",
                                    "name": "Siyang Qin"
                                },
                                {
                                    "authorId": "2307269819",
                                    "name": "Tianhong Li"
                                },
                                {
                                    "authorId": "2350843695",
                                    "name": "Xuan Yang"
                                },
                                {
                                    "authorId": "2266238535",
                                    "name": "Siyuan Qiao"
                                },
                                {
                                    "authorId": "2350755056",
                                    "name": "Andreas Steiner"
                                },
                                {
                                    "authorId": "2333512682",
                                    "name": "Chen Sun"
                                },
                                {
                                    "authorId": "2281034398",
                                    "name": "Yuanzhen Li"
                                },
                                {
                                    "authorId": "2351408307",
                                    "name": "Tao Zhu"
                                },
                                {
                                    "authorId": "2325950285",
                                    "name": "Michael Rubinstein"
                                },
                                {
                                    "authorId": "3063676",
                                    "name": "Michalis Raptis"
                                },
                                {
                                    "authorId": "2262516617",
                                    "name": "Deqing Sun"
                                },
                                {
                                    "authorId": "1737285",
                                    "name": "Radu Soricut"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.9794921875
                    },
                    {
                        "id": "(Huang et al., 2023)",
                        "snippets": [
                            "GPT4Tools [63] enables open-source language models to effectively use multimodal tools. It constructs a tool-related instructional dataset from advanced language models and utilizes Low-Rank Adaptation (LoRA) optimization to enhance the language models' tool-usage capabilities. Additionally, it proposes a benchmark to evaluate the accuracy of language models in using tools, demonstrating significant improvements in tool usage across various visual tasks. As shown in Figure 17, the GPT4Tools framework involves constructing a tool-related instruction dataset by prompting an advanced language model with various multimodal contexts. This dataset is then used to fine-tune open-source language models using Low-Rank Adaptation (LoRA) optimization, enabling them to effectively use tools for visual tasks such as comprehension and image generation.\n\nTextBind [72] enhances large language models with multi-turn interleaved multimodal instruction-following capabilities. It significantly reduces the need for high-quality exemplar data, making it more accessible and scalable for real-world tasks. The proposed model, MIM, trained on TextBind, outperforms recent baselines in open-world multimodal conversations, demonstrating remarkable performance in textual response generation, image generation, and overall multimodal instruction-following. As shown in Figure 18, MIM seamlessly integrates image encoder and decoder models to accommodate interleaved image-text inputs and outputs. It supplements large language models with visual input and output modules, enabling the model to process multi-turn interleaved multimodal instructions and generate coherent responses."
                        ],
                        "paper": {
                            "corpus_id": 266573642,
                            "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
                            "authors": [
                                {
                                    "authorId": "2115941903",
                                    "name": "Jiaxing Huang"
                                },
                                {
                                    "authorId": "2276743977",
                                    "name": "Jingyi Zhang"
                                },
                                {
                                    "authorId": "2276609830",
                                    "name": "Kai Jiang"
                                },
                                {
                                    "authorId": "49660254",
                                    "name": "Han Qiu"
                                },
                                {
                                    "authorId": "2237947102",
                                    "name": "Shijian Lu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 23
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Kang et al., 2024)",
                        "snippets": [
                            "More advanced vision-language tasks require a tighter integration between the visual and linguistic modalities. This has led to a surge of interest in unified multimodal representation models that can process both images and text within a single framework (Baltru\u0161aitis et al., 2017)",
                            "Recently, pretrained language models, such as BERT [11] and GPT (Brown et al., 2020), have significantly influenced the vision-language domain. Researchers have extended these models to handle multimodal inputs, leveraging their contextual understanding of text",
                            "Instruction-based VLMs like GPT-4V [33], LLaVA [28], and InstructBLIP [10] are a new type of large vision-language model that are trained to follow natural language instructions and prompts to perform various tasks. A key advantage these models have over traditional fine-tuning is they can adapt to new tasks without needing gradient updates or lots of specific training data. The instruction format allows rapid adaptation."
                        ],
                        "paper": {
                            "corpus_id": 274305669,
                            "title": "VLM-HOI: Vision Language Models for Interpretable Human-Object Interaction Analysis",
                            "authors": [
                                {
                                    "authorId": "1490936425",
                                    "name": "Donggoo Kang"
                                },
                                {
                                    "authorId": "1783599864",
                                    "name": "Dasol Jeong"
                                },
                                {
                                    "authorId": "2321673431",
                                    "name": "Hyunmin Lee"
                                },
                                {
                                    "authorId": "2205412863",
                                    "name": "Sangwoo Park"
                                },
                                {
                                    "authorId": "31237187",
                                    "name": "Hasil Park"
                                },
                                {
                                    "authorId": "2211171908",
                                    "name": "Sunkyu Kwon"
                                },
                                {
                                    "authorId": "2268097132",
                                    "name": "Yeongjoon Kim"
                                },
                                {
                                    "authorId": "2261728538",
                                    "name": "Joonki Paik"
                                }
                            ],
                            "year": 2024,
                            "venue": "ECCV Workshops",
                            "n_citations": 0
                        },
                        "score": 0.96923828125
                    },
                    {
                        "id": "(Xie et al., 2024)",
                        "snippets": [
                            "In recent years, an increasing number of studies (Ge et al., 2024;Wu et al., 2023b;(Tang et al., 2023)Ye et al., 2024a;(Dong et al., 2023)Aiello et al., 2024) have focused on unified multimodal language models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;Sun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive modeling to generate images. SEED-X (Ge et al., 2024) proposes a unified and versatile foundation system capable of handling both multimodal understanding and generation tasks. In this approach, continuous image representations from CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model (LLM) to perform next-word prediction and image representation regression. DreamLLM (Dong et al., 2023)) also explores the potential of enabling multimodal comprehension and creation based on LLMs. Chameleon (Team, 2024) introduces a family of token-based mixed-modal models capable of both comprehending and generating images. This approach represents all modalities as discrete tokens and utilizes a unified transformer-based architecture and trains the model from scratch in an end-to-end manner."
                        ],
                        "paper": {
                            "corpus_id": 271924334,
                            "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation",
                            "authors": [
                                {
                                    "authorId": "2306603347",
                                    "name": "Jinheng Xie"
                                },
                                {
                                    "authorId": "2258958963",
                                    "name": "Weijia Mao"
                                },
                                {
                                    "authorId": "2237427303",
                                    "name": "Zechen Bai"
                                },
                                {
                                    "authorId": "2248388634",
                                    "name": "David Junhao Zhang"
                                },
                                {
                                    "authorId": "2316668014",
                                    "name": "Weihao Wang"
                                },
                                {
                                    "authorId": "2298569956",
                                    "name": "Kevin Qinghong Lin"
                                },
                                {
                                    "authorId": "2248678348",
                                    "name": "Yuchao Gu"
                                },
                                {
                                    "authorId": "2316662510",
                                    "name": "Zhijie Chen"
                                },
                                {
                                    "authorId": "2309246065",
                                    "name": "Zhenheng Yang"
                                },
                                {
                                    "authorId": "2244623976",
                                    "name": "Mike Zheng Shou"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 229
                        },
                        "score": 0.9619140625
                    },
                    {
                        "id": "(Tang et al., 2023)",
                        "snippets": [
                            "We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io"
                        ],
                        "paper": {
                            "corpus_id": 258822817,
                            "title": "Any-to-Any Generation via Composable Diffusion",
                            "authors": [
                                {
                                    "authorId": "151270642",
                                    "name": "Zineng Tang"
                                },
                                {
                                    "authorId": "2155459391",
                                    "name": "Ziyi Yang"
                                },
                                {
                                    "authorId": "8652308",
                                    "name": "Chenguang Zhu"
                                },
                                {
                                    "authorId": "48262024",
                                    "name": "Michael Zeng"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 188
                        },
                        "score": 0
                    },
                    {
                        "id": "(Radford et al., 2021)",
                        "snippets": [
                            "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."
                        ],
                        "paper": {
                            "corpus_id": 231591445,
                            "title": "Learning Transferable Visual Models From Natural Language Supervision",
                            "authors": [
                                {
                                    "authorId": "38909097",
                                    "name": "Alec Radford"
                                },
                                {
                                    "authorId": "2110935237",
                                    "name": "Jong Wook Kim"
                                },
                                {
                                    "authorId": "2004021329",
                                    "name": "Chris Hallacy"
                                },
                                {
                                    "authorId": "1992922591",
                                    "name": "A. Ramesh"
                                },
                                {
                                    "authorId": "40087786",
                                    "name": "Gabriel Goh"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "144864359",
                                    "name": "Girish Sastry"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "2051714782",
                                    "name": "Pamela Mishkin"
                                },
                                {
                                    "authorId": "2115193883",
                                    "name": "Jack Clark"
                                },
                                {
                                    "authorId": "2064404342",
                                    "name": "Gretchen Krueger"
                                },
                                {
                                    "authorId": "1701686",
                                    "name": "I. Sutskever"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 29867
                        },
                        "score": 0
                    },
                    {
                        "id": "(Dong et al., 2023)",
                        "snippets": [
                            "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."
                        ],
                        "paper": {
                            "corpus_id": 261975252,
                            "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
                            "authors": [
                                {
                                    "authorId": "2056965063",
                                    "name": "Runpei Dong"
                                },
                                {
                                    "authorId": "2118643247",
                                    "name": "Chunrui Han"
                                },
                                {
                                    "authorId": "2211415443",
                                    "name": "Yuang Peng"
                                },
                                {
                                    "authorId": "3424017",
                                    "name": "Zekun Qi"
                                },
                                {
                                    "authorId": "2242581956",
                                    "name": "Zheng Ge"
                                },
                                {
                                    "authorId": "2161319190",
                                    "name": "Jinrong Yang"
                                },
                                {
                                    "authorId": "48096671",
                                    "name": "Liang Zhao"
                                },
                                {
                                    "authorId": "26913717",
                                    "name": "Jian\u2010Yuan Sun"
                                },
                                {
                                    "authorId": "2157473950",
                                    "name": "Hongyu Zhou"
                                },
                                {
                                    "authorId": "134085586",
                                    "name": "Hao-Ran Wei"
                                },
                                {
                                    "authorId": "2069029697",
                                    "name": "Xiangwen Kong"
                                },
                                {
                                    "authorId": "2185865433",
                                    "name": "Xiangyu Zhang"
                                },
                                {
                                    "authorId": "2244131405",
                                    "name": "Kaisheng Ma"
                                },
                                {
                                    "authorId": "2242612318",
                                    "name": "Li Yi"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 201
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "Another line of research (Koh et al., 2023)(Dong et al., 2023)Sun et al., 2023;(Wu et al., 2023) has proposed augmenting MLLMs with the ability to generate images, videos, and audio from text instructions. These approaches introduce additional visual tokens for generation and align these generative tokens as conditional information with a pretrained generator, such as Stable Diffusion (Esser et al., 2024;Podell et al., 2023) for text-to-image generation. Consequently, the generation capabilities heavily rely on the external expert generator rather than MLLMs themselves, resulting in inconsistent and inferior generation results. To combine the strength of both approaches, our model aims to learn both understanding and generation of images using an MLLM with native multimodal capabilities, drawing inspiration from Chameleon (Team, 2024a), a mixed-modal early-fusion foundation model."
                        ],
                        "paper": {
                            "corpus_id": 271709823,
                            "title": "Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining",
                            "authors": [
                                {
                                    "authorId": "2308683426",
                                    "name": "Dongyang Liu"
                                },
                                {
                                    "authorId": "2283419424",
                                    "name": "Shitian Zhao"
                                },
                                {
                                    "authorId": "2300371391",
                                    "name": "Le Zhuo"
                                },
                                {
                                    "authorId": "2284068796",
                                    "name": "Weifeng Lin"
                                },
                                {
                                    "authorId": "2312346585",
                                    "name": "Yu Qiao"
                                },
                                {
                                    "authorId": "2266421952",
                                    "name": "Hongsheng Li"
                                },
                                {
                                    "authorId": "2310909023",
                                    "name": "Peng Gao"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 59
                        },
                        "score": 0.96044921875
                    },
                    {
                        "id": "(Koh et al., 2023)",
                        "snippets": [
                            "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs."
                        ],
                        "paper": {
                            "corpus_id": 258959284,
                            "title": "Generating Images with Multimodal Language Models",
                            "authors": [
                                {
                                    "authorId": "23978705",
                                    "name": "Jing Yu Koh"
                                },
                                {
                                    "authorId": "47070750",
                                    "name": "Daniel Fried"
                                },
                                {
                                    "authorId": "145124475",
                                    "name": "R. Salakhutdinov"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 259
                        },
                        "score": 0.98779296875
                    },
                    {
                        "id": "(Li et al._1, 2024)",
                        "snippets": [
                            "Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: https://github.com/LTH14/mar."
                        ],
                        "paper": {
                            "corpus_id": 270560593,
                            "title": "Autoregressive Image Generation without Vector Quantization",
                            "authors": [
                                {
                                    "authorId": "2307269819",
                                    "name": "Tianhong Li"
                                },
                                {
                                    "authorId": "2307043887",
                                    "name": "Yonglong Tian"
                                },
                                {
                                    "authorId": "2307146098",
                                    "name": "He Li"
                                },
                                {
                                    "authorId": "2306970309",
                                    "name": "Mingyang Deng"
                                },
                                {
                                    "authorId": "2270025109",
                                    "name": "Kaiming He"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 238
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhai et al., 2023)",
                        "snippets": [
                            "We propose a simple pairwise sigmoid loss for imagetext pre-training. Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. With only four TPUv4 chips, we can train a Base CLIP model at 4k batch size and a Large LiT model at 20k batch size, the latter achieves 84.5% ImageNet zero-shot accuracy in two days. This disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We hope our research motivates further explorations in improving the quality and efficiency of language-image pre-training."
                        ],
                        "paper": {
                            "corpus_id": 257767223,
                            "title": "Sigmoid Loss for Language Image Pre-Training",
                            "authors": [
                                {
                                    "authorId": "2743563",
                                    "name": "Xiaohua Zhai"
                                },
                                {
                                    "authorId": "40608942",
                                    "name": "Basil Mustafa"
                                },
                                {
                                    "authorId": "144629422",
                                    "name": "Alexander Kolesnikov"
                                },
                                {
                                    "authorId": "39611591",
                                    "name": "Lucas Beyer"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 1204
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Capabilities and Applications of Multimodal Generative Models",
                "tldr": "Advanced multimodal generative models enable diverse applications ranging from text-image interleaved generation to visual editing via natural language instructions. These capabilities span creative content generation, visual reasoning, multimodal instruction following, and specialized applications like UML-to-code conversion. (17 sources)",
                "text": "\n## Text-Image Generation and Interleaving\nModern multimodal language models excel at generating coherent content that seamlessly interleaves text and images. Models like Emu, Chameleon, and Unified-IO can process and generate arbitrary sequences of text and images, enabling rich multimedia content creation <Paper corpusId=\"258959284\" paperTitle=\"(Koh et al., 2023)\" isShortName></Paper> <Paper corpusId=\"274860024\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249848272\" paperTitle=\"(Lu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"266573555\" paperTitle=\"(Lu et al., 2023)\" isShortName></Paper>. This capability represents a significant advancement over traditional models that could only generate single modalities or required separate models for different tasks.\n\n## Visual Editing and Manipulation\nSeveral multimodal models offer innovative capabilities for image editing through natural language instructions. Systems like InstructPix2Pix enable users to modify existing images by describing desired changes rather than requiring pixel-level manipulation <Paper corpusId=\"276929408\" paperTitle=\"(Fang et al._1, 2025)\" isShortName></Paper> <Paper corpusId=\"253581213\" paperTitle=\"(Brooks et al., 2022)\" isShortName></Paper>. This natural language interface for visual editing makes sophisticated image manipulation accessible to users without technical expertise in image editing software.\n\n## Multimodal Instruction Following\nAdvanced multimodal models demonstrate strong capabilities in following complex instructions that involve multiple modalities. TextBind enhances large language models with multi-turn interleaved multimodal instruction-following capabilities, significantly reducing the need for high-quality exemplar data <Paper corpusId=\"266573642\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. Similarly, MIM shows remarkable performance in textual response generation, image generation, and overall multimodal instruction-following within open-world multimodal conversations <Paper corpusId=\"266573642\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\n## Visual Question Answering and Reasoning\nMultimodal models excel at visual question answering tasks, demonstrating sophisticated reasoning about visual content. Notable examples include BLIP, one of the earliest unified vision-language encoder-decoder architectures, and LLaVA, an end-to-end multimodal model that connects a vision encoder to a language model for versatile visual-language understanding <Paper corpusId=\"273022981\" paperTitle=\"(Khaldi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246411402\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>. These models can answer complex questions about images, describe scenes, identify objects, and reason about spatial relationships.\n\n## Multimodal In-Context Learning\nSome advanced models like Emu2 exhibit strong multimodal in-context learning abilities, solving tasks that require on-the-fly reasoning with minimal examples <Paper corpusId=\"276782523\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"266374640\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>. This capability allows models to adapt to new tasks through demonstrations rather than requiring fine-tuning, enabling applications like visual prompting and object-grounded generation.\n\n## Cross-Modal Retrieval and Alignment\nMultimodal models can perform sophisticated cross-modal retrieval tasks, finding relevant images based on textual descriptions or generating appropriate textual content for given images. Models like CLIP align visual embeddings with textual embeddings, enabling zero-shot image classification and other cross-modal tasks <Paper corpusId=\"277065986\" paperTitle=\"(Bates et al., 2025)\" isShortName></Paper> <Paper corpusId=\"231591445\" paperTitle=\"(Radford et al., 2021)\" isShortName></Paper>. These alignment capabilities form the foundation for many practical applications in content recommendation, search, and organization.\n\n## Specialized Technical Applications\nMultimodal models are increasingly applied to specialized technical domains. For example, some models can interpret UML diagrams and generate corresponding executable code, aligning diagram structures with text-based outputs by combining visual encoders with language models <Paper corpusId=\"277065986\" paperTitle=\"(Bates et al., 2025)\" isShortName></Paper>. This capability enhances productivity in software development workflows.\n\n## End-to-End Multimodal Pipelines\nAdvanced models are moving beyond cascaded pipelines toward end-to-end optimization approaches. While earlier systems like Visual ChatGPT connected LLMs with image generation tools in a cascaded pipeline that could introduce instability and noise, newer approaches optimize models in an end-to-end manner by aligning the output space with image diffusion models <Paper corpusId=\"266210376\" paperTitle=\"(Zhu et al._1, 2023)\" isShortName></Paper>. This end-to-end approach allows models to both perceive and generate images and text with greater coherence.\n\n## Multimodal Reasoning and Planning\nLatest multimodal models demonstrate advanced reasoning capabilities across different modalities. LMMs can perform cross-modal reasoning and prediction tasks, such as generating images or audio from text descriptions <Paper corpusId=\"278339632\" paperTitle=\"(Jiang et al., 2025)\" isShortName></Paper>. This ability enables applications in creative content generation, educational tools, and assistive technologies.\n\n## Unified Autoregressive Frameworks\nModels like UniFluid demonstrate the capabilities of unified autoregressive frameworks that leverage continuous visual tokens to jointly handle vision-language generation and understanding tasks <Paper corpusId=\"277103675\" paperTitle=\"(Fan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270560593\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>. These approaches enable efficient modeling of multimodal sequences through a single architectural framework, where both text and continuous visual inputs are embedded as tokens in the same representational space.\n\n## Tool-Enhanced Multimodal Processing\nSome models incorporate tool-usage capabilities to enhance their multimodal processing abilities. GPT4Tools enables open-source language models to effectively use multimodal tools for visual tasks such as comprehension and image generation through Low-Rank Adaptation optimization <Paper corpusId=\"266573642\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. This approach expands the practical applications of multimodal models by allowing them to leverage specialized tools for different tasks.",
                "citations": [
                    {
                        "id": "(Koh et al., 2023)",
                        "snippets": [
                            "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs."
                        ],
                        "paper": {
                            "corpus_id": 258959284,
                            "title": "Generating Images with Multimodal Language Models",
                            "authors": [
                                {
                                    "authorId": "23978705",
                                    "name": "Jing Yu Koh"
                                },
                                {
                                    "authorId": "47070750",
                                    "name": "Daniel Fried"
                                },
                                {
                                    "authorId": "145124475",
                                    "name": "R. Salakhutdinov"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 259
                        },
                        "score": 0.98779296875
                    },
                    {
                        "id": "(Shi et al., 2024)",
                        "snippets": [
                            "Over the past few years, we have seen significant progress in multimodal generative models capable of understanding and generating interleaved text and images in arbitrary sequences (Dong et al., 2023;(Koh et al., 2023)Lin et al., 2024b). Models like Transfusion (Zhou et al., 2024), Chameleon (Team, 2024b), and Unified-IO (Lu et al., 2022)(Lu et al., 2023) demonstrate the potential of unified architectures that seamlessly handle both image and text modalities."
                        ],
                        "paper": {
                            "corpus_id": 274860024,
                            "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
                            "authors": [
                                {
                                    "authorId": "2254168373",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "2290545053",
                                    "name": "Xiaochuang Han"
                                },
                                {
                                    "authorId": "2333891614",
                                    "name": "Chunting Zhou"
                                },
                                {
                                    "authorId": "151253861",
                                    "name": "Weixin Liang"
                                },
                                {
                                    "authorId": "2255374957",
                                    "name": "Xi Victoria Lin"
                                },
                                {
                                    "authorId": "2256736753",
                                    "name": "Luke S. Zettlemoyer"
                                },
                                {
                                    "authorId": "2296724476",
                                    "name": "Lili Yu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 28
                        },
                        "score": 0.9736328125
                    },
                    {
                        "id": "(Lu et al., 2022)",
                        "snippets": [
                            "We propose Unified-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region captioning and referring expression, to natural language processing tasks such as question answering and paraphrasing. Developing a single unified model for such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs pertaining to each task, including RGB images, per-pixel maps, binary masks, bounding boxes, and language. We achieve this unification by homogenizing every supported input and output into a sequence of discrete vocabulary tokens. This common representation across all tasks allows us to train a single transformer-based architecture, jointly on over 90 diverse datasets in the vision and language fields. Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning. Code and demos for Unified-IO are available at: https://unified-io.allenai.org."
                        ],
                        "paper": {
                            "corpus_id": 249848272,
                            "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks",
                            "authors": [
                                {
                                    "authorId": "2117718562",
                                    "name": "Jiasen Lu"
                                },
                                {
                                    "authorId": "143997772",
                                    "name": "Christopher Clark"
                                },
                                {
                                    "authorId": "2545335",
                                    "name": "Rowan Zellers"
                                },
                                {
                                    "authorId": "3012475",
                                    "name": "Roozbeh Mottaghi"
                                },
                                {
                                    "authorId": "2684226",
                                    "name": "Aniruddha Kembhavi"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 413
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lu et al., 2023)",
                        "snippets": [
                            "We present Unified-IO 2,the. first autoregressive multi-modal model that is capable of understanding and generating image, text, audio, and action. To unify different modalities, we tokenize inputs and outputs - images, text, audio, action, bounding boxes etc., into a shared semantic space and then process them with a single encoder-decoder transformer model. Since training with such diverse modalities is challenging, we propose various architectural improvements to stabilize model training. We train our model from scratch on a large multimodal pre-training corpus from diverse sources with a multimodal mixture of denoisers objective. To learn an expansive set of skills, such as following multimodal instructions, we construct and. finetune on an ensemble of 120 datasets with prompts and augmentations. With a single unified model, Unified-io 2 achieves state-of-the-art performance on the GRIT benchmark and strong results in more than 35 benchmarks, including image generation and understanding, natural language understanding, video and audio understanding, and robotic manipulation. We release all our models to the research community."
                        ],
                        "paper": {
                            "corpus_id": 266573555,
                            "title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action",
                            "authors": [
                                {
                                    "authorId": "2117718562",
                                    "name": "Jiasen Lu"
                                },
                                {
                                    "authorId": "143997772",
                                    "name": "Christopher Clark"
                                },
                                {
                                    "authorId": "2144567767",
                                    "name": "Sangho Lee"
                                },
                                {
                                    "authorId": "5630943",
                                    "name": "Zichen Zhang"
                                },
                                {
                                    "authorId": "2056070459",
                                    "name": "Savya Khosla"
                                },
                                {
                                    "authorId": "2135169537",
                                    "name": "Ryan Marten"
                                },
                                {
                                    "authorId": "2261388484",
                                    "name": "Derek Hoiem"
                                },
                                {
                                    "authorId": "2684226",
                                    "name": "Aniruddha Kembhavi"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 174
                        },
                        "score": 0
                    },
                    {
                        "id": "(Fang et al._1, 2025)",
                        "snippets": [
                            "Recent works [1,11,26,(Sun et al., 2023)[40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs (Brooks et al., 2022)13,(Mokady et al., 2023) offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation."
                        ],
                        "paper": {
                            "corpus_id": 276929408,
                            "title": "Recognition-Synergistic Scene Text Editing",
                            "authors": [
                                {
                                    "authorId": "2313679002",
                                    "name": "Zhengyao Fang"
                                },
                                {
                                    "authorId": "10344582",
                                    "name": "Pengyuan Lyu"
                                },
                                {
                                    "authorId": "2313920076",
                                    "name": "Jingjing Wu"
                                },
                                {
                                    "authorId": "2248958848",
                                    "name": "Chengquan Zhang"
                                },
                                {
                                    "authorId": "2349736964",
                                    "name": "Jun Yu"
                                },
                                {
                                    "authorId": "2261251059",
                                    "name": "Guan-Lun Lu"
                                },
                                {
                                    "authorId": "1678473",
                                    "name": "Wenjie Pei"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9736328125
                    },
                    {
                        "id": "(Brooks et al., 2022)",
                        "snippets": [
                            "We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models\u2014a language model (GPT-3) and a text-to-image model (Stable Diffusion)\u2014to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions."
                        ],
                        "paper": {
                            "corpus_id": 253581213,
                            "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions",
                            "authors": [
                                {
                                    "authorId": "2679394",
                                    "name": "Tim Brooks"
                                },
                                {
                                    "authorId": "2248172435",
                                    "name": "Aleksander Holynski"
                                },
                                {
                                    "authorId": "1763086",
                                    "name": "Alexei A. Efros"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 1833
                        },
                        "score": 0
                    },
                    {
                        "id": "(Huang et al., 2023)",
                        "snippets": [
                            "GPT4Tools [63] enables open-source language models to effectively use multimodal tools. It constructs a tool-related instructional dataset from advanced language models and utilizes Low-Rank Adaptation (LoRA) optimization to enhance the language models' tool-usage capabilities. Additionally, it proposes a benchmark to evaluate the accuracy of language models in using tools, demonstrating significant improvements in tool usage across various visual tasks. As shown in Figure 17, the GPT4Tools framework involves constructing a tool-related instruction dataset by prompting an advanced language model with various multimodal contexts. This dataset is then used to fine-tune open-source language models using Low-Rank Adaptation (LoRA) optimization, enabling them to effectively use tools for visual tasks such as comprehension and image generation.\n\nTextBind [72] enhances large language models with multi-turn interleaved multimodal instruction-following capabilities. It significantly reduces the need for high-quality exemplar data, making it more accessible and scalable for real-world tasks. The proposed model, MIM, trained on TextBind, outperforms recent baselines in open-world multimodal conversations, demonstrating remarkable performance in textual response generation, image generation, and overall multimodal instruction-following. As shown in Figure 18, MIM seamlessly integrates image encoder and decoder models to accommodate interleaved image-text inputs and outputs. It supplements large language models with visual input and output modules, enabling the model to process multi-turn interleaved multimodal instructions and generate coherent responses."
                        ],
                        "paper": {
                            "corpus_id": 266573642,
                            "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
                            "authors": [
                                {
                                    "authorId": "2115941903",
                                    "name": "Jiaxing Huang"
                                },
                                {
                                    "authorId": "2276743977",
                                    "name": "Jingyi Zhang"
                                },
                                {
                                    "authorId": "2276609830",
                                    "name": "Kai Jiang"
                                },
                                {
                                    "authorId": "49660254",
                                    "name": "Han Qiu"
                                },
                                {
                                    "authorId": "2237947102",
                                    "name": "Shijian Lu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 23
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Khaldi et al., 2024)",
                        "snippets": [
                            "LVLMs have emerged as a natural evolution of integrating LLMs with LVMs, enabling seamless multimodal interaction. By utilizing machine-generated instruction-following data during training, these models facilitate more dynamic and interactive communication between humans and machines. LVLMs are capable of processing both visual and textual inputs, generating coherent and contextually relevant responses in natural language",
                            "Notable LVLMs include BLIP (Li et al., 2022), one of the earliest unified vision-language encoder-decoder architectures; LLaVA (Large Language and Vision Assistant) (Liu et al. 2023), an end-to-end multimodal model that connects a vision encoder to a language model for versatile visual-language understanding; and GPT-4 (Achiam et al. 2023), a proprietary foundation model capable of multimodal reasoning, accessible exclusively via APIs such as ChatGPT."
                        ],
                        "paper": {
                            "corpus_id": 273022981,
                            "title": "Exploring Social Media Image Categorization Using Large Models with Different Adaptation Methods: A Case Study on Cultural Nature's Contributions to People",
                            "authors": [
                                {
                                    "authorId": "35729419",
                                    "name": "Rohaifa Khaldi"
                                },
                                {
                                    "authorId": "2273276043",
                                    "name": "Domingo Alcaraz-Segura"
                                },
                                {
                                    "authorId": "2323737781",
                                    "name": "Ignacio S'anchez-Herrera"
                                },
                                {
                                    "authorId": "2271941340",
                                    "name": "Javier Mart\u00ednez-L\u00f3pez"
                                },
                                {
                                    "authorId": "2323747675",
                                    "name": "Carlos Javier Navarro"
                                },
                                {
                                    "authorId": "3030006",
                                    "name": "S. Tabik"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.9697265625
                    },
                    {
                        "id": "(Li et al., 2022)",
                        "snippets": [
                            "Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP."
                        ],
                        "paper": {
                            "corpus_id": 246411402,
                            "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation",
                            "authors": [
                                {
                                    "authorId": "49299019",
                                    "name": "Junnan Li"
                                },
                                {
                                    "authorId": "2981509",
                                    "name": "Dongxu Li"
                                },
                                {
                                    "authorId": "2054594326",
                                    "name": "Caiming Xiong"
                                },
                                {
                                    "authorId": "1741126",
                                    "name": "S. Hoi"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 4413
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhao et al., 2025)",
                        "snippets": [
                            "Unified multimodal generative models aim to bridge the gap between understanding and generation tasks, and integrate vision and language into a single framework, enabling the model to learn shared representations across modalities [1,(Aiello et al., 2023)(Dong et al., 2023)19,(Sun et al., 2023)62,71,72,74,76,77,79,85]. SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM (Dong et al., 2023) extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities."
                        ],
                        "paper": {
                            "corpus_id": 276782523,
                            "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
                            "authors": [
                                {
                                    "authorId": "2248946985",
                                    "name": "Rui Zhao"
                                },
                                {
                                    "authorId": "2258958963",
                                    "name": "Weijia Mao"
                                },
                                {
                                    "authorId": "2344762475",
                                    "name": "Mike Zheng Shou"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9658203125
                    },
                    {
                        "id": "(Sun et al., 2023)",
                        "snippets": [
                            "The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research."
                        ],
                        "paper": {
                            "corpus_id": 266374640,
                            "title": "Generative Multimodal Models are In-Context Learners",
                            "authors": [
                                {
                                    "authorId": "2264040642",
                                    "name": "Quan Sun"
                                },
                                {
                                    "authorId": "2263702215",
                                    "name": "Yufeng Cui"
                                },
                                {
                                    "authorId": "2264589602",
                                    "name": "Xiaosong Zhang"
                                },
                                {
                                    "authorId": "2264274388",
                                    "name": "Fan Zhang"
                                },
                                {
                                    "authorId": "23716915",
                                    "name": "Qiying Yu"
                                },
                                {
                                    "authorId": "2275808712",
                                    "name": "Zhengxiong Luo"
                                },
                                {
                                    "authorId": "2217456303",
                                    "name": "Yueze Wang"
                                },
                                {
                                    "authorId": "2275170369",
                                    "name": "Yongming Rao"
                                },
                                {
                                    "authorId": "2222717281",
                                    "name": "Jingjing Liu"
                                },
                                {
                                    "authorId": "2257137554",
                                    "name": "Tiejun Huang"
                                },
                                {
                                    "authorId": "2263959521",
                                    "name": "Xinlong Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 290
                        },
                        "score": 0
                    },
                    {
                        "id": "(Bates et al., 2025)",
                        "snippets": [
                            "Researchers developed MM-LLMs that incorporate visual, auditory, and textual data into unified frameworks to address these limitations. Models like CLIP (Radford et al., 2021) align visual embeddings with textual embeddings, enabling cross-modal tasks such as visual question answering (VQA) and zero-shot image classification. For example, CLIP (Contrastive Language-Image Pretraining) learns shared visual and textual semantics from large-scale datasets of image-text pairs, effectively bridging linguistic context and visual perception. MM-LLMs leverage these multimodal capabilities to tackle tasks that traditional LLMs cannot address, such as interpreting UML diagrams and generating corresponding executable code. MM-LLMs align diagram structures with text-based outputs by combining visual encoders with language models, enhancing accuracy and flexibility for applications like UML-to-code automation."
                        ],
                        "paper": {
                            "corpus_id": 277065986,
                            "title": "Unified Modeling Language Code Generation from Diagram Images Using Multimodal Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2309151434",
                                    "name": "Averi Bates"
                                },
                                {
                                    "authorId": "2363651648",
                                    "name": "Ryan Vavricka"
                                },
                                {
                                    "authorId": "2350512740",
                                    "name": "Shane Carleton"
                                },
                                {
                                    "authorId": "2350505892",
                                    "name": "Ruosi Shao"
                                },
                                {
                                    "authorId": "2084643963",
                                    "name": "Chongle Pan"
                                }
                            ],
                            "year": 2025,
                            "venue": "Machine Learning with Applications",
                            "n_citations": 0
                        },
                        "score": 0.97314453125
                    },
                    {
                        "id": "(Radford et al., 2021)",
                        "snippets": [
                            "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."
                        ],
                        "paper": {
                            "corpus_id": 231591445,
                            "title": "Learning Transferable Visual Models From Natural Language Supervision",
                            "authors": [
                                {
                                    "authorId": "38909097",
                                    "name": "Alec Radford"
                                },
                                {
                                    "authorId": "2110935237",
                                    "name": "Jong Wook Kim"
                                },
                                {
                                    "authorId": "2004021329",
                                    "name": "Chris Hallacy"
                                },
                                {
                                    "authorId": "1992922591",
                                    "name": "A. Ramesh"
                                },
                                {
                                    "authorId": "40087786",
                                    "name": "Gabriel Goh"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "144864359",
                                    "name": "Girish Sastry"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "2051714782",
                                    "name": "Pamela Mishkin"
                                },
                                {
                                    "authorId": "2115193883",
                                    "name": "Jack Clark"
                                },
                                {
                                    "authorId": "2064404342",
                                    "name": "Gretchen Krueger"
                                },
                                {
                                    "authorId": "1701686",
                                    "name": "I. Sutskever"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 29867
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhu et al._1, 2023)",
                        "snippets": [
                            "To expand the capabilities of generating image in a multimodal context, certain efforts, e.g., Visual ChatGPT [47], attempt to connect LLMs with image generation tools in a cascaded pipeline by transferring text messages, which inevitably introduce instability and noise. Alternatively, another line of research achieves it by optimizing models in an end-to-end manner [9,18,23,30]48]. By aligning the output space with the image diffusion models, VL models can not only perceive but also generate images and text."
                        ],
                        "paper": {
                            "corpus_id": 266210376,
                            "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
                            "authors": [
                                {
                                    "authorId": "2274151094",
                                    "name": "Jinguo Zhu"
                                },
                                {
                                    "authorId": "2258784456",
                                    "name": "Xiaohan Ding"
                                },
                                {
                                    "authorId": "152988335",
                                    "name": "Yixiao Ge"
                                },
                                {
                                    "authorId": "51123495",
                                    "name": "Yuying Ge"
                                },
                                {
                                    "authorId": "2254048096",
                                    "name": "Sijie Zhao"
                                },
                                {
                                    "authorId": "2310758544",
                                    "name": "Hengshuang Zhao"
                                },
                                {
                                    "authorId": "2274490604",
                                    "name": "Xiaohua Wang"
                                },
                                {
                                    "authorId": "2265579883",
                                    "name": "Ying Shan"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 37
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Jiang et al., 2025)",
                        "snippets": [
                            "LMMs are capable of simultaneously processing and understanding data from different modalities, such as vision, language, haptic and auditory. These models achieve comprehensive processing and reasoning of multimodal information by integrating the features of various modalities in a unified high-dimensional space. They utilize advanced neural network architectures, such as transformers and diffusion models, to extract features from each modality and optimize their representations through techniques such as contrastive learning and self-supervised learning",
                            "these LMMs can also perform cross-modal reasoning and prediction, such as generating images or audio from text."
                        ],
                        "paper": {
                            "corpus_id": 278339632,
                            "title": "A Comprehensive Survey of Large AI Models for Future Communications: Foundations, Applications and Challenges",
                            "authors": [
                                {
                                    "authorId": "40540111",
                                    "name": "Feibo Jiang"
                                },
                                {
                                    "authorId": "2290889674",
                                    "name": "Cunhua Pan"
                                },
                                {
                                    "authorId": "2152288497",
                                    "name": "Li Dong"
                                },
                                {
                                    "authorId": "2244014700",
                                    "name": "Kezhi Wang"
                                },
                                {
                                    "authorId": "145118318",
                                    "name": "M. Debbah"
                                },
                                {
                                    "authorId": "2340230621",
                                    "name": "Dusit Niyato"
                                },
                                {
                                    "authorId": "2267902891",
                                    "name": "Zhu Han"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9619140625
                    },
                    {
                        "id": "(Fan et al., 2025)",
                        "snippets": [
                            "We present UniFluid, a unified autoregressive framework for joint visual generation and understanding leveraging continuous visual tokens. Our unified autoregressive architecture processes multimodal image and text inputs, generating discrete tokens for text and continuous tokens for image",
                            "UniFluid, a unified framework that leverages continuous visual tokens within an autoregressive architecture to jointly handle vision-language generation and understanding tasks. Building upon pre-trained Gemma [48] on large-scale text corpus, UniFluid unlocks powerful visual generation and understanding capabilities through training with paired image-text data, and further allows these two tasks to mutually benefit each other within a single architecture. Specifically, UniFluid adopts a unified autoregressive framework where both text and continuous visual inputs are embedded as tokens in the same space, enabling seamless joint training of image generation and understanding tasks. UniFluid integrates a continuous tokenizer [12,(Li et al., 2024) for image generation and a pre-trained SigLIP (Zhai et al., 2023) image encoder for understanding tasks, while textual inputs are processed using a standard SentencePiece tokenizer [20]. The resulting multimodal sequences are modeled autoregressively using Gemma [48] as the underlying transformer backbone. Task-specific prediction heads-a diffusion-based head for image generation and a cross-entropy head for text generation-ensure effective modality-specific training and inference, enabling UniFluid to efficiently learn shared representations that mutually enhance its generation and understanding capabilities."
                        ],
                        "paper": {
                            "corpus_id": 277103675,
                            "title": "Unified Autoregressive Visual Generation and Understanding with Continuous Tokens",
                            "authors": [
                                {
                                    "authorId": "2347484699",
                                    "name": "Lijie Fan"
                                },
                                {
                                    "authorId": "34689393",
                                    "name": "Luming Tang"
                                },
                                {
                                    "authorId": "2333872078",
                                    "name": "Siyang Qin"
                                },
                                {
                                    "authorId": "2307269819",
                                    "name": "Tianhong Li"
                                },
                                {
                                    "authorId": "2350843695",
                                    "name": "Xuan Yang"
                                },
                                {
                                    "authorId": "2266238535",
                                    "name": "Siyuan Qiao"
                                },
                                {
                                    "authorId": "2350755056",
                                    "name": "Andreas Steiner"
                                },
                                {
                                    "authorId": "2333512682",
                                    "name": "Chen Sun"
                                },
                                {
                                    "authorId": "2281034398",
                                    "name": "Yuanzhen Li"
                                },
                                {
                                    "authorId": "2351408307",
                                    "name": "Tao Zhu"
                                },
                                {
                                    "authorId": "2325950285",
                                    "name": "Michael Rubinstein"
                                },
                                {
                                    "authorId": "3063676",
                                    "name": "Michalis Raptis"
                                },
                                {
                                    "authorId": "2262516617",
                                    "name": "Deqing Sun"
                                },
                                {
                                    "authorId": "1737285",
                                    "name": "Radu Soricut"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.9794921875
                    },
                    {
                        "id": "(Li et al._1, 2024)",
                        "snippets": [
                            "Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: https://github.com/LTH14/mar."
                        ],
                        "paper": {
                            "corpus_id": 270560593,
                            "title": "Autoregressive Image Generation without Vector Quantization",
                            "authors": [
                                {
                                    "authorId": "2307269819",
                                    "name": "Tianhong Li"
                                },
                                {
                                    "authorId": "2307043887",
                                    "name": "Yonglong Tian"
                                },
                                {
                                    "authorId": "2307146098",
                                    "name": "He Li"
                                },
                                {
                                    "authorId": "2306970309",
                                    "name": "Mingyang Deng"
                                },
                                {
                                    "authorId": "2270025109",
                                    "name": "Kaiming He"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 238
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.32117100000000004
    }
}