{
    "query": "Are there empirical studies comparing the effectiveness of Retrieval-Augmented Generation (RAG) and fine-tuning approaches for improving generative AI models?",
    "user_id": "lib_user",
    "task_id": "624a0adb-84d4-4ad6-ac45-f0b48dd387bb",
    "timestamp": "2025-06-23T21:34:31.171582",
    "n_retrieval": 256,
    "n_retrieved": 270,
    "n_candidates": 46,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.323064,
    "decomposed_query": {
        "rewritten_query": "Empirical studies comparing the effectiveness of Retrieval-Augmented Generation (RAG) and fine-tuning approaches for improving generative AI models.",
        "keyword_query": "empirical studies comparing effectiveness Retrieval-Augmented Generation RAG fine-tuning approaches improving generative AI models",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010113,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.10497, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316485338",
                    "name": "Mohammad Baqar"
                },
                {
                    "authorId": "69923048",
                    "name": "Rajat Khanda"
                }
            ],
            "abstract": "Recent advancements in Generative AI have significantly improved the efficiency and adaptability of natural language processing (NLP) systems, particularly through Retrieval-Augmented Generation (RAG), Low-Rank Adaptation (LoRA), and Weight-Decomposed Low-Rank Adaptation (DoRA). RAG integrates external knowledge to enhance factual consistency in generative outputs, while LoRA enables parameter-efficient fine-tuning of large language models (LLMs). DoRA further refines this process by optimizing fine-tuning through adaptive parameter ranking and domain-aware weight adjustments, improving learning efficiency while maintaining inference performance. This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications. Furthermore, this study examines the trade-offs between fine-tuning efficiency, computational cost, and real-time adaptability across different models. Findings highlight RAG's effectiveness in knowledge grounding, LoRA's cost-efficient domain adaptation, and DoRA's ability to balance fine-tuning efficiency with model precision. These insights provide practical guidance for deploying AI-driven generative systems in accuracy-critical domains such as healthcare, finance, and legal services, ensuring scalability, reliability, and optimal performance in dynamic environments.",
            "corpus_id": 276408784,
            "sentences": [
                {
                    "corpus_id": "276408784",
                    "title": "Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA",
                    "text": "Recent advancements in Generative AI have significantly improved the efficiency and adaptability of natural language processing (NLP) systems, particularly through Retrieval-Augmented Generation (RAG), Low-Rank Adaptation (LoRA), and Weight-Decomposed Low-Rank Adaptation (DoRA). RAG integrates external knowledge to enhance factual consistency in generative outputs, while LoRA enables parameter-efficient fine-tuning of large language models (LLMs). DoRA further refines this process by optimizing fine-tuning through adaptive parameter ranking and domain-aware weight adjustments, improving learning efficiency while maintaining inference performance. This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications. Furthermore, this study examines the trade-offs between fine-tuning efficiency, computational cost, and real-time adaptability across different models. Findings highlight RAG's effectiveness in knowledge grounding, LoRA's cost-efficient domain adaptation, and DoRA's ability to balance fine-tuning efficiency with model precision. These insights provide practical guidance for deploying AI-driven generative systems in accuracy-critical domains such as healthcare, finance, and legal services, ensuring scalability, reliability, and optimal performance in dynamic environments.",
                    "score": 0.6858715337062964,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9873046875
                }
            ],
            "relevance_judgement": 0.9873046875,
            "relevance_judgment_input_expanded": "# Title: Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA\n# Venue: arXiv.org\n# Authors: Mohammad Baqar, Rajat Khanda\n## Abstract\nRecent advancements in Generative AI have significantly improved the efficiency and adaptability of natural language processing (NLP) systems, particularly through Retrieval-Augmented Generation (RAG), Low-Rank Adaptation (LoRA), and Weight-Decomposed Low-Rank Adaptation (DoRA). RAG integrates external knowledge to enhance factual consistency in generative outputs, while LoRA enables parameter-efficient fine-tuning of large language models (LLMs). DoRA further refines this process by optimizing fine-tuning through adaptive parameter ranking and domain-aware weight adjustments, improving learning efficiency while maintaining inference performance. This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications. Furthermore, this study examines the trade-offs between fine-tuning efficiency, computational cost, and real-time adaptability across different models. Findings highlight RAG's effectiveness in knowledge grounding, LoRA's cost-efficient domain adaptation, and DoRA's ability to balance fine-tuning efficiency with model precision. These insights provide practical guidance for deploying AI-driven generative systems in accuracy-critical domains such as healthcare, finance, and legal services, ensuring scalability, reliability, and optimal performance in dynamic environments.\n",
            "reference_string": "[276408784 | Baqar et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Unveiling the Power of Large Language Models: A Comparative Study of Retrieval-Augmented Generation, Fine-Tuning, and Their Synergistic Fusion for Enhanced Performance",
            "venue": "IEEE Access",
            "year": 2025,
            "reference_count": 36,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3542334?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3542334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2345470851",
                    "name": "G\u00fcls\u00fcm Budakoglu"
                },
                {
                    "authorId": "2345472890",
                    "name": "Hakan Emekci"
                }
            ],
            "abstract": "Large-language model optimization for a particular application is crucial and challenging in natural language processing. This study compares two salient techniques for retrieve-augmented generation (RAG) and fine-tuning along with a new hybrid method that combines both. In this study, we investigate the effectiveness of various methods using the Stanford Question Answering Dataset (SQuAD), Microsoft Machine Reading Comprehension (MS MARCO) and SQL CREATE TABLE statements. RAG is used because it enriches the model responses with external data without much computational load during the inference. Fine-tuning updates the model parameters to improve the contextual accuracy. Our hybrid model balances the accuracy and efficiency of the two techniques. While fine-tuning entails semantic precision, RAG is more resource efficient. The hybrid approach while it may not offer surpassing results over fine-tuning-offers a balanced solution in scenarios where the application demands both efficiency and accuracy. These findings represent the trade-off involved in LLM optimization and offers a scope for further studies and practical applications.",
            "corpus_id": 276355526,
            "sentences": [],
            "relevance_judgement": 0.970703125,
            "relevance_judgment_input_expanded": "# Title: Unveiling the Power of Large Language Models: A Comparative Study of Retrieval-Augmented Generation, Fine-Tuning, and Their Synergistic Fusion for Enhanced Performance\n# Venue: IEEE Access\n# Authors: G\u00fcls\u00fcm Budakoglu, Hakan Emekci\n## Abstract\nLarge-language model optimization for a particular application is crucial and challenging in natural language processing. This study compares two salient techniques for retrieve-augmented generation (RAG) and fine-tuning along with a new hybrid method that combines both. In this study, we investigate the effectiveness of various methods using the Stanford Question Answering Dataset (SQuAD), Microsoft Machine Reading Comprehension (MS MARCO) and SQL CREATE TABLE statements. RAG is used because it enriches the model responses with external data without much computational load during the inference. Fine-tuning updates the model parameters to improve the contextual accuracy. Our hybrid model balances the accuracy and efficiency of the two techniques. While fine-tuning entails semantic precision, RAG is more resource efficient. The hybrid approach while it may not offer surpassing results over fine-tuning-offers a balanced solution in scenarios where the application demands both efficiency and accuracy. These findings represent the trade-off involved in LLM optimization and offers a scope for further studies and practical applications.\n",
            "reference_string": "[276355526 | Budakoglu et al. | 2025 | Citations: 0]"
        },
        {
            "title": "RAG or Fine-tuning? A Comparative Study on LCMs-based Code Completion in Industry",
            "venue": "",
            "year": 2025,
            "reference_count": 72,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.15179, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2135764153",
                    "name": "Chaozheng Wang"
                },
                {
                    "authorId": "2155450982",
                    "name": "Zezhou Yang"
                },
                {
                    "authorId": "2112314113",
                    "name": "Shuzheng Gao"
                },
                {
                    "authorId": "2267893922",
                    "name": "Cuiyun Gao"
                },
                {
                    "authorId": "2299029254",
                    "name": "Ting Peng"
                },
                {
                    "authorId": "2265772572",
                    "name": "Hailiang Huang"
                },
                {
                    "authorId": "2299160326",
                    "name": "Yuetang Deng"
                },
                {
                    "authorId": "2338266828",
                    "name": "Michael R. Lyu"
                }
            ],
            "abstract": "Code completion, a crucial practice in industrial settings, helps developers improve programming efficiency by automatically suggesting code snippets during development. With the emergence of Large Code Models (LCMs), this field has witnessed significant advancements. Due to the natural differences between open-source and industrial codebases, such as coding patterns and unique internal dependencies, it is a common practice for developers to conduct domain adaptation when adopting LCMs in industry. There exist multiple adaptation approaches, among which retrieval-augmented generation (RAG) and fine-tuning are the two most popular paradigms. However, no prior research has explored the trade-off of the two approaches in industrial scenarios. To mitigate the gap, we comprehensively compare the two paradigms including Retrieval-Augmented Generation (RAG) and Fine-tuning (FT), for industrial code completion in this paper. In collaboration with Tencent's WXG department, we collect over 160,000 internal C++ files as our codebase. We then compare the two types of adaptation approaches from three dimensions that are concerned by industrial practitioners, including effectiveness, efficiency, and parameter sensitivity, using six LCMs. Our findings reveal that RAG, when implemented with appropriate embedding models that map code snippets into dense vector representations, can achieve higher accuracy than fine-tuning alone. Specifically, BM25 presents superior retrieval effectiveness and efficiency among studied RAG methods. Moreover, RAG and fine-tuning are orthogonal and their combination leads to further improvement. We also observe that RAG demonstrates better scalability than FT, showing more sustained performance gains with larger scales of codebase.",
            "corpus_id": 278782961,
            "sentences": [],
            "relevance_judgement": 0.970703125,
            "relevance_judgment_input_expanded": "# Title: RAG or Fine-tuning? A Comparative Study on LCMs-based Code Completion in Industry\n# Venue: \n# Authors: Chaozheng Wang, Zezhou Yang, Shuzheng Gao, Cuiyun Gao, Ting Peng, Hailiang Huang, Yuetang Deng, Michael R. Lyu\n## Abstract\nCode completion, a crucial practice in industrial settings, helps developers improve programming efficiency by automatically suggesting code snippets during development. With the emergence of Large Code Models (LCMs), this field has witnessed significant advancements. Due to the natural differences between open-source and industrial codebases, such as coding patterns and unique internal dependencies, it is a common practice for developers to conduct domain adaptation when adopting LCMs in industry. There exist multiple adaptation approaches, among which retrieval-augmented generation (RAG) and fine-tuning are the two most popular paradigms. However, no prior research has explored the trade-off of the two approaches in industrial scenarios. To mitigate the gap, we comprehensively compare the two paradigms including Retrieval-Augmented Generation (RAG) and Fine-tuning (FT), for industrial code completion in this paper. In collaboration with Tencent's WXG department, we collect over 160,000 internal C++ files as our codebase. We then compare the two types of adaptation approaches from three dimensions that are concerned by industrial practitioners, including effectiveness, efficiency, and parameter sensitivity, using six LCMs. Our findings reveal that RAG, when implemented with appropriate embedding models that map code snippets into dense vector representations, can achieve higher accuracy than fine-tuning alone. Specifically, BM25 presents superior retrieval effectiveness and efficiency among studied RAG methods. Moreover, RAG and fine-tuning are orthogonal and their combination leads to further improvement. We also observe that RAG demonstrates better scalability than FT, showing more sustained performance gains with larger scales of codebase.\n",
            "reference_string": "[278782961 | Wang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought",
            "venue": "International Symposium on Chinese Spoken Language Processing",
            "year": 2024,
            "reference_count": 21,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.15569, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2312343839",
                    "name": "Yuetong Zhao"
                },
                {
                    "authorId": "2312344958",
                    "name": "Hongyu Cao"
                },
                {
                    "authorId": "2312340862",
                    "name": "Xianyu Zhao"
                },
                {
                    "authorId": "2243267608",
                    "name": "Zhijian Ou"
                }
            ],
            "abstract": "Since the launch of ChatGPT at the end of 2022, generative dialogue models represented by ChatGPT have quickly become widely used. As user expectations increase, enhancing the capability of generative dialogue models to solve complex problems has become a focal point of current research. This paper delves into the effectiveness of the RAFT (Retrieval Aug-mented Fine-Tuning) method in improving the performance of Generative dialogue models. RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and retrieval augmented generation (RAG), which significantly enhanced the model's information extraction and logical reasoning abilities. We evaluated the RAFT method across multiple datasets and analysed its performance in various reasoning tasks, including long-form QA and short-form QA tasks, tasks in both Chinese and English, and supportive and comparison reasoning tasks. Notably, it addresses the gaps in previous research regarding long-form QA tasks and Chinese datasets. Moreover, we also evaluate the benefit of the chain-of-thought (CoT) in the RAFT method. This work offers valuable insights for studies focused on enhancing the performance of generative dialogue models.",
            "corpus_id": 271329121,
            "sentences": [],
            "relevance_judgement": 0.9560546875,
            "relevance_judgment_input_expanded": "# Title: An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought\n# Venue: International Symposium on Chinese Spoken Language Processing\n# Authors: Yuetong Zhao, Hongyu Cao, Xianyu Zhao, Zhijian Ou\n## Abstract\nSince the launch of ChatGPT at the end of 2022, generative dialogue models represented by ChatGPT have quickly become widely used. As user expectations increase, enhancing the capability of generative dialogue models to solve complex problems has become a focal point of current research. This paper delves into the effectiveness of the RAFT (Retrieval Aug-mented Fine-Tuning) method in improving the performance of Generative dialogue models. RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and retrieval augmented generation (RAG), which significantly enhanced the model's information extraction and logical reasoning abilities. We evaluated the RAFT method across multiple datasets and analysed its performance in various reasoning tasks, including long-form QA and short-form QA tasks, tasks in both Chinese and English, and supportive and comparison reasoning tasks. Notably, it addresses the gaps in previous research regarding long-form QA tasks and Chinese datasets. Moreover, we also evaluate the benefit of the chain-of-thought (CoT) in the RAFT method. This work offers valuable insights for studies focused on enhancing the performance of generative dialogue models.\n",
            "reference_string": "[271329121 | Zhao et al. | 2024 | Citations: 4]"
        },
        {
            "title": "ARAGOG: Advanced RAG Output Grading",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 17,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.01037, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294361167",
                    "name": "Matouvs Eibich"
                },
                {
                    "authorId": "2294361283",
                    "name": "Shivay Nagpal"
                },
                {
                    "authorId": "2294362877",
                    "name": "Alexander Fred-Ojala"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) is essential for integrating external knowledge into Large Language Model (LLM) outputs. While the literature on RAG is growing, it primarily focuses on systematic reviews and comparisons of new state-of-the-art (SoTA) techniques against their predecessors, with a gap in extensive experimental comparisons. This study begins to address this gap by assessing various RAG methods' impacts on retrieval precision and answer similarity. We found that Hypothetical Document Embedding (HyDE) and LLM reranking significantly enhance retrieval precision. However, Maximal Marginal Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a baseline Naive RAG system, and Multi-query approaches underperformed. Sentence Window Retrieval emerged as the most effective for retrieval precision, despite its variable performance on answer similarity. The study confirms the potential of the Document Summary Index as a competent retrieval approach. All resources related to this research are publicly accessible for further investigation through our GitHub repository ARAGOG (https://github.com/predlico/ARAGOG). We welcome the community to further this exploratory study in RAG systems.",
            "corpus_id": 268819923,
            "sentences": [
                {
                    "corpus_id": "268819923",
                    "title": "ARAGOG: Advanced RAG Output Grading",
                    "text": "Large Language Models (LLMs) have significantly advanced the field of natural language processing, enabling a wide range of applications from text generation to question answering.However, integrating dynamic, external information remains a challenge for these models.Retrieval Augmented Generation (RAG) techniques address this limitation by incorporating external knowledge sources into the generation process, thus enhancing the models' ability to produce contextually relevant and informed outputs.This integration of retrieval mechanisms with generative models is a key development in improving the performance and versatility of LLMs, facilitating more accurate and context-aware responses.See Figure 1 for an overview of the standard RAG workflow.\n\nDespite the growing interest in RAG techniques within the domain of LLMs, the existing body of literature primarily consists of systematic reviews (Gao et al., 2024) and direct comparisons between successive state-of-the-art (SoTA) models (Gao et al., 2022;Jiang et al., 2023).This pattern reveals a notable gap: a comprehensive experimental comparison across a broad spectrum of advanced RAG techniques is missing.Such a comparison is crucial for understanding the relative strengths and weaknesses of these techniques in enhancing LLMs' performance across various tasks.This study seeks to contribute to bridging this gap by providing an extensive evaluation of multiple RAG techniques and their combinations, thereby offering insights into their efficacy and applicability in real-world scenarios.\n\nThe focus of this investigation is a spectrum of advanced RAG techniques aimed at optimizing the retrieval process.These techniques can be categorized into several areas: To evaluate the RAG techniques, this study leverages two metrics: Retrieval Precision and Answer Similarity (Tonic AI, 2023).Retrieval Precision measures the relevance of the retrieved context to the question asked, while Answer Similarity assesses how closely the system's answers align with reference responses, on a scale from 0 to 5.\n\nFigure 1: A high-level overview of the workflow within a Retrieval-Augmented Generation (RAG) system.This process diagram shows how a user query is processed by the system to retrieve relevant documents from a database and how these documents inform the generation of a response.",
                    "score": 0.653401819039765,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 180
                        },
                        {
                            "start": 180,
                            "end": 268
                        },
                        {
                            "start": 268,
                            "end": 502
                        },
                        {
                            "start": 502,
                            "end": 696
                        },
                        {
                            "start": 696,
                            "end": 754
                        },
                        {
                            "start": 756,
                            "end": 1033
                        },
                        {
                            "start": 1033,
                            "end": 1171
                        },
                        {
                            "start": 1171,
                            "end": 1328
                        },
                        {
                            "start": 1328,
                            "end": 1556
                        },
                        {
                            "start": 1558,
                            "end": 1673
                        },
                        {
                            "start": 1673,
                            "end": 1854
                        },
                        {
                            "start": 1854,
                            "end": 2066
                        },
                        {
                            "start": 2068,
                            "end": 2169
                        },
                        {
                            "start": 2169,
                            "end": 2347
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94677734375
                }
            ],
            "relevance_judgement": 0.94677734375,
            "relevance_judgment_input_expanded": "# Title: ARAGOG: Advanced RAG Output Grading\n# Venue: arXiv.org\n# Authors: Matouvs Eibich, Shivay Nagpal, Alexander Fred-Ojala\n## Abstract\nRetrieval-Augmented Generation (RAG) is essential for integrating external knowledge into Large Language Model (LLM) outputs. While the literature on RAG is growing, it primarily focuses on systematic reviews and comparisons of new state-of-the-art (SoTA) techniques against their predecessors, with a gap in extensive experimental comparisons. This study begins to address this gap by assessing various RAG methods' impacts on retrieval precision and answer similarity. We found that Hypothetical Document Embedding (HyDE) and LLM reranking significantly enhance retrieval precision. However, Maximal Marginal Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a baseline Naive RAG system, and Multi-query approaches underperformed. Sentence Window Retrieval emerged as the most effective for retrieval precision, despite its variable performance on answer similarity. The study confirms the potential of the Document Summary Index as a competent retrieval approach. All resources related to this research are publicly accessible for further investigation through our GitHub repository ARAGOG (https://github.com/predlico/ARAGOG). We welcome the community to further this exploratory study in RAG systems.\n## Introduction\nLarge Language Models (LLMs) have significantly advanced the field of natural language processing, enabling a wide range of applications from text generation to question answering.However, integrating dynamic, external information remains a challenge for these models.Retrieval Augmented Generation (RAG) techniques address this limitation by incorporating external knowledge sources into the generation process, thus enhancing the models' ability to produce contextually relevant and informed outputs.This integration of retrieval mechanisms with generative models is a key development in improving the performance and versatility of LLMs, facilitating more accurate and context-aware responses.See Figure 1 for an overview of the standard RAG workflow.\n\nDespite the growing interest in RAG techniques within the domain of LLMs, the existing body of literature primarily consists of systematic reviews (Gao et al., 2024) and direct comparisons between successive state-of-the-art (SoTA) models (Gao et al., 2022;Jiang et al., 2023).This pattern reveals a notable gap: a comprehensive experimental comparison across a broad spectrum of advanced RAG techniques is missing.Such a comparison is crucial for understanding the relative strengths and weaknesses of these techniques in enhancing LLMs' performance across various tasks.This study seeks to contribute to bridging this gap by providing an extensive evaluation of multiple RAG techniques and their combinations, thereby offering insights into their efficacy and applicability in real-world scenarios.\n\nThe focus of this investigation is a spectrum of advanced RAG techniques aimed at optimizing the retrieval process.These techniques can be categorized into several areas: To evaluate the RAG techniques, this study leverages two metrics: Retrieval Precision and Answer Similarity (Tonic AI, 2023).Retrieval Precision measures the relevance of the retrieved context to the question asked, while Answer Similarity assesses how closely the system's answers align with reference responses, on a scale from 0 to 5.\n\nFigure 1: A high-level overview of the workflow within a Retrieval-Augmented Generation (RAG) system.This process diagram shows how a user query is processed by the system to retrieve relevant documents from a database and how these documents inform the generation of a response.",
            "reference_string": "[268819923 | Eibich et al. | 2024 | Citations: 4]"
        },
        {
            "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
            "venue": "ACM Trans. Inf. Syst.",
            "year": 2024,
            "reference_count": 64,
            "citation_count": 40,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3701228",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.17043, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2187857206",
                    "name": "Yuanjie Lyu"
                },
                {
                    "authorId": "2268429641",
                    "name": "Zhiyu Li"
                },
                {
                    "authorId": "2268393907",
                    "name": "Simin Niu"
                },
                {
                    "authorId": "2268399953",
                    "name": "Feiyu Xiong"
                },
                {
                    "authorId": "2268400606",
                    "name": "Bo Tang"
                },
                {
                    "authorId": "2117833477",
                    "name": "Wenjin Wang"
                },
                {
                    "authorId": "2282083454",
                    "name": "Hao Wu"
                },
                {
                    "authorId": "2304320758",
                    "name": "Huan Liu"
                },
                {
                    "authorId": "2277237058",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "2265580543",
                    "name": "Enhong Chen"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate \u201challucinated\u201d content. However, evaluating RAG systems is a challenge. Most benchmarks focus primarily on question-answering applications, neglecting other potential scenarios where RAG could be beneficial. Accordingly, in the experiments, these benchmarks often assess only the LLM components of the RAG pipeline or the retriever in knowledge-intensive scenarios, overlooking the impact of external knowledge base construction and the retrieval component on the entire RAG pipeline in non-knowledge-intensive scenarios. To address these issues, this article constructs a large-scale and more comprehensive benchmark and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we refer to the CRUD actions that describe interactions between users and knowledge bases and also categorize the range of RAG applications into four distinct types\u2014create, read, update, and delete (CRUD). \u201cCreate\u201d refers to scenarios requiring the generation of original, varied content. \u201cRead\u201d involves responding to intricate questions in knowledge-intensive situations. \u201cUpdate\u201d focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. \u201cDelete\u201d pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed different datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, context length, knowledge base construction, and LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios. The source code is available at GitHub: https://github.com/IAAR-Shanghai/CRUD_RAG.",
            "corpus_id": 267320876,
            "sentences": [
                {
                    "corpus_id": "267320876",
                    "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
                    "text": "In this paper, we have introduced an innovative framework (CRUD-RAG) for evaluating retrievalaugmented generation (RAG) systems that is both comprehensive and scenario-specific. Our unique categorization of text generation tasks into the CRUD-Create, Read, Update, and Delete-types provides a structured approach to assess the capabilities and limitations of RAG systems in handling a variety of textual contexts. To facilitate this evaluation, we have meticulously constructed largescale datasets for each CRUD category, which are tailored to challenge and reflect the performance of RAG systems under different operational conditions. Through rigorous experimental comparisons, we have demonstrated that RAG systems can significantly enhance the quality of generated content by effectively incorporating information from external knowledge sources. \n\nOur study delves into the intricate balance required in the fine-tuning process of RAG systems, highlighting the importance of optimizing the retrieval model, context length, construction of the knowledge base, and the deployment of the underlying large language model to achieve the best results. The insights provided by our findings offer a valuable roadmap for researchers and practitioners in the field, guiding them in the development and refinement of RAG systems. We believe that the methodologies and results presented in this paper will spur further exploration and innovation in the realm of RAG technologies. Our work aims to catalyze advancements in text generation applications, pushing the envelope of what is possible with the integration of retrieval mechanisms and language models. We hope that this contribution will serve as a cornerstone for future research efforts, fostering the creation of more intelligent, adaptive, and context-aware generative systems.",
                    "score": 0.5823002185682042,
                    "section_title": "CONCLUSION",
                    "char_start_offset": 67227,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 177
                        },
                        {
                            "start": 178,
                            "end": 413
                        },
                        {
                            "start": 414,
                            "end": 636
                        },
                        {
                            "start": 637,
                            "end": 850
                        },
                        {
                            "start": 853,
                            "end": 1150
                        },
                        {
                            "start": 1151,
                            "end": 1324
                        },
                        {
                            "start": 1325,
                            "end": 1473
                        },
                        {
                            "start": 1474,
                            "end": 1652
                        },
                        {
                            "start": 1653,
                            "end": 1832
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94677734375
                },
                {
                    "corpus_id": "267320876",
                    "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
                    "text": "Retrieval-augmented generation (RAG) is an advanced technique that leverages external knowledge sources to enhance the text generation capabilities of large language models(LLMs). It retrieves relevant paragraphs from a corpus based on the input, and feeds them to the LLMs along with the input. With the help of external knowledge, LLMs can generate more accurate and credible responses and effectively address challenges such as outdated knowledge [19], hallucinations [3,9,35,62], and lack of domain expertise [30,46]. Therefore, RAG technology is attracting increasing attention. \n\nAlthough the effectiveness of retrieval-augmented strategies has been proven through extensive practice, their implementation still requires a significant amount of tuning. The overall performance of the RAG system is affected by multiple factors, such as the retrieval model, construction of the external knowledge base, and language model. Therefore, automatic evaluation of RAG systems is crucial. Currently, there are only a few existing benchmarks for evaluating RAG performance, as creating high-quality datasets and experimenting with them entail significant costs. These benchmarks can be classified into two types: reference-required and reference-free evaluation. Reference-free evaluation frameworks, such as RAGAS [13] and ARES [44], use LLM-generated data to evaluate RAG systems on contextual relevance, faithfulness, and informativeness. These frameworks do not depend on ground truth references, but only assess the coherence of the generated text with the retrieved context. This approach may be unreliable if the retrieved external information is low-quality. \n\nConsequently, reference-required evaluations remain the predominant method for assessing RAG systems. Existing benchmarks for reference-required evaluations, such as RGB [8] and NQ [26]. do have their limitations. First, they all rely on question answering tasks to measure the performance of RAG systems. Question answering is not the only RAG application scenario, and an optimization strategy that works well for question answering may not be generalized to other scenarios. Thus, these benchmarks may not capture the full potential of RAG systems.",
                    "score": 0.5938684103142063,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 295
                        },
                        {
                            "start": 296,
                            "end": 521
                        },
                        {
                            "start": 522,
                            "end": 583
                        },
                        {
                            "start": 586,
                            "end": 758
                        },
                        {
                            "start": 759,
                            "end": 927
                        },
                        {
                            "start": 928,
                            "end": 986
                        },
                        {
                            "start": 987,
                            "end": 1158
                        },
                        {
                            "start": 1159,
                            "end": 1259
                        },
                        {
                            "start": 1260,
                            "end": 1438
                        },
                        {
                            "start": 1439,
                            "end": 1577
                        },
                        {
                            "start": 1578,
                            "end": 1663
                        },
                        {
                            "start": 1666,
                            "end": 1767
                        },
                        {
                            "start": 1768,
                            "end": 1852
                        },
                        {
                            "start": 1853,
                            "end": 1879
                        },
                        {
                            "start": 1880,
                            "end": 1971
                        },
                        {
                            "start": 1972,
                            "end": 2143
                        },
                        {
                            "start": 2144,
                            "end": 2217
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 471,
                            "end": 474,
                            "matchedPaperCorpusId": "259075950"
                        },
                        {
                            "start": 474,
                            "end": 476,
                            "matchedPaperCorpusId": "264350686"
                        },
                        {
                            "start": 476,
                            "end": 479,
                            "matchedPaperCorpusId": "252904927"
                        },
                        {
                            "start": 479,
                            "end": 482,
                            "matchedPaperCorpusId": "261891399"
                        },
                        {
                            "start": 513,
                            "end": 517,
                            "matchedPaperCorpusId": "263835169"
                        },
                        {
                            "start": 1847,
                            "end": 1851,
                            "matchedPaperCorpusId": "86611921"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.875
                }
            ],
            "relevance_judgement": 0.94677734375,
            "relevance_judgment_input_expanded": "# Title: CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models\n# Venue: ACM Trans. Inf. Syst.\n# Authors: Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huan Liu, Tong Xu, Enhong Chen\n## Abstract\nRetrieval-augmented generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate \u201challucinated\u201d content. However, evaluating RAG systems is a challenge. Most benchmarks focus primarily on question-answering applications, neglecting other potential scenarios where RAG could be beneficial. Accordingly, in the experiments, these benchmarks often assess only the LLM components of the RAG pipeline or the retriever in knowledge-intensive scenarios, overlooking the impact of external knowledge base construction and the retrieval component on the entire RAG pipeline in non-knowledge-intensive scenarios. To address these issues, this article constructs a large-scale and more comprehensive benchmark and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we refer to the CRUD actions that describe interactions between users and knowledge bases and also categorize the range of RAG applications into four distinct types\u2014create, read, update, and delete (CRUD). \u201cCreate\u201d refers to scenarios requiring the generation of original, varied content. \u201cRead\u201d involves responding to intricate questions in knowledge-intensive situations. \u201cUpdate\u201d focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. \u201cDelete\u201d pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed different datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, context length, knowledge base construction, and LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios. The source code is available at GitHub: https://github.com/IAAR-Shanghai/CRUD_RAG.\n## INTRODUCTION\nRetrieval-augmented generation (RAG) is an advanced technique that leverages external knowledge sources to enhance the text generation capabilities of large language models(LLMs). It retrieves relevant paragraphs from a corpus based on the input, and feeds them to the LLMs along with the input. With the help of external knowledge, LLMs can generate more accurate and credible responses and effectively address challenges such as outdated knowledge [19], hallucinations [3,9,35,62], and lack of domain expertise [30,46]. Therefore, RAG technology is attracting increasing attention. \n\nAlthough the effectiveness of retrieval-augmented strategies has been proven through extensive practice, their implementation still requires a significant amount of tuning. The overall performance of the RAG system is affected by multiple factors, such as the retrieval model, construction of the external knowledge base, and language model. Therefore, automatic evaluation of RAG systems is crucial. Currently, there are only a few existing benchmarks for evaluating RAG performance, as creating high-quality datasets and experimenting with them entail significant costs. These benchmarks can be classified into two types: reference-required and reference-free evaluation. Reference-free evaluation frameworks, such as RAGAS [13] and ARES [44], use LLM-generated data to evaluate RAG systems on contextual relevance, faithfulness, and informativeness. These frameworks do not depend on ground truth references, but only assess the coherence of the generated text with the retrieved context. This approach may be unreliable if the retrieved external information is low-quality. \n\nConsequently, reference-required evaluations remain the predominant method for assessing RAG systems. Existing benchmarks for reference-required evaluations, such as RGB [8] and NQ [26]. do have their limitations. First, they all rely on question answering tasks to measure the performance of RAG systems. Question answering is not the only RAG application scenario, and an optimization strategy that works well for question answering may not be generalized to other scenarios. Thus, these benchmarks may not capture the full potential of RAG systems.\n\n## CONCLUSION\nIn this paper, we have introduced an innovative framework (CRUD-RAG) for evaluating retrievalaugmented generation (RAG) systems that is both comprehensive and scenario-specific. Our unique categorization of text generation tasks into the CRUD-Create, Read, Update, and Delete-types provides a structured approach to assess the capabilities and limitations of RAG systems in handling a variety of textual contexts. To facilitate this evaluation, we have meticulously constructed largescale datasets for each CRUD category, which are tailored to challenge and reflect the performance of RAG systems under different operational conditions. Through rigorous experimental comparisons, we have demonstrated that RAG systems can significantly enhance the quality of generated content by effectively incorporating information from external knowledge sources. \n\nOur study delves into the intricate balance required in the fine-tuning process of RAG systems, highlighting the importance of optimizing the retrieval model, context length, construction of the knowledge base, and the deployment of the underlying large language model to achieve the best results. The insights provided by our findings offer a valuable roadmap for researchers and practitioners in the field, guiding them in the development and refinement of RAG systems. We believe that the methodologies and results presented in this paper will spur further exploration and innovation in the realm of RAG technologies. Our work aims to catalyze advancements in text generation applications, pushing the envelope of what is possible with the integration of retrieval mechanisms and language models. We hope that this contribution will serve as a cornerstone for future research efforts, fostering the creation of more intelligent, adaptive, and context-aware generative systems.",
            "reference_string": "[267320876 | Lyu et al. | 2024 | Citations: 40]"
        },
        {
            "title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 106,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03780, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324796381",
                    "name": "Thang Nguyen"
                },
                {
                    "authorId": "2324790937",
                    "name": "Peter Chin"
                },
                {
                    "authorId": "2324792268",
                    "name": "Yu-Wing Tai"
                }
            ],
            "abstract": "In this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences. The versatility of our approach allows it to be effectively applied across various domains through domain-specific fine-tuning. We evaluate Reward-RAG on publicly available benchmarks from multiple domains, comparing it to state-of-the-art methods. Our experimental results demonstrate significant improvements in performance, highlighting the effectiveness of Reward-RAG in improving the relevance and quality of generated responses. These findings underscore the potential of integrating reward models with RAG to achieve superior outcomes in natural language generation tasks.",
            "corpus_id": 273186680,
            "sentences": [
                {
                    "corpus_id": "273186680",
                    "title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision",
                    "text": "In this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences. The versatility of our approach allows it to be effectively applied across various domains through domain-specific fine-tuning. We evaluate Reward-RAG on publicly available benchmarks from multiple domains, comparing it to state-of-the-art methods. Our experimental results demonstrate significant improvements in performance, highlighting the effectiveness of Reward-RAG in improving the relevance and quality of generated responses. These findings underscore the potential of integrating reward models with RAG to achieve superior outcomes in natural language generation tasks.",
                    "score": 0.6139348963513055,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9384765625
                }
            ],
            "relevance_judgement": 0.9384765625,
            "relevance_judgment_input_expanded": "# Title: Reward-RAG: Enhancing RAG with Reward Driven Supervision\n# Venue: arXiv.org\n# Authors: Thang Nguyen, Peter Chin, Yu-Wing Tai\n## Abstract\nIn this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences. The versatility of our approach allows it to be effectively applied across various domains through domain-specific fine-tuning. We evaluate Reward-RAG on publicly available benchmarks from multiple domains, comparing it to state-of-the-art methods. Our experimental results demonstrate significant improvements in performance, highlighting the effectiveness of Reward-RAG in improving the relevance and quality of generated responses. These findings underscore the potential of integrating reward models with RAG to achieve superior outcomes in natural language generation tasks.\n",
            "reference_string": "[273186680 | Nguyen et al. | 2024 | Citations: 5]"
        },
        {
            "title": "RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.12322, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280268220",
                    "name": "Ioannis Papadimitriou"
                },
                {
                    "authorId": "1988554",
                    "name": "Ilias Gialampoukidis"
                },
                {
                    "authorId": "3019137",
                    "name": "S. Vrochidis"
                },
                {
                    "authorId": "1715604",
                    "name": "Y. Kompatsiaris"
                }
            ],
            "abstract": "We present RAG Playground, an open-source framework for systematic evaluation of Retrieval-Augmented Generation (RAG) systems. The framework implements and compares three retrieval approaches: naive vector search, reranking, and hybrid vector-keyword search, combined with ReAct agents using different prompting strategies. We introduce a comprehensive evaluation framework with novel metrics and provide empirical results comparing different language models (Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our experiments demonstrate significant performance improvements through hybrid search methods and structured self-evaluation prompting, achieving up to 72.7% pass rate on our multi-metric evaluation framework. The results also highlight the importance of prompt engineering in RAG systems, with our custom-prompted agents showing consistent improvements in retrieval accuracy and response quality.",
            "corpus_id": 274788878,
            "sentences": [
                {
                    "corpus_id": "274788878",
                    "title": "RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems",
                    "text": "We present RAG Playground, an open-source framework for systematic evaluation of Retrieval-Augmented Generation (RAG) systems. The framework implements and compares three retrieval approaches: naive vector search, reranking, and hybrid vector-keyword search, combined with ReAct agents using different prompting strategies. We introduce a comprehensive evaluation framework with novel metrics and provide empirical results comparing different language models (Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our experiments demonstrate significant performance improvements through hybrid search methods and structured self-evaluation prompting, achieving up to 72.7% pass rate on our multi-metric evaluation framework. The results also highlight the importance of prompt engineering in RAG systems, with our custom-prompted agents showing consistent improvements in retrieval accuracy and response quality.",
                    "score": 0.6410772020485098,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9375
                }
            ],
            "relevance_judgement": 0.9375,
            "relevance_judgment_input_expanded": "# Title: RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems\n# Venue: arXiv.org\n# Authors: Ioannis Papadimitriou, Ilias Gialampoukidis, S. Vrochidis, Y. Kompatsiaris\n## Abstract\nWe present RAG Playground, an open-source framework for systematic evaluation of Retrieval-Augmented Generation (RAG) systems. The framework implements and compares three retrieval approaches: naive vector search, reranking, and hybrid vector-keyword search, combined with ReAct agents using different prompting strategies. We introduce a comprehensive evaluation framework with novel metrics and provide empirical results comparing different language models (Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our experiments demonstrate significant performance improvements through hybrid search methods and structured self-evaluation prompting, achieving up to 72.7% pass rate on our multi-metric evaluation framework. The results also highlight the importance of prompt engineering in RAG systems, with our custom-prompted agents showing consistent improvements in retrieval accuracy and response quality.\n",
            "reference_string": "[274788878 | Papadimitriou et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 78,
            "citation_count": 61,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01219, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2273537815",
                    "name": "Xiaohua Wang"
                },
                {
                    "authorId": "2308276345",
                    "name": "Zhenghua Wang"
                },
                {
                    "authorId": "2292070745",
                    "name": "Xuan Gao"
                },
                {
                    "authorId": "2308226671",
                    "name": "Feiran Zhang"
                },
                {
                    "authorId": "2308043953",
                    "name": "Yixin Wu"
                },
                {
                    "authorId": "2308044030",
                    "name": "Zhibo Xu"
                },
                {
                    "authorId": "2308036711",
                    "name": "Tianyuan Shi"
                },
                {
                    "authorId": "2309182278",
                    "name": "Zhengyuan Wang"
                },
                {
                    "authorId": "2309656885",
                    "name": "Shizheng Li"
                },
                {
                    "authorId": "2309176521",
                    "name": "Qi Qian"
                },
                {
                    "authorId": "2292032843",
                    "name": "Ruicheng Yin"
                },
                {
                    "authorId": "2220896023",
                    "name": "Changze Lv"
                },
                {
                    "authorId": "2257315404",
                    "name": "Xiaoqing Zheng"
                },
                {
                    "authorId": "2257129987",
                    "name": "Xuanjing Huang"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a \u201cretrieval as generation\u201d strategy.",
            "corpus_id": 270870251,
            "sentences": [
                {
                    "corpus_id": "270870251",
                    "title": "Searching for Best Practices in Retrieval-Augmented Generation",
                    "text": "The contributions of this study are three-fold:\n\n\u2022 Through extensive experimentation, we thoroughly investigated existing RAG approaches and their combinations to identify and recommend optimal RAG practices.\n\n\u2022 We introduce a comprehensive framework of evaluation metrics and corresponding datasets to comprehensively assess the performance of retrieval-augmented generation models, covering general, specialized (or domain-specific), and RAG-related capabilities.\u2022 We demonstrate that the integration of multimodal retrieval techniques can substantially improve question-answering capabilities on visual inputs and speed up the generation of multimodal content through a strategy of \"retrieval as generation\".",
                    "score": 0.6314901017668331,
                    "section_title": "Retrieval Source",
                    "char_start_offset": 4117,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 47
                        },
                        {
                            "start": 49,
                            "end": 208
                        },
                        {
                            "start": 210,
                            "end": 465
                        },
                        {
                            "start": 465,
                            "end": 711
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9375
                },
                {
                    "corpus_id": "270870251",
                    "title": "Searching for Best Practices in Retrieval-Augmented Generation",
                    "text": "In this study, we aim to identify optimal practices for implementing retrieval-augmented generation in order to improve the quality and reliability of content produced by large language models.We systematically assessed a range of potential solutions for each module within the RAG framework and recommended the most effective approach for each module.Furthermore, we introduced a comprehensive evaluation benchmark for RAG systems and conducted extensive experiments to determine the best practices among various alternatives.Our findings not only contribute to a deeper understanding of retrieval-augmented generation systems but also establish a foundation for future research.",
                    "score": 0.6254129495633101,
                    "section_title": "Conclusion",
                    "char_start_offset": 32122,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 193
                        },
                        {
                            "start": 193,
                            "end": 352
                        },
                        {
                            "start": 352,
                            "end": 527
                        },
                        {
                            "start": 527,
                            "end": 680
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8974609375
                }
            ],
            "relevance_judgement": 0.9375,
            "relevance_judgment_input_expanded": "# Title: Searching for Best Practices in Retrieval-Augmented Generation\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng, Xuanjing Huang\n## Abstract\nRetrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a \u201cretrieval as generation\u201d strategy.\n## Retrieval Source\nThe contributions of this study are three-fold:\n\n\u2022 Through extensive experimentation, we thoroughly investigated existing RAG approaches and their combinations to identify and recommend optimal RAG practices.\n\n\u2022 We introduce a comprehensive framework of evaluation metrics and corresponding datasets to comprehensively assess the performance of retrieval-augmented generation models, covering general, specialized (or domain-specific), and RAG-related capabilities.\u2022 We demonstrate that the integration of multimodal retrieval techniques can substantially improve question-answering capabilities on visual inputs and speed up the generation of multimodal content through a strategy of \"retrieval as generation\".\n\n## Conclusion\nIn this study, we aim to identify optimal practices for implementing retrieval-augmented generation in order to improve the quality and reliability of content produced by large language models.We systematically assessed a range of potential solutions for each module within the RAG framework and recommended the most effective approach for each module.Furthermore, we introduced a comprehensive evaluation benchmark for RAG systems and conducted extensive experiments to determine the best practices among various alternatives.Our findings not only contribute to a deeper understanding of retrieval-augmented generation systems but also establish a foundation for future research.",
            "reference_string": "[270870251 | Wang et al. | 2024 | Citations: 61]"
        },
        {
            "title": "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 44,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.04528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2186740325",
                    "name": "Aleksander Ficek"
                },
                {
                    "authorId": "2266881428",
                    "name": "Jiaqi Zeng"
                },
                {
                    "authorId": "2787022",
                    "name": "Oleksii Kuchaiev"
                }
            ],
            "abstract": "Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG) have become popular methods for adapting large language models while minimizing compute requirements. In this paper, we apply PEFT methods (P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer (RETRO) and a baseline GPT model across several sizes, ranging from 823 million to 48 billion parameters. We show that RETRO models outperform GPT models in zero-shot settings due to their unique pre-training process but GPT models have higher performance potential with PEFT. Additionally, our study indicates that 8B parameter models strike an optimal balance between cost and performance and P-tuning lags behind other PEFT techniques. We further provide a comparative analysis of between applying PEFT to Instruction-tuned RETRO model and base RETRO model. This work presents the first comprehensive comparison of various PEFT methods integrated with RAG, applied to both GPT and RETRO models, highlighting their relative performance.",
            "corpus_id": 271039066,
            "sentences": [
                {
                    "corpus_id": "271039066",
                    "title": "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning",
                    "text": "This study explores Parameter-Efficient Fine-Tuning (PEFT) methods applied to Retrieval-Augmented Generation (RAG) models, comparing GPT and RETRO architectures. RETRO generally outperforms GPT in zero-shot settings due to their pre-training process that integrates external retrieval, enhancing contextual understanding. However, GPT models show a higher performance potential with PEFT, indicating more room for improvement during fine-tuning. Both RETRO and GPT models perform optimally around the 8B parameter mark, balancing cost and performance. While P-tuning is effective in larger models, it lags behind other methods in smaller models, particularly for RETRO. Applying PEFT to Instructiontuned RETRO yields limited improvement compared to base RETRO, suggesting a saturation point in leveraging pre-training and fine-tuning benefits. Our comprehensive analysis offers valuable insights for optimizing large language models with PEFT and RAG to the community.",
                    "score": 0.5843186912469251,
                    "section_title": "Conclusion",
                    "char_start_offset": 13407,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 321
                        },
                        {
                            "start": 322,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 551
                        },
                        {
                            "start": 552,
                            "end": 669
                        },
                        {
                            "start": 670,
                            "end": 843
                        },
                        {
                            "start": 844,
                            "end": 968
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9375
                }
            ],
            "relevance_judgement": 0.9375,
            "relevance_judgment_input_expanded": "# Title: GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Aleksander Ficek, Jiaqi Zeng, Oleksii Kuchaiev\n## Abstract\nParameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG) have become popular methods for adapting large language models while minimizing compute requirements. In this paper, we apply PEFT methods (P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer (RETRO) and a baseline GPT model across several sizes, ranging from 823 million to 48 billion parameters. We show that RETRO models outperform GPT models in zero-shot settings due to their unique pre-training process but GPT models have higher performance potential with PEFT. Additionally, our study indicates that 8B parameter models strike an optimal balance between cost and performance and P-tuning lags behind other PEFT techniques. We further provide a comparative analysis of between applying PEFT to Instruction-tuned RETRO model and base RETRO model. This work presents the first comprehensive comparison of various PEFT methods integrated with RAG, applied to both GPT and RETRO models, highlighting their relative performance.\n## Conclusion\nThis study explores Parameter-Efficient Fine-Tuning (PEFT) methods applied to Retrieval-Augmented Generation (RAG) models, comparing GPT and RETRO architectures. RETRO generally outperforms GPT in zero-shot settings due to their pre-training process that integrates external retrieval, enhancing contextual understanding. However, GPT models show a higher performance potential with PEFT, indicating more room for improvement during fine-tuning. Both RETRO and GPT models perform optimally around the 8B parameter mark, balancing cost and performance. While P-tuning is effective in larger models, it lags behind other methods in smaller models, particularly for RETRO. Applying PEFT to Instructiontuned RETRO yields limited improvement compared to base RETRO, suggesting a saturation point in leveraging pre-training and fine-tuning benefits. Our comprehensive analysis offers valuable insights for optimizing large language models with PEFT and RAG to the community.",
            "reference_string": "[271039066 | Ficek et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Retrieval-Augmented Generation-based Relation Extraction",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 32,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.13397, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2189018699",
                    "name": "Sefika Efeoglu"
                },
                {
                    "authorId": "2259621860",
                    "name": "Adrian Paschke"
                }
            ],
            "abstract": "Information Extraction (IE) is a transformative process that converts unstructured text data into a structured format by employing entity and relation extraction (RE) methodologies. The identification of the relation between a pair of entities plays a crucial role within this framework. Despite the existence of various techniques for relation extraction, their efficacy heavily relies on access to labeled data and substantial computational resources. In addressing these challenges, Large Language Models (LLMs) emerge as promising solutions; however, they might return hallucinating responses due to their own training data. To overcome these limitations, Retrieved-Augmented Generation-based Relation Extraction (RAG4RE) in this work is proposed, offering a pathway to enhance the performance of relation extraction tasks. This work evaluated the effectiveness of our RAG4RE approach utilizing different LLMs. Through the utilization of established benchmarks, such as TACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to comprehensively evaluate the efficacy of our RAG4RE approach. In particularly, we leverage prominent LLMs including Flan T5, Llama2, and Mistral in our investigation. The results of our study demonstrate that our RAG4RE approach surpasses performance of traditional RE approaches based solely on LLMs, particularly evident in the TACRED dataset and its variations. Furthermore, our approach exhibits remarkable performance compared to previous RE methodologies across both TACRED and TACREV datasets, underscoring its efficacy and potential for advancing RE tasks in natural language processing.",
            "corpus_id": 269292881,
            "sentences": [
                {
                    "corpus_id": "269292881",
                    "title": "Retrieval-Augmented Generation-based Relation Extraction",
                    "text": "Retrieval-Augmented Generation (RAG) for large language models can be classified into two categories: i) naive RAG and ii) advanced RAG.Naive RAG has basic steps: retrieve, augmentation, and generation, while the advanced version includes a post-processing step before sending the retrieved information to a user [24].The concept of RAG has been suggested as a way to minimize the undesired alterations in Language Models (LLMs) when conversational systems built on LLMs generate arbitrary responses to a query [7].RAG is an example of open-book exams which are applied to the usage of LLMs.The retriever mechanism in RAG finds an example of the user query (prompt), and then the user query is regenerated along with the example by the data-augmentation module in RAG.Ovadia et al. [25] evaluates the knowledge injection capacities of both fine-tuning and the RAG approach and found that LLMs dealt with performance problems through unsupervised fine-tuning while RAG outperformed the fine-tuning approach in unsupervised learning.\n\nIn this work, we introduce a Retrieval-Augmented Generation-based Relation Extraction (RAG4RE) approach to identify the relationship between a pair of entities in a sentence.",
                    "score": 0.6231551873965765,
                    "section_title": "Retrieval-Augmented Generation",
                    "char_start_offset": 8393,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 136
                        },
                        {
                            "start": 136,
                            "end": 318
                        },
                        {
                            "start": 318,
                            "end": 515
                        },
                        {
                            "start": 515,
                            "end": 591
                        },
                        {
                            "start": 591,
                            "end": 768
                        },
                        {
                            "start": 768,
                            "end": 1031
                        },
                        {
                            "start": 1033,
                            "end": 1207
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 511,
                            "end": 514,
                            "matchedPaperCorpusId": "218869575"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93310546875
                }
            ],
            "relevance_judgement": 0.93310546875,
            "relevance_judgment_input_expanded": "# Title: Retrieval-Augmented Generation-based Relation Extraction\n# Venue: arXiv.org\n# Authors: Sefika Efeoglu, Adrian Paschke\n## Abstract\nInformation Extraction (IE) is a transformative process that converts unstructured text data into a structured format by employing entity and relation extraction (RE) methodologies. The identification of the relation between a pair of entities plays a crucial role within this framework. Despite the existence of various techniques for relation extraction, their efficacy heavily relies on access to labeled data and substantial computational resources. In addressing these challenges, Large Language Models (LLMs) emerge as promising solutions; however, they might return hallucinating responses due to their own training data. To overcome these limitations, Retrieved-Augmented Generation-based Relation Extraction (RAG4RE) in this work is proposed, offering a pathway to enhance the performance of relation extraction tasks. This work evaluated the effectiveness of our RAG4RE approach utilizing different LLMs. Through the utilization of established benchmarks, such as TACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to comprehensively evaluate the efficacy of our RAG4RE approach. In particularly, we leverage prominent LLMs including Flan T5, Llama2, and Mistral in our investigation. The results of our study demonstrate that our RAG4RE approach surpasses performance of traditional RE approaches based solely on LLMs, particularly evident in the TACRED dataset and its variations. Furthermore, our approach exhibits remarkable performance compared to previous RE methodologies across both TACRED and TACREV datasets, underscoring its efficacy and potential for advancing RE tasks in natural language processing.\n## Retrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) for large language models can be classified into two categories: i) naive RAG and ii) advanced RAG.Naive RAG has basic steps: retrieve, augmentation, and generation, while the advanced version includes a post-processing step before sending the retrieved information to a user [24].The concept of RAG has been suggested as a way to minimize the undesired alterations in Language Models (LLMs) when conversational systems built on LLMs generate arbitrary responses to a query [7].RAG is an example of open-book exams which are applied to the usage of LLMs.The retriever mechanism in RAG finds an example of the user query (prompt), and then the user query is regenerated along with the example by the data-augmentation module in RAG.Ovadia et al. [25] evaluates the knowledge injection capacities of both fine-tuning and the RAG approach and found that LLMs dealt with performance problems through unsupervised fine-tuning while RAG outperformed the fine-tuning approach in unsupervised learning.\n\nIn this work, we introduce a Retrieval-Augmented Generation-based Relation Extraction (RAG4RE) approach to identify the relationship between a pair of entities in a sentence.",
            "reference_string": "[269292881 | Efeoglu et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
            "venue": "SIGIR-AP",
            "year": 2024,
            "reference_count": 66,
            "citation_count": 37,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3673791.3698415",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.01432, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2165569122",
                    "name": "Heydar Soudani"
                },
                {
                    "authorId": "1713134",
                    "name": "E. Kanoulas"
                },
                {
                    "authorId": "1951737",
                    "name": "Faegheh Hasibi"
                }
            ],
            "abstract": "Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LMs in handling low-frequency entities on question answering tasks. We conduct extensive experiments on twelve LMs of varying size and type and different FT methods, data augmentation, and retrieval models. Our findings indicate that while FT boosts the performance across entities of varying popularity, RAG surpasses FT by a large margin particularly for least popular factual knowledge. Additionally, the success of both RAG and FT approaches is amplified by improving retrieval and data augmentation techniques. Fine tuning, while beneficial for small LMs, requires extensive resources. To address this issue, we propose the new Stimulus RAG approach that surpasses the effectiveness of fine tuning based approaches, thereby eliminating the need for the costly data augmentation and fine tuning step for enriching LMs with less popular factual knowledge.",
            "corpus_id": 268248396,
            "sentences": [
                {
                    "corpus_id": "268248396",
                    "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
                    "text": "Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LMs in handling low-frequency entities on question answering tasks. We conduct extensive experiments on twelve LMs of varying size and type and different FT methods, data augmentation, and retrieval models. Our findings indicate that while FT boosts the performance across entities of varying popularity, RAG surpasses FT by a large margin particularly for least popular factual knowledge. Additionally, the success of both RAG and FT approaches is amplified by improving retrieval and data augmentation techniques. Fine tuning, while beneficial for small LMs, requires extensive resources. To address this issue, we propose the new Stimulus RAG approach that surpasses the effectiveness of fine tuning based approaches, thereby eliminating the need for the costly data augmentation and fine tuning step for enriching LMs with less popular factual knowledge.",
                    "score": 0.5966274498692409,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92822265625
                },
                {
                    "corpus_id": "268248396",
                    "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
                    "text": "Interestingly, larger LMs generally do not benefit from finetuning, while smaller ones do. Therefore, a small fine-tuned LM with RAG can perform on par or better than a large LM; e.g., StableLM2 (1.6B) vs. Llama3 (8B) (Table 3). \u2022 Retrieval model: Comparing retrievers with varying performance in the RAG system, we observe that as the popularity of factual knowledge increases, the performance of the retriever decreases (Figure 7). Moreover, the performance of the RAG system increases by using higher performance retriever (Figures 1 and 8). \u2022 Fine-tuning vs. RAG: Comparing these two knowledge injection methods, RAG substantially outperforms fine-tuning. Fine-tuned LMs combined with RAG either outperform or perform on par with vanilla LMs with RAG in all but one case (Figure 1). \n\nWhile fine-tuning improves accuracy in answering factual questions, both with and without RAG, it demands a considerable amount of effort and resources. This leads us to our second research question: (RQ2): Can we avoid the cost of fine-tuning by developing an advanced RAG approach that surpass the performance of a fine-tuned LM with RAG? To answer this question, we develop Stimulus RAG (SRAG), a new RAG approach that stimulates an LM to generate the correct response based on the provided hint in the prompt. The hint is extracted from the top retrieved documents by the retrieval model. Our results demonstrate that Stimulus RAG outperforms all other combinations of fine-tuning, both with and without retrievethen-generate RAG. \n\nTo summarize, this paper makes the following contributions: \n\n\u2022 We study the effectiveness of fine-tuning and RAG approaches for question answering over less popular factual knowledge and compare the performance of these models across distinct setups: vanilla and fine-tuned models, both with and without RAG, using different data augmentation methods.",
                    "score": 0.6056043704672425,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 4089,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 90
                        },
                        {
                            "start": 91,
                            "end": 228
                        },
                        {
                            "start": 229,
                            "end": 433
                        },
                        {
                            "start": 434,
                            "end": 544
                        },
                        {
                            "start": 545,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 786
                        },
                        {
                            "start": 789,
                            "end": 941
                        },
                        {
                            "start": 942,
                            "end": 1129
                        },
                        {
                            "start": 1130,
                            "end": 1302
                        },
                        {
                            "start": 1303,
                            "end": 1381
                        },
                        {
                            "start": 1382,
                            "end": 1523
                        },
                        {
                            "start": 1526,
                            "end": 1585
                        },
                        {
                            "start": 1588,
                            "end": 1878
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.923828125
                }
            ],
            "relevance_judgement": 0.92822265625,
            "relevance_judgment_input_expanded": "# Title: Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge\n# Venue: SIGIR-AP\n# Authors: Heydar Soudani, E. Kanoulas, Faegheh Hasibi\n## Abstract\nLanguage Models (LMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LMs in handling low-frequency entities on question answering tasks. We conduct extensive experiments on twelve LMs of varying size and type and different FT methods, data augmentation, and retrieval models. Our findings indicate that while FT boosts the performance across entities of varying popularity, RAG surpasses FT by a large margin particularly for least popular factual knowledge. Additionally, the success of both RAG and FT approaches is amplified by improving retrieval and data augmentation techniques. Fine tuning, while beneficial for small LMs, requires extensive resources. To address this issue, we propose the new Stimulus RAG approach that surpasses the effectiveness of fine tuning based approaches, thereby eliminating the need for the costly data augmentation and fine tuning step for enriching LMs with less popular factual knowledge.\n## INTRODUCTION\nInterestingly, larger LMs generally do not benefit from finetuning, while smaller ones do. Therefore, a small fine-tuned LM with RAG can perform on par or better than a large LM; e.g., StableLM2 (1.6B) vs. Llama3 (8B) (Table 3). \u2022 Retrieval model: Comparing retrievers with varying performance in the RAG system, we observe that as the popularity of factual knowledge increases, the performance of the retriever decreases (Figure 7). Moreover, the performance of the RAG system increases by using higher performance retriever (Figures 1 and 8). \u2022 Fine-tuning vs. RAG: Comparing these two knowledge injection methods, RAG substantially outperforms fine-tuning. Fine-tuned LMs combined with RAG either outperform or perform on par with vanilla LMs with RAG in all but one case (Figure 1). \n\nWhile fine-tuning improves accuracy in answering factual questions, both with and without RAG, it demands a considerable amount of effort and resources. This leads us to our second research question: (RQ2): Can we avoid the cost of fine-tuning by developing an advanced RAG approach that surpass the performance of a fine-tuned LM with RAG? To answer this question, we develop Stimulus RAG (SRAG), a new RAG approach that stimulates an LM to generate the correct response based on the provided hint in the prompt. The hint is extracted from the top retrieved documents by the retrieval model. Our results demonstrate that Stimulus RAG outperforms all other combinations of fine-tuning, both with and without retrievethen-generate RAG. \n\nTo summarize, this paper makes the following contributions: \n\n\u2022 We study the effectiveness of fine-tuning and RAG approaches for question answering over less popular factual knowledge and compare the performance of these models across distinct setups: vanilla and fine-tuned models, both with and without RAG, using different data augmentation methods.",
            "reference_string": "[268248396 | Soudani et al. | 2024 | Citations: 37]"
        },
        {
            "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 127,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.07688, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2270181751",
                    "name": "Zilun Zhang"
                },
                {
                    "authorId": "2174678931",
                    "name": "Haozhan Shen"
                },
                {
                    "authorId": "8200875",
                    "name": "Tiancheng Zhao"
                },
                {
                    "authorId": "2330774884",
                    "name": "Yuhao Wang"
                },
                {
                    "authorId": "2330612748",
                    "name": "Bin Chen"
                },
                {
                    "authorId": "2149196373",
                    "name": "Yuxiang Cai"
                },
                {
                    "authorId": "2093090552",
                    "name": "Yongheng Shang"
                },
                {
                    "authorId": "2111612160",
                    "name": "Jianwei Yin"
                }
            ],
            "abstract": "Ultra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000 $\\times$ 100,000 pixels or more) poses a significant challenge for current Remote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize the UHR image to standard input image size, the extensive spatial and contextual information that UHR images contain will be neglected. Otherwise, the original size of these images often exceeds the token limits of standard RSMLLMs, making it difficult to process the entire image and capture long-range dependencies to answer the query based on the abundant visual context. In this paper, we introduce ImageRAG for RS, a training-free framework to address the complexities of analyzing UHR remote sensing imagery. By transforming UHR remote sensing image analysis task to image's long context selection task, we design an innovative image contextual retrieval mechanism based on the Retrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's core innovation lies in its ability to selectively retrieve and focus on the most relevant portions of the UHR image as visual contexts that pertain to a given query. Fast path and slow path are proposed in this framework to handle this task efficiently and effectively. ImageRAG allows RSMLLMs to manage extensive context and spatial information from UHR RSI, ensuring the analysis is both accurate and efficient. Codebase will be released in https://github.com/om-ai-lab/ImageRAG",
            "corpus_id": 273969615,
            "sentences": [
                {
                    "corpus_id": "273969615",
                    "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
                    "text": "Retrieval-Augmented Generation (RAG) addresses the limitations of traditional generative models in handling specialized or long-tail knowledge. Early models like GPT, trained on vast corpora, excel at general queries but struggle with domain-specific or rare information, often generating hallucinations [95]. RAG, introduced by Facebook AI Research in 2020 [96], enhances generative models by integrating realtime document retrieval, improving accuracy and contextual grounding. Gao et al. [97] categorize RAG into Naive, Advanced, and Modular paradigms, detailing key components like retrievers, generators, and augmentation methods. A comparative study by Ovadia et al. [98] shows that RAG outperforms unsupervised fine-tuning, particularly in scenarios involving new or unseen knowledge, underscoring its superiority in knowledge injection and model adaptation. \n\nThe effectiveness of RAG systems heavily depends on the quality and relevance of the retrieved knowledge, which directly influences the accuracy and factual grounding of generated content. To enhance retrieval efficiency and overcome the limitations of traditional methods, several advancements have been proposed, particularly for zero-shot and few-shot retrieval tasks. Techniques such as HyDE [99] and REINA [100] utilize LLMs to generate hypothetical documents, improving retrieval performance without requiring labeled data. The Rewrite-Retrieve-Read [101] framework introduces a query rewriting step, allowing the input query to be better aligned with retrieval modules. By using reinforcement learning to adapt queries, R3 enhances retrieval quality, improving performance in open-domain and multiple-choice question answering tasks. Promptagator [102] demonstrates the effectiveness of few-shot learning in dense retrieval, utilizing LLMs to generate synthetic training data from minimal examples, surpassing models trained on large-scale datasets like MS MARCO. This underscores the viability of few-shot learning and LLM-generated synthetic data in resource-constrained settings. To bridge the preference gap between retrievers and LLMs, Zixuan Ke et al. [103] introduce the BGM framework, which employs a sequence-to-sequence model to align retrieved information with LLM preferences.",
                    "score": 0.8958595735737092,
                    "section_title": "C. Retrieval-Augmented Generation",
                    "char_start_offset": 65622,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 309
                        },
                        {
                            "start": 310,
                            "end": 479
                        },
                        {
                            "start": 480,
                            "end": 635
                        },
                        {
                            "start": 636,
                            "end": 865
                        },
                        {
                            "start": 868,
                            "end": 1056
                        },
                        {
                            "start": 1057,
                            "end": 1239
                        },
                        {
                            "start": 1240,
                            "end": 1397
                        },
                        {
                            "start": 1398,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1708
                        },
                        {
                            "start": 1709,
                            "end": 1938
                        },
                        {
                            "start": 1939,
                            "end": 2057
                        },
                        {
                            "start": 2058,
                            "end": 2263
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 358,
                            "end": 362,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 1264,
                            "end": 1268,
                            "matchedPaperCorpusId": "254877046"
                        },
                        {
                            "start": 1424,
                            "end": 1429,
                            "matchedPaperCorpusId": "258841283"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9248046875
                }
            ],
            "relevance_judgement": 0.9248046875,
            "relevance_judgment_input_expanded": "# Title: Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG\n# Venue: arXiv.org\n# Authors: Zilun Zhang, Haozhan Shen, Tiancheng Zhao, Yuhao Wang, Bin Chen, Yuxiang Cai, Yongheng Shang, Jianwei Yin\n## Abstract\nUltra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000 $\\times$ 100,000 pixels or more) poses a significant challenge for current Remote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize the UHR image to standard input image size, the extensive spatial and contextual information that UHR images contain will be neglected. Otherwise, the original size of these images often exceeds the token limits of standard RSMLLMs, making it difficult to process the entire image and capture long-range dependencies to answer the query based on the abundant visual context. In this paper, we introduce ImageRAG for RS, a training-free framework to address the complexities of analyzing UHR remote sensing imagery. By transforming UHR remote sensing image analysis task to image's long context selection task, we design an innovative image contextual retrieval mechanism based on the Retrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's core innovation lies in its ability to selectively retrieve and focus on the most relevant portions of the UHR image as visual contexts that pertain to a given query. Fast path and slow path are proposed in this framework to handle this task efficiently and effectively. ImageRAG allows RSMLLMs to manage extensive context and spatial information from UHR RSI, ensuring the analysis is both accurate and efficient. Codebase will be released in https://github.com/om-ai-lab/ImageRAG\n## C. Retrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) addresses the limitations of traditional generative models in handling specialized or long-tail knowledge. Early models like GPT, trained on vast corpora, excel at general queries but struggle with domain-specific or rare information, often generating hallucinations [95]. RAG, introduced by Facebook AI Research in 2020 [96], enhances generative models by integrating realtime document retrieval, improving accuracy and contextual grounding. Gao et al. [97] categorize RAG into Naive, Advanced, and Modular paradigms, detailing key components like retrievers, generators, and augmentation methods. A comparative study by Ovadia et al. [98] shows that RAG outperforms unsupervised fine-tuning, particularly in scenarios involving new or unseen knowledge, underscoring its superiority in knowledge injection and model adaptation. \n\nThe effectiveness of RAG systems heavily depends on the quality and relevance of the retrieved knowledge, which directly influences the accuracy and factual grounding of generated content. To enhance retrieval efficiency and overcome the limitations of traditional methods, several advancements have been proposed, particularly for zero-shot and few-shot retrieval tasks. Techniques such as HyDE [99] and REINA [100] utilize LLMs to generate hypothetical documents, improving retrieval performance without requiring labeled data. The Rewrite-Retrieve-Read [101] framework introduces a query rewriting step, allowing the input query to be better aligned with retrieval modules. By using reinforcement learning to adapt queries, R3 enhances retrieval quality, improving performance in open-domain and multiple-choice question answering tasks. Promptagator [102] demonstrates the effectiveness of few-shot learning in dense retrieval, utilizing LLMs to generate synthetic training data from minimal examples, surpassing models trained on large-scale datasets like MS MARCO. This underscores the viability of few-shot learning and LLM-generated synthetic data in resource-constrained settings. To bridge the preference gap between retrievers and LLMs, Zixuan Ke et al. [103] introduce the BGM framework, which employs a sequence-to-sequence model to align retrieved information with LLM preferences.",
            "reference_string": "[273969615 | Zhang et al. | 2024 | Citations: 3]"
        },
        {
            "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.18365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275628230",
                    "name": "Yiteng Tu"
                },
                {
                    "authorId": "2147219374",
                    "name": "Weihang Su"
                },
                {
                    "authorId": "2290870875",
                    "name": "Yujia Zhou"
                },
                {
                    "authorId": "2260835922",
                    "name": "Yiqun Liu"
                },
                {
                    "authorId": "2256982003",
                    "name": "Qingyao Ai"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.",
            "corpus_id": 275993994,
            "sentences": [
                {
                    "corpus_id": "275993994",
                    "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects",
                    "text": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.",
                    "score": 0.6617351671827728,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9208984375
                }
            ],
            "relevance_judgement": 0.9208984375,
            "relevance_judgment_input_expanded": "# Title: RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects\n# Venue: arXiv.org\n# Authors: Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Qingyao Ai\n## Abstract\nRetrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.\n",
            "reference_string": "[275993994 | Tu et al. | 2025 | Citations: 6]"
        },
        {
            "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11201, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279752649",
                    "name": "Scott Barnett"
                },
                {
                    "authorId": "2279020735",
                    "name": "Zach Brannelly"
                },
                {
                    "authorId": "2266469333",
                    "name": "Stefanus Kurniawan"
                },
                {
                    "authorId": "2307101480",
                    "name": "Sheng Wong"
                }
            ],
            "abstract": "Large Language Models (LLMs) have the unique capability to understand and generate human-like text from input queries. When fine-tuned, these models show enhanced performance on domain-specific queries. OpenAI highlights the process of fine-tuning, stating:\"To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples, but the right number varies greatly based on the exact use case.\"This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI. This study highlights the need for vigorous investigation and validation of fine-tuned models for domain-specific tasks.",
            "corpus_id": 270560495,
            "sentences": [
                {
                    "corpus_id": "270560495",
                    "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
                    "text": "Large Language Models (LLMs) have the unique capability to understand and generate human-like text from input queries. When fine-tuned, these models show enhanced performance on domain-specific queries. OpenAI highlights the process of fine-tuning, stating:\"To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples, but the right number varies greatly based on the exact use case.\"This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI. This study highlights the need for vigorous investigation and validation of fine-tuned models for domain-specific tasks.",
                    "score": 0.5988536024455067,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9189453125
                }
            ],
            "relevance_judgement": 0.9189453125,
            "relevance_judgment_input_expanded": "# Title: Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models\n# Venue: arXiv.org\n# Authors: Scott Barnett, Zach Brannelly, Stefanus Kurniawan, Sheng Wong\n## Abstract\nLarge Language Models (LLMs) have the unique capability to understand and generate human-like text from input queries. When fine-tuned, these models show enhanced performance on domain-specific queries. OpenAI highlights the process of fine-tuning, stating:\"To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples, but the right number varies greatly based on the exact use case.\"This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI. This study highlights the need for vigorous investigation and validation of fine-tuned models for domain-specific tasks.\n",
            "reference_string": "[270560495 | Barnett et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Class-RAG: Real-Time Content Moderation with Retrieval Augmented Generation",
            "venue": "",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.14881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327003851",
                    "name": "Jianfa Chen"
                },
                {
                    "authorId": "2326992786",
                    "name": "Emily Shen"
                },
                {
                    "authorId": "2297187181",
                    "name": "Trupti Bavalatti"
                },
                {
                    "authorId": "2327028660",
                    "name": "Xiaowen Lin"
                },
                {
                    "authorId": "2326986310",
                    "name": "Yongkai Wang"
                },
                {
                    "authorId": "2327158340",
                    "name": "Shuming Hu"
                },
                {
                    "authorId": "2322094813",
                    "name": "Harihar Subramanyam"
                },
                {
                    "authorId": "2149726609",
                    "name": "Ksheeraj Sai Vepuri"
                },
                {
                    "authorId": "2327303021",
                    "name": "Ming Jiang"
                },
                {
                    "authorId": "2327505613",
                    "name": "Ji Qi"
                },
                {
                    "authorId": "2287762612",
                    "name": "Li Chen"
                },
                {
                    "authorId": "2326964342",
                    "name": "Nan Jiang"
                },
                {
                    "authorId": "2287848816",
                    "name": "Ankit Jain"
                }
            ],
            "abstract": "Robust content moderation classifiers are essential for the safety of Generative AI systems. In this task, differences between safe and unsafe inputs are often extremely subtle, making it difficult for classifiers (and indeed, even humans) to properly distinguish violating vs. benign samples without context or explanation. Scaling risk discovery and mitigation through continuous model fine-tuning is also slow, challenging and costly, preventing developers from being able to respond quickly and effectively to emergent harms. We propose a Classification approach employing Retrieval-Augmented Generation (Class-RAG). Class-RAG extends the capability of its base LLM through access to a retrieval library which can be dynamically updated to enable semantic hotfixing for immediate, flexible risk mitigation. Compared to model fine-tuning, Class-RAG demonstrates flexibility and transparency in decision-making, outperforms on classification and is more robust against adversarial attack, as evidenced by empirical studies. Our findings also suggest that Class-RAG performance scales with retrieval library size, indicating that increasing the library size is a viable and low-cost approach to improve content moderation.",
            "corpus_id": 273502659,
            "sentences": [
                {
                    "corpus_id": "273502659",
                    "title": "Class-RAG: Real-Time Content Moderation with Retrieval Augmented Generation",
                    "text": "Robust content moderation classifiers are essential for the safety of Generative AI systems. In this task, differences between safe and unsafe inputs are often extremely subtle, making it difficult for classifiers (and indeed, even humans) to properly distinguish violating vs. benign samples without context or explanation. Scaling risk discovery and mitigation through continuous model fine-tuning is also slow, challenging and costly, preventing developers from being able to respond quickly and effectively to emergent harms. We propose a Classification approach employing Retrieval-Augmented Generation (Class-RAG). Class-RAG extends the capability of its base LLM through access to a retrieval library which can be dynamically updated to enable semantic hotfixing for immediate, flexible risk mitigation. Compared to model fine-tuning, Class-RAG demonstrates flexibility and transparency in decision-making, outperforms on classification and is more robust against adversarial attack, as evidenced by empirical studies. Our findings also suggest that Class-RAG performance scales with retrieval library size, indicating that increasing the library size is a viable and low-cost approach to improve content moderation.",
                    "score": 0.6110743234244418,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91845703125
                }
            ],
            "relevance_judgement": 0.91845703125,
            "relevance_judgment_input_expanded": "# Title: Class-RAG: Real-Time Content Moderation with Retrieval Augmented Generation\n# Venue: \n# Authors: Jianfa Chen, Emily Shen, Trupti Bavalatti, Xiaowen Lin, Yongkai Wang, Shuming Hu, Harihar Subramanyam, Ksheeraj Sai Vepuri, Ming Jiang, Ji Qi, Li Chen, Nan Jiang, Ankit Jain\n## Abstract\nRobust content moderation classifiers are essential for the safety of Generative AI systems. In this task, differences between safe and unsafe inputs are often extremely subtle, making it difficult for classifiers (and indeed, even humans) to properly distinguish violating vs. benign samples without context or explanation. Scaling risk discovery and mitigation through continuous model fine-tuning is also slow, challenging and costly, preventing developers from being able to respond quickly and effectively to emergent harms. We propose a Classification approach employing Retrieval-Augmented Generation (Class-RAG). Class-RAG extends the capability of its base LLM through access to a retrieval library which can be dynamically updated to enable semantic hotfixing for immediate, flexible risk mitigation. Compared to model fine-tuning, Class-RAG demonstrates flexibility and transparency in decision-making, outperforms on classification and is more robust against adversarial attack, as evidenced by empirical studies. Our findings also suggest that Class-RAG performance scales with retrieval library size, indicating that increasing the library size is a viable and low-cost approach to improve content moderation.\n",
            "reference_string": "[273502659 | Chen et al. | 2024 | Citations: 2]"
        },
        {
            "title": "RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 74,
            "citation_count": 11,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.13509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261354998",
                    "name": "Xinze Li"
                },
                {
                    "authorId": "2124028252",
                    "name": "Senkun Mei"
                },
                {
                    "authorId": "49047064",
                    "name": "Zhenghao Liu"
                },
                {
                    "authorId": "2277242040",
                    "name": "Yukun Yan"
                },
                {
                    "authorId": "2267033597",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2314785970",
                    "name": "Shi Yu"
                },
                {
                    "authorId": "1633538428",
                    "name": "Zheni Zeng"
                },
                {
                    "authorId": "2327546188",
                    "name": "Hao Chen"
                },
                {
                    "authorId": "2204644192",
                    "name": "Ge Yu"
                },
                {
                    "authorId": "2290295914",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2273551430",
                    "name": "Maosong Sun"
                },
                {
                    "authorId": "2139787803",
                    "name": "Chenyan Xiong"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has proven its effectiveness in mitigating hallucinations in Large Language Models (LLMs) by retrieving knowledge from external resources. To adapt LLMs for the RAG systems, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system. In this paper, we propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG systems by aligning data preferences between different RAG modules. DDR works by collecting the rewards to optimize each agent in the RAG system with the rollout method, which prompts agents to sample some potential responses as perturbations, evaluates the impact of these perturbations on the whole RAG system, and subsequently optimizes the agent to produce outputs that improve the performance of the RAG system. Our experiments on various knowledge-intensive tasks demonstrate that DDR significantly outperforms the SFT method, particularly for LLMs with smaller-scale parameters that depend more on the retrieved knowledge. Additionally, DDR exhibits a stronger capability to align the data preference between RAG modules. The DDR method makes the generation module more effective in extracting key information from documents and mitigating conflicts between parametric memory and external knowledge. All codes are available at https://github.com/OpenMatch/RAG-DDR.",
            "corpus_id": 273403480,
            "sentences": [
                {
                    "corpus_id": "273403480",
                    "title": "RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards",
                    "text": "Retrieval-Augmented Generation (RAG) has proven its effectiveness in mitigating hallucinations in Large Language Models (LLMs) by retrieving knowledge from external resources. To adapt LLMs for the RAG systems, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system. In this paper, we propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG systems by aligning data preferences between different RAG modules. DDR works by collecting the rewards to optimize each agent in the RAG system with the rollout method, which prompts agents to sample some potential responses as perturbations, evaluates the impact of these perturbations on the whole RAG system, and subsequently optimizes the agent to produce outputs that improve the performance of the RAG system. Our experiments on various knowledge-intensive tasks demonstrate that DDR significantly outperforms the SFT method, particularly for LLMs with smaller-scale parameters that depend more on the retrieved knowledge. Additionally, DDR exhibits a stronger capability to align the data preference between RAG modules. The DDR method makes the generation module more effective in extracting key information from documents and mitigating conflicts between parametric memory and external knowledge. All codes are available at https://github.com/OpenMatch/RAG-DDR.",
                    "score": 0.5880863644434837,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91650390625
                }
            ],
            "relevance_judgement": 0.91650390625,
            "relevance_judgment_input_expanded": "# Title: RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards\n# Venue: International Conference on Learning Representations\n# Authors: Xinze Li, Senkun Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan Liu, Maosong Sun, Chenyan Xiong\n## Abstract\nRetrieval-Augmented Generation (RAG) has proven its effectiveness in mitigating hallucinations in Large Language Models (LLMs) by retrieving knowledge from external resources. To adapt LLMs for the RAG systems, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system. In this paper, we propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG systems by aligning data preferences between different RAG modules. DDR works by collecting the rewards to optimize each agent in the RAG system with the rollout method, which prompts agents to sample some potential responses as perturbations, evaluates the impact of these perturbations on the whole RAG system, and subsequently optimizes the agent to produce outputs that improve the performance of the RAG system. Our experiments on various knowledge-intensive tasks demonstrate that DDR significantly outperforms the SFT method, particularly for LLMs with smaller-scale parameters that depend more on the retrieved knowledge. Additionally, DDR exhibits a stronger capability to align the data preference between RAG modules. The DDR method makes the generation module more effective in extracting key information from documents and mitigating conflicts between parametric memory and external knowledge. All codes are available at https://github.com/OpenMatch/RAG-DDR.\n",
            "reference_string": "[273403480 | Li et al. | 2024 | Citations: 11]"
        },
        {
            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 74,
            "citation_count": 99,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.07437, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301195826",
                    "name": "Hao Yu"
                },
                {
                    "authorId": "2301156375",
                    "name": "Aoran Gan"
                },
                {
                    "authorId": "2263584690",
                    "name": "Kai Zhang"
                },
                {
                    "authorId": "66187823",
                    "name": "Shiwei Tong"
                },
                {
                    "authorId": "2301169021",
                    "name": "Qi Liu"
                },
                {
                    "authorId": "2301158163",
                    "name": "Zhaofeng Liu"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external information retrieval. Evaluating these RAG systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of RAG (Auepora) and aim to provide a comprehensive overview of the evaluation and benchmarks of RAG systems. Specifically, we examine and compare several quantifiable metrics of the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the current RAG benchmarks, encompassing the possible output and ground truth pairs. We then analyze the various datasets and metrics, discuss the limitations of current benchmarks, and suggest potential directions to advance the field of RAG benchmarks.",
            "corpus_id": 269758033,
            "sentences": [
                {
                    "corpus_id": "269758033",
                    "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                    "text": "Retrieval-Augmented Generation (RAG) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external information retrieval. Evaluating these RAG systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of RAG (Auepora) and aim to provide a comprehensive overview of the evaluation and benchmarks of RAG systems. Specifically, we examine and compare several quantifiable metrics of the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the current RAG benchmarks, encompassing the possible output and ground truth pairs. We then analyze the various datasets and metrics, discuss the limitations of current benchmarks, and suggest potential directions to advance the field of RAG benchmarks.",
                    "score": 0.8132694539343907,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9140625
                }
            ],
            "relevance_judgement": 0.9140625,
            "relevance_judgment_input_expanded": "# Title: Evaluation of Retrieval-Augmented Generation: A Survey\n# Venue: arXiv.org\n# Authors: Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, Zhaofeng Liu\n## Abstract\nRetrieval-Augmented Generation (RAG) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external information retrieval. Evaluating these RAG systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of RAG (Auepora) and aim to provide a comprehensive overview of the evaluation and benchmarks of RAG systems. Specifically, we examine and compare several quantifiable metrics of the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the current RAG benchmarks, encompassing the possible output and ground truth pairs. We then analyze the various datasets and metrics, discuss the limitations of current benchmarks, and suggest potential directions to advance the field of RAG benchmarks.\n",
            "reference_string": "[269758033 | Yu et al. | 2024 | Citations: 99]"
        },
        {
            "title": "ENHANCED RETRIEVAL-AUGMENTED GENERATION FOR STUDENT MENTAL HEALTH SUPPORT USING GENERATIVE ARTIFICIAL INTELLIGENCE",
            "venue": "INTERNATIONAL JOURNAL OF COMPUTER ENGINEERING & TECHNOLOGY",
            "year": 2025,
            "reference_count": 4,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.34218/ijcet_16_01_277",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.34218/ijcet_16_01_277?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.34218/ijcet_16_01_277, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2359473811",
                    "name": "Dr. P. Swathi"
                },
                {
                    "authorId": "2359477815",
                    "name": "Dr. P. Jyotsna"
                }
            ],
            "abstract": "Generative AI, combined with Retrieval-Augmented Generation (RAG), enhances large language models (LLMs) by integrating dynamic information retrieval, significantly improving response accuracy and contextual relevance. This study explores the implementation of Generative AI-powered RAG in student mental health support systems, demonstrating how it mitigates hallucinations, incorporates real-time knowledge updates, and personalizes assistance. The proposed system integrates retrieval and generation components, leveraging vector-based search mechanisms to access domain-specific mental health knowledge. Comparative analysis with traditional LLMs highlights RAG\u2019s superior accuracy, reduced misinformation, and improved response reliability. Experimental evaluation using context precision, hit rate, faithfulness, and user satisfaction metrics validates the system's effectiveness. This research underscores the transformative potential of Generative AI-driven RAG in delivering scalable, evidence-based mental health support for students.",
            "corpus_id": 278020653,
            "sentences": [
                {
                    "corpus_id": "278020653",
                    "title": "ENHANCED RETRIEVAL-AUGMENTED GENERATION FOR STUDENT MENTAL HEALTH SUPPORT USING GENERATIVE ARTIFICIAL INTELLIGENCE",
                    "text": "Generative AI, combined with Retrieval-Augmented Generation (RAG), enhances large language models (LLMs) by integrating dynamic information retrieval, significantly improving response accuracy and contextual relevance. This study explores the implementation of Generative AI-powered RAG in student mental health support systems, demonstrating how it mitigates hallucinations, incorporates real-time knowledge updates, and personalizes assistance. The proposed system integrates retrieval and generation components, leveraging vector-based search mechanisms to access domain-specific mental health knowledge. Comparative analysis with traditional LLMs highlights RAG\u2019s superior accuracy, reduced misinformation, and improved response reliability. Experimental evaluation using context precision, hit rate, faithfulness, and user satisfaction metrics validates the system's effectiveness. This research underscores the transformative potential of Generative AI-driven RAG in delivering scalable, evidence-based mental health support for students.",
                    "score": 0.64075589794761,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9130859375
                }
            ],
            "relevance_judgement": 0.9130859375,
            "relevance_judgment_input_expanded": "# Title: ENHANCED RETRIEVAL-AUGMENTED GENERATION FOR STUDENT MENTAL HEALTH SUPPORT USING GENERATIVE ARTIFICIAL INTELLIGENCE\n# Venue: INTERNATIONAL JOURNAL OF COMPUTER ENGINEERING & TECHNOLOGY\n# Authors: Dr. P. Swathi, Dr. P. Jyotsna\n## Abstract\nGenerative AI, combined with Retrieval-Augmented Generation (RAG), enhances large language models (LLMs) by integrating dynamic information retrieval, significantly improving response accuracy and contextual relevance. This study explores the implementation of Generative AI-powered RAG in student mental health support systems, demonstrating how it mitigates hallucinations, incorporates real-time knowledge updates, and personalizes assistance. The proposed system integrates retrieval and generation components, leveraging vector-based search mechanisms to access domain-specific mental health knowledge. Comparative analysis with traditional LLMs highlights RAG\u2019s superior accuracy, reduced misinformation, and improved response reliability. Experimental evaluation using context precision, hit rate, faithfulness, and user satisfaction metrics validates the system's effectiveness. This research underscores the transformative potential of Generative AI-driven RAG in delivering scalable, evidence-based mental health support for students.\n",
            "reference_string": "[278020653 | Swathi et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Adapting Generative Large Language Models for Information Extraction from Unstructured Electronic Health Records in Residential Aged Care: A Comparative Analysis of Training Approaches",
            "venue": "J. Heal. Informatics Res.",
            "year": 2025,
            "reference_count": 69,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1007/s41666-025-00190-z",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12037947, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277877538",
                    "name": "D. Vithanage"
                },
                {
                    "authorId": "144551284",
                    "name": "C. Deng"
                },
                {
                    "authorId": "2278214328",
                    "name": "Lei Wang"
                },
                {
                    "authorId": "1972725075",
                    "name": "M. Yin"
                },
                {
                    "authorId": "2069756289",
                    "name": "M. Alkhalaf"
                },
                {
                    "authorId": "2109338789",
                    "name": "Zhenyu Zhang"
                },
                {
                    "authorId": "2117078401",
                    "name": "Yunshu Zhu"
                },
                {
                    "authorId": "2231452266",
                    "name": "P. Yu"
                }
            ],
            "abstract": "Information extraction (IE) of unstructured electronic health records is challenging due to the semantic complexity of textual data. Generative large language models (LLMs) offer promising solutions to address this challenge. However, identifying the best training methods to adapt LLMs for IE in residential aged care settings remains underexplored. This research addresses this challenge by evaluating the effects of zero-shot and few-shot learning, both with and without parameter-efficient fine-tuning (PEFT) and retrieval-augmented generation (RAG) using Llama 3.1-8B. The study performed named entity recognition (NER) to nursing notes from Australian aged care facilities (RACFs), focusing on agitation in dementia and malnutrition risk factors. Performance evaluation includes accuracy, macro-averaged precision, recall, and F1 score. We used non-parametric statistical methods to compare if the differences were statistically significant. Results show that zero-shot and few-shot learning, whether combined with PEFT or RAG, achieve comparable performance across the clinical domains when the same prompting template is used. Few-shot learning significantly outperforms zero-shot learning when neither PEFT nor RAG is applied. Notably, PEFT significantly improves model performance in both zero-shot and few-shot learning; however, RAG significantly improves performance only in few-shot learning. After PEFT, the performance of zero-shot learning reaches a comparable level with few-shot learning. However, few-shot learning with RAG significantly outperforms zero-shot learning with RAG. We also found a similar level of performance between few-shot learning with RAG and zero-shot learning with PEFT. These findings provide valuable insights for researchers, practitioners, and stakeholders to optimize the use of generative LLMs in clinical IE. Supplementary Information The online version contains supplementary material available at 10.1007/s41666-025-00190-z.",
            "corpus_id": 276526965,
            "sentences": [],
            "relevance_judgement": 0.9130859375,
            "relevance_judgment_input_expanded": "# Title: Adapting Generative Large Language Models for Information Extraction from Unstructured Electronic Health Records in Residential Aged Care: A Comparative Analysis of Training Approaches\n# Venue: J. Heal. Informatics Res.\n# Authors: D. Vithanage, C. Deng, Lei Wang, M. Yin, M. Alkhalaf, Zhenyu Zhang, Yunshu Zhu, P. Yu\n## Abstract\nInformation extraction (IE) of unstructured electronic health records is challenging due to the semantic complexity of textual data. Generative large language models (LLMs) offer promising solutions to address this challenge. However, identifying the best training methods to adapt LLMs for IE in residential aged care settings remains underexplored. This research addresses this challenge by evaluating the effects of zero-shot and few-shot learning, both with and without parameter-efficient fine-tuning (PEFT) and retrieval-augmented generation (RAG) using Llama 3.1-8B. The study performed named entity recognition (NER) to nursing notes from Australian aged care facilities (RACFs), focusing on agitation in dementia and malnutrition risk factors. Performance evaluation includes accuracy, macro-averaged precision, recall, and F1 score. We used non-parametric statistical methods to compare if the differences were statistically significant. Results show that zero-shot and few-shot learning, whether combined with PEFT or RAG, achieve comparable performance across the clinical domains when the same prompting template is used. Few-shot learning significantly outperforms zero-shot learning when neither PEFT nor RAG is applied. Notably, PEFT significantly improves model performance in both zero-shot and few-shot learning; however, RAG significantly improves performance only in few-shot learning. After PEFT, the performance of zero-shot learning reaches a comparable level with few-shot learning. However, few-shot learning with RAG significantly outperforms zero-shot learning with RAG. We also found a similar level of performance between few-shot learning with RAG and zero-shot learning with PEFT. These findings provide valuable insights for researchers, practitioners, and stakeholders to optimize the use of generative LLMs in clinical IE. Supplementary Information The online version contains supplementary material available at 10.1007/s41666-025-00190-z.\n",
            "reference_string": "[276526965 | Vithanage et al. | 2025 | Citations: 0]"
        },
        {
            "title": "ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 42,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.11929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2341534946",
                    "name": "Peter Devine"
                }
            ],
            "abstract": "Retrieval Augmented Generation (RAG) systems have been shown to improve the accuracy of Large Language Model (LLM) outputs. However, these models can often achieve low accuracy when applied to new data domains. We introduce the Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG) framework, designed to improve the accuracy of RAG systems on a given domain by training LLMs without manually labeled data or using larger teacher models. By generating and filtering synthetic training data and performing LoRA fine-tuning, ALoFTRAG improves citation and answer accuracy across 20 datasets in 26 languages by, on average, 8.3% and 3.0% respectively. Our results demonstrate that ALoFTRAG offers a practical, cost-effective, and data-secure solution for improving RAG accuracy, making it particularly applicable to sensitive domains such as healthcare and finance.",
            "corpus_id": 275788867,
            "sentences": [
                {
                    "corpus_id": "275788867",
                    "title": "ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation",
                    "text": "Retrieval augmented generation (RAG) models are a subset of large language models (LLMs) which combine the generation capabilities of conventional LLMs with the factual grounding of information retrieval (IR) models to create more factually accurate outputs from LLMs (Lewis et al., 2020). RAG models work by taking a user question as input, and then selecting several reference texts with high semantic similarity (determined by an IR model) from a database. An LLM is then given these texts with the original question and is instructed to answer the question basing the answer on the relevant reference texts. \n\nRAG not only allows for more accurate answers to questions regarding general public knowledge (Guu et al., 2020;Ram et al., 2023), it also allows LLMs to generate responses based on locally available or domain specific information that it has not necessarily been trained upon (Gao et al., 2023;Zhang et al., 2024). \n\nHowever, the models that have exhibited the highest performance in RAG tasks are based on proprietary cloud-based LLMs, meaning that LLMs run locally are more likely to generate hallucinations or other untruthful outputs when being used for RAG (Hughes et al., 2023). Moreover, LLMs that are not trained using data from a specific domain exhibit lower RAG accuracy in that domain (Zhang et al., 2024). \n\nTo address this, we propose a framework called Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG). ALoF-TRAG improves the accuracy of base RAG systems by automatically training on the data which the system will later be used, all without using larger models or labelled data. \n\nWe demonstrate the effectiveness of ALoF-TRAG by performing experiments on 20 datasets in 26 languages across a variety of domains and comparing the accuracy to simply using the base LLM for RAG. We show that the ALoFTRAG approach improves both the citation accuracy and answer accuracy of RAG models across almost all datasets compared to the base RAG model. \n\nOur findings inform the future implementation of RAG systems, allowing users to fine-tune their RAG models on local data using modest hardware, enabling improved RAG accuracy while preserving data security.",
                    "score": 0.5804842388852964,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 289
                        },
                        {
                            "start": 290,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 611
                        },
                        {
                            "start": 614,
                            "end": 929
                        },
                        {
                            "start": 932,
                            "end": 1199
                        },
                        {
                            "start": 1200,
                            "end": 1333
                        },
                        {
                            "start": 1336,
                            "end": 1463
                        },
                        {
                            "start": 1464,
                            "end": 1640
                        },
                        {
                            "start": 1643,
                            "end": 1838
                        },
                        {
                            "start": 1839,
                            "end": 2002
                        },
                        {
                            "start": 2005,
                            "end": 2211
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 268,
                            "end": 288,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 708,
                            "end": 726,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 726,
                            "end": 743,
                            "matchedPaperCorpusId": "256459451"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9072265625
                }
            ],
            "relevance_judgement": 0.9072265625,
            "relevance_judgment_input_expanded": "# Title: ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation\n# Venue: arXiv.org\n# Authors: Peter Devine\n## Abstract\nRetrieval Augmented Generation (RAG) systems have been shown to improve the accuracy of Large Language Model (LLM) outputs. However, these models can often achieve low accuracy when applied to new data domains. We introduce the Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG) framework, designed to improve the accuracy of RAG systems on a given domain by training LLMs without manually labeled data or using larger teacher models. By generating and filtering synthetic training data and performing LoRA fine-tuning, ALoFTRAG improves citation and answer accuracy across 20 datasets in 26 languages by, on average, 8.3% and 3.0% respectively. Our results demonstrate that ALoFTRAG offers a practical, cost-effective, and data-secure solution for improving RAG accuracy, making it particularly applicable to sensitive domains such as healthcare and finance.\n## Introduction\nRetrieval augmented generation (RAG) models are a subset of large language models (LLMs) which combine the generation capabilities of conventional LLMs with the factual grounding of information retrieval (IR) models to create more factually accurate outputs from LLMs (Lewis et al., 2020). RAG models work by taking a user question as input, and then selecting several reference texts with high semantic similarity (determined by an IR model) from a database. An LLM is then given these texts with the original question and is instructed to answer the question basing the answer on the relevant reference texts. \n\nRAG not only allows for more accurate answers to questions regarding general public knowledge (Guu et al., 2020;Ram et al., 2023), it also allows LLMs to generate responses based on locally available or domain specific information that it has not necessarily been trained upon (Gao et al., 2023;Zhang et al., 2024). \n\nHowever, the models that have exhibited the highest performance in RAG tasks are based on proprietary cloud-based LLMs, meaning that LLMs run locally are more likely to generate hallucinations or other untruthful outputs when being used for RAG (Hughes et al., 2023). Moreover, LLMs that are not trained using data from a specific domain exhibit lower RAG accuracy in that domain (Zhang et al., 2024). \n\nTo address this, we propose a framework called Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG). ALoF-TRAG improves the accuracy of base RAG systems by automatically training on the data which the system will later be used, all without using larger models or labelled data. \n\nWe demonstrate the effectiveness of ALoF-TRAG by performing experiments on 20 datasets in 26 languages across a variety of domains and comparing the accuracy to simply using the base LLM for RAG. We show that the ALoFTRAG approach improves both the citation accuracy and answer accuracy of RAG models across almost all datasets compared to the base RAG model. \n\nOur findings inform the future implementation of RAG systems, allowing users to fine-tune their RAG models on local data using modest hardware, enabling improved RAG accuracy while preserving data security.",
            "reference_string": "[275788867 | Devine | 2025 | Citations: 0]"
        },
        {
            "title": "How Does Knowledge Selection Help Retrieval Augmented Generation?",
            "venue": "",
            "year": 2024,
            "reference_count": 24,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.13258, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "89919188",
                    "name": "Xiangci Li"
                },
                {
                    "authorId": "2284862335",
                    "name": "Jessica Ouyang"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) is a powerful method for enhancing natural language generation by integrating external knowledge into a model's output. While prior work has demonstrated the importance of improving knowledge retrieval for boosting generation quality, the role of knowledge selection remains less clear. This paper empirically analyzes how knowledge selection influences downstream generation performance in RAG systems. By simulating different retrieval and selection conditions through a controlled mixture of gold and distractor knowledge, we assess the impact of these factors on generation outcomes. Our findings indicate that the downstream generator model's capability, as well as the complexity of the task and dataset, significantly influence the impact of knowledge selection on the overall RAG system performance. In typical scenarios, improving the knowledge recall score is key to enhancing generation outcomes, with the knowledge selector providing limited benefit when a strong generator model is used on clear, well-defined tasks. For weaker generator models or more ambiguous tasks and datasets, the knowledge F1 score becomes a critical factor, and the knowledge selector plays a more prominent role in improving overall performance.",
            "corpus_id": 273403839,
            "sentences": [
                {
                    "corpus_id": "273403839",
                    "title": "How Does Knowledge Selection Help Retrieval Augmented Generation?",
                    "text": "Retrieval-augmented generation (RAG) is a powerful method for enhancing natural language generation by integrating external knowledge into a model's output. While prior work has demonstrated the importance of improving knowledge retrieval for boosting generation quality, the role of knowledge selection remains less clear. This paper empirically analyzes how knowledge selection influences downstream generation performance in RAG systems. By simulating different retrieval and selection conditions through a controlled mixture of gold and distractor knowledge, we assess the impact of these factors on generation outcomes. Our findings indicate that the downstream generator model's capability, as well as the complexity of the task and dataset, significantly influence the impact of knowledge selection on the overall RAG system performance. In typical scenarios, improving the knowledge recall score is key to enhancing generation outcomes, with the knowledge selector providing limited benefit when a strong generator model is used on clear, well-defined tasks. For weaker generator models or more ambiguous tasks and datasets, the knowledge F1 score becomes a critical factor, and the knowledge selector plays a more prominent role in improving overall performance.",
                    "score": 0.6113333185323225,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90625
                }
            ],
            "relevance_judgement": 0.90625,
            "relevance_judgment_input_expanded": "# Title: How Does Knowledge Selection Help Retrieval Augmented Generation?\n# Venue: \n# Authors: Xiangci Li, Jessica Ouyang\n## Abstract\nRetrieval-augmented generation (RAG) is a powerful method for enhancing natural language generation by integrating external knowledge into a model's output. While prior work has demonstrated the importance of improving knowledge retrieval for boosting generation quality, the role of knowledge selection remains less clear. This paper empirically analyzes how knowledge selection influences downstream generation performance in RAG systems. By simulating different retrieval and selection conditions through a controlled mixture of gold and distractor knowledge, we assess the impact of these factors on generation outcomes. Our findings indicate that the downstream generator model's capability, as well as the complexity of the task and dataset, significantly influence the impact of knowledge selection on the overall RAG system performance. In typical scenarios, improving the knowledge recall score is key to enhancing generation outcomes, with the knowledge selector providing limited benefit when a strong generator model is used on clear, well-defined tasks. For weaker generator models or more ambiguous tasks and datasets, the knowledge F1 score becomes a critical factor, and the knowledge selector plays a more prominent role in improving overall performance.\n",
            "reference_string": "[273403839 | Li et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Enhancing LLM Code Generation: A Systematic Evaluation of Multi-Agent Collaboration and Runtime Debugging for Improved Accuracy, Reliability, and Latency",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 45,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.02133, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2359150080",
                    "name": "Nazmus Ashrafi"
                },
                {
                    "authorId": "2359148921",
                    "name": "Salah Bouktif"
                },
                {
                    "authorId": "2329167767",
                    "name": "Mohammed Mediani"
                }
            ],
            "abstract": "The use of large language models (LLMs) for automated code generation has emerged as a significant focus within AI research. As these pretrained models continue to evolve, their ability to understand and generate complex code structures has opened new possibilities for automating intricate programming tasks for the sake of accurate code generation. Although contemporary foundational models demonstrate promoting results, researchers continue to explore optimal post-training strategies to enhance code quality. These include supervised fine-tuning, retrieval-augmented generation (RAG), debugging, and many others. In this paper, we combine two widely used approaches namely multi-agent collaboration and runtime execution information-based debugging, for improving code generation functionality, reliability, and practical applicability. We perform an empirical study in order to extend the evaluation of the individual strategies as well as the proposed composition of the activities of both strategies. Our study use 19 LLMs to examines the performance of individual and the proposed strategies, offering comprehensive insights into how different programming activities compositions and training paradigms influence code generation effectiveness. In particular, we implement a chained system that combines both strategies to assess their combined impact on functional accuracy, code reliability, and generation latency using two benchmark datasets commonly used for code generation. Our findings provide valuable insights for organizations seeking robust AI-driven coding solutions by guiding them in selecting models that can better adapt to complex post-training strategies, ultimately fostering the adoption of more effective and reliable code generation technologies.",
            "corpus_id": 278327536,
            "sentences": [],
            "relevance_judgement": 0.90576171875,
            "relevance_judgment_input_expanded": "# Title: Enhancing LLM Code Generation: A Systematic Evaluation of Multi-Agent Collaboration and Runtime Debugging for Improved Accuracy, Reliability, and Latency\n# Venue: arXiv.org\n# Authors: Nazmus Ashrafi, Salah Bouktif, Mohammed Mediani\n## Abstract\nThe use of large language models (LLMs) for automated code generation has emerged as a significant focus within AI research. As these pretrained models continue to evolve, their ability to understand and generate complex code structures has opened new possibilities for automating intricate programming tasks for the sake of accurate code generation. Although contemporary foundational models demonstrate promoting results, researchers continue to explore optimal post-training strategies to enhance code quality. These include supervised fine-tuning, retrieval-augmented generation (RAG), debugging, and many others. In this paper, we combine two widely used approaches namely multi-agent collaboration and runtime execution information-based debugging, for improving code generation functionality, reliability, and practical applicability. We perform an empirical study in order to extend the evaluation of the individual strategies as well as the proposed composition of the activities of both strategies. Our study use 19 LLMs to examines the performance of individual and the proposed strategies, offering comprehensive insights into how different programming activities compositions and training paradigms influence code generation effectiveness. In particular, we implement a chained system that combines both strategies to assess their combined impact on functional accuracy, code reliability, and generation latency using two benchmark datasets commonly used for code generation. Our findings provide valuable insights for organizations seeking robust AI-driven coding solutions by guiding them in selecting models that can better adapt to complex post-training strategies, ultimately fostering the adoption of more effective and reliable code generation technologies.\n",
            "reference_string": "[278327536 | Ashrafi et al. | 2025 | Citations: 1]"
        },
        {
            "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 65,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.19670, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1900406",
                    "name": "Yutao Zhu"
                },
                {
                    "authorId": "2187935160",
                    "name": "Zhaoheng Huang"
                },
                {
                    "authorId": "1897235",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "2186578511",
                    "name": "Ji-Rong Wen"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capabilities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across 12 question-answering tasks demonstrate the superiority of our approach.",
            "corpus_id": 270123034,
            "sentences": [
                {
                    "corpus_id": "270123034",
                    "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
                    "text": "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capabilities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across 12 question-answering tasks demonstrate the superiority of our approach.",
                    "score": 0.5829331542784741,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90380859375
                }
            ],
            "relevance_judgement": 0.90380859375,
            "relevance_judgment_input_expanded": "# Title: One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Yutao Zhu, Zhaoheng Huang, Zhicheng Dou, Ji-Rong Wen\n## Abstract\nRetrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capabilities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across 12 question-answering tasks demonstrate the superiority of our approach.\n",
            "reference_string": "[270123034 | Zhu et al. | 2024 | Citations: 6]"
        },
        {
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 11,
            "citation_count": 23,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.12837, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2311997786",
                    "name": "Shailja Gupta"
                },
                {
                    "authorId": "2311893279",
                    "name": "Rajesh Ranjan"
                },
                {
                    "authorId": "2321535962",
                    "name": "Surya Narayan Singh"
                }
            ],
            "abstract": "This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs. The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant technological advancements in RAG is provided, including key innovations in retrieval-augmented language models and applications across various domains such as question-answering, summarization, and knowledge-based tasks. Recent research breakthroughs are discussed, highlighting novel methods for improving retrieval efficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethical concerns in deployment. Future research directions are proposed, focusing on improving the robustness of RAG models, expanding the scope of application of RAG models, and addressing societal implications. This survey aims to serve as a foundational resource for researchers and practitioners in understanding the potential of RAG and its trajectory in natural language processing.",
            "corpus_id": 273403982,
            "sentences": [
                {
                    "corpus_id": "273403982",
                    "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
                    "text": "RAFT demonstrates consistent performance improvements in domain-specific RAG tasks, including PubMed, HotpotQA, and Gorilla datasets, serving as a post-training enhancement for LLMs. FILCO (Wang et. al. 2023) , a method designed to enhance the quality of context provided to generative models in tasks like open-domain question answering and fact verification, addresses issues of over-or under-reliance on retrieved passages, which can lead to problems such as hallucinations in the generated outputs. The method improves context quality by identifying useful context through lexical and information-theoretic approaches and training context filtering models to refine retrieved contexts during test time. Reflection Token is a key attribute of Self-reflective Retrieval Augmented-Generation (Self-RAG) (Asai et. al. 2023), a novel framework designed to improve the factual accuracy of large language models (LLMs) by combining retrieval with self-reflection. Unlike traditional methods that retrieve and incorporate a fixed number of passages, Self-RAG adaptively retrieves relevant passages and uses reflection tokens to evaluate and refine its responses, allowing the model to adjust its behavior according to task-specific needs and has shown superior performance in open-domain question-answering, reasoning, fact verification, and long-form generation tasks. Intelligence and effectiveness of RAG are dependent a lot on the quality of retrieval and more meta-data understanding of the repository would enhance the effectiveness of the RAG system. A novel data-centric Retrieval-Augmented Generation (RAG) workflow advances beyond the traditional retrieve-then-read mode and employs a prepare-then-rewrite-then-retrieve-then-read framework, enhancing LLMs by integrating contextually relevant, time-critical, or domain-specific information. Key innovations include generating metadata, synthetic Questions and Answers (QA), and introducing the Meta Knowledge Summary (MK Summary) for clusters of documents (Mombaerts et. al. 2024). A recent paper introduces CommunityKG-RAG (Chang et. al. 2024), a zero-shot framework that integrates community structures within Knowledge Graphs (KGs) into Retrieval-Augmented Generation (RAG) systems.",
                    "score": 0.5935456070758318,
                    "section_title": "Recent Advancement in the field:",
                    "char_start_offset": 25127,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 502
                        },
                        {
                            "start": 503,
                            "end": 706
                        },
                        {
                            "start": 707,
                            "end": 960
                        },
                        {
                            "start": 961,
                            "end": 1365
                        },
                        {
                            "start": 1366,
                            "end": 1553
                        },
                        {
                            "start": 1554,
                            "end": 1846
                        },
                        {
                            "start": 1847,
                            "end": 2037
                        },
                        {
                            "start": 2038,
                            "end": 2241
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90185546875
                },
                {
                    "corpus_id": "273403982",
                    "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
                    "text": "Retrieval-augmented generation (RAG) is an advanced hybrid model architecture that augments natural language generation (NLG) with external retrieval mechanisms to enhance the model's knowledge base. Traditional large language models (LLMs) such as GPT-3 and BERT, which are pre-trained on vast corpora, rely entirely on their internal representations of knowledge, making them susceptible to issues like hallucinations-where the models generate plausible but incorrect information. These models cannot efficiently update their knowledge bases without retraining, making them less practical for dynamic, knowledge-intensive tasks like open-domain question answering and fact verification (Brown, T., et al. 2020). To overcome these limitations, the paper (Lewis et al. 2020) proposed the RAG architecture, which retrieves real-time, relevant external documents to ground the generated text in factual information. \n\nThe RAG model incorporates two key components: \n\n1. Retriever: This retrieves the most relevant documents from a corpus using techniques such as dense passage retrieval (DPR) (Karpukhin et. al. 2020) or traditional BM25 algorithms. 2. Generator: It synthesizes the retrieved documents into coherent, contextually relevant responses. \n\nRAG's strength lies in its ability to leverage external knowledge dynamically, allowing it to outperform generative models like GPT-3 and knowledge-grounded systems like BERT, which rely on static datasets. \n\nIn open-domain question answering, RAG has been demonstrated to be highly effective, consistently retrieving relevant information and improving the factual accuracy of the generated responses (Guu, K., et al. 2020). In addition to knowledge retrieval, RAG models excel at updating knowledge bases. Since the model fetches external documents for each query, it requires no retraining to incorporate the latest information. This flexibility makes RAG models particularly suitable for domains where information is constantly evolving, such as medical research, financial news, and legal proceedings. Furthermore, studies have shown that RAG models achieve superior results in a variety of knowledge-intensive tasks, including document summarization and, knowledge-grounded dialogues",
                    "score": 0.65278911598739,
                    "section_title": "Overview of RAG Models",
                    "char_start_offset": 10564,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 199
                        },
                        {
                            "start": 200,
                            "end": 482
                        },
                        {
                            "start": 483,
                            "end": 713
                        },
                        {
                            "start": 714,
                            "end": 913
                        },
                        {
                            "start": 916,
                            "end": 962
                        },
                        {
                            "start": 965,
                            "end": 1147
                        },
                        {
                            "start": 1148,
                            "end": 1248
                        },
                        {
                            "start": 1251,
                            "end": 1457
                        },
                        {
                            "start": 1460,
                            "end": 1675
                        },
                        {
                            "start": 1676,
                            "end": 1757
                        },
                        {
                            "start": 1758,
                            "end": 1881
                        },
                        {
                            "start": 1882,
                            "end": 2056
                        },
                        {
                            "start": 2057,
                            "end": 2239
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 755,
                            "end": 773,
                            "matchedPaperCorpusId": "218869575"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87548828125
                }
            ],
            "relevance_judgement": 0.90185546875,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions\n# Venue: arXiv.org\n# Authors: Shailja Gupta, Rajesh Ranjan, Surya Narayan Singh\n## Abstract\nThis paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs. The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant technological advancements in RAG is provided, including key innovations in retrieval-augmented language models and applications across various domains such as question-answering, summarization, and knowledge-based tasks. Recent research breakthroughs are discussed, highlighting novel methods for improving retrieval efficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethical concerns in deployment. Future research directions are proposed, focusing on improving the robustness of RAG models, expanding the scope of application of RAG models, and addressing societal implications. This survey aims to serve as a foundational resource for researchers and practitioners in understanding the potential of RAG and its trajectory in natural language processing.\n## Overview of RAG Models\nRetrieval-augmented generation (RAG) is an advanced hybrid model architecture that augments natural language generation (NLG) with external retrieval mechanisms to enhance the model's knowledge base. Traditional large language models (LLMs) such as GPT-3 and BERT, which are pre-trained on vast corpora, rely entirely on their internal representations of knowledge, making them susceptible to issues like hallucinations-where the models generate plausible but incorrect information. These models cannot efficiently update their knowledge bases without retraining, making them less practical for dynamic, knowledge-intensive tasks like open-domain question answering and fact verification (Brown, T., et al. 2020). To overcome these limitations, the paper (Lewis et al. 2020) proposed the RAG architecture, which retrieves real-time, relevant external documents to ground the generated text in factual information. \n\nThe RAG model incorporates two key components: \n\n1. Retriever: This retrieves the most relevant documents from a corpus using techniques such as dense passage retrieval (DPR) (Karpukhin et. al. 2020) or traditional BM25 algorithms. 2. Generator: It synthesizes the retrieved documents into coherent, contextually relevant responses. \n\nRAG's strength lies in its ability to leverage external knowledge dynamically, allowing it to outperform generative models like GPT-3 and knowledge-grounded systems like BERT, which rely on static datasets. \n\nIn open-domain question answering, RAG has been demonstrated to be highly effective, consistently retrieving relevant information and improving the factual accuracy of the generated responses (Guu, K., et al. 2020). In addition to knowledge retrieval, RAG models excel at updating knowledge bases. Since the model fetches external documents for each query, it requires no retraining to incorporate the latest information. This flexibility makes RAG models particularly suitable for domains where information is constantly evolving, such as medical research, financial news, and legal proceedings. Furthermore, studies have shown that RAG models achieve superior results in a variety of knowledge-intensive tasks, including document summarization and, knowledge-grounded dialogues\n\n## Recent Advancement in the field:\nRAFT demonstrates consistent performance improvements in domain-specific RAG tasks, including PubMed, HotpotQA, and Gorilla datasets, serving as a post-training enhancement for LLMs. FILCO (Wang et. al. 2023) , a method designed to enhance the quality of context provided to generative models in tasks like open-domain question answering and fact verification, addresses issues of over-or under-reliance on retrieved passages, which can lead to problems such as hallucinations in the generated outputs. The method improves context quality by identifying useful context through lexical and information-theoretic approaches and training context filtering models to refine retrieved contexts during test time. Reflection Token is a key attribute of Self-reflective Retrieval Augmented-Generation (Self-RAG) (Asai et. al. 2023), a novel framework designed to improve the factual accuracy of large language models (LLMs) by combining retrieval with self-reflection. Unlike traditional methods that retrieve and incorporate a fixed number of passages, Self-RAG adaptively retrieves relevant passages and uses reflection tokens to evaluate and refine its responses, allowing the model to adjust its behavior according to task-specific needs and has shown superior performance in open-domain question-answering, reasoning, fact verification, and long-form generation tasks. Intelligence and effectiveness of RAG are dependent a lot on the quality of retrieval and more meta-data understanding of the repository would enhance the effectiveness of the RAG system. A novel data-centric Retrieval-Augmented Generation (RAG) workflow advances beyond the traditional retrieve-then-read mode and employs a prepare-then-rewrite-then-retrieve-then-read framework, enhancing LLMs by integrating contextually relevant, time-critical, or domain-specific information. Key innovations include generating metadata, synthetic Questions and Answers (QA), and introducing the Meta Knowledge Summary (MK Summary) for clusters of documents (Mombaerts et. al. 2024). A recent paper introduces CommunityKG-RAG (Chang et. al. 2024), a zero-shot framework that integrates community structures within Knowledge Graphs (KGs) into Retrieval-Augmented Generation (RAG) systems.",
            "reference_string": "[273403982 | Gupta et al. | 2024 | Citations: 23]"
        },
        {
            "title": "RAG-KG-IL: A Multi-Agent Hybrid Framework for Reducing Hallucinations and Enhancing LLM Reasoning through RAG and Incremental Knowledge Graph Learning Integration",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 37,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.13514, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2351728696",
                    "name": "Hong Qing Yu"
                },
                {
                    "authorId": "2350756862",
                    "name": "Frank McQuade"
                }
            ],
            "abstract": "This paper presents RAG-KG-IL, a novel multi-agent hybrid framework designed to enhance the reasoning capabilities of Large Language Models (LLMs) by integrating Retrieval-Augmented Generation (RAG) and Knowledge Graphs (KGs) with an Incremental Learning (IL) approach. Despite recent advancements, LLMs still face significant challenges in reasoning with structured data, handling dynamic knowledge evolution, and mitigating hallucinations, particularly in mission-critical domains. Our proposed RAG-KG-IL framework addresses these limitations by employing a multi-agent architecture that enables continuous knowledge updates, integrates structured knowledge, and incorporates autonomous agents for enhanced explainability and reasoning. The framework utilizes RAG to ensure the generated responses are grounded in verifiable information, while KGs provide structured domain knowledge for improved consistency and depth of understanding. The Incremental Learning approach allows for dynamic updates to the knowledge base without full retraining, significantly reducing computational overhead and improving the model's adaptability. We evaluate the framework using real-world case studies involving health-related queries, comparing it to state-of-the-art models like GPT-4o and a RAG-only baseline. Experimental results demonstrate that our approach significantly reduces hallucination rates and improves answer completeness and reasoning accuracy. The results underscore the potential of combining RAG, KGs, and multi-agent systems to create intelligent, adaptable systems capable of real-time knowledge integration and reasoning in complex domains.",
            "corpus_id": 277104712,
            "sentences": [
                {
                    "corpus_id": "277104712",
                    "title": "RAG-KG-IL: A Multi-Agent Hybrid Framework for Reducing Hallucinations and Enhancing LLM Reasoning through RAG and Incremental Knowledge Graph Learning Integration",
                    "text": "Jesson et al. presented a Bayesian model to estimate the probability of hallucination in ICL tasks by calculating response probabilities based on context [19]. \n\nAnother approach to measuring hallucination includes the use of a cross-encoder model and n-gram overlap metrics to promote more grounded responses [20]. These measurements provide an empirical framework to understand hallucination better and evaluate the mitigation approaches. \n\nAddressing hallucination in generative AI has received significant attention. Several strategies have been proposed to mitigate hallucinations, ranging from improvements in training methodologies to more advanced inference techniques. \n\nRetrieval-Augmented Generation (RAG): RAG combines generative models with information retrieval techniques to ensure that the generated output is grounded in real, verifiable data. B\u00e9chard et al. [21] demonstrated the effectiveness of RAG in reducing hallucinations in structured outputs like workflow generation from natural language instructions. \n\nChannel-Aware Domain-Adaptive Generative Adversarial Network (CADAGAN): Grayson et al. [22] proposed CADA-GAN, which modifies generative models by introducing channel-aware processing and domain-adaptive learning. This technique helps in mitigating hallucinations by aligning generated output with domain-specific knowledge while maintaining linguistic coherence. In addition, the research shows that selfrefining approach is another approach for training an agent incrementally rather than batch process. \n\nGenetic Algorithm for Grounded Answer Generation (GAuGE): Kulkarni et al. [20] introduced a genetic algorithmbased grounded answer generation method to minimize hallucination. This method effectively maintains high relevance by cross-checking with retrieved search engine results and encouraging grounding through a balanced fitness function. \n\nDomain-Specific Adaption and Knowledge Graph Utilization: Techniques such as domain adaptation and knowledge graph integration have shown promise in controlling hallucination. Towhidul Islam Tonmoy et al. [23] reviewed multiple mitigation strategies, including knowledge retrieval and domain adaptation to enhance fact consistency in generated texts.",
                    "score": 0.5814121876542511,
                    "section_title": "A. Hallucination in Generative AI",
                    "char_start_offset": 4947,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 159
                        },
                        {
                            "start": 162,
                            "end": 315
                        },
                        {
                            "start": 316,
                            "end": 440
                        },
                        {
                            "start": 443,
                            "end": 520
                        },
                        {
                            "start": 521,
                            "end": 677
                        },
                        {
                            "start": 680,
                            "end": 860
                        },
                        {
                            "start": 861,
                            "end": 1028
                        },
                        {
                            "start": 1031,
                            "end": 1244
                        },
                        {
                            "start": 1245,
                            "end": 1394
                        },
                        {
                            "start": 1395,
                            "end": 1536
                        },
                        {
                            "start": 1539,
                            "end": 1714
                        },
                        {
                            "start": 1715,
                            "end": 1881
                        },
                        {
                            "start": 1884,
                            "end": 2059
                        },
                        {
                            "start": 2060,
                            "end": 2234
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 154,
                            "end": 158,
                            "matchedPaperCorpusId": "270379551"
                        },
                        {
                            "start": 310,
                            "end": 314,
                            "matchedPaperCorpusId": "272203021"
                        },
                        {
                            "start": 876,
                            "end": 880,
                            "matchedPaperCorpusId": "269137180"
                        },
                        {
                            "start": 1613,
                            "end": 1617,
                            "matchedPaperCorpusId": "272203021"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8974609375
                }
            ],
            "relevance_judgement": 0.8974609375,
            "relevance_judgment_input_expanded": "# Title: RAG-KG-IL: A Multi-Agent Hybrid Framework for Reducing Hallucinations and Enhancing LLM Reasoning through RAG and Incremental Knowledge Graph Learning Integration\n# Venue: arXiv.org\n# Authors: Hong Qing Yu, Frank McQuade\n## Abstract\nThis paper presents RAG-KG-IL, a novel multi-agent hybrid framework designed to enhance the reasoning capabilities of Large Language Models (LLMs) by integrating Retrieval-Augmented Generation (RAG) and Knowledge Graphs (KGs) with an Incremental Learning (IL) approach. Despite recent advancements, LLMs still face significant challenges in reasoning with structured data, handling dynamic knowledge evolution, and mitigating hallucinations, particularly in mission-critical domains. Our proposed RAG-KG-IL framework addresses these limitations by employing a multi-agent architecture that enables continuous knowledge updates, integrates structured knowledge, and incorporates autonomous agents for enhanced explainability and reasoning. The framework utilizes RAG to ensure the generated responses are grounded in verifiable information, while KGs provide structured domain knowledge for improved consistency and depth of understanding. The Incremental Learning approach allows for dynamic updates to the knowledge base without full retraining, significantly reducing computational overhead and improving the model's adaptability. We evaluate the framework using real-world case studies involving health-related queries, comparing it to state-of-the-art models like GPT-4o and a RAG-only baseline. Experimental results demonstrate that our approach significantly reduces hallucination rates and improves answer completeness and reasoning accuracy. The results underscore the potential of combining RAG, KGs, and multi-agent systems to create intelligent, adaptable systems capable of real-time knowledge integration and reasoning in complex domains.\n## A. Hallucination in Generative AI\nJesson et al. presented a Bayesian model to estimate the probability of hallucination in ICL tasks by calculating response probabilities based on context [19]. \n\nAnother approach to measuring hallucination includes the use of a cross-encoder model and n-gram overlap metrics to promote more grounded responses [20]. These measurements provide an empirical framework to understand hallucination better and evaluate the mitigation approaches. \n\nAddressing hallucination in generative AI has received significant attention. Several strategies have been proposed to mitigate hallucinations, ranging from improvements in training methodologies to more advanced inference techniques. \n\nRetrieval-Augmented Generation (RAG): RAG combines generative models with information retrieval techniques to ensure that the generated output is grounded in real, verifiable data. B\u00e9chard et al. [21] demonstrated the effectiveness of RAG in reducing hallucinations in structured outputs like workflow generation from natural language instructions. \n\nChannel-Aware Domain-Adaptive Generative Adversarial Network (CADAGAN): Grayson et al. [22] proposed CADA-GAN, which modifies generative models by introducing channel-aware processing and domain-adaptive learning. This technique helps in mitigating hallucinations by aligning generated output with domain-specific knowledge while maintaining linguistic coherence. In addition, the research shows that selfrefining approach is another approach for training an agent incrementally rather than batch process. \n\nGenetic Algorithm for Grounded Answer Generation (GAuGE): Kulkarni et al. [20] introduced a genetic algorithmbased grounded answer generation method to minimize hallucination. This method effectively maintains high relevance by cross-checking with retrieved search engine results and encouraging grounding through a balanced fitness function. \n\nDomain-Specific Adaption and Knowledge Graph Utilization: Techniques such as domain adaptation and knowledge graph integration have shown promise in controlling hallucination. Towhidul Islam Tonmoy et al. [23] reviewed multiple mitigation strategies, including knowledge retrieval and domain adaptation to enhance fact consistency in generated texts.",
            "reference_string": "[277104712 | Yu et al. | 2025 | Citations: 3]"
        },
        {
            "title": "Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework for Medical Applications",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.21055, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313916812",
                    "name": "Cui Long"
                },
                {
                    "authorId": "47909171",
                    "name": "Yongbin Liu"
                },
                {
                    "authorId": "16318808",
                    "name": "Chunping Ouyang"
                },
                {
                    "authorId": "2296267575",
                    "name": "Ying Yu"
                }
            ],
            "abstract": "Large Language Models (LLMs) have exhibited remarkable proficiency in natural language understanding, prompting extensive exploration of their potential applications across diverse domains. In the medical domain, open-source LLMs have demonstrated moderate efficacy following domain-specific fine-tuning; however, they remain substantially inferior to proprietary models such as GPT-4 and GPT-3.5. These open-source models encounter limitations in the comprehensiveness of domain-specific knowledge and exhibit a propensity for 'hallucinations' during text generation. To mitigate these issues, researchers have implemented the Retrieval-Augmented Generation (RAG) approach, which augments LLMs with background information from external knowledge bases while preserving the model's internal parameters. However, document noise can adversely affect performance, and the application of RAG in the medical field remains in its nascent stages. This study presents the Bailicai framework: a novel integration of retrieval-augmented generation with large language models optimized for the medical domain. The Bailicai framework augments the performance of LLMs in medicine through the implementation of four sub-modules. Experimental results demonstrate that the Bailicai approach surpasses existing medical domain LLMs across multiple medical benchmarks and exceeds the performance of GPT-3.5. Furthermore, the Bailicai method effectively attenuates the prevalent issue of hallucinations in medical applications of LLMs and ameliorates the noise-related challenges associated with traditional RAG techniques when processing irrelevant or pseudo-relevant documents.",
            "corpus_id": 271571143,
            "sentences": [
                {
                    "corpus_id": "271571143",
                    "title": "Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework for Medical Applications",
                    "text": "Retrieval-Augmented Generation (RAG) enhances the performance of large language models by leveraging external knowledge bases to retrieve relevant document segments through semantic similarity computations [23], [29]. This approach significantly reduces the incidence of hallucinations-defined as instances where generated content deviates from factual accuracy [38], [42]. Early research on RAG primarily focused on developing sparse or dense retrievers [19], [20], whereas contemporary studies have emphasized optimizing the inte-gration of RAG with Large Language Models (LLMs). These optimizations encompass the timing of retrieval, methodological enhancements, and refined utilization of contextual information to mitigate noise within retrieved documents [10], [12]. \n\nWith respect to adaptive retrieval strategies, the FLARE project has proposed two novel methods: proactive retrieval based on retrieval instructions and confidence-based proactive retrieval, both designed to mitigate unnecessary retrievals [18]. Furthermore, process optimization has evolved from the \"Rewrite-Retrieve-Read\" paradigm [24] to \"ITER-RETGEN\" implementing iterative retrieval to incrementally access more granular and comprehensive knowledge [30]. Concerning the management of noise within retrieved documents, studies conducted demonstrated that irrelevant noise documents do not necessarily deteriorate system performance; conversely, they can enhance accuracy by up to 35%. Conversely, documents incorrectly classified as relevant to the query introduce significant interference, substantially impacting the model's generative performance [9]. \n\nIn the medical domain, the application of RAG technology remains in its nascent stages. The MEDRAG system assessed performance variations across diverse retrievers and corpora in medical question-answering tasks [37]. The Self-BioRAG [16] project integrated Self-RAG [5] technology in medicine, optimizing it through reflective tokens that address retrieval timing, evaluate the relevance and supporting capacity of retrieved documents in answer generation, and assess the quality of generated outputs. Nevertheless, empirical evidence suggests that this approach fails to surpass the performance of models specifically optimized for medical datasets, potentially attributable to the limitations of smaller models in multitask integration, as demonstrated by the AUTOACT study [28].",
                    "score": 0.6565256034082569,
                    "section_title": "B. Retrieval-augmented Generation",
                    "char_start_offset": 9292,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 217
                        },
                        {
                            "start": 218,
                            "end": 373
                        },
                        {
                            "start": 374,
                            "end": 581
                        },
                        {
                            "start": 582,
                            "end": 772
                        },
                        {
                            "start": 775,
                            "end": 1020
                        },
                        {
                            "start": 1021,
                            "end": 1235
                        },
                        {
                            "start": 1236,
                            "end": 1464
                        },
                        {
                            "start": 1465,
                            "end": 1634
                        },
                        {
                            "start": 1637,
                            "end": 1724
                        },
                        {
                            "start": 1725,
                            "end": 1854
                        },
                        {
                            "start": 1855,
                            "end": 2139
                        },
                        {
                            "start": 2140,
                            "end": 2419
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 206,
                            "end": 210,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 212,
                            "end": 216,
                            "matchedPaperCorpusId": "256459451"
                        },
                        {
                            "start": 455,
                            "end": 459,
                            "matchedPaperCorpusId": "259316759"
                        },
                        {
                            "start": 1871,
                            "end": 1875,
                            "matchedPaperCorpusId": "267312134"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8955078125
                }
            ],
            "relevance_judgement": 0.8955078125,
            "relevance_judgment_input_expanded": "# Title: Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework for Medical Applications\n# Venue: arXiv.org\n# Authors: Cui Long, Yongbin Liu, Chunping Ouyang, Ying Yu\n## Abstract\nLarge Language Models (LLMs) have exhibited remarkable proficiency in natural language understanding, prompting extensive exploration of their potential applications across diverse domains. In the medical domain, open-source LLMs have demonstrated moderate efficacy following domain-specific fine-tuning; however, they remain substantially inferior to proprietary models such as GPT-4 and GPT-3.5. These open-source models encounter limitations in the comprehensiveness of domain-specific knowledge and exhibit a propensity for 'hallucinations' during text generation. To mitigate these issues, researchers have implemented the Retrieval-Augmented Generation (RAG) approach, which augments LLMs with background information from external knowledge bases while preserving the model's internal parameters. However, document noise can adversely affect performance, and the application of RAG in the medical field remains in its nascent stages. This study presents the Bailicai framework: a novel integration of retrieval-augmented generation with large language models optimized for the medical domain. The Bailicai framework augments the performance of LLMs in medicine through the implementation of four sub-modules. Experimental results demonstrate that the Bailicai approach surpasses existing medical domain LLMs across multiple medical benchmarks and exceeds the performance of GPT-3.5. Furthermore, the Bailicai method effectively attenuates the prevalent issue of hallucinations in medical applications of LLMs and ameliorates the noise-related challenges associated with traditional RAG techniques when processing irrelevant or pseudo-relevant documents.\n## B. Retrieval-augmented Generation\nRetrieval-Augmented Generation (RAG) enhances the performance of large language models by leveraging external knowledge bases to retrieve relevant document segments through semantic similarity computations [23], [29]. This approach significantly reduces the incidence of hallucinations-defined as instances where generated content deviates from factual accuracy [38], [42]. Early research on RAG primarily focused on developing sparse or dense retrievers [19], [20], whereas contemporary studies have emphasized optimizing the inte-gration of RAG with Large Language Models (LLMs). These optimizations encompass the timing of retrieval, methodological enhancements, and refined utilization of contextual information to mitigate noise within retrieved documents [10], [12]. \n\nWith respect to adaptive retrieval strategies, the FLARE project has proposed two novel methods: proactive retrieval based on retrieval instructions and confidence-based proactive retrieval, both designed to mitigate unnecessary retrievals [18]. Furthermore, process optimization has evolved from the \"Rewrite-Retrieve-Read\" paradigm [24] to \"ITER-RETGEN\" implementing iterative retrieval to incrementally access more granular and comprehensive knowledge [30]. Concerning the management of noise within retrieved documents, studies conducted demonstrated that irrelevant noise documents do not necessarily deteriorate system performance; conversely, they can enhance accuracy by up to 35%. Conversely, documents incorrectly classified as relevant to the query introduce significant interference, substantially impacting the model's generative performance [9]. \n\nIn the medical domain, the application of RAG technology remains in its nascent stages. The MEDRAG system assessed performance variations across diverse retrievers and corpora in medical question-answering tasks [37]. The Self-BioRAG [16] project integrated Self-RAG [5] technology in medicine, optimizing it through reflective tokens that address retrieval timing, evaluate the relevance and supporting capacity of retrieved documents in answer generation, and assess the quality of generated outputs. Nevertheless, empirical evidence suggests that this approach fails to surpass the performance of models specifically optimized for medical datasets, potentially attributable to the limitations of smaller models in multitask integration, as demonstrated by the AUTOACT study [28].",
            "reference_string": "[271571143 | Long et al. | 2024 | Citations: 5]"
        },
        {
            "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 153,
            "citation_count": 51,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.10981, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2260272949",
                    "name": "Yizheng Huang"
                },
                {
                    "authorId": "2259653248",
                    "name": "Jimmy X. Huang"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.",
            "corpus_id": 269188036,
            "sentences": [
                {
                    "corpus_id": "269188036",
                    "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
                    "text": "To provide clarity and structure, this paper is organized as follows: Section 2 outlines the overall RAG workflow, dividing the methodologies into pre-retrieval, retrieval, post-retrieval, and generation phases. Sections 3 through 6 explore the core techniques within each phase. Section 7 focuses on the evaluation methodologies for RAG. Section 8 summarizes the reviewed studies, detailing the retrievers and generators used, while Section 9 discusses challenges and future research directions, extending beyond text-based studies to include multimodal data applications. The paper concludes with Section 10. \n\nOther related surveys provide valuable insights into the evolving RAG landscape from different angles. Gao et al. [38] identified three key stages in RAG development: pre-training enhancement, inference, and fine-tuning. Zhao et al. [162] focused on the diverse applications of RAG, including text, code, image, and video generation, emphasizing augmented intelligence in generative tasks. Meanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how interactions between retrievers, language models, and augmentations influence model architectures and applications. \n\nIn this paper, we aim to offer a comprehensive and unified framework for understanding RAG from an information retrieval (IR) perspective, identifying key challenges and areas for improvement. We delve into the core technologies that drive RAG, assessing their effectiveness in addressing retrieval and generation tasks. Additionally, this survey introduces the evaluation methods employed in RAG research, highlights current limitations, and proposes promising avenues for future exploration.",
                    "score": 0.6225227730856241,
                    "section_title": "Introduction",
                    "char_start_offset": 2330,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 279
                        },
                        {
                            "start": 280,
                            "end": 338
                        },
                        {
                            "start": 339,
                            "end": 573
                        },
                        {
                            "start": 574,
                            "end": 610
                        },
                        {
                            "start": 613,
                            "end": 715
                        },
                        {
                            "start": 716,
                            "end": 833
                        },
                        {
                            "start": 834,
                            "end": 1002
                        },
                        {
                            "start": 1003,
                            "end": 1212
                        },
                        {
                            "start": 1215,
                            "end": 1407
                        },
                        {
                            "start": 1408,
                            "end": 1535
                        },
                        {
                            "start": 1536,
                            "end": 1708
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89306640625
                }
            ],
            "relevance_judgement": 0.89306640625,
            "relevance_judgment_input_expanded": "# Title: A Survey on Retrieval-Augmented Text Generation for Large Language Models\n# Venue: arXiv.org\n# Authors: Yizheng Huang, Jimmy X. Huang\n## Abstract\nRetrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.\n## Introduction\nTo provide clarity and structure, this paper is organized as follows: Section 2 outlines the overall RAG workflow, dividing the methodologies into pre-retrieval, retrieval, post-retrieval, and generation phases. Sections 3 through 6 explore the core techniques within each phase. Section 7 focuses on the evaluation methodologies for RAG. Section 8 summarizes the reviewed studies, detailing the retrievers and generators used, while Section 9 discusses challenges and future research directions, extending beyond text-based studies to include multimodal data applications. The paper concludes with Section 10. \n\nOther related surveys provide valuable insights into the evolving RAG landscape from different angles. Gao et al. [38] identified three key stages in RAG development: pre-training enhancement, inference, and fine-tuning. Zhao et al. [162] focused on the diverse applications of RAG, including text, code, image, and video generation, emphasizing augmented intelligence in generative tasks. Meanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how interactions between retrievers, language models, and augmentations influence model architectures and applications. \n\nIn this paper, we aim to offer a comprehensive and unified framework for understanding RAG from an information retrieval (IR) perspective, identifying key challenges and areas for improvement. We delve into the core technologies that drive RAG, assessing their effectiveness in addressing retrieval and generation tasks. Additionally, this survey introduces the evaluation methods employed in RAG research, highlights current limitations, and proposes promising avenues for future exploration.",
            "reference_string": "[269188036 | Huang et al. | 2024 | Citations: 51]"
        },
        {
            "title": "On the Evaluation of Machine-Generated Reports",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2024,
            "reference_count": 99,
            "citation_count": 16,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2405.00982",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.00982, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2298905578",
                    "name": "James Mayfield"
                },
                {
                    "authorId": "2296599846",
                    "name": "Eugene Yang"
                },
                {
                    "authorId": "2539674",
                    "name": "Dawn J Lawrie"
                },
                {
                    "authorId": "2290916915",
                    "name": "Sean MacAvaney"
                },
                {
                    "authorId": "145324163",
                    "name": "Paul McNamee"
                },
                {
                    "authorId": "1737250",
                    "name": "Douglas W. Oard"
                },
                {
                    "authorId": "3328733",
                    "name": "Luca Soldaini"
                },
                {
                    "authorId": "2299328116",
                    "name": "Ian Soboroff"
                },
                {
                    "authorId": "47433471",
                    "name": "Orion Weller"
                },
                {
                    "authorId": "1396112798",
                    "name": "Efsun Kayi"
                },
                {
                    "authorId": "2187060946",
                    "name": "Kate Sanders"
                },
                {
                    "authorId": "2215823149",
                    "name": "Marc Mason"
                },
                {
                    "authorId": "2299332019",
                    "name": "Noah Hibbler"
                }
            ],
            "abstract": "Large Language Models (LLMs) have enabled new ways to satisfy information needs. Although great strides have been made in applying them to settings like document ranking and short-form text generation, they still struggle to compose complete, accurate, and verifiable long-form reports. Reports with these qualities are necessary to satisfy the complex, nuanced, or multi-faceted information needs of users. In this perspective paper, we draw together opinions from industry and academia, and from a variety of related research areas, to present our vision for automatic report generation, and---critically---a flexible framework by which such reports can be evaluated. In contrast with other summarization tasks, automatic report generation starts with a detailed description of an information need, stating the necessary background, requirements, and scope of the report. Further, the generated reports should be complete, accurate, and verifiable. These qualities, which are desirable---if not required---in many analytic report-writing settings, require rethinking how to build and evaluate systems that exhibit these qualities. To foster new efforts in building these systems, we present an evaluation framework that draws on ideas found in various evaluations. To test completeness and accuracy, the framework uses nuggets of information, expressed as questions and answers, that need to be part of any high-quality generated report. Additionally, evaluation of citations that map claims made in the report to their source documents ensures verifiability.",
            "corpus_id": 269502216,
            "sentences": [
                {
                    "corpus_id": "269502216",
                    "title": "On the Evaluation of Machine-Generated Reports",
                    "text": "Retrieval-Augmented Generation.Early retrieval augmented generation systems have been evaluated using task-specific metrics on end-to-end tasks.For example, in the context of question answering, exact match and  1 metrics have been used [30,41].For summarization, ROUGE and BERTScore on reference summaries are common [26].These approaches have two limitations: they only measure ability to complete end tasks, and thus cannot assess intermediate stages or evaluate generation across multiple dimensions; and they are not well-suited to capture failures that can be introduced by current generative models [27].\n\nMore recently, techniques have proposed to more holistically evaluate RAG systems.Gienapp et al. [25] introduce a theoretical framework for evaluating ad hoc generative retrieval.Chen et al. [11] focus on robustness of RAG systems against various perturbations.Thakur et al. [82] benchmark hallucinations and the ability of RAG systems to identify relevant information for 18 languages.Others have introduced benchmarks to measure the ability of RAG systems to provide citations [6,23,53,90].While not specifically  designed for RAG applications, metrics designed to evaluate factuality (e.g., FactScore [58]) or faithful manipulation of long inputs (e.g., BooookScore [10]) can complement application-specific evaluation frameworks.\n\nMost approaches to automated evaluation aim to estimate the effectiveness of RAG systems across desirable dimensions (e.g., faithfulness, answer relevance, and context relevance).Techniques include prompting LLMs to evaluate generated summaries [76], and fine-tuning lightweight models on synthetic data [73].Downstream applications, such as question answering, can also be used to evaluate the effectiveness of RAG systems [74].",
                    "score": 0.7470241739468626,
                    "section_title": "3.2.4",
                    "char_start_offset": 19611,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 31
                        },
                        {
                            "start": 31,
                            "end": 144
                        },
                        {
                            "start": 144,
                            "end": 245
                        },
                        {
                            "start": 245,
                            "end": 323
                        },
                        {
                            "start": 323,
                            "end": 611
                        },
                        {
                            "start": 613,
                            "end": 695
                        },
                        {
                            "start": 695,
                            "end": 792
                        },
                        {
                            "start": 792,
                            "end": 874
                        },
                        {
                            "start": 874,
                            "end": 999
                        },
                        {
                            "start": 999,
                            "end": 1105
                        },
                        {
                            "start": 1105,
                            "end": 1346
                        },
                        {
                            "start": 1348,
                            "end": 1527
                        },
                        {
                            "start": 1527,
                            "end": 1657
                        },
                        {
                            "start": 1657,
                            "end": 1777
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 241,
                            "end": 244,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 318,
                            "end": 322,
                            "matchedPaperCorpusId": "258865156"
                        },
                        {
                            "start": 1101,
                            "end": 1104,
                            "matchedPaperCorpusId": "258587884"
                        },
                        {
                            "start": 1772,
                            "end": 1776,
                            "matchedPaperCorpusId": "238207962"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.892578125
                }
            ],
            "relevance_judgement": 0.892578125,
            "relevance_judgment_input_expanded": "# Title: On the Evaluation of Machine-Generated Reports\n# Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\n# Authors: James Mayfield, Eugene Yang, Dawn J Lawrie, Sean MacAvaney, Paul McNamee, Douglas W. Oard, Luca Soldaini, Ian Soboroff, Orion Weller, Efsun Kayi, Kate Sanders, Marc Mason, Noah Hibbler\n## Abstract\nLarge Language Models (LLMs) have enabled new ways to satisfy information needs. Although great strides have been made in applying them to settings like document ranking and short-form text generation, they still struggle to compose complete, accurate, and verifiable long-form reports. Reports with these qualities are necessary to satisfy the complex, nuanced, or multi-faceted information needs of users. In this perspective paper, we draw together opinions from industry and academia, and from a variety of related research areas, to present our vision for automatic report generation, and---critically---a flexible framework by which such reports can be evaluated. In contrast with other summarization tasks, automatic report generation starts with a detailed description of an information need, stating the necessary background, requirements, and scope of the report. Further, the generated reports should be complete, accurate, and verifiable. These qualities, which are desirable---if not required---in many analytic report-writing settings, require rethinking how to build and evaluate systems that exhibit these qualities. To foster new efforts in building these systems, we present an evaluation framework that draws on ideas found in various evaluations. To test completeness and accuracy, the framework uses nuggets of information, expressed as questions and answers, that need to be part of any high-quality generated report. Additionally, evaluation of citations that map claims made in the report to their source documents ensures verifiability.\n## 3.2.4\nRetrieval-Augmented Generation.Early retrieval augmented generation systems have been evaluated using task-specific metrics on end-to-end tasks.For example, in the context of question answering, exact match and  1 metrics have been used [30,41].For summarization, ROUGE and BERTScore on reference summaries are common [26].These approaches have two limitations: they only measure ability to complete end tasks, and thus cannot assess intermediate stages or evaluate generation across multiple dimensions; and they are not well-suited to capture failures that can be introduced by current generative models [27].\n\nMore recently, techniques have proposed to more holistically evaluate RAG systems.Gienapp et al. [25] introduce a theoretical framework for evaluating ad hoc generative retrieval.Chen et al. [11] focus on robustness of RAG systems against various perturbations.Thakur et al. [82] benchmark hallucinations and the ability of RAG systems to identify relevant information for 18 languages.Others have introduced benchmarks to measure the ability of RAG systems to provide citations [6,23,53,90].While not specifically  designed for RAG applications, metrics designed to evaluate factuality (e.g., FactScore [58]) or faithful manipulation of long inputs (e.g., BooookScore [10]) can complement application-specific evaluation frameworks.\n\nMost approaches to automated evaluation aim to estimate the effectiveness of RAG systems across desirable dimensions (e.g., faithfulness, answer relevance, and context relevance).Techniques include prompting LLMs to evaluate generated summaries [76], and fine-tuning lightweight models on synthetic data [73].Downstream applications, such as question answering, can also be used to evaluate the effectiveness of RAG systems [74].",
            "reference_string": "[269502216 | Mayfield et al. | 2024 | Citations: 16]"
        },
        {
            "title": "Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge Reasoning and Text Generation",
            "venue": "2024 5th International Symposium on Computer Engineering and Intelligent Communications (ISCEIC)",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2411.03572",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.03572, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326920545",
                    "name": "Yuxin Dong"
                },
                {
                    "authorId": "2327007198",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2327003963",
                    "name": "Hongye Zheng"
                },
                {
                    "authorId": "2322450076",
                    "name": "Jiajing Chen"
                },
                {
                    "authorId": "2322450970",
                    "name": "Zhenhong Zhang"
                },
                {
                    "authorId": "2322612972",
                    "name": "Chihang Wang"
                }
            ],
            "abstract": "This study aims to optimize the existing retrieval-augmented generation model (RAG) by introducing a graph structure to improve the performance of the model in dealing with complex knowledge reasoning tasks. The traditional RAG model has the problem of insufficient processing efficiency when facing complex graph structure information (such as knowledge graphs, hierarchical relationships, etc.), which affects the quality and consistency of the generated results. This study proposes a scheme to process graph structure data by combining graph neural network (GNN), so that the model can capture the complex relationship between entities, thereby improving the knowledge consistency and reasoning ability of the generated text. The experiment used the Natural Questions (NQ) dataset and compared it with multiple existing generation models. The results show that the graph-based RAG model proposed in this paper is superior to the traditional generation model in terms of quality, knowledge consistency, and reasoning ability, especially when dealing with tasks that require multi-dimensional reasoning. Through the combination of the enhancement of the retrieval module and the graph neural network, the model in this study can better handle complex knowledge background information and has broad potential value in multiple practical application scenarios.",
            "corpus_id": 273850363,
            "sentences": [
                {
                    "corpus_id": "273850363",
                    "title": "Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge Reasoning and Text Generation",
                    "text": "We will select five different generative models to conduct comparative experiments with the RAG model based on graph structure optimization proposed in this paper (labeled as Ours). By comparing their performance on the same dataset, we can verify the superiority of our model. The evaluation indicators used in the comparative experiments include Quality, Knowledge Consistency (KC), and Reasoning Capability(RC). 1, the performance of each model in the three indicators of Quality, Knowledge Consistency, and Reasoning Capability varies. It can be observed that traditional generative models such as BART and T5 performed relatively weakly in the experiment, especially in terms of knowledge consistency and reasoning capability. BART's quality score is 0.74, KC score is 0.65, and reasoning capability is 0.68, reflecting that although its generated text is fluent, it cannot provide sufficient external support in knowledge-intensive tasks. The T5 model has a slight improvement in the three indicators, especially in reasoning capability (0.72), which shows that the T5 model can improve its generalization ability by unifying task processing, but it is still not enough to fully handle complex knowledge reasoning tasks. \n\nIn contrast, the RAG model and its improved version RAG+T (RAG+Text) perform significantly better than BART and T5 in these three indicators. The RAG model combines the retrieval module to enable it to dynamically access the external knowledge base during the generation process, thereby significantly improving knowledge consistency (0.73) and reasoning capability (0.80). The performance of RAG+T is further improved, especially in terms of reasoning ability, which reaches 0.84, which shows the great potential of retrieval-enhanced generative models in complex tasks. By incorporating more relevant knowledge into the generation process, RAG+T shows stronger ability to deal with complex background information and deep reasoning, further narrowing the limitations of generative models in dealing with knowledge-intensive tasks. \n\nFinally, the graph-based RAG optimization model (ours) proposed in this paper performs well in all indicators, with a quality score of 0.90, knowledge consistency of 0.85, and reasoning ability of 0.91, which is significantly better than other models. This result verifies the effectiveness of our introduction of graph structure information.",
                    "score": 0.5826018312171315,
                    "section_title": "B. Experimental Results",
                    "char_start_offset": 11942,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 181
                        },
                        {
                            "start": 182,
                            "end": 277
                        },
                        {
                            "start": 278,
                            "end": 414
                        },
                        {
                            "start": 415,
                            "end": 539
                        },
                        {
                            "start": 540,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 944
                        },
                        {
                            "start": 945,
                            "end": 1226
                        },
                        {
                            "start": 1229,
                            "end": 1370
                        },
                        {
                            "start": 1371,
                            "end": 1602
                        },
                        {
                            "start": 1603,
                            "end": 1800
                        },
                        {
                            "start": 1801,
                            "end": 2061
                        },
                        {
                            "start": 2064,
                            "end": 2315
                        },
                        {
                            "start": 2316,
                            "end": 2406
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89208984375
                }
            ],
            "relevance_judgement": 0.89208984375,
            "relevance_judgment_input_expanded": "# Title: Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge Reasoning and Text Generation\n# Venue: 2024 5th International Symposium on Computer Engineering and Intelligent Communications (ISCEIC)\n# Authors: Yuxin Dong, Shuo Wang, Hongye Zheng, Jiajing Chen, Zhenhong Zhang, Chihang Wang\n## Abstract\nThis study aims to optimize the existing retrieval-augmented generation model (RAG) by introducing a graph structure to improve the performance of the model in dealing with complex knowledge reasoning tasks. The traditional RAG model has the problem of insufficient processing efficiency when facing complex graph structure information (such as knowledge graphs, hierarchical relationships, etc.), which affects the quality and consistency of the generated results. This study proposes a scheme to process graph structure data by combining graph neural network (GNN), so that the model can capture the complex relationship between entities, thereby improving the knowledge consistency and reasoning ability of the generated text. The experiment used the Natural Questions (NQ) dataset and compared it with multiple existing generation models. The results show that the graph-based RAG model proposed in this paper is superior to the traditional generation model in terms of quality, knowledge consistency, and reasoning ability, especially when dealing with tasks that require multi-dimensional reasoning. Through the combination of the enhancement of the retrieval module and the graph neural network, the model in this study can better handle complex knowledge background information and has broad potential value in multiple practical application scenarios.\n## B. Experimental Results\nWe will select five different generative models to conduct comparative experiments with the RAG model based on graph structure optimization proposed in this paper (labeled as Ours). By comparing their performance on the same dataset, we can verify the superiority of our model. The evaluation indicators used in the comparative experiments include Quality, Knowledge Consistency (KC), and Reasoning Capability(RC). 1, the performance of each model in the three indicators of Quality, Knowledge Consistency, and Reasoning Capability varies. It can be observed that traditional generative models such as BART and T5 performed relatively weakly in the experiment, especially in terms of knowledge consistency and reasoning capability. BART's quality score is 0.74, KC score is 0.65, and reasoning capability is 0.68, reflecting that although its generated text is fluent, it cannot provide sufficient external support in knowledge-intensive tasks. The T5 model has a slight improvement in the three indicators, especially in reasoning capability (0.72), which shows that the T5 model can improve its generalization ability by unifying task processing, but it is still not enough to fully handle complex knowledge reasoning tasks. \n\nIn contrast, the RAG model and its improved version RAG+T (RAG+Text) perform significantly better than BART and T5 in these three indicators. The RAG model combines the retrieval module to enable it to dynamically access the external knowledge base during the generation process, thereby significantly improving knowledge consistency (0.73) and reasoning capability (0.80). The performance of RAG+T is further improved, especially in terms of reasoning ability, which reaches 0.84, which shows the great potential of retrieval-enhanced generative models in complex tasks. By incorporating more relevant knowledge into the generation process, RAG+T shows stronger ability to deal with complex background information and deep reasoning, further narrowing the limitations of generative models in dealing with knowledge-intensive tasks. \n\nFinally, the graph-based RAG optimization model (ours) proposed in this paper performs well in all indicators, with a quality score of 0.90, knowledge consistency of 0.85, and reasoning ability of 0.91, which is significantly better than other models. This result verifies the effectiveness of our introduction of graph structure information.",
            "reference_string": "[273850363 | Dong et al. | 2024 | Citations: 13]"
        },
        {
            "title": "RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 21,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.01262, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2214586034",
                    "name": "Kunlun Zhu"
                },
                {
                    "authorId": "2281837494",
                    "name": "Yifan Luo"
                },
                {
                    "authorId": "2314779448",
                    "name": "Dingling Xu"
                },
                {
                    "authorId": "2314784069",
                    "name": "Ruobing Wang"
                },
                {
                    "authorId": "2314785970",
                    "name": "Shi Yu"
                },
                {
                    "authorId": "2267033597",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2277242040",
                    "name": "Yukun Yan"
                },
                {
                    "authorId": "49047064",
                    "name": "Zhenghao Liu"
                },
                {
                    "authorId": "48506411",
                    "name": "Xu Han"
                },
                {
                    "authorId": "2301534001",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2273551430",
                    "name": "Maosong Sun"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) is a powerful approach that enables large language models (LLMs) to incorporate external knowledge. However, evaluating the effectiveness of RAG systems in specialized scenarios remains challenging due to the high costs of data construction and the lack of suitable evaluation metrics. This paper introduces RAGEval, a framework designed to assess RAG systems across diverse scenarios by generating high-quality documents, questions, answers, and references through a schema-based pipeline. With a focus on factual accuracy, we propose three novel metrics: Completeness, Hallucination, and Irrelevance to evaluate LLM generated responses rigorously. Experimental results show that RAGEval outperforms zero-shot and one-shot methods in terms of clarity, safety, conformity, and richness of generated samples. Furthermore, the use of LLMs for scoring the proposed metrics demonstrates a high level of consistency with human evaluations. RAGEval establishes a new paradigm for evaluating RAG systems in real-world applications. The code and dataset are released at https://github.com/OpenBMB/RAGEval.",
            "corpus_id": 271693372,
            "sentences": [
                {
                    "corpus_id": "271693372",
                    "title": "RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework",
                    "text": "Retrieval-Augmented Generation (RAG) is a powerful approach that enables large language models (LLMs) to incorporate external knowledge. However, evaluating the effectiveness of RAG systems in specialized scenarios remains challenging due to the high costs of data construction and the lack of suitable evaluation metrics. This paper introduces RAGEval, a framework designed to assess RAG systems across diverse scenarios by generating high-quality documents, questions, answers, and references through a schema-based pipeline. With a focus on factual accuracy, we propose three novel metrics: Completeness, Hallucination, and Irrelevance to evaluate LLM generated responses rigorously. Experimental results show that RAGEval outperforms zero-shot and one-shot methods in terms of clarity, safety, conformity, and richness of generated samples. Furthermore, the use of LLMs for scoring the proposed metrics demonstrates a high level of consistency with human evaluations. RAGEval establishes a new paradigm for evaluating RAG systems in real-world applications. The code and dataset are released at https://github.com/OpenBMB/RAGEval.",
                    "score": 0.666338692813319,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.890625
                }
            ],
            "relevance_judgement": 0.890625,
            "relevance_judgment_input_expanded": "# Title: RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework\n# Venue: arXiv.org\n# Authors: Kunlun Zhu, Yifan Luo, Dingling Xu, Ruobing Wang, Shi Yu, Shuo Wang, Yukun Yan, Zhenghao Liu, Xu Han, Zhiyuan Liu, Maosong Sun\n## Abstract\nRetrieval-Augmented Generation (RAG) is a powerful approach that enables large language models (LLMs) to incorporate external knowledge. However, evaluating the effectiveness of RAG systems in specialized scenarios remains challenging due to the high costs of data construction and the lack of suitable evaluation metrics. This paper introduces RAGEval, a framework designed to assess RAG systems across diverse scenarios by generating high-quality documents, questions, answers, and references through a schema-based pipeline. With a focus on factual accuracy, we propose three novel metrics: Completeness, Hallucination, and Irrelevance to evaluate LLM generated responses rigorously. Experimental results show that RAGEval outperforms zero-shot and one-shot methods in terms of clarity, safety, conformity, and richness of generated samples. Furthermore, the use of LLMs for scoring the proposed metrics demonstrates a high level of consistency with human evaluations. RAGEval establishes a new paradigm for evaluating RAG systems in real-world applications. The code and dataset are released at https://github.com/OpenBMB/RAGEval.\n",
            "reference_string": "[271693372 | Zhu et al. | 2024 | Citations: 21]"
        },
        {
            "title": "Adaptive Control of Retrieval-Augmented Generation for Large Language Models Through Reflective Tags",
            "venue": "Electronics",
            "year": 2024,
            "reference_count": 29,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3390/electronics13234643",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics13234643?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics13234643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2332684729",
                    "name": "Chengyuan Yao"
                },
                {
                    "authorId": "2275134724",
                    "name": "Satoshi Fujita"
                }
            ],
            "abstract": "While retrieval-augmented generation (RAG) enhances large language models (LLMs), it also introduces challenges that can impact accuracy and performance. In practice, RAG can obscure the intrinsic strengths of LLMs. Firstly, LLMs may become too reliant on external retrieval, underutilizing their own knowledge and reasoning, which can diminish responsiveness. Secondly, RAG may introduce irrelevant or low-quality data, adding noise that disrupts generation, especially with complex tasks. This paper proposes an RAG framework that uses reflective tags to manage retrieval, evaluating documents in parallel and applying the chain-of-thought (CoT) technique for step-by-step generation. The model selects the highest quality content for final output. The key contributions are as follows: (1) reducing hallucinations by focusing on high-scoring documents; (2) improving real-time performance through efficient retrieval; and (3) mitigating negative effects by filtering out irrelevant information using parallel generation and reflective tagging. These innovations aim to optimize RAG for more reliable, high-quality results.",
            "corpus_id": 274283400,
            "sentences": [
                {
                    "corpus_id": "274283400",
                    "title": "Adaptive Control of Retrieval-Augmented Generation for Large Language Models Through Reflective Tags",
                    "text": "This paper investigates retrieval-augmented generation (RAG) as a means to mitigate the hallucination and latency challenges inherent to large language models (LLMs). To address the limitations introduced by RAG, such as the potential masking of LLM capabilities, we propose a novel framework employing four reflective tags to control the retrieval and evaluation of external sources. A search tag enables adaptive search, mitigating the overreliance on irrelevant information. Evaluation tags facilitate a comprehensive assessment of retrieved documents based on relevance, support, and overall quality. The framework incorporates chain-of-thought (CoT) reasoning to decompose queries and generate responses incrementally, further enhancing output quality and reliability. \n\nTo evaluate the performance of the proposed framework, we conducted experiments on four benchmark datasets: ARC-Challenge, PubHealth, PopQA, and TriviaQA, where GPT-3.5 and Qwen served as baselines, with accuracy as the primary evaluation metric. The experimental results shown in Section 4 demonstrate the effectiveness of the proposed method, especially in improving the accuracy of the fact-checking benchmarks. The performance of the proposed method depends on the value of the hyperparameter k, and the optimal value of k depends on the baseline LLM. These findings collectively provide compelling evidence of the effectiveness of the proposed framework. \n\nFuture work includes comparisons with existing methods for improving the performance of RAGs and evaluation experiments using a wider range of datasets. We plan to compare our method with existing robust approaches, including RAG by Facebook AI [18], REALM by Google [1], and various domain-specific retrieval systems such as PubMed [19], BioBERT [20] PatentBERT [21], and FinancialBERT [22]. Additionally, we will evaluate against the latest state-of-the-art LLMs, such as Phi-3.5 [23], LLaMA 3.2 [24], and OLMo [25]. Specifically, our investigation will focus on the influence of prepared inputs, such as keywords, on generation performance, particularly as it relates to solving retrieval problems. Furthermore, it is essential to evaluate our method using domain-specific datasets in areas such as law, medicine, and science.",
                    "score": 0.605711253434436,
                    "section_title": "Concluding Remarks",
                    "char_start_offset": 27075,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 166
                        },
                        {
                            "start": 167,
                            "end": 384
                        },
                        {
                            "start": 385,
                            "end": 477
                        },
                        {
                            "start": 478,
                            "end": 604
                        },
                        {
                            "start": 605,
                            "end": 773
                        },
                        {
                            "start": 776,
                            "end": 1022
                        },
                        {
                            "start": 1023,
                            "end": 1190
                        },
                        {
                            "start": 1191,
                            "end": 1331
                        },
                        {
                            "start": 1332,
                            "end": 1435
                        },
                        {
                            "start": 1438,
                            "end": 1590
                        },
                        {
                            "start": 1591,
                            "end": 1830
                        },
                        {
                            "start": 1831,
                            "end": 1956
                        },
                        {
                            "start": 1957,
                            "end": 2139
                        },
                        {
                            "start": 2140,
                            "end": 2267
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1705,
                            "end": 1708,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 1771,
                            "end": 1775,
                            "matchedPaperCorpusId": "224820417"
                        },
                        {
                            "start": 1785,
                            "end": 1789,
                            "matchedPaperCorpusId": "59291975"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.890625
                }
            ],
            "relevance_judgement": 0.890625,
            "relevance_judgment_input_expanded": "# Title: Adaptive Control of Retrieval-Augmented Generation for Large Language Models Through Reflective Tags\n# Venue: Electronics\n# Authors: Chengyuan Yao, Satoshi Fujita\n## Abstract\nWhile retrieval-augmented generation (RAG) enhances large language models (LLMs), it also introduces challenges that can impact accuracy and performance. In practice, RAG can obscure the intrinsic strengths of LLMs. Firstly, LLMs may become too reliant on external retrieval, underutilizing their own knowledge and reasoning, which can diminish responsiveness. Secondly, RAG may introduce irrelevant or low-quality data, adding noise that disrupts generation, especially with complex tasks. This paper proposes an RAG framework that uses reflective tags to manage retrieval, evaluating documents in parallel and applying the chain-of-thought (CoT) technique for step-by-step generation. The model selects the highest quality content for final output. The key contributions are as follows: (1) reducing hallucinations by focusing on high-scoring documents; (2) improving real-time performance through efficient retrieval; and (3) mitigating negative effects by filtering out irrelevant information using parallel generation and reflective tagging. These innovations aim to optimize RAG for more reliable, high-quality results.\n## Concluding Remarks\nThis paper investigates retrieval-augmented generation (RAG) as a means to mitigate the hallucination and latency challenges inherent to large language models (LLMs). To address the limitations introduced by RAG, such as the potential masking of LLM capabilities, we propose a novel framework employing four reflective tags to control the retrieval and evaluation of external sources. A search tag enables adaptive search, mitigating the overreliance on irrelevant information. Evaluation tags facilitate a comprehensive assessment of retrieved documents based on relevance, support, and overall quality. The framework incorporates chain-of-thought (CoT) reasoning to decompose queries and generate responses incrementally, further enhancing output quality and reliability. \n\nTo evaluate the performance of the proposed framework, we conducted experiments on four benchmark datasets: ARC-Challenge, PubHealth, PopQA, and TriviaQA, where GPT-3.5 and Qwen served as baselines, with accuracy as the primary evaluation metric. The experimental results shown in Section 4 demonstrate the effectiveness of the proposed method, especially in improving the accuracy of the fact-checking benchmarks. The performance of the proposed method depends on the value of the hyperparameter k, and the optimal value of k depends on the baseline LLM. These findings collectively provide compelling evidence of the effectiveness of the proposed framework. \n\nFuture work includes comparisons with existing methods for improving the performance of RAGs and evaluation experiments using a wider range of datasets. We plan to compare our method with existing robust approaches, including RAG by Facebook AI [18], REALM by Google [1], and various domain-specific retrieval systems such as PubMed [19], BioBERT [20] PatentBERT [21], and FinancialBERT [22]. Additionally, we will evaluate against the latest state-of-the-art LLMs, such as Phi-3.5 [23], LLaMA 3.2 [24], and OLMo [25]. Specifically, our investigation will focus on the influence of prepared inputs, such as keywords, on generation performance, particularly as it relates to solving retrieval problems. Furthermore, it is essential to evaluate our method using domain-specific datasets in areas such as law, medicine, and science.",
            "reference_string": "[274283400 | Yao et al. | 2024 | Citations: 4]"
        },
        {
            "title": "SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity Reduction",
            "venue": "International Conference on Learning Representations",
            "year": 2025,
            "reference_count": 84,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01478, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2247963050",
                    "name": "Lu Dai"
                },
                {
                    "authorId": "2294806563",
                    "name": "Yijie Xu"
                },
                {
                    "authorId": "2348899671",
                    "name": "Jinhui Ye"
                },
                {
                    "authorId": "2247930299",
                    "name": "Hao Liu"
                },
                {
                    "authorId": "2247992230",
                    "name": "Hui Xiong"
                }
            ],
            "abstract": "Large Language Models (LLMs) have demonstrated improved generation performance by incorporating externally retrieved knowledge, a process known as retrieval-augmented generation (RAG). Despite the potential of this approach, existing studies evaluate RAG effectiveness by 1) assessing retrieval and generation components jointly, which obscures retrieval's distinct contribution, or 2) examining retrievers using traditional metrics such as NDCG, which creates a gap in understanding retrieval's true utility in the overall generation process. To address the above limitations, in this work, we introduce an automatic evaluation method that measures retrieval quality through the lens of information gain within the RAG framework. Specifically, we propose Semantic Perplexity (SePer), a metric that captures the LLM's internal belief about the correctness of the retrieved information. We quantify the utility of retrieval by the extent to which it reduces semantic perplexity post-retrieval. Extensive experiments demonstrate that SePer not only aligns closely with human preferences but also offers a more precise and efficient evaluation of retrieval utility across diverse RAG scenarios.",
            "corpus_id": 276775668,
            "sentences": [
                {
                    "corpus_id": "276775668",
                    "title": "SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity Reduction",
                    "text": "Large Language Models (LLMs) have demonstrated improved generation performance by incorporating externally retrieved knowledge, a process known as retrieval-augmented generation (RAG). Despite the potential of this approach, existing studies evaluate RAG effectiveness by 1) assessing retrieval and generation components jointly, which obscures retrieval's distinct contribution, or 2) examining retrievers using traditional metrics such as NDCG, which creates a gap in understanding retrieval's true utility in the overall generation process. To address the above limitations, in this work, we introduce an automatic evaluation method that measures retrieval quality through the lens of information gain within the RAG framework. Specifically, we propose Semantic Perplexity (SePer), a metric that captures the LLM's internal belief about the correctness of the retrieved information. We quantify the utility of retrieval by the extent to which it reduces semantic perplexity post-retrieval. Extensive experiments demonstrate that SePer not only aligns closely with human preferences but also offers a more precise and efficient evaluation of retrieval utility across diverse RAG scenarios.",
                    "score": 0.6558539948721187,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8896484375
                }
            ],
            "relevance_judgement": 0.8896484375,
            "relevance_judgment_input_expanded": "# Title: SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity Reduction\n# Venue: International Conference on Learning Representations\n# Authors: Lu Dai, Yijie Xu, Jinhui Ye, Hao Liu, Hui Xiong\n## Abstract\nLarge Language Models (LLMs) have demonstrated improved generation performance by incorporating externally retrieved knowledge, a process known as retrieval-augmented generation (RAG). Despite the potential of this approach, existing studies evaluate RAG effectiveness by 1) assessing retrieval and generation components jointly, which obscures retrieval's distinct contribution, or 2) examining retrievers using traditional metrics such as NDCG, which creates a gap in understanding retrieval's true utility in the overall generation process. To address the above limitations, in this work, we introduce an automatic evaluation method that measures retrieval quality through the lens of information gain within the RAG framework. Specifically, we propose Semantic Perplexity (SePer), a metric that captures the LLM's internal belief about the correctness of the retrieved information. We quantify the utility of retrieval by the extent to which it reduces semantic perplexity post-retrieval. Extensive experiments demonstrate that SePer not only aligns closely with human preferences but also offers a more precise and efficient evaluation of retrieval utility across diverse RAG scenarios.\n",
            "reference_string": "[276775668 | Dai et al. | 2025 | Citations: 3]"
        },
        {
            "title": "RAG vs. GraphRAG: A Systematic Evaluation and Key Insights",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 46,
            "citation_count": 3,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.11371, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2049039664",
                    "name": "Haoyu Han"
                },
                {
                    "authorId": "2220302956",
                    "name": "Harry Shomer"
                },
                {
                    "authorId": "2346107355",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "2338562947",
                    "name": "Yongjia Lei"
                },
                {
                    "authorId": "2338271219",
                    "name": "Kai Guo"
                },
                {
                    "authorId": "2293482433",
                    "name": "Zhigang Hua"
                },
                {
                    "authorId": "2338267824",
                    "name": "Bo Long"
                },
                {
                    "authorId": "2298005501",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "2330147642",
                    "name": "Jiliang Tang"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) enhances the performance of LLMs across various tasks by retrieving relevant information from external sources, particularly on text-based data. For structured data, such as knowledge graphs, GraphRAG has been widely used to retrieve relevant information. However, recent studies have revealed that structuring implicit knowledge from text into graphs can benefit certain tasks, extending the application of GraphRAG from graph data to general text-based data. Despite their successful extensions, most applications of GraphRAG for text data have been designed for specific tasks and datasets, lacking a systematic evaluation and comparison between RAG and GraphRAG on widely used text-based benchmarks. In this paper, we systematically evaluate RAG and GraphRAG on well-established benchmark tasks, such as Question Answering and Query-based Summarization. Our results highlight the distinct strengths of RAG and GraphRAG across different tasks and evaluation perspectives. Inspired by these observations, we investigate strategies to integrate their strengths to improve downstream tasks. Additionally, we provide an in-depth discussion of the shortcomings of current GraphRAG approaches and outline directions for future research.",
            "corpus_id": 276408622,
            "sentences": [
                {
                    "corpus_id": "276408622",
                    "title": "RAG vs. GraphRAG: A Systematic Evaluation and Key Insights",
                    "text": "Retrieval-Augmented Generation (RAG) has been widely applied to enhance the performance of Large Language Models (LLMs) by retrieving relevant information from external sources, addressing the limitation of LLMs' restricted context windows, improving factual accuracy, and mitigating hallucinations (Fan et al., 2024;Gao et al., 2023). Most RAG systems primarily process text data by first splitting it into chunks (Finardi et al., 2024). When a query is received, RAG retrieves relevant chunks either through lexical search (Ram et al., 2023) or by computing semantic similarity (Karpukhin et al., 2020), embeddings both the query and text chunks into a shared vector space. Advanced techniques, such as pre-retrieval processing (Ma et al., 2023;Zheng et al., 2023a) and post-retrieval processing (Dong et al., 2024;Xu et al., 2023), as well as fine-tuning strategies (Li et al., 2023), have further enhanced RAG's effectiveness across various domains, including QA) (Yan et al., 2024), dialogue generation (Izacard et al., 2023), and text summarization (Jiang et al., 2023). \n\nSeveral studies have evaluated the effectiveness of RAG systems across various tasks (Yu et al., 2024;Chen et al., 2024;Es et al., 2023), such as multi-hop question answering (Tang and Yang, 2024), biomedical question answering (Xiong et al., 2024), and text generation (Liu et al., 2023). However, no existing study has simultaneously and systematically evaluated and compared RAG and GraphRAG on these general text-based tasks.",
                    "score": 0.6964858316403251,
                    "section_title": "Retrieval-Augmented Generation",
                    "char_start_offset": 4780,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 438
                        },
                        {
                            "start": 439,
                            "end": 675
                        },
                        {
                            "start": 676,
                            "end": 1076
                        },
                        {
                            "start": 1079,
                            "end": 1368
                        },
                        {
                            "start": 1369,
                            "end": 1508
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 299,
                            "end": 317,
                            "matchedPaperCorpusId": "269740933"
                        },
                        {
                            "start": 525,
                            "end": 543,
                            "matchedPaperCorpusId": "256459451"
                        },
                        {
                            "start": 1008,
                            "end": 1030,
                            "matchedPaperCorpusId": "251371732"
                        },
                        {
                            "start": 1181,
                            "end": 1199,
                            "matchedPaperCorpusId": "261530434"
                        },
                        {
                            "start": 1254,
                            "end": 1275,
                            "matchedPaperCorpusId": "263152125"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88916015625
                }
            ],
            "relevance_judgement": 0.88916015625,
            "relevance_judgment_input_expanded": "# Title: RAG vs. GraphRAG: A Systematic Evaluation and Key Insights\n# Venue: arXiv.org\n# Authors: Haoyu Han, Harry Shomer, Yu Wang, Yongjia Lei, Kai Guo, Zhigang Hua, Bo Long, Hui Liu, Jiliang Tang\n## Abstract\nRetrieval-Augmented Generation (RAG) enhances the performance of LLMs across various tasks by retrieving relevant information from external sources, particularly on text-based data. For structured data, such as knowledge graphs, GraphRAG has been widely used to retrieve relevant information. However, recent studies have revealed that structuring implicit knowledge from text into graphs can benefit certain tasks, extending the application of GraphRAG from graph data to general text-based data. Despite their successful extensions, most applications of GraphRAG for text data have been designed for specific tasks and datasets, lacking a systematic evaluation and comparison between RAG and GraphRAG on widely used text-based benchmarks. In this paper, we systematically evaluate RAG and GraphRAG on well-established benchmark tasks, such as Question Answering and Query-based Summarization. Our results highlight the distinct strengths of RAG and GraphRAG across different tasks and evaluation perspectives. Inspired by these observations, we investigate strategies to integrate their strengths to improve downstream tasks. Additionally, we provide an in-depth discussion of the shortcomings of current GraphRAG approaches and outline directions for future research.\n## Retrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) has been widely applied to enhance the performance of Large Language Models (LLMs) by retrieving relevant information from external sources, addressing the limitation of LLMs' restricted context windows, improving factual accuracy, and mitigating hallucinations (Fan et al., 2024;Gao et al., 2023). Most RAG systems primarily process text data by first splitting it into chunks (Finardi et al., 2024). When a query is received, RAG retrieves relevant chunks either through lexical search (Ram et al., 2023) or by computing semantic similarity (Karpukhin et al., 2020), embeddings both the query and text chunks into a shared vector space. Advanced techniques, such as pre-retrieval processing (Ma et al., 2023;Zheng et al., 2023a) and post-retrieval processing (Dong et al., 2024;Xu et al., 2023), as well as fine-tuning strategies (Li et al., 2023), have further enhanced RAG's effectiveness across various domains, including QA) (Yan et al., 2024), dialogue generation (Izacard et al., 2023), and text summarization (Jiang et al., 2023). \n\nSeveral studies have evaluated the effectiveness of RAG systems across various tasks (Yu et al., 2024;Chen et al., 2024;Es et al., 2023), such as multi-hop question answering (Tang and Yang, 2024), biomedical question answering (Xiong et al., 2024), and text generation (Liu et al., 2023). However, no existing study has simultaneously and systematically evaluated and compared RAG and GraphRAG on these general text-based tasks.",
            "reference_string": "[276408622 | Han et al. | 2025 | Citations: 3]"
        },
        {
            "title": "LoRE: Logit-Ranked Retriever Ensemble for Enhancing Open-Domain Question Answering",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 18,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.10042, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325905180",
                    "name": "Saikrishna Sanniboina"
                },
                {
                    "authorId": "2325907509",
                    "name": "Shiv Trivedi"
                },
                {
                    "authorId": "2325903482",
                    "name": "Sreenidhi Vijayaraghavan"
                }
            ],
            "abstract": "Retrieval-based question answering systems often suffer from positional bias, leading to suboptimal answer generation. We propose LoRE (Logit-Ranked Retriever Ensemble), a novel approach that improves answer accuracy and relevance by mitigating positional bias. LoRE employs an ensemble of diverse retrievers, such as BM25 and sentence transformers with FAISS indexing. A key innovation is a logit-based answer ranking algorithm that combines the logit scores from a large language model (LLM), with the retrieval ranks of the passages. Experimental results on NarrativeQA, SQuAD demonstrate that LoRE significantly outperforms existing retrieval-based methods in terms of exact match and F1 scores. On SQuAD, LoRE achieves 14.5\\%, 22.83\\%, and 14.95\\% improvements over the baselines for ROUGE-L, EM, and F1, respectively. Qualitatively, LoRE generates more relevant and accurate answers, especially for complex queries.",
            "corpus_id": 273345967,
            "sentences": [
                {
                    "corpus_id": "273345967",
                    "title": "LoRE: Logit-Ranked Retriever Ensemble for Enhancing Open-Domain Question Answering",
                    "text": "The development of retrieval-augmented LLMs has advanced significantly, focusing on improving the synergy between retrieval and generation processes. Shao et al. [17] introduced ITER-RETGEN, which iteratively enhances retrieval and generation for tasks like multi-hop QA and commonsense reasoning. It alternates between retrieval-augmented generation and generation-augmented retrieval, refining each step with the previous output. But, it faces challenges such as suboptimal utilization of retrieved information, with about 20% of contexts lacking actual answers, leading to inaccuracies or hallucinations due to positional bias and non-optimized iterative retrieval. \n\nAddressing similar issues, Sawarkar et al. [16] proposed Blended RAG, which incorporates semantic search and hybrid query-based retrievers to enhance the accuracy and relevancy of retrieved information. By using a combination of keyword-based, vector-based, and semantic-based searches, Blended RAG aims to mitigate the retrieval limitations observed in ITER-RETGEN by ensuring that more relevant documents are retrieved, thus improving the overall effectiveness of the RAG system. However, the complexity and computational intensity of implementing multiple retrieval strategies, as well as scalability concerns in real-world applications, remain significant challenges for Blended RAG. \n\nAnother approach by Ma et al. [9] focused on Query Rewriting within retrieval-augmented settings. Their \"Rewrite-Retrieve-Read\" framework aims to adapt the search query itself to better align with the information needs, thereby enhancing the relevancy and effectiveness of retrieval. This method reduces the dependency on extensive, domain-specific datasets by refining queries to work with existing open-domain data. Nonetheless, the system's reliance on high-quality training data and its limited ability to generalize across diverse datasets highlight persistent issues in the field. \n\nCheng et al. [3] introduced Lift Yourself Up, a retrieval-augmented text generation model using self-memory and external metrics like ROUGE/BLEU for more reliable and contextually relevant generation. \n\nThese studies highlight the need to refine retrieval and generation integration, addressing challenges like dynamic retrieval contexts, iterative process optimization, and scalability.",
                    "score": 0.6290220721986022,
                    "section_title": "Related Work",
                    "char_start_offset": 4291,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 149
                        },
                        {
                            "start": 150,
                            "end": 297
                        },
                        {
                            "start": 298,
                            "end": 431
                        },
                        {
                            "start": 432,
                            "end": 668
                        },
                        {
                            "start": 671,
                            "end": 873
                        },
                        {
                            "start": 874,
                            "end": 1152
                        },
                        {
                            "start": 1153,
                            "end": 1358
                        },
                        {
                            "start": 1361,
                            "end": 1458
                        },
                        {
                            "start": 1459,
                            "end": 1644
                        },
                        {
                            "start": 1645,
                            "end": 1778
                        },
                        {
                            "start": 1779,
                            "end": 1947
                        },
                        {
                            "start": 1950,
                            "end": 2150
                        },
                        {
                            "start": 2153,
                            "end": 2337
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1963,
                            "end": 1966,
                            "matchedPaperCorpusId": "258479968"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88916015625
                }
            ],
            "relevance_judgement": 0.88916015625,
            "relevance_judgment_input_expanded": "# Title: LoRE: Logit-Ranked Retriever Ensemble for Enhancing Open-Domain Question Answering\n# Venue: arXiv.org\n# Authors: Saikrishna Sanniboina, Shiv Trivedi, Sreenidhi Vijayaraghavan\n## Abstract\nRetrieval-based question answering systems often suffer from positional bias, leading to suboptimal answer generation. We propose LoRE (Logit-Ranked Retriever Ensemble), a novel approach that improves answer accuracy and relevance by mitigating positional bias. LoRE employs an ensemble of diverse retrievers, such as BM25 and sentence transformers with FAISS indexing. A key innovation is a logit-based answer ranking algorithm that combines the logit scores from a large language model (LLM), with the retrieval ranks of the passages. Experimental results on NarrativeQA, SQuAD demonstrate that LoRE significantly outperforms existing retrieval-based methods in terms of exact match and F1 scores. On SQuAD, LoRE achieves 14.5\\%, 22.83\\%, and 14.95\\% improvements over the baselines for ROUGE-L, EM, and F1, respectively. Qualitatively, LoRE generates more relevant and accurate answers, especially for complex queries.\n## Related Work\nThe development of retrieval-augmented LLMs has advanced significantly, focusing on improving the synergy between retrieval and generation processes. Shao et al. [17] introduced ITER-RETGEN, which iteratively enhances retrieval and generation for tasks like multi-hop QA and commonsense reasoning. It alternates between retrieval-augmented generation and generation-augmented retrieval, refining each step with the previous output. But, it faces challenges such as suboptimal utilization of retrieved information, with about 20% of contexts lacking actual answers, leading to inaccuracies or hallucinations due to positional bias and non-optimized iterative retrieval. \n\nAddressing similar issues, Sawarkar et al. [16] proposed Blended RAG, which incorporates semantic search and hybrid query-based retrievers to enhance the accuracy and relevancy of retrieved information. By using a combination of keyword-based, vector-based, and semantic-based searches, Blended RAG aims to mitigate the retrieval limitations observed in ITER-RETGEN by ensuring that more relevant documents are retrieved, thus improving the overall effectiveness of the RAG system. However, the complexity and computational intensity of implementing multiple retrieval strategies, as well as scalability concerns in real-world applications, remain significant challenges for Blended RAG. \n\nAnother approach by Ma et al. [9] focused on Query Rewriting within retrieval-augmented settings. Their \"Rewrite-Retrieve-Read\" framework aims to adapt the search query itself to better align with the information needs, thereby enhancing the relevancy and effectiveness of retrieval. This method reduces the dependency on extensive, domain-specific datasets by refining queries to work with existing open-domain data. Nonetheless, the system's reliance on high-quality training data and its limited ability to generalize across diverse datasets highlight persistent issues in the field. \n\nCheng et al. [3] introduced Lift Yourself Up, a retrieval-augmented text generation model using self-memory and external metrics like ROUGE/BLEU for more reliable and contextually relevant generation. \n\nThese studies highlight the need to refine retrieval and generation integration, addressing challenges like dynamic retrieval contexts, iterative process optimization, and scalability.",
            "reference_string": "[273345967 | Sanniboina et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Maximizing RAG efficiency: A comparative analysis of RAG methods",
            "venue": "Natural Language Processing",
            "year": 2024,
            "reference_count": 21,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/D7B259BCD35586E04358DF06006E0A85/S2977042424000530a.pdf/div-class-title-maximizing-rag-efficiency-a-comparative-analysis-of-rag-methods-div.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1017/nlp.2024.53?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1017/nlp.2024.53, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2328607527",
                    "name": "Tolga \u015eakar"
                },
                {
                    "authorId": "2328604444",
                    "name": "Hakan Emekci"
                }
            ],
            "abstract": "\n This paper addresses the optimization of retrieval-augmented generation (RAG) processes by exploring various methodologies, including advanced RAG methods. The research, driven by the need to enhance RAG processes as highlighted by recent studies, involved a grid-search optimization of 23,625 iterations. We evaluated multiple RAG methods across different vectorstores, embedding models, and large language models, using cross-domain datasets and contextual compression filters. The findings emphasize the importance of balancing context quality with similarity-based ranking methods, as well as understanding tradeoffs between similarity scores, token usage, runtime, and hardware utilization. Additionally, contextual compression filters were found to be crucial for efficient hardware utilization and reduced token consumption, despite the evident impacts on similarity scores, which may be acceptable depending on specific use cases and RAG methods.",
            "corpus_id": 273749074,
            "sentences": [
                {
                    "corpus_id": "273749074",
                    "title": "Maximizing RAG efficiency: A comparative analysis of RAG methods",
                    "text": "This paper addresses the optimization of retrieval-augmented generation (RAG) processes by exploring various methodologies, including advanced RAG methods. The research, driven by the need to enhance RAG processes as highlighted by recent studies, involved a grid-search optimization of 23,625 iterations. We evaluated multiple RAG methods across different vectorstores, embedding models, and large language models, using cross-domain datasets and contextual compression filters. The findings emphasize the importance of balancing context quality with similarity-based ranking methods, as well as understanding tradeoffs between similarity scores, token usage, runtime, and hardware utilization. Additionally, contextual compression filters were found to be crucial for efficient hardware utilization and reduced token consumption, despite the evident impacts on similarity scores, which may be acceptable depending on specific use cases and RAG methods.",
                    "score": 0.7039701279753574,
                    "section_title": "abstract",
                    "char_start_offset": 2,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88623046875
                }
            ],
            "relevance_judgement": 0.88623046875,
            "relevance_judgment_input_expanded": "# Title: Maximizing RAG efficiency: A comparative analysis of RAG methods\n# Venue: Natural Language Processing\n# Authors: Tolga \u015eakar, Hakan Emekci\n## Abstract\n\n This paper addresses the optimization of retrieval-augmented generation (RAG) processes by exploring various methodologies, including advanced RAG methods. The research, driven by the need to enhance RAG processes as highlighted by recent studies, involved a grid-search optimization of 23,625 iterations. We evaluated multiple RAG methods across different vectorstores, embedding models, and large language models, using cross-domain datasets and contextual compression filters. The findings emphasize the importance of balancing context quality with similarity-based ranking methods, as well as understanding tradeoffs between similarity scores, token usage, runtime, and hardware utilization. Additionally, contextual compression filters were found to be crucial for efficient hardware utilization and reduced token consumption, despite the evident impacts on similarity scores, which may be acceptable depending on specific use cases and RAG methods.\n",
            "reference_string": "[273749074 | Sakar et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 10,
            "citation_count": 20,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01722, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279813822",
                    "name": "Liang Zhang"
                },
                {
                    "authorId": "2279831793",
                    "name": "Katherine Jijo"
                },
                {
                    "authorId": "2282528163",
                    "name": "Spurthi Setty"
                },
                {
                    "authorId": "2279830841",
                    "name": "Eden Chung"
                },
                {
                    "authorId": "2282539958",
                    "name": "Fatima Javid"
                },
                {
                    "authorId": "2279830757",
                    "name": "Natan Vidra"
                },
                {
                    "authorId": "2279838243",
                    "name": "Thomas Clifford"
                }
            ],
            "abstract": "Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions. To address these challenges, a fine-tuning process is employed, involving feedback and examples to refine models. The objective is to enhance AI models through continuous feedback loops, utilizing metrics such as cosine similarity, LLM evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on financial datasets, including the FinanceBench and RAG Instruct Benchmark Tester Dataset, illustrating the necessity of fine-tuning. The results showcase the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs, providing superior question and answering capabilities. Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy.",
            "corpus_id": 267412954,
            "sentences": [
                {
                    "corpus_id": "267412954",
                    "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
                    "text": "Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions. To address these challenges, a fine-tuning process is employed, involving feedback and examples to refine models. The objective is to enhance AI models through continuous feedback loops, utilizing metrics such as cosine similarity, LLM evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on financial datasets, including the FinanceBench and RAG Instruct Benchmark Tester Dataset, illustrating the necessity of fine-tuning. The results showcase the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs, providing superior question and answering capabilities. Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy.",
                    "score": 0.6086768921072092,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88623046875
                }
            ],
            "relevance_judgement": 0.88623046875,
            "relevance_judgment_input_expanded": "# Title: Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately\n# Venue: arXiv.org\n# Authors: Liang Zhang, Katherine Jijo, Spurthi Setty, Eden Chung, Fatima Javid, Natan Vidra, Thomas Clifford\n## Abstract\nLarge Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions. To address these challenges, a fine-tuning process is employed, involving feedback and examples to refine models. The objective is to enhance AI models through continuous feedback loops, utilizing metrics such as cosine similarity, LLM evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on financial datasets, including the FinanceBench and RAG Instruct Benchmark Tester Dataset, illustrating the necessity of fine-tuning. The results showcase the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs, providing superior question and answering capabilities. Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy.\n",
            "reference_string": "[267412954 | Zhang et al. | 2024 | Citations: 20]"
        },
        {
            "title": "Enhancing Policy Generation with GraphRAG and YouTube Data: A Logistics Case Study",
            "venue": "Electronics",
            "year": 2025,
            "reference_count": 36,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics14071241?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics14071241, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2295941874",
                    "name": "Hisatoshi Naganawa"
                },
                {
                    "authorId": "120962479",
                    "name": "Enna Hirata"
                }
            ],
            "abstract": "Graph-based retrieval-augmented generation (GraphRAG) represents an innovative advancement in natural language processing, leveraging the power of large language models (LLMs) for complex tasks such as policy generation. This research presents a GraphRAG model trained on YouTube data containing keywords related to logistics issues to generate policy proposals addressing these challenges. The collected data include both video subtitles and user comments, which are used to fine-tune the GraphRAG model. To evaluate the effectiveness of this approach, the performance of the proposed model is compared to a standard generative pre-trained transformer (GPT) model. The results show that the GraphRAG model outperforms the GPT model in most prompts, highlighting its potential to generate more accurate and contextually relevant policy recommendations. This study not only contributes to the evolving field of LLM-based natural language processing (NLP) applications but also explores new methods for improving model efficiency and scalability in real-world domains like logistics policy making.",
            "corpus_id": 277231843,
            "sentences": [],
            "relevance_judgement": 0.8818359375,
            "relevance_judgment_input_expanded": "# Title: Enhancing Policy Generation with GraphRAG and YouTube Data: A Logistics Case Study\n# Venue: Electronics\n# Authors: Hisatoshi Naganawa, Enna Hirata\n## Abstract\nGraph-based retrieval-augmented generation (GraphRAG) represents an innovative advancement in natural language processing, leveraging the power of large language models (LLMs) for complex tasks such as policy generation. This research presents a GraphRAG model trained on YouTube data containing keywords related to logistics issues to generate policy proposals addressing these challenges. The collected data include both video subtitles and user comments, which are used to fine-tune the GraphRAG model. To evaluate the effectiveness of this approach, the performance of the proposed model is compared to a standard generative pre-trained transformer (GPT) model. The results show that the GraphRAG model outperforms the GPT model in most prompts, highlighting its potential to generate more accurate and contextually relevant policy recommendations. This study not only contributes to the evolving field of LLM-based natural language processing (NLP) applications but also explores new methods for improving model efficiency and scalability in real-world domains like logistics policy making.\n",
            "reference_string": "[277231843 | Naganawa et al. | 2025 | Citations: 3]"
        },
        {
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 251,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1491233507",
                    "name": "Mingyue Cheng"
                },
                {
                    "authorId": "2208917508",
                    "name": "Yucong Luo"
                },
                {
                    "authorId": "2322501286",
                    "name": "Ouyang Jie"
                },
                {
                    "authorId": "2332691115",
                    "name": "Qi Liu"
                },
                {
                    "authorId": "2312648865",
                    "name": "Huijie Liu"
                },
                {
                    "authorId": "2291070758",
                    "name": "Li Li"
                },
                {
                    "authorId": "2322429208",
                    "name": "Shuo Yu"
                },
                {
                    "authorId": "2351226328",
                    "name": "Bohou Zhang"
                },
                {
                    "authorId": "2350426005",
                    "name": "Jiawei Cao"
                },
                {
                    "authorId": "2350427710",
                    "name": "Jie Ma"
                },
                {
                    "authorId": "2322524150",
                    "name": "Daoyu Wang"
                },
                {
                    "authorId": "2258714945",
                    "name": "Enhong Chen"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has gained significant attention in recent years for its potential to enhance natural language understanding and generation by combining large-scale retrieval systems with generative models. RAG leverages external knowledge sources, such as documents, databases, or structured data, to improve model performance and generate more accurate and contextually relevant outputs. This survey aims to provide a comprehensive overview of RAG by examining its fundamental components, including retrieval mechanisms, generation processes, and the integration between the two. We discuss the key characteristics of RAG, such as its ability to augment generative models with dynamic external knowledge, and the challenges associated with aligning retrieved information with generative objectives. We also present a taxonomy that categorizes RAG methods, ranging from basic retrieval-augmented approaches to more advanced models incorporating multi-modal data and reasoning capabilities. Additionally, we review the evaluation benchmarks and datasets commonly used to assess RAG systems, along with a detailed exploration of its applications in fields such as question answering, summarization, and information retrieval. Finally, we highlight emerging research directions and opportunities for improving RAG systems, such as enhanced retrieval efficiency, model interpretability, and domain-specific adaptations. This paper concludes by outlining the prospects for RAG in addressing real-world challenges and its potential to drive further advancements in natural language processing.",
            "corpus_id": 277043297,
            "sentences": [
                {
                    "corpus_id": "277043297",
                    "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
                    "text": "Retrieval-Augmented Generation (RAG) has gained significant attention in recent years for its potential to enhance natural language understanding and generation by combining large-scale retrieval systems with generative models. RAG leverages external knowledge sources, such as documents, databases, or structured data, to improve model performance and generate more accurate and contextually relevant outputs. This survey aims to provide a comprehensive overview of RAG by examining its fundamental components, including retrieval mechanisms, generation processes, and the integration between the two. We discuss the key characteristics of RAG, such as its ability to augment generative models with dynamic external knowledge, and the challenges associated with aligning retrieved information with generative objectives. We also present a taxonomy that categorizes RAG methods, ranging from basic retrieval-augmented approaches to more advanced models incorporating multi-modal data and reasoning capabilities. Additionally, we review the evaluation benchmarks and datasets commonly used to assess RAG systems, along with a detailed exploration of its applications in fields such as question answering, summarization, and information retrieval. Finally, we highlight emerging research directions and opportunities for improving RAG systems, such as enhanced retrieval efficiency, model interpretability, and domain-specific adaptations. This paper concludes by outlining the prospects for RAG in addressing real-world challenges and its potential to drive further advancements in natural language processing.",
                    "score": 0.6916683631819612,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88037109375
                }
            ],
            "relevance_judgement": 0.88037109375,
            "relevance_judgment_input_expanded": "# Title: A Survey on Knowledge-Oriented Retrieval-Augmented Generation\n# Venue: arXiv.org\n# Authors: Mingyue Cheng, Yucong Luo, Ouyang Jie, Qi Liu, Huijie Liu, Li Li, Shuo Yu, Bohou Zhang, Jiawei Cao, Jie Ma, Daoyu Wang, Enhong Chen\n## Abstract\nRetrieval-Augmented Generation (RAG) has gained significant attention in recent years for its potential to enhance natural language understanding and generation by combining large-scale retrieval systems with generative models. RAG leverages external knowledge sources, such as documents, databases, or structured data, to improve model performance and generate more accurate and contextually relevant outputs. This survey aims to provide a comprehensive overview of RAG by examining its fundamental components, including retrieval mechanisms, generation processes, and the integration between the two. We discuss the key characteristics of RAG, such as its ability to augment generative models with dynamic external knowledge, and the challenges associated with aligning retrieved information with generative objectives. We also present a taxonomy that categorizes RAG methods, ranging from basic retrieval-augmented approaches to more advanced models incorporating multi-modal data and reasoning capabilities. Additionally, we review the evaluation benchmarks and datasets commonly used to assess RAG systems, along with a detailed exploration of its applications in fields such as question answering, summarization, and information retrieval. Finally, we highlight emerging research directions and opportunities for improving RAG systems, such as enhanced retrieval efficiency, model interpretability, and domain-specific adaptations. This paper concludes by outlining the prospects for RAG in addressing real-world challenges and its potential to drive further advancements in natural language processing.\n",
            "reference_string": "[277043297 | Cheng et al. | 2025 | Citations: 6]"
        },
        {
            "title": "Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 28,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.11792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2297189569",
                    "name": "Zooey Nguyen"
                },
                {
                    "authorId": "2297188041",
                    "name": "Anthony Annunziata"
                },
                {
                    "authorId": "69442223",
                    "name": "Vinh Luong"
                },
                {
                    "authorId": "2297188221",
                    "name": "Sang Dinh"
                },
                {
                    "authorId": "2297190249",
                    "name": "Quynh Le"
                },
                {
                    "authorId": "2297189614",
                    "name": "A. Ha"
                },
                {
                    "authorId": "2297189915",
                    "name": "Chanh Le"
                },
                {
                    "authorId": "2297189697",
                    "name": "Hong An Phan"
                },
                {
                    "authorId": "2058395065",
                    "name": "Shruti Raghavan"
                },
                {
                    "authorId": "2297324474",
                    "name": "Christopher Nguyen"
                }
            ],
            "abstract": "This paper investigates the impact of domain-specific model fine-tuning and of reasoning mechanisms on the performance of question-answering (Q&A) systems powered by large language models (LLMs) and Retrieval-Augmented Generation (RAG). Using the FinanceBench SEC financial filings dataset, we observe that, for RAG, combining a fine-tuned embedding model with a fine-tuned LLM achieves better accuracy than generic models, with relatively greater gains attributable to fine-tuned embedding models. Additionally, employing reasoning iterations on top of RAG delivers an even bigger jump in performance, enabling the Q&A systems to get closer to human-expert quality. We discuss the implications of such findings, propose a structured technical design space capturing major technical components of Q&A AI, and provide recommendations for making high-impact technical choices for such components. We plan to follow up on this work with actionable guides for AI teams and further investigations into the impact of domain-specific augmentation in RAG and into agentic AI capabilities such as advanced planning and reasoning.",
            "corpus_id": 269214364,
            "sentences": [
                {
                    "corpus_id": "269214364",
                    "title": "Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study",
                    "text": "AI-powered question-answering (Q&A) systems have emerged as important tools, alongside established search technologies, to enable quick access to relevant information and knowledge from large digital sources that are complex and time-consuming for humans to navigate.Advancements in large language models (LLMs) have revolutionized the field of Q&A, with models like GPT-3 (Brown et al. 2020), BERT (Devlin et al. 2018), and RoBERTa (Liu et al. 2019) demonstrating remarkable abilities in understanding and generating human-like text.However, the effectiveness of such models in handling domain-specific questions that require specialized knowledge is limited.\n\nRetrieval-augmented generation (RAG) techniques, which combine information retrieval and generative models (Lewis et al. 2021), have shown promise in boosting the quality of LLM output in Q&A tasks.RAG systems leverage the strengths of both retrieval and generation components to provide contextually relevant and informative responses.While there is a lack of established quantification of RAG accuracy, early findings suggest that generic RAG does not perform well in complex domains such as finance.In one instance, RAG based on generic LLMs such as GPT-4-Turbo fails to answer 81% of the questions derived from Securities and Exchange Commission (SEC) financial filings (Islam et al. 2023).\n\nThe underperformance of generic LLMs and RAG in domain-specific Q&A has motivated us to research into and create methods to adapt and extend such models and techniques.In this paper, we describe and quantify the gains in accuracy from two major methods: model fine-tuning and iterative reasoning.\n\nFine-tuning is a way to adapt language models to specific domains and tasks (Devlin et al. 2018;Liu et al. 2019) by training them on domain-specific data and having them capture the nuances and intricacies of a particular field.In a typical RAG workflow, there are two principal models that can be considered for fine-tuning: the Embedding Model, whose tasks are indexing the information in the corpus and retrieving information relevant to the posed question, and the Generative Model, whose task is synthesizing an answer.",
                    "score": 0.6027630799883393,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 267
                        },
                        {
                            "start": 267,
                            "end": 534
                        },
                        {
                            "start": 534,
                            "end": 660
                        },
                        {
                            "start": 662,
                            "end": 860
                        },
                        {
                            "start": 860,
                            "end": 998
                        },
                        {
                            "start": 998,
                            "end": 1164
                        },
                        {
                            "start": 1164,
                            "end": 1356
                        },
                        {
                            "start": 1358,
                            "end": 1526
                        },
                        {
                            "start": 1526,
                            "end": 1654
                        },
                        {
                            "start": 1656,
                            "end": 1884
                        },
                        {
                            "start": 1884,
                            "end": 2180
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8779296875
                }
            ],
            "relevance_judgement": 0.8779296875,
            "relevance_judgment_input_expanded": "# Title: Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study\n# Venue: arXiv.org\n# Authors: Zooey Nguyen, Anthony Annunziata, Vinh Luong, Sang Dinh, Quynh Le, A. Ha, Chanh Le, Hong An Phan, Shruti Raghavan, Christopher Nguyen\n## Abstract\nThis paper investigates the impact of domain-specific model fine-tuning and of reasoning mechanisms on the performance of question-answering (Q&A) systems powered by large language models (LLMs) and Retrieval-Augmented Generation (RAG). Using the FinanceBench SEC financial filings dataset, we observe that, for RAG, combining a fine-tuned embedding model with a fine-tuned LLM achieves better accuracy than generic models, with relatively greater gains attributable to fine-tuned embedding models. Additionally, employing reasoning iterations on top of RAG delivers an even bigger jump in performance, enabling the Q&A systems to get closer to human-expert quality. We discuss the implications of such findings, propose a structured technical design space capturing major technical components of Q&A AI, and provide recommendations for making high-impact technical choices for such components. We plan to follow up on this work with actionable guides for AI teams and further investigations into the impact of domain-specific augmentation in RAG and into agentic AI capabilities such as advanced planning and reasoning.\n## Introduction\nAI-powered question-answering (Q&A) systems have emerged as important tools, alongside established search technologies, to enable quick access to relevant information and knowledge from large digital sources that are complex and time-consuming for humans to navigate.Advancements in large language models (LLMs) have revolutionized the field of Q&A, with models like GPT-3 (Brown et al. 2020), BERT (Devlin et al. 2018), and RoBERTa (Liu et al. 2019) demonstrating remarkable abilities in understanding and generating human-like text.However, the effectiveness of such models in handling domain-specific questions that require specialized knowledge is limited.\n\nRetrieval-augmented generation (RAG) techniques, which combine information retrieval and generative models (Lewis et al. 2021), have shown promise in boosting the quality of LLM output in Q&A tasks.RAG systems leverage the strengths of both retrieval and generation components to provide contextually relevant and informative responses.While there is a lack of established quantification of RAG accuracy, early findings suggest that generic RAG does not perform well in complex domains such as finance.In one instance, RAG based on generic LLMs such as GPT-4-Turbo fails to answer 81% of the questions derived from Securities and Exchange Commission (SEC) financial filings (Islam et al. 2023).\n\nThe underperformance of generic LLMs and RAG in domain-specific Q&A has motivated us to research into and create methods to adapt and extend such models and techniques.In this paper, we describe and quantify the gains in accuracy from two major methods: model fine-tuning and iterative reasoning.\n\nFine-tuning is a way to adapt language models to specific domains and tasks (Devlin et al. 2018;Liu et al. 2019) by training them on domain-specific data and having them capture the nuances and intricacies of a particular field.In a typical RAG workflow, there are two principal models that can be considered for fine-tuning: the Embedding Model, whose tasks are indexing the information in the corpus and retrieving information relevant to the posed question, and the Generative Model, whose task is synthesizing an answer.",
            "reference_string": "[269214364 | Nguyen et al. | 2024 | Citations: 4]"
        },
        {
            "title": "EnterpriseEM: Fine-tuned Embeddings for Enterprise Semantic Search",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 17,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.00010, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66945149",
                    "name": "Kamalkumar Rathinasamy"
                },
                {
                    "authorId": "2304474062",
                    "name": "Jayarama Nettar"
                },
                {
                    "authorId": "2304471263",
                    "name": "Amit Kumar"
                },
                {
                    "authorId": "2304472624",
                    "name": "Vishal Manchanda"
                },
                {
                    "authorId": "2304472592",
                    "name": "Arun Vijayakumar"
                },
                {
                    "authorId": "2275256433",
                    "name": "Ayush Kataria"
                },
                {
                    "authorId": "2304471029",
                    "name": "Venkateshprasanna Manjunath"
                },
                {
                    "authorId": "2304472432",
                    "name": "GS Chidambaram"
                },
                {
                    "authorId": "31722494",
                    "name": "Jaskirat Sodhi"
                },
                {
                    "authorId": "2304453176",
                    "name": "Shoeb Shaikh"
                },
                {
                    "authorId": "2304474234",
                    "name": "Wasim Akhtar Khan"
                },
                {
                    "authorId": "2304542089",
                    "name": "Prashant Singh"
                },
                {
                    "authorId": "2304475433",
                    "name": "Tanishq Dattatray Ige"
                },
                {
                    "authorId": "2334485471",
                    "name": "V. Tiwari"
                },
                {
                    "authorId": "2304471138",
                    "name": "Rajab Ali Mondal"
                },
                {
                    "authorId": "2304472540",
                    "name": "K. Harshini"
                },
                {
                    "authorId": "2129197171",
                    "name": "S. Reka"
                },
                {
                    "authorId": "2304472698",
                    "name": "Chetana Amancharla"
                },
                {
                    "authorId": "2302622929",
                    "name": "Faiz ur Rahman"
                },
                {
                    "authorId": "2304471871",
                    "name": "A. HarikrishnanP"
                },
                {
                    "authorId": "2304475424",
                    "name": "Indraneel Saha"
                },
                {
                    "authorId": "2304471930",
                    "name": "Bhavya Tiwary"
                },
                {
                    "authorId": "2305129400",
                    "name": "Navin Shankar Patel"
                },
                {
                    "authorId": "2304470943",
                    "name": "S. PradeepT"
                },
                {
                    "authorId": "2304475392",
                    "name": "J. BalajiA"
                },
                {
                    "authorId": "2304472627",
                    "name": "Priyapravas"
                },
                {
                    "authorId": "2304472453",
                    "name": "Mohammed Rafee Tarafdar"
                }
            ],
            "abstract": "Enterprises grapple with the significant challenge of managing proprietary unstructured data, hindering efficient information retrieval. This has led to the emergence of AI-driven information retrieval solutions, designed to adeptly extract relevant insights to address employee inquiries. These solutions often leverage pre-trained embedding models and generative models as foundational components. While pre-trained embeddings may exhibit proximity or disparity based on their original training objectives, they might not fully align with the unique characteristics of enterprise-specific data, leading to suboptimal alignment with the retrieval goals of enterprise environments. In this paper, we propose a comprehensive methodology for contextualizing pre-trained embedding models to enterprise environments, covering the entire process from data preparation to model fine-tuning and evaluation. By adapting the embeddings to better suit the retrieval tasks prevalent in enterprises, we aim to enhance the performance of information retrieval solutions. We discuss the process of fine-tuning, its effect on retrieval accuracy, and the potential benefits for enterprise information management. Our findings demonstrate the efficacy of fine-tuned embedding models in improving the precision and relevance of search results in enterprise settings.",
            "corpus_id": 270214689,
            "sentences": [
                {
                    "corpus_id": "270214689",
                    "title": "EnterpriseEM: Fine-tuned Embeddings for Enterprise Semantic Search",
                    "text": "In the context of enterprises accumulating proprietary unstructured data, AI-driven information retrieval solutions have emerged as vital tools for extracting relevant answers to employee queries. Traditional methods for developing such solutions often involve choosing between Retrieval Augmented Generation (RAG) or fine-tuned Large Language Models (LLMs). However, fine-tuned LLMs, comprising only generative models, lack a guarantee of factual accuracy, while RAG, comprising an embedding model and a generative model, assures factual precision (Lewis at al., 2020 [1]). Despite their superior performance in general, RAG based solutions often rely on pre-trained models, potentially leading to suboptimal alignment with enterprise-specific data. \n\nAddressing this challenge entails exploring two potential avenues: Firstly, recent studies such as RAFT (Zhang et al., 2024 [2]) explore the integration of fine-tuned generative models within a RAG pipeline to enhance accuracy, albeit requiring substantial domain-specific data to fine-tune the generative models. Alternatively, leveraging domain-specific embedding models within a RAG pipeline to enhance accuracy remains an underexplored area. \n\nEarlier efforts, such as BioBERT (Lee et al., 2019 [3]), SciBERT (Beltagy et al., 2019 [4]), and LEGAL-BERT (Chalkidis et al., 2020 [5]) have effectively demonstrated the efficacy of domain-specific embeddings in information retrieval tasks. These endeavors primarily investigated two methodologies: (a) extending the pre-training of BERT and (b) pre-training BERT from scratch, both employing domain-specific corpora. Despite yielding commendable results, these methodologies necessitated substantial domainspecific corpora, with figures as staggering as 21.3B words for BioBERT, 3.17B tokens for SciBERT, and 11.5GB of text data for LEGAL-BERT, thereby posing significant challenges, particularly in lowresource domains like enterprises.",
                    "score": 0.5830881355556112,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 196
                        },
                        {
                            "start": 197,
                            "end": 358
                        },
                        {
                            "start": 359,
                            "end": 574
                        },
                        {
                            "start": 575,
                            "end": 750
                        },
                        {
                            "start": 753,
                            "end": 1066
                        },
                        {
                            "start": 1067,
                            "end": 1198
                        },
                        {
                            "start": 1201,
                            "end": 1442
                        },
                        {
                            "start": 1443,
                            "end": 1619
                        },
                        {
                            "start": 1620,
                            "end": 1940
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 569,
                            "end": 572,
                            "matchedPaperCorpusId": "218869575"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8759765625
                }
            ],
            "relevance_judgement": 0.8759765625,
            "relevance_judgment_input_expanded": "# Title: EnterpriseEM: Fine-tuned Embeddings for Enterprise Semantic Search\n# Venue: arXiv.org\n# Authors: Kamalkumar Rathinasamy, Jayarama Nettar, Amit Kumar, Vishal Manchanda, Arun Vijayakumar, Ayush Kataria, Venkateshprasanna Manjunath, GS Chidambaram, Jaskirat Sodhi, Shoeb Shaikh, Wasim Akhtar Khan, Prashant Singh, Tanishq Dattatray Ige, V. Tiwari, Rajab Ali Mondal, K. Harshini, S. Reka, Chetana Amancharla, Faiz ur Rahman, A. HarikrishnanP, Indraneel Saha, Bhavya Tiwary, Navin Shankar Patel, S. PradeepT, J. BalajiA, Priyapravas, Mohammed Rafee Tarafdar\n## Abstract\nEnterprises grapple with the significant challenge of managing proprietary unstructured data, hindering efficient information retrieval. This has led to the emergence of AI-driven information retrieval solutions, designed to adeptly extract relevant insights to address employee inquiries. These solutions often leverage pre-trained embedding models and generative models as foundational components. While pre-trained embeddings may exhibit proximity or disparity based on their original training objectives, they might not fully align with the unique characteristics of enterprise-specific data, leading to suboptimal alignment with the retrieval goals of enterprise environments. In this paper, we propose a comprehensive methodology for contextualizing pre-trained embedding models to enterprise environments, covering the entire process from data preparation to model fine-tuning and evaluation. By adapting the embeddings to better suit the retrieval tasks prevalent in enterprises, we aim to enhance the performance of information retrieval solutions. We discuss the process of fine-tuning, its effect on retrieval accuracy, and the potential benefits for enterprise information management. Our findings demonstrate the efficacy of fine-tuned embedding models in improving the precision and relevance of search results in enterprise settings.\n## Introduction\nIn the context of enterprises accumulating proprietary unstructured data, AI-driven information retrieval solutions have emerged as vital tools for extracting relevant answers to employee queries. Traditional methods for developing such solutions often involve choosing between Retrieval Augmented Generation (RAG) or fine-tuned Large Language Models (LLMs). However, fine-tuned LLMs, comprising only generative models, lack a guarantee of factual accuracy, while RAG, comprising an embedding model and a generative model, assures factual precision (Lewis at al., 2020 [1]). Despite their superior performance in general, RAG based solutions often rely on pre-trained models, potentially leading to suboptimal alignment with enterprise-specific data. \n\nAddressing this challenge entails exploring two potential avenues: Firstly, recent studies such as RAFT (Zhang et al., 2024 [2]) explore the integration of fine-tuned generative models within a RAG pipeline to enhance accuracy, albeit requiring substantial domain-specific data to fine-tune the generative models. Alternatively, leveraging domain-specific embedding models within a RAG pipeline to enhance accuracy remains an underexplored area. \n\nEarlier efforts, such as BioBERT (Lee et al., 2019 [3]), SciBERT (Beltagy et al., 2019 [4]), and LEGAL-BERT (Chalkidis et al., 2020 [5]) have effectively demonstrated the efficacy of domain-specific embeddings in information retrieval tasks. These endeavors primarily investigated two methodologies: (a) extending the pre-training of BERT and (b) pre-training BERT from scratch, both employing domain-specific corpora. Despite yielding commendable results, these methodologies necessitated substantial domainspecific corpora, with figures as staggering as 21.3B words for BioBERT, 3.17B tokens for SciBERT, and 11.5GB of text data for LEGAL-BERT, thereby posing significant challenges, particularly in lowresource domains like enterprises.",
            "reference_string": "[270214689 | Rathinasamy et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency",
            "venue": "",
            "year": 2025,
            "reference_count": 23,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.08445, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2139579614",
                    "name": "A. Ammar"
                },
                {
                    "authorId": "2260523016",
                    "name": "Anis Koubaa"
                },
                {
                    "authorId": "2302559379",
                    "name": "Omer Nacar"
                },
                {
                    "authorId": "3151219",
                    "name": "Wadii Boulila"
                }
            ],
            "abstract": "Large language models achieve high task performance yet often hallucinate or rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses these gaps by coupling generation with external search. We analyse how hyperparameters influence speed and quality in RAG systems, covering Chroma and Faiss vector stores, chunking policies, cross-encoder re-ranking, and temperature, and we evaluate six metrics: faithfulness, answer correctness, answer relevancy, context precision, context recall, and answer similarity. Chroma processes queries 13% faster, whereas Faiss yields higher retrieval precision, revealing a clear speed-accuracy trade-off. Naive fixed-length chunking with small windows and minimal overlap outperforms semantic segmentation while remaining the quickest option. Re-ranking provides modest gains in retrieval quality yet increases runtime by roughly a factor of 5, so its usefulness depends on latency constraints. These results help practitioners balance computational cost and accuracy when tuning RAG systems for transparent, up-to-date responses. Finally, we re-evaluate the top configurations with a corrective RAG workflow and show that their advantages persist when the model can iteratively request additional evidence. We obtain a near-perfect context precision (99%), which demonstrates that RAG systems can achieve extremely high retrieval accuracy with the right combination of hyperparameters, with significant implications for applications where retrieval quality directly impacts downstream task performance, such as clinical decision support in healthcare.",
            "corpus_id": 278534742,
            "sentences": [
                {
                    "corpus_id": "278534742",
                    "title": "Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency",
                    "text": "The emergence of Large Language Models (LLMs) represents a paradigm shift in artificial intelligence, demonstrating unprecedented capabilities in text generation, summarization, translation and complex reasoning. Despite training on vast corpora, LLMs remain prone to hallucinations-plausible yet incorrect outputs-and their static knowledge cutoff renders them unaware of developments beyond their training data. Moreover, the opaque nature of their reasoning hinders verification and accountability in high-stakes domains such as healthcare, robotics, legal analysis, and scientific research [1]- [5]. \n\nRetrieval-Augmented Generation (RAG) integrates external knowledge retrieval with generative models to mitigate these issues. First introduced by Lewis et al. [6], RAG systems retrieve semantically relevant document fragments prior to generation, grounding outputs in verifiable sources and enabling continuous updates. This dynamic interface improves factual accuracy, relevance and trustworthiness without sacrificing generative flexibility. \n\nThe adoption of RAG has expanded LLM applications in conversational AI, enterprise knowledge management and specialized professional domains. By incorporating retrieval, these systems handle complex, knowledge-intensive queries more precisely. However, retrieval incurs increased latency, complicates system architecture and raises ethical concerns around data privacy, consent and information quality-factors that demand careful design and governance. \n\nDespite growing deployment, the impact of hyperparameter and configuration choices on RAG performance and efficiency remains underexplored. Real-world constraints-computational resources, latency requirements and accuracy thresholds-necessitate an empirical understanding of how implementation decisions affect the trade-off between responsiveness and output quality. \n\nTo fill this gap, we conducted a systematic investigation varying key components-vector stores (Chroma, Faiss), chunking strategies (naive, semantic), cross-encoder re-ranking and temperature settings-while monitoring six performance metrics: faithfulness, answer correctness, answer relevancy, context precision, context recall and answer similarity. This multidimensional analysis reveals the accuracy-latency tradeoffs inherent in each choice and provides actionable insights for practitioners. \n\nThe contributions of this paper are fourfold. First, we introduce a unified evaluation framework that jointly measures RAG quality along six dimensions and tracks computational efficiency.",
                    "score": 0.586314129543506,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 18,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 212
                        },
                        {
                            "start": 213,
                            "end": 413
                        },
                        {
                            "start": 414,
                            "end": 603
                        },
                        {
                            "start": 606,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 925
                        },
                        {
                            "start": 926,
                            "end": 1049
                        },
                        {
                            "start": 1052,
                            "end": 1193
                        },
                        {
                            "start": 1194,
                            "end": 1295
                        },
                        {
                            "start": 1296,
                            "end": 1504
                        },
                        {
                            "start": 1507,
                            "end": 1646
                        },
                        {
                            "start": 1647,
                            "end": 1874
                        },
                        {
                            "start": 1877,
                            "end": 2228
                        },
                        {
                            "start": 2229,
                            "end": 2374
                        },
                        {
                            "start": 2377,
                            "end": 2422
                        },
                        {
                            "start": 2423,
                            "end": 2565
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 599,
                            "end": 602,
                            "matchedPaperCorpusId": "272957646"
                        },
                        {
                            "start": 765,
                            "end": 768,
                            "matchedPaperCorpusId": "218869575"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.875
                }
            ],
            "relevance_judgement": 0.875,
            "relevance_judgment_input_expanded": "# Title: Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency\n# Venue: \n# Authors: A. Ammar, Anis Koubaa, Omer Nacar, Wadii Boulila\n## Abstract\nLarge language models achieve high task performance yet often hallucinate or rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses these gaps by coupling generation with external search. We analyse how hyperparameters influence speed and quality in RAG systems, covering Chroma and Faiss vector stores, chunking policies, cross-encoder re-ranking, and temperature, and we evaluate six metrics: faithfulness, answer correctness, answer relevancy, context precision, context recall, and answer similarity. Chroma processes queries 13% faster, whereas Faiss yields higher retrieval precision, revealing a clear speed-accuracy trade-off. Naive fixed-length chunking with small windows and minimal overlap outperforms semantic segmentation while remaining the quickest option. Re-ranking provides modest gains in retrieval quality yet increases runtime by roughly a factor of 5, so its usefulness depends on latency constraints. These results help practitioners balance computational cost and accuracy when tuning RAG systems for transparent, up-to-date responses. Finally, we re-evaluate the top configurations with a corrective RAG workflow and show that their advantages persist when the model can iteratively request additional evidence. We obtain a near-perfect context precision (99%), which demonstrates that RAG systems can achieve extremely high retrieval accuracy with the right combination of hyperparameters, with significant implications for applications where retrieval quality directly impacts downstream task performance, such as clinical decision support in healthcare.\n## I. INTRODUCTION\nThe emergence of Large Language Models (LLMs) represents a paradigm shift in artificial intelligence, demonstrating unprecedented capabilities in text generation, summarization, translation and complex reasoning. Despite training on vast corpora, LLMs remain prone to hallucinations-plausible yet incorrect outputs-and their static knowledge cutoff renders them unaware of developments beyond their training data. Moreover, the opaque nature of their reasoning hinders verification and accountability in high-stakes domains such as healthcare, robotics, legal analysis, and scientific research [1]- [5]. \n\nRetrieval-Augmented Generation (RAG) integrates external knowledge retrieval with generative models to mitigate these issues. First introduced by Lewis et al. [6], RAG systems retrieve semantically relevant document fragments prior to generation, grounding outputs in verifiable sources and enabling continuous updates. This dynamic interface improves factual accuracy, relevance and trustworthiness without sacrificing generative flexibility. \n\nThe adoption of RAG has expanded LLM applications in conversational AI, enterprise knowledge management and specialized professional domains. By incorporating retrieval, these systems handle complex, knowledge-intensive queries more precisely. However, retrieval incurs increased latency, complicates system architecture and raises ethical concerns around data privacy, consent and information quality-factors that demand careful design and governance. \n\nDespite growing deployment, the impact of hyperparameter and configuration choices on RAG performance and efficiency remains underexplored. Real-world constraints-computational resources, latency requirements and accuracy thresholds-necessitate an empirical understanding of how implementation decisions affect the trade-off between responsiveness and output quality. \n\nTo fill this gap, we conducted a systematic investigation varying key components-vector stores (Chroma, Faiss), chunking strategies (naive, semantic), cross-encoder re-ranking and temperature settings-while monitoring six performance metrics: faithfulness, answer correctness, answer relevancy, context precision, context recall and answer similarity. This multidimensional analysis reveals the accuracy-latency tradeoffs inherent in each choice and provides actionable insights for practitioners. \n\nThe contributions of this paper are fourfold. First, we introduce a unified evaluation framework that jointly measures RAG quality along six dimensions and tracks computational efficiency.",
            "reference_string": "[278534742 | Ammar et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Auto-GDA: Automatic Domain Adaptation for Efficient Grounding Verification in Retrieval Augmented Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 53,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03461, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324579020",
                    "name": "Tobias Leemann"
                },
                {
                    "authorId": "2189080460",
                    "name": "Periklis Petridis"
                },
                {
                    "authorId": "2072372278",
                    "name": "Giuseppe Vietri"
                },
                {
                    "authorId": "41073684",
                    "name": "Dionysis Manousakas"
                },
                {
                    "authorId": "2272478086",
                    "name": "Aaron Roth"
                },
                {
                    "authorId": "120169766",
                    "name": "Serg\u00fcl Ayd\u00f6re"
                }
            ],
            "abstract": "While retrieval-augmented generation (RAG) has been shown to enhance factuality of large language model (LLM) outputs, LLMs still suffer from hallucination, generating incorrect or irrelevant information. A common detection strategy involves prompting the LLM again to assess whether its response is grounded in the retrieved evidence, but this approach is costly. Alternatively, lightweight natural language inference (NLI) models for efficient grounding verification can be used at inference time. While existing pre-trained NLI models offer potential solutions, their performance remains subpar compared to larger models on realistic RAG inputs. RAG inputs are more complex than most datasets used for training NLI models and have characteristics specific to the underlying knowledge base, requiring adaptation of the NLI models to a specific target domain. Additionally, the lack of labeled instances in the target domain makes supervised domain adaptation, e.g., through fine-tuning, infeasible. To address these challenges, we introduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework enables unsupervised domain adaptation through synthetic data generation. Unlike previous methods that rely on handcrafted filtering and augmentation strategies, Auto-GDA employs an iterative process to continuously improve the quality of generated samples using weak labels from less efficient teacher models and discrete optimization to select the most promising augmented samples. Experimental results demonstrate the effectiveness of our approach, with models fine-tuned on synthetic data using Auto-GDA often surpassing the performance of the teacher model and reaching the performance level of LLMs at 10% of their computational cost.",
            "corpus_id": 273162404,
            "sentences": [],
            "relevance_judgement": 0.87255859375,
            "relevance_judgment_input_expanded": "# Title: Auto-GDA: Automatic Domain Adaptation for Efficient Grounding Verification in Retrieval Augmented Generation\n# Venue: arXiv.org\n# Authors: Tobias Leemann, Periklis Petridis, Giuseppe Vietri, Dionysis Manousakas, Aaron Roth, Serg\u00fcl Ayd\u00f6re\n## Abstract\nWhile retrieval-augmented generation (RAG) has been shown to enhance factuality of large language model (LLM) outputs, LLMs still suffer from hallucination, generating incorrect or irrelevant information. A common detection strategy involves prompting the LLM again to assess whether its response is grounded in the retrieved evidence, but this approach is costly. Alternatively, lightweight natural language inference (NLI) models for efficient grounding verification can be used at inference time. While existing pre-trained NLI models offer potential solutions, their performance remains subpar compared to larger models on realistic RAG inputs. RAG inputs are more complex than most datasets used for training NLI models and have characteristics specific to the underlying knowledge base, requiring adaptation of the NLI models to a specific target domain. Additionally, the lack of labeled instances in the target domain makes supervised domain adaptation, e.g., through fine-tuning, infeasible. To address these challenges, we introduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework enables unsupervised domain adaptation through synthetic data generation. Unlike previous methods that rely on handcrafted filtering and augmentation strategies, Auto-GDA employs an iterative process to continuously improve the quality of generated samples using weak labels from less efficient teacher models and discrete optimization to select the most promising augmented samples. Experimental results demonstrate the effectiveness of our approach, with models fine-tuned on synthetic data using Auto-GDA often surpassing the performance of the teacher model and reaching the performance level of LLMs at 10% of their computational cost.\n",
            "reference_string": "[273162404 | Leemann et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability",
            "venue": "IEEE India Conference",
            "year": 2024,
            "reference_count": 17,
            "citation_count": 14,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2406.11424",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11424, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2356633197",
                    "name": "Gautam Balakrishnan"
                },
                {
                    "authorId": "33856997",
                    "name": "A. Purwar"
                }
            ],
            "abstract": "This paper presents an analysis of open-source large language models (LLMs) and their application in Retrieval-Augmented Generation (RAG) tasks, specific for enterprise-specific data sets scraped from their websites. With the increasing reliance on LLMs in natural language processing, it is crucial to evaluate their performance, accessibility, and integration within specific organizational contexts. This study examines various open-source LLMs, explores their integration into RAG frame-works using enterprise-specific data, and assesses the performance of different open-source embeddings in enhancing the retrieval and generation process. Our findings indicate that open-source LLMs, combined with effective embedding techniques, can significantly improve the accuracy and efficiency of RAG systems, offering a viable alternative to proprietary solutions for enterprises.",
            "corpus_id": 270560505,
            "sentences": [
                {
                    "corpus_id": "270560505",
                    "title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability",
                    "text": "The rapid advancements in natural language processing (NLP) have led to the development of sophisticated large language models (LLMs) that excel in tasks such as text generation, summarization, and question answering.Among these advancements, Retrieval-Augmented Generation (RAG) has emerged as a promising approach for the retrieval-based systems with generative models to produce highly accurate and contextually relevant outputs.The concept of Retrieval-Augmented Generation (RAG) was introduced by Lewis et al.In their seminar 2020 paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\" [20].In their research, Lewis et al. present a method that combines retrieval-based and generative models to enhance the performance of knowledge-intensive tasks.By integrating non-parametric memory (retrieved documents) with parametric memory (the generative model's internal parameters), RAG models achieve superior accuracy and flexibility in tasks such as open-domain question answering and abstract question answering.Karpukhin et al. (2020) developed dense passage retrieval for open-domain question answering, which significantly boosts retrieval accuracy by using dense vector representations and a neural retriever [18].More recent work further advances the field by introducing novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments [24].There has been efficient ways to improve the retrieval process such as the Keyword Augmented Retrieval (KAR), which integrates keyword generation using transformer models with document metadata to identify the right context quickly and cost-effectively [23].Also, approach to handle sparse information where classical RAG using hybrid retriever fails to generate correct answers have been reported [17].More recent work by Tay et al. (2023) on the UL2 model and studies on ColBERT by Khattab and Zaharia (2020) have further pushed the boundaries of retrieval and generation synergies in RAG frameworks [19] [25].",
                    "score": 0.7961675997943802,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 217
                        },
                        {
                            "start": 217,
                            "end": 514
                        },
                        {
                            "start": 514,
                            "end": 615
                        },
                        {
                            "start": 615,
                            "end": 779
                        },
                        {
                            "start": 779,
                            "end": 1040
                        },
                        {
                            "start": 1040,
                            "end": 1414
                        },
                        {
                            "start": 1414,
                            "end": 1672
                        },
                        {
                            "start": 1672,
                            "end": 1817
                        },
                        {
                            "start": 1817,
                            "end": 2026
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87158203125
                }
            ],
            "relevance_judgement": 0.87158203125,
            "relevance_judgment_input_expanded": "# Title: Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability\n# Venue: IEEE India Conference\n# Authors: Gautam Balakrishnan, A. Purwar\n## Abstract\nThis paper presents an analysis of open-source large language models (LLMs) and their application in Retrieval-Augmented Generation (RAG) tasks, specific for enterprise-specific data sets scraped from their websites. With the increasing reliance on LLMs in natural language processing, it is crucial to evaluate their performance, accessibility, and integration within specific organizational contexts. This study examines various open-source LLMs, explores their integration into RAG frame-works using enterprise-specific data, and assesses the performance of different open-source embeddings in enhancing the retrieval and generation process. Our findings indicate that open-source LLMs, combined with effective embedding techniques, can significantly improve the accuracy and efficiency of RAG systems, offering a viable alternative to proprietary solutions for enterprises.\n## INTRODUCTION\nThe rapid advancements in natural language processing (NLP) have led to the development of sophisticated large language models (LLMs) that excel in tasks such as text generation, summarization, and question answering.Among these advancements, Retrieval-Augmented Generation (RAG) has emerged as a promising approach for the retrieval-based systems with generative models to produce highly accurate and contextually relevant outputs.The concept of Retrieval-Augmented Generation (RAG) was introduced by Lewis et al.In their seminar 2020 paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\" [20].In their research, Lewis et al. present a method that combines retrieval-based and generative models to enhance the performance of knowledge-intensive tasks.By integrating non-parametric memory (retrieved documents) with parametric memory (the generative model's internal parameters), RAG models achieve superior accuracy and flexibility in tasks such as open-domain question answering and abstract question answering.Karpukhin et al. (2020) developed dense passage retrieval for open-domain question answering, which significantly boosts retrieval accuracy by using dense vector representations and a neural retriever [18].More recent work further advances the field by introducing novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments [24].There has been efficient ways to improve the retrieval process such as the Keyword Augmented Retrieval (KAR), which integrates keyword generation using transformer models with document metadata to identify the right context quickly and cost-effectively [23].Also, approach to handle sparse information where classical RAG using hybrid retriever fails to generate correct answers have been reported [17].More recent work by Tay et al. (2023) on the UL2 model and studies on ColBERT by Khattab and Zaharia (2020) have further pushed the boundaries of retrieval and generation synergies in RAG frameworks [19] [25].",
            "reference_string": "[270560505 | Balakrishnan et al. | 2024 | Citations: 14]"
        },
        {
            "title": "IRSC: A Zero-shot Evaluation Benchmark for Information Retrieval through Semantic Comprehension in Retrieval-Augmented Generation Scenarios",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 23,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.15763, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313705848",
                    "name": "Hai Lin"
                },
                {
                    "authorId": "2313634969",
                    "name": "Shaoxiong Zhan"
                },
                {
                    "authorId": "2326633382",
                    "name": "Junyou Su"
                },
                {
                    "authorId": "2313769137",
                    "name": "Hai-Tao Zheng"
                },
                {
                    "authorId": "2322882269",
                    "name": "Hui Wang"
                }
            ],
            "abstract": "In Retrieval-Augmented Generation (RAG) tasks using Large Language Models (LLMs), the quality of retrieved information is critical to the final output. This paper introduces the IRSC benchmark for evaluating the performance of embedding models in multilingual RAG tasks. The benchmark encompasses five retrieval tasks: query retrieval, title retrieval, part-of-paragraph retrieval, keyword retrieval, and summary retrieval. Our research addresses the current lack of comprehensive testing and effective comparison methods for embedding models in RAG scenarios. We introduced new metrics: the Similarity of Semantic Comprehension Index (SSCI) and the Retrieval Capability Contest Index (RCCI), and evaluated models such as Snowflake-Arctic, BGE, GTE, and M3E. Our contributions include: 1) the IRSC benchmark, 2) the SSCI and RCCI metrics, and 3) insights into the cross-lingual limitations of embedding models. The IRSC benchmark aims to enhance the understanding and development of accurate retrieval systems in RAG tasks. All code and datasets are available at: https://github.com/Jasaxion/IRSC_Benchmark",
            "corpus_id": 272831924,
            "sentences": [
                {
                    "corpus_id": "272831924",
                    "title": "IRSC: A Zero-shot Evaluation Benchmark for Information Retrieval through Semantic Comprehension in Retrieval-Augmented Generation Scenarios",
                    "text": "The field of Retrieval-Augmented Generation (RAG) has gained significant attention, especially in addressing the limitations of Large Language models (LLMs) in providing accurate and contextually relevant information. This section reviews notable works in this domain and situates our contribution within the existing research. \n\nBenchmarking in RAG Chen et al. developed the Retrieval-Augmented Generation Benchmark (RGB) to evaluate LLMs on four abilities: noise robustness, negative rejection, information integration, and counterfactual robustness. Their findings highlight the need for nuanced evaluation metrics to improve RAG capabilities, as LLMs showed weaknesses in negative rejection, information integration, and handling false information (Chen et al., 2024b) . However, RGB primarily focuses on robustness aspects and does not provide comprehensive coverage of different retrieval tasks, which is crucial for real-world RAG applications.",
                    "score": 0.6521627188465499,
                    "section_title": "Related Work",
                    "char_start_offset": 2956,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 217
                        },
                        {
                            "start": 218,
                            "end": 327
                        },
                        {
                            "start": 330,
                            "end": 552
                        },
                        {
                            "start": 553,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 951
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 752,
                            "end": 772,
                            "matchedPaperCorpusId": "261530434"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86962890625
                }
            ],
            "relevance_judgement": 0.86962890625,
            "relevance_judgment_input_expanded": "# Title: IRSC: A Zero-shot Evaluation Benchmark for Information Retrieval through Semantic Comprehension in Retrieval-Augmented Generation Scenarios\n# Venue: arXiv.org\n# Authors: Hai Lin, Shaoxiong Zhan, Junyou Su, Hai-Tao Zheng, Hui Wang\n## Abstract\nIn Retrieval-Augmented Generation (RAG) tasks using Large Language Models (LLMs), the quality of retrieved information is critical to the final output. This paper introduces the IRSC benchmark for evaluating the performance of embedding models in multilingual RAG tasks. The benchmark encompasses five retrieval tasks: query retrieval, title retrieval, part-of-paragraph retrieval, keyword retrieval, and summary retrieval. Our research addresses the current lack of comprehensive testing and effective comparison methods for embedding models in RAG scenarios. We introduced new metrics: the Similarity of Semantic Comprehension Index (SSCI) and the Retrieval Capability Contest Index (RCCI), and evaluated models such as Snowflake-Arctic, BGE, GTE, and M3E. Our contributions include: 1) the IRSC benchmark, 2) the SSCI and RCCI metrics, and 3) insights into the cross-lingual limitations of embedding models. The IRSC benchmark aims to enhance the understanding and development of accurate retrieval systems in RAG tasks. All code and datasets are available at: https://github.com/Jasaxion/IRSC_Benchmark\n## Related Work\nThe field of Retrieval-Augmented Generation (RAG) has gained significant attention, especially in addressing the limitations of Large Language models (LLMs) in providing accurate and contextually relevant information. This section reviews notable works in this domain and situates our contribution within the existing research. \n\nBenchmarking in RAG Chen et al. developed the Retrieval-Augmented Generation Benchmark (RGB) to evaluate LLMs on four abilities: noise robustness, negative rejection, information integration, and counterfactual robustness. Their findings highlight the need for nuanced evaluation metrics to improve RAG capabilities, as LLMs showed weaknesses in negative rejection, information integration, and handling false information (Chen et al., 2024b) . However, RGB primarily focuses on robustness aspects and does not provide comprehensive coverage of different retrieval tasks, which is crucial for real-world RAG applications.",
            "reference_string": "[272831924 | Lin et al. | 2024 | Citations: 1]"
        },
        {
            "title": "DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 48,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.10198, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283536492",
                    "name": "Hanghui Guo"
                },
                {
                    "authorId": "2355348492",
                    "name": "Jia Zhu"
                },
                {
                    "authorId": "2345031378",
                    "name": "Shimin Di"
                },
                {
                    "authorId": "2225352858",
                    "name": "Weijie Shi"
                },
                {
                    "authorId": "2334326664",
                    "name": "Zhangze Chen"
                },
                {
                    "authorId": "2342456093",
                    "name": "Jiajie Xu"
                }
            ],
            "abstract": "Dynamic Retrieval-augmented Generation (RAG) has shown great success in mitigating hallucinations in large language models (LLMs) during generation. However, existing dynamic RAG methods face significant limitations in two key aspects: 1) Lack of an effective mechanism to control retrieval triggers, and 2) Lack of effective scrutiny of retrieval content. To address these limitations, we propose an innovative dynamic RAG method, DioR (Adaptive Cognitive Detection and Contextual Retrieval Optimization), which consists of two main components: adaptive cognitive detection and contextual retrieval optimization, specifically designed to determine when retrieval is needed and what to retrieve for LLMs is useful. Experimental results demonstrate that DioR achieves superior performance on all tasks, demonstrating the effectiveness of our work.",
            "corpus_id": 277781671,
            "sentences": [
                {
                    "corpus_id": "277781671",
                    "title": "DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation",
                    "text": "In this paper, we investigate the effectiveness of Retrieval-Augmented Generation (RAG) techniques in mitigating hallucination issues in large language models (LLMs). However, existing dy-namic RAG methods face significant limitations in two key aspects, 1) Lack of an effective mechanism to control retrieval triggers, and 2) Lack of effective scrutiny of retrieval content. To address these limitations, we propose an innovative dynamic RAG approach, DioR (Adaptive Cognitive Detection and Contextual Retrieval Optimization), achieving when retrieval is needed and what to retrieve for LLMs is useful. Compared to existing popular dynamic RAG methods, DioR demonstrates superior performance across four knowledge-intensive generation datasets, proving the effectiveness of our method in improving. \n\nLooking ahead, we will refine DioR, especially in its ability to tackle complex problems in a stepby-step manner. By integrating more refined reasoning strategies, we aim to further elevate the model's performance on intricate tasks.",
                    "score": 0.5900244408971249,
                    "section_title": "Conclusion and Future work",
                    "char_start_offset": 24646,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 166
                        },
                        {
                            "start": 167,
                            "end": 375
                        },
                        {
                            "start": 376,
                            "end": 603
                        },
                        {
                            "start": 604,
                            "end": 799
                        },
                        {
                            "start": 802,
                            "end": 915
                        },
                        {
                            "start": 916,
                            "end": 1035
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86669921875
                }
            ],
            "relevance_judgement": 0.86669921875,
            "relevance_judgment_input_expanded": "# Title: DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation\n# Venue: arXiv.org\n# Authors: Hanghui Guo, Jia Zhu, Shimin Di, Weijie Shi, Zhangze Chen, Jiajie Xu\n## Abstract\nDynamic Retrieval-augmented Generation (RAG) has shown great success in mitigating hallucinations in large language models (LLMs) during generation. However, existing dynamic RAG methods face significant limitations in two key aspects: 1) Lack of an effective mechanism to control retrieval triggers, and 2) Lack of effective scrutiny of retrieval content. To address these limitations, we propose an innovative dynamic RAG method, DioR (Adaptive Cognitive Detection and Contextual Retrieval Optimization), which consists of two main components: adaptive cognitive detection and contextual retrieval optimization, specifically designed to determine when retrieval is needed and what to retrieve for LLMs is useful. Experimental results demonstrate that DioR achieves superior performance on all tasks, demonstrating the effectiveness of our work.\n## Conclusion and Future work\nIn this paper, we investigate the effectiveness of Retrieval-Augmented Generation (RAG) techniques in mitigating hallucination issues in large language models (LLMs). However, existing dy-namic RAG methods face significant limitations in two key aspects, 1) Lack of an effective mechanism to control retrieval triggers, and 2) Lack of effective scrutiny of retrieval content. To address these limitations, we propose an innovative dynamic RAG approach, DioR (Adaptive Cognitive Detection and Contextual Retrieval Optimization), achieving when retrieval is needed and what to retrieve for LLMs is useful. Compared to existing popular dynamic RAG methods, DioR demonstrates superior performance across four knowledge-intensive generation datasets, proving the effectiveness of our method in improving. \n\nLooking ahead, we will refine DioR, especially in its ability to tackle complex problems in a stepby-step manner. By integrating more refined reasoning strategies, we aim to further elevate the model's performance on intricate tasks.",
            "reference_string": "[277781671 | Guo et al. | 2025 | Citations: 0]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "273403982",
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "text": "Retrieval-Augmented Generation (RAG) is an emerging hybrid architecture designed to address the limitations of pure generative models. RAG integrates two key components: (i) a retrieval mechanism, which retrieves relevant documents or information from an external knowledge source, and (ii) a generation module, which processes this information to generate human-like text (Lewis et al. 2020). This combination allows RAG models to not only generate fluent text but also ground their outputs in real-world, up-to-date data. \n\nThe retrieval module in RAG typically leverages dense vector representations to identify relevant documents from large datasets, such as Wikipedia or proprietary databases. Once retrieved, these documents are passed to the generative module, often built using transformer-based architectures, to generate responses grounded in the retrieved knowledge. This methodology helps mitigate the hallucination problem and ensures that the generated text is more factual and contextually appropriate (Thakur et al. 2021). Over the period, RAG models have seen applications in various domains, including open-domain question answering (Karpukhin et al., 2020), conversational agents (Liu et al. 2021), and personalized recommendations. \n\nFigure 2: A basic flow of the RAG system along with its component",
            "score": 0.8988958102243912,
            "section_title": "Overview of Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 2068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 523
                },
                {
                    "start": 526,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1319
                }
            ],
            "ref_mentions": [
                {
                    "start": 373,
                    "end": 392,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81591796875
        },
        {
            "corpus_id": "273969615",
            "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
            "text": "Retrieval-Augmented Generation (RAG) addresses the limitations of traditional generative models in handling specialized or long-tail knowledge. Early models like GPT, trained on vast corpora, excel at general queries but struggle with domain-specific or rare information, often generating hallucinations [95]. RAG, introduced by Facebook AI Research in 2020 [96], enhances generative models by integrating realtime document retrieval, improving accuracy and contextual grounding. Gao et al. [97] categorize RAG into Naive, Advanced, and Modular paradigms, detailing key components like retrievers, generators, and augmentation methods. A comparative study by Ovadia et al. [98] shows that RAG outperforms unsupervised fine-tuning, particularly in scenarios involving new or unseen knowledge, underscoring its superiority in knowledge injection and model adaptation. \n\nThe effectiveness of RAG systems heavily depends on the quality and relevance of the retrieved knowledge, which directly influences the accuracy and factual grounding of generated content. To enhance retrieval efficiency and overcome the limitations of traditional methods, several advancements have been proposed, particularly for zero-shot and few-shot retrieval tasks. Techniques such as HyDE [99] and REINA [100] utilize LLMs to generate hypothetical documents, improving retrieval performance without requiring labeled data. The Rewrite-Retrieve-Read [101] framework introduces a query rewriting step, allowing the input query to be better aligned with retrieval modules. By using reinforcement learning to adapt queries, R3 enhances retrieval quality, improving performance in open-domain and multiple-choice question answering tasks. Promptagator [102] demonstrates the effectiveness of few-shot learning in dense retrieval, utilizing LLMs to generate synthetic training data from minimal examples, surpassing models trained on large-scale datasets like MS MARCO. This underscores the viability of few-shot learning and LLM-generated synthetic data in resource-constrained settings. To bridge the preference gap between retrievers and LLMs, Zixuan Ke et al. [103] introduce the BGM framework, which employs a sequence-to-sequence model to align retrieved information with LLM preferences.",
            "score": 0.8958595735737092,
            "section_title": "C. Retrieval-Augmented Generation",
            "char_start_offset": 65622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2263
                }
            ],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 362,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1264,
                    "end": 1268,
                    "matchedPaperCorpusId": "254877046"
                },
                {
                    "start": 1424,
                    "end": 1429,
                    "matchedPaperCorpusId": "258841283"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9248046875
        },
        {
            "corpus_id": "276949546",
            "title": "From Code Generation to Software Testing: AI Copilot With Context-Based Retrieval-Augmented Generation",
            "text": "Retrieval Augmented Generation (RAG) combines retrieval mechanisms with generative models to enhance output accuracy and contextual relevanceLewis et al. [2020]. Originally developed for natural language processing, RAG has shown promise in software engineering for tasks like bug detectionPerera et al. [2020]. By integrating relevant code snippets or bug reports, RAG provides essential context to Large Language Models, improving the precision of generated solutions. Our approach leverages RAG to refine both programming and testing processes, ensuring AI-generated outputs are highly informed and effective.",
            "score": 0.8575080871113956,
            "section_title": "Retrieval Augmented Generation",
            "char_start_offset": 5027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 612
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7685546875
        },
        {
            "corpus_id": "269758033",
            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
            "text": "Retrieval-Augmented Generation (RAG) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external information retrieval. Evaluating these RAG systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of RAG (Auepora) and aim to provide a comprehensive overview of the evaluation and benchmarks of RAG systems. Specifically, we examine and compare several quantifiable metrics of the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the current RAG benchmarks, encompassing the possible output and ground truth pairs. We then analyze the various datasets and metrics, discuss the limitations of current benchmarks, and suggest potential directions to advance the field of RAG benchmarks.",
            "score": 0.8132694539343907,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9140625
        },
        {
            "corpus_id": "266741703",
            "title": "Enhancing Multilingual Information Retrieval in Mixed Human Resources Environments: A RAG Model Implementation for Multicultural Enterprise",
            "text": "Retrieval-Augmented Generation (RAG) models have shown promise in bridging the gap between retrieval-based and generative models, offering a hybrid approach to information retrieval. Lee et al. introduced RAG, demonstrating its effectiveness in improving pre-trained language models by incorporating information retrieval components [5]. This approach aligns with our goal of enhancing information retrieval in a mixed human resources environment.",
            "score": 0.8084053543135811,
            "section_title": "Retrieval-Augmented Generation (RAG) Models",
            "char_start_offset": 5206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 447
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75244140625
        },
        {
            "corpus_id": "270560505",
            "title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability",
            "text": "The rapid advancements in natural language processing (NLP) have led to the development of sophisticated large language models (LLMs) that excel in tasks such as text generation, summarization, and question answering.Among these advancements, Retrieval-Augmented Generation (RAG) has emerged as a promising approach for the retrieval-based systems with generative models to produce highly accurate and contextually relevant outputs.The concept of Retrieval-Augmented Generation (RAG) was introduced by Lewis et al.In their seminar 2020 paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\" [20].In their research, Lewis et al. present a method that combines retrieval-based and generative models to enhance the performance of knowledge-intensive tasks.By integrating non-parametric memory (retrieved documents) with parametric memory (the generative model's internal parameters), RAG models achieve superior accuracy and flexibility in tasks such as open-domain question answering and abstract question answering.Karpukhin et al. (2020) developed dense passage retrieval for open-domain question answering, which significantly boosts retrieval accuracy by using dense vector representations and a neural retriever [18].More recent work further advances the field by introducing novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments [24].There has been efficient ways to improve the retrieval process such as the Keyword Augmented Retrieval (KAR), which integrates keyword generation using transformer models with document metadata to identify the right context quickly and cost-effectively [23].Also, approach to handle sparse information where classical RAG using hybrid retriever fails to generate correct answers have been reported [17].More recent work by Tay et al. (2023) on the UL2 model and studies on ColBERT by Khattab and Zaharia (2020) have further pushed the boundaries of retrieval and generation synergies in RAG frameworks [19] [25].",
            "score": 0.7961675997943802,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 217,
                    "end": 514
                },
                {
                    "start": 514,
                    "end": 615
                },
                {
                    "start": 615,
                    "end": 779
                },
                {
                    "start": 779,
                    "end": 1040
                },
                {
                    "start": 1040,
                    "end": 1414
                },
                {
                    "start": 1414,
                    "end": 1672
                },
                {
                    "start": 1672,
                    "end": 1817
                },
                {
                    "start": 1817,
                    "end": 2026
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87158203125
        },
        {
            "corpus_id": "276107364",
            "title": "Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) combines retrieval and generation to enhance language models in knowledge-intensive tasks. Early RAG systems [57] retrieve context documents relevant to a query and use these as input for generation, enabling models to answer queries beyond their training data. More recent approaches explore the integration of retrieval with both encoder-decoder [41] and decoder-only [87] architectures, optimizing performance for a variety of downstream tasks. \n\nOne key area of focus in RAG is mitigating hallucinations and improving the factuality of generated responses [69]. Techniques such as self-consistency [113] and noisy context filtering [28] have been proposed to enhance the reliability of RAG outputs. However, the quality of retrieved documents remains a critical factor in the overall effectiveness of RAG pipelines. Recent works emphasize the importance of combining retrieval with robust re-ranking techniques to ensure that the retrieved context is both relevant and precise [45].",
            "score": 0.7889316398755417,
            "section_title": "Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 8442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 484
                },
                {
                    "start": 487,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1023
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 407,
                    "end": 411,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7724609375
        },
        {
            "corpus_id": "267069204",
            "title": "Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented Generation in Niche Domains, Exemplified by Korean Medicine",
            "text": "Retrieval-Augmented Generation (RAG) models combine a generative model with an information retrieval function, designed to overcome the inherent constraints of generative models.(1) They integrate the robustness of a large language model (LLM) with the relevance and up-to-dateness of external information sources, resulting in responses that are not only natural and human-like but also the latest, accurate, and contextually relevant to the query. (1)(2)(3)(4) The interaction of the two modules (retrieval and generation) enables responses that would not be achievable with either module alone, making RAG more than just the sum of its components. This approach represents a significant milestone in the field of generative models by enabling the induction of high-quality responses in less-explored domains at a low expense. (5,6) In the conventional RAG operation, the initial step involves converting input queries into vector embeddings, which are then used to retrieve relevant data from the vectorized database. Following this, the generative part of RAG utilizes the retrieved external data for producing contextually rich responses. (7) Thus, both the embedding and generative models are considered crucial factors in the performance of RAG, directly affecting the retrieval process.(8) However, in niche domains, the performance of generic LLM-based embedding models appears suboptimal compared to their effectiveness in more general fields. The lack of specialized training data in these domains results in embeddings that do not adequately capture the nuances and specificity of the domain (9), leading to less accurate and contextually relevant information retrieval. Despite the evident presence of these functional limitations, they have not been much identified through experiments, therefore the optimality of the conventional LLM-based vector embedding RAG methods for niche domains has remained in obscurity. Researchers have been aware of these shortcomings of LLMs and have explored supplementary processes such as fine-tuning to improve the performance. (8,(10)(11)(12) However, the cost of fine-tuning, especially when it involves adjusting the entire or majority of parameters in LLM, has rapidly become expensive, thereby increasing the demand for alternative solutions.",
            "score": 0.7792908868777463,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 1604,
                    "end": 1607,
                    "matchedPaperCorpusId": "227277273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77587890625
        },
        {
            "corpus_id": "270521566",
            "title": "HIRO: Hierarchical Information Retrieval Optimization",
            "text": "Retrieval-Augmented Generation (RAG) [8], enhances Large Language Models (LLMs) by integrating real-world data from external databases, thus establishing Retrieval Augmented Language Models (RALMs). This approach ensures generated content is both contextually relevant and factually accurate, addressing the issue of LLMs producing plausible but incorrect \"hallucinated\" in-formation [16]. By enhancing the accuracy and reliability of AI-generated text, RAG heralds a new era of trustworthy machine-generated communications. It's widespread use in information retrieval and question-answering showcases its utility. However, challenges remain, particularly in scalability and efficiency when dealing with large datasets and complex structures. These issues highlight the need for ongoing research in optimization techniques, an area where Hierarchical Information Retrieval shows potential.",
            "score": 0.7712972716326882,
            "section_title": "Related Work",
            "char_start_offset": 3331,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 890
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 40,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68603515625
        },
        {
            "corpus_id": "269758033",
            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
            "text": "Retrieval-Augmented Generation (RAG) [34] efficiently enhances the performance of generative language models through integrating information retrieval techniques.It addresses a critical challenge faced by standalone generative language models: the tendency to produce responses that, while plausible, may not be grounded in facts.By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [23] or factually incorrect outputs, thereby improving the content's reliability and richness.[73] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a development in the pursuit of more intelligent and versatile language models [73,64].\n\nNumerous studies of RAG systems have emerged from various perspectives since the advent of Large Language Models (LLMs) [55,45,59,42,41,69,16].The RAG system comprises two primary components: Retrieval and Generation.The retrieval component aims to extract relevant information from various external knowledge sources.It involves two main phases, indexing and searching.Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval [16,12,28].The searching component utilizes these indexes to fetch relevant documents on the user's query, often incorporating the optional rerankers [4,39,6,52] to refine the ranking of the retrieved documents.The generation component utilizes the retrieved content and question query to formulate coherent and contextually relevant responses with the prompting and inferencing phases.As the \"Emerging\" ability [59] of LLMs and the breakthrough in aligning human commands [42], LLMs are the best performance choices model for the generation stage.Prompting methods like Chain of Thought (CoT) [60], Tree of Thgouht [65], Rephrase and Respond (RaR) [8] guide better generation results.In the inferencing step, LLMs interpret the prompted input to generate accurate and in-depth responses that align with the query's intent and integrate the extracted information [35,9] without further finetuning, such as fully finetuning [16,1,67,68] or LoRA [21].",
            "score": 0.7694720710257963,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 162,
                    "end": 330
                },
                {
                    "start": 330,
                    "end": 540
                },
                {
                    "start": 540,
                    "end": 847
                },
                {
                    "start": 849,
                    "end": 992
                },
                {
                    "start": 992,
                    "end": 1066
                },
                {
                    "start": 1066,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1219
                },
                {
                    "start": 1219,
                    "end": 1390
                },
                {
                    "start": 1390,
                    "end": 1590
                },
                {
                    "start": 1590,
                    "end": 1765
                },
                {
                    "start": 1765,
                    "end": 1927
                },
                {
                    "start": 1927,
                    "end": 2064
                },
                {
                    "start": 2064,
                    "end": 2328
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 540,
                    "end": 544,
                    "matchedPaperCorpusId": "264426669"
                },
                {
                    "start": 839,
                    "end": 843,
                    "matchedPaperCorpusId": "264426669"
                },
                {
                    "start": 973,
                    "end": 976,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82958984375
        },
        {
            "corpus_id": "276576280",
            "title": "Balancing Content Size in RAG-Text2SQL System",
            "text": "Retrieval-Augmented Generation (RAG) introduces an information retrieval process that enhances the generative model's accuracy and robustness by fetching relevant objects from external data stores. This integration allows RAG systems to dynamically incorporate up-to-date and domain-specific knowledge, significantly improving their performance, particularly in knowledge-intensive tasks (Zhao et al. [2024b]). \n\nA retrieval mechanism is embedded into the model pipeline in an RAG system. It fetches contextually relevant information from an external knowledge base or document corpus based on the user query. The retrieved content is then combined with the original query and passed to a generative model, which uses this enriched context to produce its output. This approach represents a transformative shift in Generative AI, creating more transparent (\"glass-box\") models that excel in accuracy and reliability, especially in domains requiring precise information (Khan et al. [2024]). RAG systems also mitigate the need for frequent retraining of large models, reducing both computational and financial costs. This adaptability makes RAG particularly appealing for enterprise applications, where maintaining up-to-date models is essential. \n\nWe designed our RAG system using the following components: \n\n1. Framework: We utilized LangChain (Chase [2022]), a robust framework that simplifies the development of advanced applications integrating language models. Its modular design allows seamless integration of RAG components. 2. Embeddings: Semantic search in RAG systems relies on vector embeddings. For our implementation, we used all-MiniLM-L12-v2, a sentence-transformer model capable of converting textual data into fixed-size embeddings. This model is ideal for clustering and semantic search tasks and has demonstrated superior performance among open-source embedding models (Aperdannier et al. [2024]). 3. Vector Store: Efficient storage and querying of embeddings are essential for the RAG pipeline. We employed FAISS (Facebook AI Similarity Search) (Douze et al. [2024]), an open-source library optimized for fast and lightweight similarity searches. FAISS retrieves relevant document chunks during query processing with high precision.",
            "score": 0.7660869992900892,
            "section_title": "Description of RAG system",
            "char_start_offset": 18265,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 410
                },
                {
                    "start": 413,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1244
                },
                {
                    "start": 1247,
                    "end": 1305
                },
                {
                    "start": 1308,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 1887,
                    "end": 1913,
                    "matchedPaperCorpusId": "276934091"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68798828125
        },
        {
            "corpus_id": "278367823",
            "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
            "text": "Retrieval-augmented generation (RAG) enhances generation performance by integrating relevant external knowledge into the generation pipeline. Early research primarily adopted prompt-based approaches, guiding LLMs through processes such as query generation, query decomposition, and multi-turn information retrieval [44,28,43,15,33,22]. Despite their effectiveness, these methods often require intricate prompt engineering and impose substantial demands on the model's reasoning capabilities. To improve efficiency and reduce dependency on strong black-box LLMs, subsequent work has proposed supervised fine-tuning strategies for smaller LLMs. For instance, Self-RAG [1] employs a self-reflection mechanism, iteratively refining model outputs through predicted reflection tokens. RetroLLM [24] integrates retrieval and generation by enabling the model to directly generate fine-grained evidence from the corpus via constrained decoding. Recent advances also include test-time scaling techniques [25,14,47,13], notably Monte Carlo Tree Search (MCTS), which dynamically expands the search space during inference. For example, RAG-star [13] integrates retrieved information into a tree-based reasoning process, while AirRAG [5] employs MCTS to activate intrinsic reasoning capabilities and expand the solution space. Despite promising results, these approaches introduce significant computational overhead, limiting their practical applicability.",
            "score": 0.7650093352803269,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 6269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1442
                }
            ],
            "ref_mentions": [
                {
                    "start": 325,
                    "end": 328,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 666,
                    "end": 669,
                    "matchedPaperCorpusId": "264288947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79443359375
        },
        {
            "corpus_id": "273798011",
            "title": "Unified Generative and Discriminative Training for Multi-modal Large Language Models",
            "text": "Early models for retrieval primarily focused on word representations [16,64,74], with minimal generative capabilities. Some recent works have endeavored to fine-tune generative pre-trained LLMs to generate discriminative embeddings, albeit at the expense of compromising the model's original generative capabilities [44,70,65,63,24,71]. GRIT [66] integrates generative and discriminative tasks in NLP and demonstrates mutual benefits between them. However, its training cost is prohibitively high compared to individual tasks. Moreover, due to its specialized attention mechanism, the model can only be trained from scratch. \n\nRetrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) [25,69], which harnesses the advanced inference capabilities of LLMs along with external knowledge, has the potential to significantly mitigate issues related to long-tail entities and reduce the occurrence of hallucina-tory responses [29,36,101,77,90,92,97]. Recently, there have also been related studies in the multimodal domain attempting to utilize retrieval augmentation [93,96]. These methods typically require an additional retrieval module (e.g., CLIP), leading to component optimization challenges where the overall model performance is affected by the performance of the retrieval model, as well as concerns regarding the compatibility between the retrieval model and the MLLMs. Furthermore, retrieval modules like CLIP struggle to handle compositional or fine-grained scenarios, posing certain challenges for retrieval.",
            "score": 0.7639910356120067,
            "section_title": "Related Work",
            "char_start_offset": 8945,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 624
                },
                {
                    "start": 627,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1527
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 76,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 931,
                    "end": 935,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 935,
                    "end": 938,
                    "matchedPaperCorpusId": "261033863"
                },
                {
                    "start": 951,
                    "end": 954,
                    "matchedPaperCorpusId": "265445443"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65478515625
        },
        {
            "corpus_id": "270703323",
            "title": "Context-augmented Retrieval: A Novel Framework for Fast Information Retrieval based Response Generation using Large Language Model",
            "text": "With the launch of OpenAI's ChatGPT, Large Language Models (LLMs) such as the GPT and LLAMA series have garnered significant attention.These LLMs can perform various impressive tasks, including language translation, content summarization, and other functions that typically require human intelligence.In certain aspects, these models can sometimes outperform humans.However, LLMs face challenges such as factual hallucination, knowledge obsolescence, and a lack of domain-specific expertise [29][15] [35].Two primary solutions to address these problems are fine-tuning LLMs on domain-specific data and using Retrieval-Augmented Generation (RAG).Based on previous research, RAG is considered the better option [13].\n\nRetrieval-augmented generation (RAG) is an advanced technique in NLP that combines retrieval-based and generative models to improve the quality and accuracy of responses, especially in applications requiring precise information retrieval within a specific knowledge domain (e.g., a book or other provided sources for factual correctness) [14].\n\nTraditional generative models like the GPT series and the LLAMA series generate responses based solely on the input they receive.This can sometimes lead to inaccuracies or hallucinations where the model produces irrelevant information [8][34] [30].In contrast, retrieval-based models focus on fetching relevant information from a pre-existing database, ensuring factual correctness.While generative models can provide faster answers, incorrect information can be more damaging than no information at all.RAG bridges these two approaches.It employs a retrieval mechanism to fetch relevant documents or passages from a large database based on the input query.These retrieved documents provide factual grounding for response.The generative model takes these documents and generates a contextually appropriate response.A typical RAG workflow is depicted in Figure 1.",
            "score": 0.7611154052436964,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 135,
                    "end": 301
                },
                {
                    "start": 301,
                    "end": 366
                },
                {
                    "start": 366,
                    "end": 505
                },
                {
                    "start": 505,
                    "end": 645
                },
                {
                    "start": 645,
                    "end": 714
                },
                {
                    "start": 716,
                    "end": 1059
                },
                {
                    "start": 1061,
                    "end": 1190
                },
                {
                    "start": 1190,
                    "end": 1309
                },
                {
                    "start": 1309,
                    "end": 1443
                },
                {
                    "start": 1443,
                    "end": 1565
                },
                {
                    "start": 1565,
                    "end": 1598
                },
                {
                    "start": 1598,
                    "end": 1718
                },
                {
                    "start": 1718,
                    "end": 1783
                },
                {
                    "start": 1783,
                    "end": 1876
                },
                {
                    "start": 1876,
                    "end": 1923
                }
            ],
            "ref_mentions": [
                {
                    "start": 1054,
                    "end": 1058,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81884765625
        },
        {
            "corpus_id": "272955388",
            "title": "Application of RAG Model Based on Retrieval Enhanced Generation Technique in Complex Query Processing",
            "text": "Compared with traditional generative models, RAG models not only rely on pre-trained language models, but also dynamically acquire the most relevant knowledge to the query, which gives them an obvious advantage when facing diverse and high-complexity queries. Retrieval-Augmented Generation (RAG) models are employed in various applications, including question-answering systems, dialogue systems, information retrieval, and knowledge graph construction. They are particularly effective for tasks that require the integration of extensive background information to produce accurate responses.",
            "score": 0.7575186521206927,
            "section_title": "Overview of the RAG model and its retrieval enhancement generation technique",
            "char_start_offset": 4522,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 592
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68701171875
        },
        {
            "corpus_id": "273403982",
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "text": "Retrieval-Augmented Generation (RAG) has undergone significant evolution, with extensive research dedicated to improving retrieval effectiveness and enhancing coherent generation to minimize hallucinations. From its early iterations to recent advancements, RAG has been instrumental in integrating external knowledge into Large Language Models (LLMs), thereby boosting accuracy and reliability. In particular, recent domain-specific work has showcased RAG's potential in specialized areas such as legal, medical, and low-resource language applications, highlighting its adaptability and scope. However, despite these advances, this paper identifies clear gaps that remain unresolved. Challenges such as the integration of ambiguous or unstructured information, effective handling of domain-specific contexts, and the high computational overhead of complex retrieval tasks still persist. These limitations constrain the broader applicability of RAG systems, particularly in diverse and dynamic real-world environments. The future research directions outlined in this paper-ranging from improving retrieval mechanisms to enhancing context management and ensuring scalability-will serve as a critical guide for the next phase of innovation in this space. By addressing these gaps, the next generation of RAG models has the potential to drive more reliable, efficient, and domain-adaptable LLM systems, further pushing the boundaries of what is possible in retrieval-augmented AI applications.",
            "score": 0.747507173707867,
            "section_title": "Conclusion",
            "char_start_offset": 38729,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1489
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77294921875
        },
        {
            "corpus_id": "269502216",
            "title": "On the Evaluation of Machine-Generated Reports",
            "text": "Retrieval-Augmented Generation.Early retrieval augmented generation systems have been evaluated using task-specific metrics on end-to-end tasks.For example, in the context of question answering, exact match and  1 metrics have been used [30,41].For summarization, ROUGE and BERTScore on reference summaries are common [26].These approaches have two limitations: they only measure ability to complete end tasks, and thus cannot assess intermediate stages or evaluate generation across multiple dimensions; and they are not well-suited to capture failures that can be introduced by current generative models [27].\n\nMore recently, techniques have proposed to more holistically evaluate RAG systems.Gienapp et al. [25] introduce a theoretical framework for evaluating ad hoc generative retrieval.Chen et al. [11] focus on robustness of RAG systems against various perturbations.Thakur et al. [82] benchmark hallucinations and the ability of RAG systems to identify relevant information for 18 languages.Others have introduced benchmarks to measure the ability of RAG systems to provide citations [6,23,53,90].While not specifically  designed for RAG applications, metrics designed to evaluate factuality (e.g., FactScore [58]) or faithful manipulation of long inputs (e.g., BooookScore [10]) can complement application-specific evaluation frameworks.\n\nMost approaches to automated evaluation aim to estimate the effectiveness of RAG systems across desirable dimensions (e.g., faithfulness, answer relevance, and context relevance).Techniques include prompting LLMs to evaluate generated summaries [76], and fine-tuning lightweight models on synthetic data [73].Downstream applications, such as question answering, can also be used to evaluate the effectiveness of RAG systems [74].",
            "score": 0.7470241739468626,
            "section_title": "3.2.4",
            "char_start_offset": 19611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 31,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 245
                },
                {
                    "start": 245,
                    "end": 323
                },
                {
                    "start": 323,
                    "end": 611
                },
                {
                    "start": 613,
                    "end": 695
                },
                {
                    "start": 695,
                    "end": 792
                },
                {
                    "start": 792,
                    "end": 874
                },
                {
                    "start": 874,
                    "end": 999
                },
                {
                    "start": 999,
                    "end": 1105
                },
                {
                    "start": 1105,
                    "end": 1346
                },
                {
                    "start": 1348,
                    "end": 1527
                },
                {
                    "start": 1527,
                    "end": 1657
                },
                {
                    "start": 1657,
                    "end": 1777
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "258865156"
                },
                {
                    "start": 1101,
                    "end": 1104,
                    "matchedPaperCorpusId": "258587884"
                },
                {
                    "start": 1772,
                    "end": 1776,
                    "matchedPaperCorpusId": "238207962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.892578125
        },
        {
            "corpus_id": "277104989",
            "title": "A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM",
            "text": "Retrieval-augmented generation (RAG) can be used for knowledge-intensive NLP tasks [14]. RAG is a technique for enhancing the accuracy and reliability of generative AI models with facts fetched from predetermined external sources. \n\nLLMs have some limitations, such as; presenting false information when they do not have the answer, presenting out-of-date or generic information, and creating a response from non-authoritative sources in train data. RAG addresses these challenges by redirecting the LLM to retrieve relevant information from authoritative, predetermined knowledge sources. This way, LLM can use the retrieved data as context for generating a response that is more relevant, accurate, and useful in various contexts. RAG applications potentially provide user transparency by revealing the sources of the retrieved data, offering insight into how the LLM generates its responses.",
            "score": 0.7407291391099268,
            "section_title": "Vector Databases and RAG as a Solution",
            "char_start_offset": 5123,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 894
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 87,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.426025390625
        },
        {
            "corpus_id": "267061186",
            "title": "Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency",
            "text": "Moreover, they discuss the application of retrieval-augmented generation (RAG) techniques, which combine information retrieval with generative modeling to produce contextually relevant recommendations. Overall, these insights underscore the significant advancements and ongoing challenges in utilizing generative models for RS.",
            "score": 0.7398956064382483,
            "section_title": "Leveraging Pre-trained LMs and Prompting for Recommender Systems",
            "char_start_offset": 23909,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 327
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.416259765625
        },
        {
            "corpus_id": "276576207",
            "title": "A Taxonomy of Generative AI in HEOR: Concepts, Emerging Applications, and Advanced Tools - An ISPOR Working Group Report.",
            "text": "This section examines several more advanced techniques to enhance generative AI performance in HEOR. Strategies like prompt engineering and retrieval-augmented generation (RAG) improve accuracy, factuality, and comprehensiveness. Model fine-tuning and domain-specific FMs ensure contextually relevant outputs for specialized tasks. These methods, applicable across use cases, help address key challenges in accuracy and reliability. Table 2 summarizes key approaches to enhance the quality of generative AI outputs in HEOR.",
            "score": 0.7383376128562354,
            "section_title": "Techniques to improve the use of generative AI in HEOR",
            "char_start_offset": 14533,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 523
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70068359375
        },
        {
            "corpus_id": "273403982",
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "text": "This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs. The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant technological advancements in RAG is provided, including key innovations in retrieval-augmented language models and applications across various domains such as question-answering, summarization, and knowledge-based tasks. Recent research breakthroughs are discussed, highlighting novel methods for improving retrieval efficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethical concerns in deployment. Future research directions are proposed, focusing on improving the robustness of RAG models, expanding the scope of application of RAG models, and addressing societal implications. This survey aims to serve as a foundational resource for researchers and practitioners in understanding the potential of RAG and its trajectory in natural language processing.",
            "score": 0.7347023622666097,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7470703125
        },
        {
            "corpus_id": "272367349",
            "title": "Evaluating ChatGPT on Nuclear Domain-Specific Data",
            "text": "Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by grounding their responses in external knowledge sources. This approach is",
            "score": 0.7329855598984621,
            "section_title": "Retrieval-Augmented Generation (RAG): A Background",
            "char_start_offset": 4164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 191
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.384033203125
        },
        {
            "corpus_id": "276667096",
            "title": "Exploring RAG Solutions for a Specific Language: Albanian",
            "text": "Retrieval-augmented generation (RAG) is an advanced method for enhancing the output of generative AI models by incorporating factual information from external resources [4]. This approach addresses a key limitation of large language models (LLMs): their inability to access real-time or domain-specific knowledge without additional training. \n\nLLMs, much like neural networks in the human brain, function based on their parameters, which represent patterns in language usage derived from extensive training datasets. These parameters allow LLMs to generate coherent responses quickly and efficiently, making them well-suited for general-purpose queries. However, they often fall short when tasked with providing detailed or up-to-date information on specialized topics. For example, while an LLM can summarize general knowledge, it struggles to answer context-specific questions about a newly provided document. \n\nRAG solves this problem by enabling LLMs to reference external sources dynamically during query processing. Much like footnotes in a research paper, these sources can be cited, ensuring transparency and trust. By offering verifiable facts and reducing errors-often called \"hallucinations\" in AI-the RAG approach improves the reliability and accuracy of responses. \n\nOne notable advantage of RAG is its efficiency. Instead of retraining a model with additional datasets, RAG allows developers to connect external resources to LLMs seamlessly. This lightweight implementation is not only faster but also more cost-effective, often requiring minimal code to integrate new data sources. \n\nRAG also expands the potential of AI applications across industries. For instance, integrating a medical database with an LLM could create a valuable assistant for healthcare professionals, while linking financial data could support analysts in making informed decisions. Similarly, businesses can enhance customer support, training, and productivity by converting internal documents, logs, or multimedia content into accessible knowledge bases. The concept of retrieval-augmented techniques can be traced [5] (Fig. 1).",
            "score": 0.7326144806603795,
            "section_title": "RAG solutions",
            "char_start_offset": 3203,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 341
                },
                {
                    "start": 344,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1596
                },
                {
                    "start": 1599,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2118
                }
            ],
            "ref_mentions": [
                {
                    "start": 2105,
                    "end": 2108,
                    "matchedPaperCorpusId": "267733899"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58544921875
        },
        {
            "corpus_id": "278026850",
            "title": "Beyond the model: Key differentiators in large language models and multi-agent services",
            "text": "The rapid evolution of generative AI has led to a saturation point where numerous industry and open-source Large Language Models (LLMs) exhibit similar quality levels [1,2]. If LLMs are no longer the competitive edge, what drives the advantage in AI services? The actual value in generative AI lies not in the models themselves but in the ancillary components that enhance and support these models. \n\nModel Reliability can be enhanced by integrating human-generated data through human-in-the-loop approaches to reach desired outcomes. Techniques like Retrieval-Augmented Generation (RAG) help reduce AI hallucinations and lower computational costs by dynamically retrieving information, enabling AI to provide more accurate responses without frequent retraining [5].",
            "score": 0.730895937489046,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 398
                },
                {
                    "start": 401,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 766
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 170,
                    "matchedPaperCorpusId": "238222421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46923828125
        },
        {
            "corpus_id": "270870251",
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "text": "Ensuring the accuracy of responses generated by Large Language Models (LLMs) such as Chat-GPT [13] and LLaMA [14] is essential.However, simply enlarging model size does not fundamentally address the issue of hallucinations [15,16], especially in knowledge-intensive tasks and specialized domains.Retrieval-augmented generation (RAG) addresses these challenges by retrieving relevant documents from external knowledge bases, providing accurate, real-time, domain-specific context to LLMs [6].Previous works have optimized the RAG pipeline through query and retrieval transformations, enhancing retriever performance, and fine-tuning both the retriever and generator.These optimizations improve the interaction between input queries, retrieval mechanisms, and generation processes, ensuring the accuracy and relevance of responses.",
            "score": 0.7281159121489851,
            "section_title": "Related Work",
            "char_start_offset": 4844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 127,
                    "end": 296
                },
                {
                    "start": 296,
                    "end": 491
                },
                {
                    "start": 491,
                    "end": 665
                },
                {
                    "start": 665,
                    "end": 829
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "266164171"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72412109375
        },
        {
            "corpus_id": "272753366",
            "title": "Retrieval-Augmented Test Generation: How Far Are We?",
            "text": "Retrieval Augmented Generation (RAG) is a prompting strategy that enhances the responses of Large Language Models (LLMs) by incorporating additional knowledge sources beyond the pretrained knowledge base [2]. In RAG, the query intended for the LLM is used to search and retrieve relevant documents from a database [2,5]. This process leverages external knowledge, such as documents from various sources, to produce more contextually relevant and accurate responses compared to traditional generation-only models [5]. \n\nOne advantage of RAG is that it eliminates the need for retraining, as the LLM can search and access the latest information to generate more reliable output through retrieval-based generation [42]. RAG techniques have been successful in numerous software-related tasks, including code generation [32], code search [11], commit message generation [43], code suggestions [4], assertion generation [25], and automated program repair [25,36]. However, the effectiveness of RAG relies heavily on the quality of the retrieved knowledge [19]. Therefore, it is essential to assess the impact of various knowledge resources on the performance of RAG-based techniques.",
            "score": 0.7276742748833875,
            "section_title": "Retrieval Augmented Generation (RAG)",
            "char_start_offset": 5054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1177
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "270738768"
                },
                {
                    "start": 314,
                    "end": 317,
                    "matchedPaperCorpusId": "270738768"
                },
                {
                    "start": 317,
                    "end": 319,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 512,
                    "end": 515,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 833,
                    "end": 837,
                    "matchedPaperCorpusId": "265509385"
                },
                {
                    "start": 888,
                    "end": 891,
                    "matchedPaperCorpusId": "269130676"
                },
                {
                    "start": 914,
                    "end": 918,
                    "matchedPaperCorpusId": "259860357"
                },
                {
                    "start": 949,
                    "end": 953,
                    "matchedPaperCorpusId": "259860357"
                },
                {
                    "start": 953,
                    "end": 956,
                    "matchedPaperCorpusId": "261697451"
                },
                {
                    "start": 1049,
                    "end": 1053,
                    "matchedPaperCorpusId": "265466391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79833984375
        },
        {
            "corpus_id": "274166497",
            "title": "Enhancing Translation Quality: A Comparative Study of Fine-Tuning and Prompt Engineering in Dialog-Oriented Machine Translation Systems. Insights from the MULTITAN-GML Team",
            "text": "6 Further Research 6.1 Retrieval-Augmented Generation (RAG) \n\nThe database serves as a vital resource for addressing the challenges posed by rare or complex structures that may not be well-represented in translation models (Gao et al., 2024). Retrievalaugmented generation (RAG) is a technique for enhancing the accuracy and reliability of generative AI models with facts fetched from external sources. Future improvements could involve aug-",
            "score": 0.7265356885860352,
            "section_title": "Automatic Post-editing vs. Prompt Engineering",
            "char_start_offset": 9845,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 62,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 441
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.625
        },
        {
            "corpus_id": "269740933",
            "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
            "text": "As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at: https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/",
            "score": 0.7238608964232676,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "266999736",
            "title": "Reinforcement Learning for Optimizing RAG for Domain Chatbots",
            "text": "With recent advancements in Generative AI and LLMs, the Retrieval-Augmented Generation (RAG) (Lewis et al. 2021) approach has emerged as the preferred strategy for contex-tual question answering. RAG pipeline consists of a retrieval model followed by an LLM to generate the answer. Different approaches have been experimented with to improve components of the RAG in terms of accuracy and minimize hallucinations in answer generation. Khatry et al. (kha 2023) proposed a low-rank residual adaptation approach with the pretrained embedding model to improve the retrieval model. It was shown to lead to improved task-specific retrieval as compared to a general-purpose embeddings-based baseline. Instead of using an interleaved retrieval and generation, Shao et al. (Shao et al. 2023) proposed an iterative retrieval and generation approach where the current model output acts as an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Li et al. (Li et al. 2022) extensively surveys recent RAG-based approaches. \n\nRL has been experimented with to improve RAG. Bacciu et al. (Bacciu et al. 2023) propose an RL-based approach to train an efficient retriever model to search for relevant information in an arbitrarily large database. Once this set of relevant data has been retrieved, it is forwarded to the API-based LLM to generate the answer. In particular, the authors show that RL helps reduce hallucinations by minimizing the number of damaging documents returned by the retriever. Self-RAG (Asai et al. 2023) trains a single LLM that adaptively retrieves passages and generates and reflects on retrieved passages and their generations using reflection tokens. In our work, we assume we do not have access to the gradients of the retrieval model and LLM. We only train a policy model that resides external to RAG. \n\nGPT-4 is observed to provide human-level accuracy for automated evaluation tasks. Hack at al. (Hackl et al. 2023) investigated the consistency of feedback ratings generated by GPT-4 across multiple iterations, time spans, and stylistic variations.",
            "score": 0.7218923190832305,
            "section_title": "Related works",
            "char_start_offset": 5694,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1888
                },
                {
                    "start": 1891,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2138
                }
            ],
            "ref_mentions": [
                {
                    "start": 764,
                    "end": 782,
                    "matchedPaperCorpusId": "258866037"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86083984375
        },
        {
            "corpus_id": "277940421",
            "title": "Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm",
            "text": "Retrieval augmented generation (RAG) combines the generative power of LLMs with external knowledge retrieval systems to produce more accurate and contextually informed outputs [14]. This augmented approach addresses key limitations of stand-alone generative models, such as hallucination and factual inaccuracy, by incorporating relevant information retrieved from structured databases, corpora, or the web during the generation process. The process typically involves two main steps. The first is to identify and retrieve relevant documents or information from alternative sources. The second passes these texts into the generative model as additional context. The model integrates this information with its internal knowledge to produce responses that more strongly influenced by the new knowledge. \n\nBy integrating external information sources with its inherent generative abilities, RAG can substantially reduce the risk of generating misleading information. The retrieved context acts as a dynamic, evidence-based grounding layer, enhancing the reliability of the outputs produced by the generative model. Incorporating verifiable information from external sources helps mitigate the phenomenon of hallucination that is common in large language models. The retrieved content can also provide richer context for generating nuanced responses. \n\nRetrieval augmented generation has found applications across a range of domains: conversational AI, in enhancing chatbot responses by grounding them in current events or factual databases; question answering -improving the precision of responses in systems that need to answer fact-based queries; content creation, by supporting writers and researchers by providing dynamically retrieved background information that complements the creative process. It is the ability to focus the LLMs onto specific areas that presents an interesting opportunity for educationalists and technology designers alike.",
            "score": 0.7205000382346822,
            "section_title": "Focussing LLMs",
            "char_start_offset": 9318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1345
                },
                {
                    "start": 1348,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1946
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 180,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68994140625
        },
        {
            "corpus_id": "273963542",
            "title": "Leveraging Retrieval-Augmented Generation for Persian University Knowledge Retrieval",
            "text": "Retrieval-Augmented Generation (RAG) is a novel paradigm that enhances the performance of large language models by incorporating information retrieval processes into the generation mechanism. This approach aims to",
            "score": 0.7200384563989273,
            "section_title": "A. Introduction to Retrieval-Augmented Generation",
            "char_start_offset": 3196,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 213
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.451904296875
        },
        {
            "corpus_id": "270357334",
            "title": "RAG Does Not Work for Enterprises",
            "text": "Retrieval-Augmented Generation (RAG) is an emerging paradigm that combines the strengths of pre-trained language models with external knowledge retrieval to enhance the accuracy, consistency, and contextual relevance of generated outputs [ Lewis et al., 2020 ].In a typical RAG architecture, a retriever component first selects the most relevant documents or passages based on the input query, and then a generator component conditions on both the query and the retrieved content to produce a final output [ Izacard and Grave, 2021 ].RAG has shown significant promise in improving the factual accuracy, consistency, and contextual awareness of generative models across a wide range of applications, such as question answering, dialogue systems, and content creation [ Zhao et al., 2024 ].However, implementing RAG effectively in real-world, enterprise settings poses several challenges, which this paper aims to address.",
            "score": 0.7189423312295589,
            "section_title": "Background",
            "char_start_offset": 5003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 261,
                    "end": 534
                },
                {
                    "start": 534,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 920
                }
            ],
            "ref_mentions": [
                {
                    "start": 506,
                    "end": 533,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 766,
                    "end": 787,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60205078125
        },
        {
            "corpus_id": "270285974",
            "title": "A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions",
            "text": "Retrieval-Augmented Generation (RAG) [212] is a machine learning technique that combines the strengths of retrievalbased and generative models to enhance the quality and diversity of generated text.This approach has gained significant attention in NLP tasks, particularly in areas like conversational AI, question-answering systems, and text summarization.\n\n1) General Algorithm: In general, a typical RAG system should consist of the following components:\n\n\u2022 Retrieval Component: The system starts by retrieving relevant information from a large database or corpus.This could involve indexing and efficiently searching through past conversations, documents, or web pages based on the input query.Techniques like TF-IDF [213], BM25 [214], or more advanced retrieval methods can be used for this component.\u2022 Generation Component: Once the relevant information is retrieved, a generative model (e.g., GPT-3, T5, or BERT) can use this information as context to generate a response or output [215].The generation process is augmented by conditioning the model on the retrieved data, allowing it to generate more informed, contextually accurate, and diverse responses rather than generating from scratch.\u2022 Component Integration: The integration ways of these two components can vary [212].Firstly, the retrieved information might be concatenated with the input prompt and directly fed into the generator.Others might use a more sophisticated fusion mechanism, where the retrieval and generation models interact in multiple steps, refining the context and the generated output iteratively.By leveraging external knowledge, the generated text of RAG is more likely to be contextually appropriate and accurate.Retrieval of varied sources can introduce more diversity in the generated outputs, reducing the likelihood of repetitive or generic responses [216].Retrieval models can quickly narrow down the scope of information needed, which can make the generation process more efficient compared to exploring the entire knowledge space.Incorporating specific retrieved information can provide more control over the content and tone of the generated text, aligning it better with user expectations or specific requirements.\n\n2) Specific RAG Algorithm: Clinfo.ai[51].Clinfo.ai is an open-source WebApp that answers clinical questions based on dynamically retrieved scientific literature.The information retrieval and summarization tasks are applied to evaluate the retrieval-augmented LLM systems.\n\nAlmanac [52].",
            "score": 0.7177690671963097,
            "section_title": "D. Retrieval-Augmented Generation for Med-LLMs",
            "char_start_offset": 58486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 198,
                    "end": 356
                },
                {
                    "start": 358,
                    "end": 456
                },
                {
                    "start": 458,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 697
                },
                {
                    "start": 697,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 994
                },
                {
                    "start": 994,
                    "end": 1199
                },
                {
                    "start": 1199,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1399
                },
                {
                    "start": 1399,
                    "end": 1583
                },
                {
                    "start": 1583,
                    "end": 1702
                },
                {
                    "start": 1702,
                    "end": 1850
                },
                {
                    "start": 1850,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2212
                },
                {
                    "start": 2214,
                    "end": 2250
                },
                {
                    "start": 2250,
                    "end": 2255
                },
                {
                    "start": 2255,
                    "end": 2375
                },
                {
                    "start": 2375,
                    "end": 2485
                },
                {
                    "start": 2487,
                    "end": 2500
                }
            ],
            "ref_mentions": [
                {
                    "start": 720,
                    "end": 725,
                    "matchedPaperCorpusId": "14638345"
                },
                {
                    "start": 732,
                    "end": 737,
                    "matchedPaperCorpusId": "207220720"
                },
                {
                    "start": 1844,
                    "end": 1849,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 2250,
                    "end": 2254,
                    "matchedPaperCorpusId": "264487188"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.640625
        },
        {
            "corpus_id": "265594594",
            "title": "Semantic Embeddings for Arabic Retrieval Augmented Generation (ARAG)",
            "text": "Retrieval Augmented Generation (RAG), introduced by Facebook Researchers in 2020 [1], is a pivotal AI framework facilitating information retrieval for Generative AI models, thereby enhancing their accuracy and capabilities. RAG empowers Large Language Models (LLMs) by granting them access to external knowledge sources, augmenting the content generation process. This dual functionality entails retrieval, wherein RAG meticulously selects pertinent information from provided sources and generation, whereby LLMs craft contextually relevant responses based on user input. \n\nThe advantages of RAG are multi-fold. Firstly, it bolsters the performance by grounding LLMs with factual, up-todate information from external knowledge repositories. Furthermore, RAG maintains contextual relevance in responses, contributing to a more engaging user experience in conversational AI applications. Its scalability is noteworthy, as RAG models seamlessly handle copious volumes of information, proving invaluable for data-intensive tasks. Additionally, the adaptability of RAG models allows fine-tuning for specific applications [2], rendering them versatile across diverse data and use cases. Customizability is another hallmark, permitting RAG models to specialize in particular domains or subjects through customization and fine-tuning on specific knowledge bases. Due to the importance of such a framework for enterprises, extensive research is currently being pursued to discover new algorithms and techniques to enhance the performance of such models bounded by the context-window limitations of LLMs. Although there is ongoing research to expand the window size for LLM to be able to ingest more data in the prompt, the use of techniques like RAG is still of great practical importance, not only on homogeneous unstructured data but also on heterogeneous data [3]. \n\nIn principle, at the heart of the information retrieval module is the semantic embedding module which converts a piece of text, whether a query or a context text chunk to a numeric feature vector that embodies all semantic features of the text. The development of word and sentence embeddings is a relatively recent area of research in natural language processing (NLP) and information retrieval. \n\nMost of the semantic models are English language-centred; however, in recent years, Multilingual embedding models were released [4].",
            "score": 0.7117775683588696,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2257
                },
                {
                    "start": 2260,
                    "end": 2392
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.591796875
        },
        {
            "corpus_id": "269283058",
            "title": "RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) represents a significant advancement in the field of natural language processing (NLP) and machine learning, combining LLMs with the vast information accessible in external knowledge databases.Specifically, RAG is employed to enhance the generative models' ability to produce more accurate, relevant, and contextually rich responses by dynamically retrieving information from a corpus during the generation process.This hybrid approach combines the strengths of two major strands: the deep contextual understanding of LLMs and the precision of knowledge database retrieval.Recent work [1,8,22,27,37,42] has demonstrated that RAG can significantly improve the generation quality across various benchmarks compared to solely generative models.The RAG framework has since been applied across various tasks, including question answering [39], content creation [24], and even code generation [33,43], showcasing its versatility and promise.As shown in Figure 1, RAG operates on a two-step workflow: retrieval and generation, integrating offline preparation with real-time processing for enhanced performance.Initially, in its offline phase, RAG transforms the external knowledge sources, such as documents, into high-dimensional vectors using advanced embedding models.RAG then indexes these vectors into a specialized vector database designed for efficient retrieval.Upon receiving a user request, RAG first accesses this vector database to conduct a vector similarity search, retrieving the documents that best match the request based on their semantic content.Following this, RAG combines the content of these retrieved documents with the original user request, creating an augmented request.This augmented request is then provided to an LLM, which leverages the combined information to generate a response that is more informed and contextually rich.\n\nIn an RAG workflow, the retrieval step is mainly performed on CPUs, while the generation step is executed on GPUs.From a system perspective, the end-to-end performance of RAG is affected by both the retrieval or generation steps.The retrieval time is mainly determined by the vector database's scale, and the generation time is decided by the model size and the sequence length.Our subsequent characterization will identify RAG's performance bottleneck and highlight potential areas for optimization.",
            "score": 0.7100820121802761,
            "section_title": "Background",
            "char_start_offset": 6804,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 610
                },
                {
                    "start": 610,
                    "end": 778
                },
                {
                    "start": 778,
                    "end": 972
                },
                {
                    "start": 972,
                    "end": 1140
                },
                {
                    "start": 1140,
                    "end": 1301
                },
                {
                    "start": 1301,
                    "end": 1400
                },
                {
                    "start": 1400,
                    "end": 1595
                },
                {
                    "start": 1595,
                    "end": 1727
                },
                {
                    "start": 1727,
                    "end": 1886
                },
                {
                    "start": 1888,
                    "end": 2002
                },
                {
                    "start": 2002,
                    "end": 2117
                },
                {
                    "start": 2117,
                    "end": 2266
                },
                {
                    "start": 2266,
                    "end": 2388
                }
            ],
            "ref_mentions": [
                {
                    "start": 625,
                    "end": 627,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 630,
                    "end": 633,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 928,
                    "end": 931,
                    "matchedPaperCorpusId": "247255943"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86083984375
        },
        {
            "corpus_id": "271244968",
            "title": "NinjaLLM: Fast, Scalable and Cost-effective RAG using Amazon SageMaker and AWS Trainium and Inferentia2",
            "text": "Retrieval-augmented generation (RAG) techniques are widely used today to retrieve and present information in a conversational format. This paper presents a set of enhancements to traditional RAG techniques, focusing on large language models (LLMs) fine-tuned and hosted on AWS Trainium and Inferentia2 AI chips via SageMaker. These chips are characterized by their elasticity, affordability, and efficient performance for AI compute tasks. Besides enabling deployment on these chips, this work aims to improve tool usage, add citation capabilities, and mitigate the risks of hallucinations and unsafe responses due to context bias. We benchmark our RAG system's performance on the Natural Questions and HotPotQA datasets, achieving an accuracy of 62% and 59% respectively, exceeding other models such as DBRX and Mixtral Instruct.",
            "score": 0.707886208339245,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5615234375
        },
        {
            "corpus_id": "276774736",
            "title": "Explainable Depression Detection in Clinical Interviews with Personalized Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) enhances language models (LMs) by incorporating retrieved text passages into the input, leading to significant improvements in knowledge-intensive tasks (Guu et al., 2020;Lewis et al., 2020)  instruction-tuning LMs with a fixed number of retrieved passages or jointly pre-training a retriever and LM followed by few-shot fine-tuning (Luo et al., 2023;Izacard et al., 2022). Some approaches adaptively retrieve passages during generation (Jiang et al., 2023), while others, like Schick et al. (2023), train LMs to generate API calls for named entities. However, these improvements often come with trade-offs in runtime efficiency, robustness, and contextual relevance (Mallen et al., 2023;Shi et al., 2023). To address these challenges, recent work introduces methods like SELF-RAG, which enables on-demand retrieval and filters out irrelevant passages through self-reflection, enhancing robustness and control (Lin et al., 2024;Yoran et al., 2024). SELF-RAG (Asai et al., 2023) also evaluates the factuality and quality of the generated output without relying on external models during inference, making it more efficient and customizable. Additionally, other concurrent RAG methods, such as LATS (Zhou et al., 2023), explore ways to improve retrieval for specific tasks like question answering through tree search. \n\n3 Method",
            "score": 0.7063785086020364,
            "section_title": "Retrievel Augementated Generation",
            "char_start_offset": 6546,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1363
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 208,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 208,
                    "end": 227,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 474,
                    "end": 494,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 515,
                    "end": 535,
                    "matchedPaperCorpusId": "256697342"
                },
                {
                    "start": 704,
                    "end": 725,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 725,
                    "end": 742,
                    "matchedPaperCorpusId": "256459776"
                },
                {
                    "start": 947,
                    "end": 965,
                    "matchedPaperCorpusId": "263605962"
                },
                {
                    "start": 965,
                    "end": 984,
                    "matchedPaperCorpusId": "263608822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79638671875
        },
        {
            "corpus_id": "268032903",
            "title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)",
            "text": "2.1 Retrieval-Augmented Generation (RAG) Retrieval-augmented generation (RAG), first introduced by Lewis et al. (2020), has emerged as one of the most popular approaches to enhance the generation ability of LLMs (Liu, 2022;Chase, 2022;Van Veen et al., 2023;Ram et al., 2023;Shi et al., 2023). This synergy markedly boosts the output's accuracy and relevance (Gao et al., 2023), mitigating essential issues commonly referred to as \"hal-lucinations\" of LLMs (Shuster et al., 2021). One of RAG's distinctive features is its flexible architecture, allowing for the seamless interchange or update of its three core components: the dataset, the retriever, and the LLM. This flexibility means that adjustments to any of these elements can be made without necessitating re-training or fine-tuning of the entire system (Shao et al., 2023;Cheng et al., 2023). These unique advantages have positioned RAG as a favored approach for a range of practical applications, including personal chatbots and specialized domain experts like medical diagnostic assistants (Panagoulias et al., 2024).",
            "score": 0.7051247304402345,
            "section_title": "Related Work",
            "char_start_offset": 4018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1076
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 118,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1049,
                    "end": 1075,
                    "matchedPaperCorpusId": "267112617"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5859375
        },
        {
            "corpus_id": "273749074",
            "title": "Maximizing RAG efficiency: A comparative analysis of RAG methods",
            "text": "This paper addresses the optimization of retrieval-augmented generation (RAG) processes by exploring various methodologies, including advanced RAG methods. The research, driven by the need to enhance RAG processes as highlighted by recent studies, involved a grid-search optimization of 23,625 iterations. We evaluated multiple RAG methods across different vectorstores, embedding models, and large language models, using cross-domain datasets and contextual compression filters. The findings emphasize the importance of balancing context quality with similarity-based ranking methods, as well as understanding tradeoffs between similarity scores, token usage, runtime, and hardware utilization. Additionally, contextual compression filters were found to be crucial for efficient hardware utilization and reduced token consumption, despite the evident impacts on similarity scores, which may be acceptable depending on specific use cases and RAG methods.",
            "score": 0.7039701279753574,
            "section_title": "abstract",
            "char_start_offset": 2,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88623046875
        },
        {
            "corpus_id": "276249863",
            "title": "LM2: Large Memory Models",
            "text": "Retrieval-Augmented Generation (RAG) Retrieval-Augmented Generation (RAG) Lewis et al. ( 2020) is a popular solution for language models to handle large amounts of text. The core architecture of RAG comprises a retriever module that identifies relevant information from a knowledge base, ensuring that the input to the generative model remains within the token limit while filtering out irrelevant noise, thereby improving efficiency and response quality. While Retrieval-Augmented Generation (RAG) has proven effective for many tasks, it struggles with some complicated tasks like multi-hop question-answering Mavi et al. (2024), which require retrieving and reasoning over multiple interconnected pieces of evidence.",
            "score": 0.7026325513365009,
            "section_title": "Related Work",
            "char_start_offset": 22501,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 718
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7001953125
        },
        {
            "corpus_id": "272463832",
            "title": "VERA: Validation and Evaluation of Retrieval-Augmented Systems",
            "text": "The increasing use of Retrieval-Augmented Generation (RAG) systems in various applications necessitates stringent protocols to ensure RAG systems accuracy, safety, and alignment with user intentions. In this paper, we introduce VERA (Validation and Evaluation of Retrieval-Augmented Systems), a framework designed to enhance the transparency and reliability of outputs from large language models (LLMs) that utilize retrieved information. VERA improves the way we evaluate RAG systems in two important ways: (1) it introduces a cross-encoder based mechanism that encompasses a set of multidimensional metrics into a single comprehensive ranking score, addressing the challenge of prioritizing individual metrics, and (2) it employs Bootstrap statistics on LLM-based metrics across the document repository to establish confidence bounds, ensuring the repositorys topical coverage and improving the overall reliability of retrieval systems. Through several use cases, we demonstrate how VERA can strengthen decision-making processes and trust in AI applications. Our findings not only contribute to the theoretical understanding of LLM-based RAG evaluation metric but also promote the practical implementation of responsible AI systems, marking a significant advancement in the development of reliable and transparent generative AI technologies.",
            "score": 0.6972828536893099,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8173828125
        },
        {
            "corpus_id": "277150624",
            "title": "DroidTTP: Mapping Android Applications with TTP for Cyber Threat Intelligence",
            "text": "Retrieval-Augmented Generation (RAG) [24] represents a fusion of information retrieval and language generation technologies. This approach enhances AI language models by connecting them with external knowledge sources that enable more informed and accurate responses. Unlike traditional language models that rely solely on their training data, RAG actively draws upon current information when generating responses. \n\nThe RAG workflow is illustrated in Figure 1. At its core, RAG functions through a two-stage process. The first stage involves information retrieval, where the system searches through external knowledge sources to find content relevant to the current query. This works by converting both the user's question and the available reference documents into mathematical representations called embeddings. It then scans similarly encoded documents in its knowledge base, using mathematical comparison of embeddings to identify the most semantically relevant information. \n\nThe second stage leverages an LLM's generative capabilities. Rather than relying solely on its pre-trained knowledge, the model receives both the user's query and the retrieved relevant information (context) from the vector database. This allows the LLM to craft responses that incorporate specific, factual details from the retrieved sources while maintaining natural language fluency. The result is more accurate and contextually appropriate than what could be achieved by either retrieval or generation alone. This combination of dynamic knowledge access and sophisticated language generation represents a significant leap forward in AI's ability to provide precise, contextually enriched responses.",
            "score": 0.696960091870478,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 16615,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 414
                },
                {
                    "start": 417,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1684
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50439453125
        },
        {
            "corpus_id": "276408622",
            "title": "RAG vs. GraphRAG: A Systematic Evaluation and Key Insights",
            "text": "Retrieval-Augmented Generation (RAG) has been widely applied to enhance the performance of Large Language Models (LLMs) by retrieving relevant information from external sources, addressing the limitation of LLMs' restricted context windows, improving factual accuracy, and mitigating hallucinations (Fan et al., 2024;Gao et al., 2023). Most RAG systems primarily process text data by first splitting it into chunks (Finardi et al., 2024). When a query is received, RAG retrieves relevant chunks either through lexical search (Ram et al., 2023) or by computing semantic similarity (Karpukhin et al., 2020), embeddings both the query and text chunks into a shared vector space. Advanced techniques, such as pre-retrieval processing (Ma et al., 2023;Zheng et al., 2023a) and post-retrieval processing (Dong et al., 2024;Xu et al., 2023), as well as fine-tuning strategies (Li et al., 2023), have further enhanced RAG's effectiveness across various domains, including QA) (Yan et al., 2024), dialogue generation (Izacard et al., 2023), and text summarization (Jiang et al., 2023). \n\nSeveral studies have evaluated the effectiveness of RAG systems across various tasks (Yu et al., 2024;Chen et al., 2024;Es et al., 2023), such as multi-hop question answering (Tang and Yang, 2024), biomedical question answering (Xiong et al., 2024), and text generation (Liu et al., 2023). However, no existing study has simultaneously and systematically evaluated and compared RAG and GraphRAG on these general text-based tasks.",
            "score": 0.6964858316403251,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 4780,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1508
                }
            ],
            "ref_mentions": [
                {
                    "start": 299,
                    "end": 317,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 525,
                    "end": 543,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 1008,
                    "end": 1030,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1181,
                    "end": 1199,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 1254,
                    "end": 1275,
                    "matchedPaperCorpusId": "263152125"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88916015625
        },
        {
            "corpus_id": "274965531",
            "title": "XRAG: eXamining the Core - Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation",
            "text": "Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent data with the generative capabilities of Large Language Models (LLMs), ensuring that the generated output is not only contextually relevant but also accurate and current. We introduce XRAG, an open-source, modular codebase that facilitates exhaustive evaluation of the performance of foundational components of advanced RAG modules. These components are systematically categorized into four core phases: pre-retrieval, retrieval, post-retrieval, and generation. We systematically analyse them across reconfigured datasets, providing a comprehensive benchmark for their effectiveness. As the complexity of RAG systems continues to escalate, we underscore the critical need to identify potential failure points in RAG systems. We formulate a suite of experimental methodologies and diagnostic testing protocols to dissect the failure points inherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed at bolstering the overall performance of these modules. Our work thoroughly evaluates the performance of advanced core components in RAG systems, providing insights into optimizations for prevalent failure points.",
            "score": 0.69442878887352,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7001953125
        },
        {
            "corpus_id": "270560221",
            "title": "Retrieval-Augmented Feature Generation for Domain-Specific Classification",
            "text": "Retrieval Augmented Generation (RAG) (Lewis et al., 2020) is a technique that integrates the information retrieval capabilities with the generative language models to enhance the performance of the final output.RAG effectively assists LLMs with tasks requiring extensive and specific domain knowledge (Zhang et al., 2024a;Huang and Huang, 2024).Given a domain-specific task, the LLMs can access a large external library, which might be a set of documents or knowledge related to the task (Hu and Lu, 2024).Then, the LLMs generate a query according to the task information and use it for searching based on the similarity between the query and the candidate documents.This approach helps the LLMs to produce responses that are more accurate, domain-relevant, and reduce hallucinations (Yang et al., 2024).However, the vanilla results of RAG might not be used for direct feature generation which requires additional processing to align with the existing features (Li et al., 2024).This is the main difference between the goal of general RAG and our proposed framework for feature generation.",
            "score": 0.6933574123219998,
            "section_title": "Retrieval Augmented Generation for Large Language Models",
            "char_start_offset": 7072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 506
                },
                {
                    "start": 506,
                    "end": 667
                },
                {
                    "start": 667,
                    "end": 804
                },
                {
                    "start": 804,
                    "end": 979
                },
                {
                    "start": 979,
                    "end": 1089
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 784,
                    "end": 803,
                    "matchedPaperCorpusId": "259262077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62109375
        },
        {
            "corpus_id": "277043297",
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) has gained significant attention in recent years for its potential to enhance natural language understanding and generation by combining large-scale retrieval systems with generative models. RAG leverages external knowledge sources, such as documents, databases, or structured data, to improve model performance and generate more accurate and contextually relevant outputs. This survey aims to provide a comprehensive overview of RAG by examining its fundamental components, including retrieval mechanisms, generation processes, and the integration between the two. We discuss the key characteristics of RAG, such as its ability to augment generative models with dynamic external knowledge, and the challenges associated with aligning retrieved information with generative objectives. We also present a taxonomy that categorizes RAG methods, ranging from basic retrieval-augmented approaches to more advanced models incorporating multi-modal data and reasoning capabilities. Additionally, we review the evaluation benchmarks and datasets commonly used to assess RAG systems, along with a detailed exploration of its applications in fields such as question answering, summarization, and information retrieval. Finally, we highlight emerging research directions and opportunities for improving RAG systems, such as enhanced retrieval efficiency, model interpretability, and domain-specific adaptations. This paper concludes by outlining the prospects for RAG in addressing real-world challenges and its potential to drive further advancements in natural language processing.",
            "score": 0.6916683631819612,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88037109375
        },
        {
            "corpus_id": "277272240",
            "title": "Retrieval Augmented Generation and Understanding in Vision: A Survey and New Outlook",
            "text": "Retrieval-augmented generation (RAG) is a transformative technique in generative AI, particularly in natural language processing (NLP) and recommendation systems. It improves content quality by integrating external, up-to-date information from knowledge sources [1]. Despite the impressive performance of large language models (LLMs), challenges like hallucinations, outdated knowledge, and lack of domainspecific expertise remain [1]. RAG addresses these issues by supplying LLMs with relevant, retrieved factual information, enhancing model outputs. \n\nRAG works by using a retriever to extract relevant knowledge from external databases [12], which is then combined with the model's input to provide enriched context [13]. This approach is efficient, requiring minimal adaptation and often no additional training [2]. Recent studies highlight RAG's potential, not only for knowledge-intensive tasks but also for a broad range of language-based applications [2], enabling more accurate and up-to-date outputs. \n\nWhile traditional RAG pipelines are text-based, real-world knowledge is often multimodal, in the form of images, videos, and 3D models. This creates challenges when applying RAG to computer vision (CV). In CV, visual understanding tasks like object identification [14,15], anomaly detection [16], and segmentation [17] require integrating external knowledge to improve accuracy [13]. Similarly, visual generation tasks, such as transforming textual descriptions into realistic images, can benefit from external knowledge like scene layouts, object relationships, and temporal dynamics in videos [15]. \n\nGiven the complexity of visual data, RAG can significantly improve model performance. For example, scene generation models can benefit from knowledge about object interactions and spatial relationships, while image classification models can enhance accuracy by retrieving up-to-date visual references. By integrating external knowledge, RAG enhances both visual understanding and generation, helping overcome inherent challenges in vision tasks. As shown in Figure 1, recent research has started exploring RAG's integration into CV, aiming to improve both understanding and generation. While large visionlanguage models (LVLMs) have shown promise, they still face challenges with image generalization and understanding [18].",
            "score": 0.6890081754155729,
            "section_title": "A. Background",
            "char_start_offset": 2182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 551
                },
                {
                    "start": 554,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1010
                },
                {
                    "start": 1013,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1613
                },
                {
                    "start": 1616,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2201
                },
                {
                    "start": 2202,
                    "end": 2340
                }
            ],
            "ref_mentions": [
                {
                    "start": 639,
                    "end": 643,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 815,
                    "end": 818,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 959,
                    "end": 962,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 1277,
                    "end": 1281,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 1281,
                    "end": 1284,
                    "matchedPaperCorpusId": "268531117"
                },
                {
                    "start": 1608,
                    "end": 1612,
                    "matchedPaperCorpusId": "268531117"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83203125
        },
        {
            "corpus_id": "270688478",
            "title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems",
            "text": "Retrieval Augmented Generation (RAG) represents a significant advancement in artificial intelligence combining a retrieval phase with a generative phase, with the latter typically being powered by large language models (LLMs). The current common practices in RAG involve using\"instructed\"LLMs, which are fine-tuned with supervised training to enhance their ability to follow instructions and are aligned with human preferences using state-of-the-art techniques. Contrary to popular belief, our study demonstrates that base models outperform their instructed counterparts in RAG tasks by 20% on average under our experimental settings. This finding challenges the prevailing assumptions about the superiority of instructed LLMs in RAG applications. Further investigations reveal a more nuanced situation, questioning fundamental aspects of RAG and suggesting the need for broader discussions on the topic; or, as Fromm would have it,\"Seldom is a glance at the statistics enough to understand the meaning of the figures\".",
            "score": 0.6872708138132401,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78759765625
        },
        {
            "corpus_id": "261531346",
            "title": "A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture",
            "text": "This study presents a method for implementing generative AI services by utilizing the Large Language Models (LLM) application architecture. With recent advancements in generative AI technology, LLMs have gained prominence across various domains. In this context, the research addresses the challenge of information scarcity and proposes specific remedies by harnessing LLM capabilities. The investigation delves into strategies for mitigating the issue of inadequate data, offering tailored solutions. The study delves into the efficacy of employing fine-tuning techniques and direct document integration to alleviate data insufficiency. A significant contribution of this work is the development of a Retrieval-Augmented Generation (RAG) model, which tackles the aforementioned challenges. The RAG model is carefully designed to enhance information storage and retrieval processes, ensuring improved content generation. The research elucidates the key phases of the information storage and retrieval methodology underpinned by the RAG model. A comprehensive analysis of these steps is undertaken, emphasizing their significance in addressing the scarcity of data. The study highlights the efficacy of the proposed method, showcasing its applicability through illustrative instances. By implementing the RAG model for information storage and retrieval, the research not only contributes to a deeper comprehension of generative AI technology but also facilitates its practical usability within enterprises utilizing LLMs. This work holds substantial value in advancing the field of generative AI, offering insights into enhancing data-driven content generation and fostering active utilization of LLM-based services within corporate settings.",
            "score": 0.686060901622942,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73583984375
        },
        {
            "corpus_id": "276408784",
            "title": "Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA",
            "text": "Recent advancements in Generative AI have significantly improved the efficiency and adaptability of natural language processing (NLP) systems, particularly through Retrieval-Augmented Generation (RAG), Low-Rank Adaptation (LoRA), and Weight-Decomposed Low-Rank Adaptation (DoRA). RAG integrates external knowledge to enhance factual consistency in generative outputs, while LoRA enables parameter-efficient fine-tuning of large language models (LLMs). DoRA further refines this process by optimizing fine-tuning through adaptive parameter ranking and domain-aware weight adjustments, improving learning efficiency while maintaining inference performance. This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications. Furthermore, this study examines the trade-offs between fine-tuning efficiency, computational cost, and real-time adaptability across different models. Findings highlight RAG's effectiveness in knowledge grounding, LoRA's cost-efficient domain adaptation, and DoRA's ability to balance fine-tuning efficiency with model precision. These insights provide practical guidance for deploying AI-driven generative systems in accuracy-critical domains such as healthcare, finance, and legal services, ensuring scalability, reliability, and optimal performance in dynamic environments.",
            "score": 0.6858715337062964,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9873046875
        },
        {
            "corpus_id": "271720020",
            "title": "Creating a Taxonomy for Retrieval Augmented Generation Applications",
            "text": "The rise of new modes of interaction with artificial intelligence (AI) has significantly increased its popularity and broadened its applicability. Despite these advancements, the conceptual integration of AI in organizational settings remains limited, with a notable lack of systematic application (Uba et al. 2023). Among various AI applications, Retrieval-Augmented Generation (RAG) stands out due to its potential to transform information retrieval and content generation (Shuster et al. 2021). This was particularly evident following the public unveiling of OpenAI's models like ChatGPT in November 2022, where generative AI (genAI) has garnered much attention in both academic (B\u00f6hmann et al. 2023;Wessel et al. 2023) and industry sectors (McGrath 2024;McKinsey 2023). Recent studies on the potential of genAI, which largely rely on large language model (LLM) systems, ranging from automatization (Engel et al. 2023) to improving knowledge work (Anthony et al., 2023;Dell'Acqua et al., 2023) to creating novel business models (Kanbach et al. 2023). However, LLMs are not without flaws. In recent studies, Large Language Models (LLMs) have been identified to have several core limitations. These include a tendency to generate incorrect or misleading information (hallucinations) (Blom 2010), poor arithmetic capabilities, a lack of interpretative power, the high costs associated with model revisions, limitations in handling less popular or low-resource concepts and entities, and an inability to reference sources accurately (Barnett et al. 2024;Soudani et al. 2024;Zhao et al. 2024). Several approaches have been developed to mitigate the limitations of, while retrieval augmented generation (RAG) is as of now deemed as most promising (Gao et al. 2024). RAG primarily enhances LLMs by incorporating contextual information during the retrieval process, significantly improving the generated content's accuracy and consistency. Consequently, RAG improves LLM tasks and applications in various ways, as evidenced by recent studies (Asai et al. 2023;Jiang et al. 2023;Martino et al. 2023).",
            "score": 0.6857102831763857,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 298,
                    "end": 314,
                    "matchedPaperCorpusId": "256902156"
                },
                {
                    "start": 475,
                    "end": 496,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 703,
                    "end": 721,
                    "matchedPaperCorpusId": "266784043"
                },
                {
                    "start": 744,
                    "end": 758,
                    "matchedPaperCorpusId": "268272501"
                },
                {
                    "start": 902,
                    "end": 921,
                    "matchedPaperCorpusId": "259476254"
                },
                {
                    "start": 950,
                    "end": 972,
                    "matchedPaperCorpusId": "255732846"
                },
                {
                    "start": 972,
                    "end": 996,
                    "matchedPaperCorpusId": "195208606"
                },
                {
                    "start": 1031,
                    "end": 1052,
                    "matchedPaperCorpusId": "261849160"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6611328125
        },
        {
            "corpus_id": "271710111",
            "title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation",
            "text": "Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. We introduce RAG Foundry, an open-source framework for augmenting large language models for RAG use cases. RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources. We demonstrate the framework effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets. Code is released as open-source in https://github.com/IntelLabs/RAGFoundry.",
            "score": 0.6845861583809282,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64794921875
        },
        {
            "corpus_id": "269293655",
            "title": "Evaluating Retrieval Quality in Retrieval-Augmented Generation",
            "text": "Retrieval-augmented generation (RAG) has emerged as a prominent approach in natural language processing, combining the strengths of retrieval and generation models [35], with use cases in decreasing hallucination [1,29], knowledge-grounding [9,16,34], and personalization [25,26].Evaluating RAG systems is important as it ensures the effectiveness of integrating retrieval-based methods with generative models [10,23].Traditionally, RAG evaluation has primarily relied on end-to-end assessment, which entails comparing the generated output with one or more ground truth references [20].While this is crucial, it presents several limitations, especially, for evaluating retrieval models in RAG systems.\n\nFirst, end-to-end evaluation lacks transparency regarding which retrieved document contributed to the generated output, hindering interpretability of the system's behavior.Secondly, it is resourceintensive, consuming significant time and computational power, particularly when dealing with a large set of retrieval results consumed by the LLM.To process long input sequences resulting from the utilization of all retrieved documents by the LLM, GPUs with substantial memory capacities are essential for end-to-end evaluation.Moreover, many ranking systems rely on interleaving (i.e., replacing one or more documents in the result list) for evaluation and optimization, which further complicates the evaluation, as slight variations in retrieval results necessitate re-computation of the RAG pipeline.Finally, optimizing ranking models often requires document-level feedback, such as user clicks [3,6].However, endto-end evaluation only provides list-level feedback for the retrieval results.That said, this paper studies retrieval evaluation in RAG.\n\nHuman annotations can be a potential solution for evaluating retrieval models in RAG, however, accurate annotations are often challenging and costly to obtain.More recently, with the emergence of large language models (LLMs) and their advanced capabilities in reasoning and text comprehension, they have been utilized to annotate documents for retrieval evaluation [10,23].Nevertheless, these approaches predominantly evaluate the retriever in RAG systems based on human preferences, whereas the primary objective of the retrieval model in RAG is to serve the LLM that leverages the retrieved results [35].",
            "score": 0.6807129891053961,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 418
                },
                {
                    "start": 418,
                    "end": 586
                },
                {
                    "start": 586,
                    "end": 701
                },
                {
                    "start": 703,
                    "end": 875
                },
                {
                    "start": 875,
                    "end": 1046
                },
                {
                    "start": 1046,
                    "end": 1228
                },
                {
                    "start": 1228,
                    "end": 1503
                },
                {
                    "start": 1503,
                    "end": 1604
                },
                {
                    "start": 1604,
                    "end": 1694
                },
                {
                    "start": 1694,
                    "end": 1752
                },
                {
                    "start": 1754,
                    "end": 1913
                },
                {
                    "start": 1913,
                    "end": 2127
                },
                {
                    "start": 2127,
                    "end": 2360
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 216,
                    "end": 219,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 244,
                    "end": 247,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "269605438"
                },
                {
                    "start": 272,
                    "end": 276,
                    "matchedPaperCorpusId": "269009728"
                },
                {
                    "start": 581,
                    "end": 585,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 1598,
                    "end": 1601,
                    "matchedPaperCorpusId": "258212955"
                },
                {
                    "start": 1601,
                    "end": 1603,
                    "matchedPaperCorpusId": "2979013"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8056640625
        },
        {
            "corpus_id": "275757440",
            "title": "Generative AI for Health Technology Assessment: Opportunities, Challenges, and Policy Considerations",
            "text": "\u2022 Prompt engineering: creating and adapting prompts (input) to instruct AI models to generate specific output. \n\n\u2022 Retrieval Augmented Generation (RAG): A method in natural language processing that combines a generative model with a retrieval system to improve response accuracy. The retrieval system finds relevant information, which the generative model uses to produce more contextually accurate and factual outputs. -Example: DALL-E generating images from text descriptions.",
            "score": 0.6796604943322908,
            "section_title": "Glossary",
            "char_start_offset": 29583,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 113,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 478
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3896484375
        },
        {
            "corpus_id": "271924151",
            "title": "Hierarchical Retrieval-Augmented Generation Model with Rethink for Multi-hop Question Answering",
            "text": "Retrieval-Augmented Generation (RAG) is a current research hotspot for Multi-hop QA tasks [10]. It can integrate the internal knowledge of the model with the external knowledge retrieved. LLM can retrieve external content through RAG to expand their knowledge base, thereby improving their ability to generate accurate and contextually relevant responses. Historically, various studies attempt to adapt the use of generative models to improve their performance. For instance, REPLUG [17] uses different retrieved content to generate corresponding answers and then combine them. Self-Rag [2] fine-tunes a generation model to simultaneously produce answers along with relevance, support, and usefulness scores. Concurrently, several methods for multi-hop QA emphasize the content and timing of retrieval. Self-Ask [15] lets the model generate sub-questions and queries, and continuously alternate between retrieval and generation. PROMPTAGATOR [4], Take a step back [32] focus on abstracting high-level concepts and utilizing LLMs for prompt-based query generation. Additionally, the confidence-based method, FLARE [8], generates queries using low-confidence tokens. However, most studies directly feed the retrieval content into the generation model, ignoring the evaluation and processing of the retrieval content. Unlike them, HiRAG highlights the importance of verifying retrieved content and adjusts the retriever to enhance the relevance of results when the quality of the retrieved information is subpar.",
            "score": 0.677373737399325,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 7386,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1509
                }
            ],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 94,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81689453125
        },
        {
            "corpus_id": "269137180",
            "title": "Reducing hallucination in structured outputs via Retrieval-Augmented Generation",
            "text": "A common and fundamental limitation of Generative AI (GenAI) is its propensity to hallucinate. While large language models (LLM) have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging Retrieval Augmented Generation (RAG) to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucinations in the output and improves the generalization of our LLM in out-of-domain settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive.",
            "score": 0.6760354224951901,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7021484375
        },
        {
            "corpus_id": "268091298",
            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
            "text": "Despite significant advancements in generative models, AIGC still grapples with challenges like outdated knowledge, lack of long-tail knowledge [27], and risks of leaking private training data [28]. Retrieval-Augmented Generation (RAG) aims to mitigate these issues with its flexible data repository [29]. The retrievable knowledge acts as non-parametric memory, which is easily updatable, accommodates extensive long-tail knowledge, and can encode confidential data. Moreover, retrieval can lower generation costs. RAG can reduce the size of large models [30], support long contexts [31], and eliminate certain generation steps [32]. \n\nA typical RAG process is depicted in Fig. 1. Given an input query, the retriever identifies relevant data sources, and the retrieved information interacts with the generator to improve the generation process. There are several foundational paradigms (foundations in short) according to how the retrieved results augment the generation: they can serve as augmented input to the generator [33], [34]; they can join at the middle stage of generation as latent representations [35], [36]; they can contribute to the final generation results in the form of logits [37], [38]; they can even influence or omit certain generation steps [32], [39]. Additionally, researchers have proposed various enhancements to improve the foundational RAG process. These methods encompass specific optimizations for individual components as well as holistic enhancements aimed Fig. 1: A generic RAG architecture. The user queries, spanning different modalities, serve as input to both the retriever and the generator. The retriever extracts relevant information from data sources. The generator interacts with the retrieval results and ultimately produces outcomes of various modalities. at the entire pipeline. \n\nIn addition, while the concept of RAG initially emerged in text-to-text generation [34], this technique has also found applications across various domains, including codes [40]- [42], audios [43], [44], images [45]- [47], videos [48], [49], 3D [50], [51], knowledge [52]- [54], and AI for science [55], [56].",
            "score": 0.6756283409093988,
            "section_title": "I. INTRODUCTION A. Background",
            "char_start_offset": 1995,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1825
                },
                {
                    "start": 1828,
                    "end": 2136
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 148,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 193,
                    "end": 197,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1024,
                    "end": 1028,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1110,
                    "end": 1114,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1116,
                    "end": 1120,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1202,
                    "end": 1206,
                    "matchedPaperCorpusId": "237452184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7919921875
        },
        {
            "corpus_id": "265610005",
            "title": "Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models",
            "text": "Retrieval-augmented generation (RAG) for LLMs represents an important advancement in enhancing the generative capabilities of models by integrating external knowledge sources. Early works such as REALM (Guu et al., 2020) and RAG (Lewis et al., 2020) introduced the foundational methodology for incorporating external documents into the generation process. Subsequent research expanded the retrieval-augmented paradigm to multi-modal contexts. Such as MuRAG (Chen et al., 2022) and REVEAL (Hu et al., 2022), which augment language generation with both textual and visual information from external sources. Furthermore, recent studies such as FiD-Light (Hofst\u00e4tter et al., 2022) and REPLUG (Shi et al., 2023) have focused on improving the efficiency and effectiveness of retrieval-augmented systems as well as exploring in-context retrieval-augmented mechanisms (Ram et al., 2023;de Jong et al., 2023).",
            "score": 0.6754046594348895,
            "section_title": "Retrieval-Augmented Generation for LLMs",
            "char_start_offset": 3981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 900
                }
            ],
            "ref_mentions": [
                {
                    "start": 488,
                    "end": 505,
                    "matchedPaperCorpusId": "254564204"
                },
                {
                    "start": 651,
                    "end": 676,
                    "matchedPaperCorpusId": "252568176"
                },
                {
                    "start": 860,
                    "end": 878,
                    "matchedPaperCorpusId": "256459451"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.814453125
        },
        {
            "corpus_id": "272524830",
            "title": "LLMs Will Always Hallucinate, and We Need to Live With This",
            "text": "Fine-tuning improves how large language models (LLMs) perform on specific tasks, but it does not always ensure that the information they produce is factually accurate. Retrieval-Augmented Generation (RAG) addresses this by combining the strengths of language models with information retrieval systems, allowing the model to generate content based on accurate, up-to-date information. \n\nIn simple terms, RAG works like this: \n\nHere, y is the output, G is the language model, x is the input, and R(x) is the relevant information retrieved from an external knowledge base.",
            "score": 0.6753715995748866,
            "section_title": "Retrieval-Augmented Generation (RAG): Bridging Knowledge Gaps",
            "char_start_offset": 12455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 569
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53955078125
        },
        {
            "corpus_id": "268091298",
            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
            "text": "Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.",
            "score": 0.6750035746130431,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80126953125
        },
        {
            "corpus_id": "270869499",
            "title": "First Place Solution of 2023 Global Artificial Intelligence Technology Innovation Competition Track 1",
            "text": "Retrieval Augmented Generation (RAG) technology [2,6,7,9,15] in the field of natural language processing represents an innovative breakthrough.Traditional NLP techniques primarily rely on large language models, but their accuracy and depth may be limited when dealing with complex queries that require extensive background knowledge.To overcome this limitation, RAG combines conventional information retrieval methods with modern generative language models, aiming to enhance the model's text generation capabilities by incorporating external knowledge sources.The core principle is to integrate retrieval and generation techniques, allowing the model to access and utilize a vast amount of external information before generating text.RAG excels in addressing knowledge-intensive NLP tasks such as question answering, fact verification, and more.In recent years, RAG systems have evolved from a primary stage to an advanced stage, and then to a modular stage, to improve performance, cost-effectiveness, and   crease the difficulty of the pre-training task, gradually increasing the proportion of masking as the number of epochs increased.Specifically, we set an initial mask proportion of 0.3, and after every 10 epochs of pre-training, we perform fine-tuning of the downstream task.If the performance of the fine-tuning is lower than the previous one, we increase the mask proportion by 0.05 and continue with pre-training.Ultimately, we increase the number of pre-training epochs to 140, which significantly improves the text generation performance of the downstream task.For the construction of the training set with retrieval knowledge, we use D(Description) as the query and calculate the similarity with the key of each key-value pair in the knowledge base (e.g., vector inner product, L2 distance, or cosine similarity).If the similarity is larger than the threshold k, we call it an effective retrieval.We retrieve this key-value pair and concatenate the value to the end of the query as the new training sample corresponding to the query.For the val set and test set, we use the same retrieval method to construct the val set and test set with retrieval knowledge.Retrieval Iterations.For the first retrieval augmentation, the embeddings of key-value pairs are computed using a model trained on a training set without a knowledge base.",
            "score": 0.6724627935730986,
            "section_title": "Retrieval Augmentation in NLP",
            "char_start_offset": 3769,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 143,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 561
                },
                {
                    "start": 561,
                    "end": 735
                },
                {
                    "start": 735,
                    "end": 846
                },
                {
                    "start": 846,
                    "end": 1139
                },
                {
                    "start": 1139,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1425
                },
                {
                    "start": 1425,
                    "end": 1575
                },
                {
                    "start": 1575,
                    "end": 1828
                },
                {
                    "start": 1828,
                    "end": 1912
                },
                {
                    "start": 1912,
                    "end": 2048
                },
                {
                    "start": 2048,
                    "end": 2174
                },
                {
                    "start": 2174,
                    "end": 2195
                },
                {
                    "start": 2195,
                    "end": 2345
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 51,
                    "matchedPaperCorpusId": "252735160"
                },
                {
                    "start": 51,
                    "end": 53,
                    "matchedPaperCorpusId": "264426178"
                },
                {
                    "start": 53,
                    "end": 55,
                    "matchedPaperCorpusId": "252568176"
                },
                {
                    "start": 55,
                    "end": 57,
                    "matchedPaperCorpusId": "267053546"
                },
                {
                    "start": 57,
                    "end": 60,
                    "matchedPaperCorpusId": "247058346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.560546875
        },
        {
            "corpus_id": "276742664",
            "title": "Pseudo-Knowledge Graph: Meta-Path Guided Retrieval and In-Graph Text for RAG-Equipped LLM",
            "text": "Shortly after the introduction of pre-trained language models [13], Large Language Models (LLMs) [5] have significantly advanced natural language processing, excelling in tasks like translation and summarization [9,81]. However, they often struggle with factual accuracy, generating outdated or incorrect information due to reliance on learned patterns. To address these challenges, the Retrieval-Augmented Generation (RAG) framework [31] is introduced. RAG enhances the generative capabilities of LLMs by incorporating a retrieval mechanism that accesses relevant information from external knowledge bases [32]. This two-step process retrieves relevant documents based on the input query and uses them to inform response generation. By integrating retrieval and generation, RAG enhances factual accuracy and enriches content with current, contextually relevant information [27]. RAG has shown promising results in applications like questionanswering and conversational agents, setting a new standard for combining retrieval and generative techniques. The integration of vector databases with RAG holds significant potential for improving the efficiency and effectiveness of information retrieval alongside LLMs [52].",
            "score": 0.6709308082411806,
            "section_title": "RELATED WORK 2.1 Retrieval Augmented Generation",
            "char_start_offset": 6333,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1217
                }
            ],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 66,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 434,
                    "end": 438,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 874,
                    "end": 878,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 1212,
                    "end": 1216,
                    "matchedPaperCorpusId": "269293655"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81103515625
        },
        {
            "corpus_id": "273323961",
            "title": "A Methodology for Evaluating RAG Systems: A Case Study On Configuration Dependency Validation",
            "text": "Retrieval-augmented generation (RAG) is an emerging novel approach, driven by recent advancements in large language models (LLMs), which have enhanced their capabilities in various software engineering tasks, such as code generation [6,49], configuration validation [26], database tuning [16,25], and program repair [10,22,48]. Despite these advancements, LLMs still face challenges, such as hallucination [20,58], outdated data [45,57], and untraceable reasoning [14,23], which undermine their ability to provide accurate and reliable outputs. RAG offers a promising solution to these challenges by integrating techniques from information retrieval with the generative capabilities of LLMs. This enables the delivery of contextually relevant, up-to-date, and factually accurate information to an LLM, effectively mitigating its limitations. \n\nTypically, a RAG system comprises of an ingestion part where data is preprocess, embedded, and stored as context documents in a vector database including meta-data, a retrieval part where context documents are retrieved and ranked according to their relevance to a query, and a query part where a prompt with a query is combined with the retrieval results and sent to the LLM. So, RAG is not a single technology, but an umbrella of different components, design decisions, and domain-specific adaptations. It can be composed from different embedding models, search types, filters, and re-rankers. Developers must define which data sources are relevant and how to preprocess them prior to ingestion into a vector database. Moreover, how to prompt the LLM, how to rank which contextual information is most beneficial, and whether and how to provide the LLM with examples to the query at hand, often requires multiple evaluation and refinement steps [2]. The final RAG system is, thus, a product of several design decisions and iterations based on empirical evidence about the RAG's effectiveness collected on the way. \n\nSuch an iterative, and empirical-driven development process requires a sound research methodology when a RAG system is not only built but also evaluated in a sound way.",
            "score": 0.669051648791436,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 841
                },
                {
                    "start": 844,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1958
                },
                {
                    "start": 1961,
                    "end": 2129
                }
            ],
            "ref_mentions": [
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "247158549"
                },
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "255372224"
                },
                {
                    "start": 320,
                    "end": 323,
                    "matchedPaperCorpusId": "251765058"
                },
                {
                    "start": 323,
                    "end": 326,
                    "matchedPaperCorpusId": "259860439"
                },
                {
                    "start": 406,
                    "end": 410,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "254877753"
                },
                {
                    "start": 1790,
                    "end": 1793,
                    "matchedPaperCorpusId": "266933076"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.859375
        },
        {
            "corpus_id": "271693372",
            "title": "RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework",
            "text": "Retrieval-Augmented Generation (RAG) is a powerful approach that enables large language models (LLMs) to incorporate external knowledge. However, evaluating the effectiveness of RAG systems in specialized scenarios remains challenging due to the high costs of data construction and the lack of suitable evaluation metrics. This paper introduces RAGEval, a framework designed to assess RAG systems across diverse scenarios by generating high-quality documents, questions, answers, and references through a schema-based pipeline. With a focus on factual accuracy, we propose three novel metrics: Completeness, Hallucination, and Irrelevance to evaluate LLM generated responses rigorously. Experimental results show that RAGEval outperforms zero-shot and one-shot methods in terms of clarity, safety, conformity, and richness of generated samples. Furthermore, the use of LLMs for scoring the proposed metrics demonstrates a high level of consistency with human evaluations. RAGEval establishes a new paradigm for evaluating RAG systems in real-world applications. The code and dataset are released at https://github.com/OpenBMB/RAGEval.",
            "score": 0.666338692813319,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.890625
        },
        {
            "corpus_id": "270764659",
            "title": "Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation",
            "text": "To address the misalignment between different components of retrieval-augmented generation (RAG) and improve overall generation performance, we propose the DPA-RAG framework, which is illustrated in Figure 2. In general, DPA-RAG improves traditional RAG architecture in two main aspects: (1) we fine-tune a preference-aligned reranker between the retriever and the LLM to selectively filter out knowledge that aligns with LLMs' knowledge preferences ( \u00a73.3); and (2) we design a self-alignment mechanism that fine-tunes LLMs to better recognize and utilize knowledge consistent with their reasoning preferences ( \u00a73.4).To acquire the LLM's preference knowledge, we devise a three-step construction method, motivated by our preliminary analysis of how different types of retrieved documents affect RAG performance ( \u00a73.2).Below, we will first introduce the task definition ( \u00a73.1) and then we delve into the specifics of our approach.",
            "score": 0.6656682658024744,
            "section_title": "Methodology",
            "char_start_offset": 8100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 619
                },
                {
                    "start": 619,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 933
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.544921875
        },
        {
            "corpus_id": "270062436",
            "title": "SynthAI: A Multi Agent Generative AI Framework for Automated Modular HLS Design Generation",
            "text": "Another technique we use in SynthAI is Retrieval Augmented Generation (RAG) [11]- [13], a method that augments LLMs knowledge by incorporating external knowledge retrieval into their generative process. In practice, RAG involves constructing a vector database from semantic data. This database acts as a search tool for the LLM, enabling it to supplement its inherent knowledge (embedded in its parameters) with external, task-specific information. RAG allows us to integrate diverse resources like textbooks and coding best practices and other domain-specific knowledge.",
            "score": 0.6643782769056219,
            "section_title": "C. Retrieval Augmented Generation (RAG)",
            "char_start_offset": 7784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 571
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4873046875
        },
        {
            "corpus_id": "264288947",
            "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
            "text": "Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) augments the input space of LMs with retrieved text passages (Guu et al., 2020;Lewis et al., 2020), leading to large improvements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram et al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by fewshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only once at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation on top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named entities. Yet, the improved task performance of such approaches often comes at the expense of runtime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of attributions (Liu et al., 2023a;Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve generation quality and attributions.",
            "score": 0.6639800004713787,
            "section_title": "RELATED WORK",
            "char_start_offset": 4683,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1265
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 148,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 148,
                    "end": 167,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 881,
                    "end": 902,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 937,
                    "end": 955,
                    "matchedPaperCorpusId": "256459776"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70654296875
        },
        {
            "corpus_id": "269740933",
            "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
            "text": "As one of the most fundamental data mining techniques, retrieval aims to understand the input query and extract relevant information from external data sources [24,30,67,140].It has found extensive application in various fields [8,28,106,179], such as search, question answering, and recommender systems.For instance, search engines (e.g., Google, Bing, and Baidu) are the most successful applications of retrieval in the industry; they can filter and retrieve the most relevant web pages or documents that can match a user's query [19,179], enabling users to find the desired information effectively.Meanwhile, retrieval models, through effective data maintenance in external databases, can provide faithful and timely external knowledge, thereby serving vital functions in various knowledge-intensive tasks.Due to their powerful capacities, retrieval techniques have been successfully incorporated into advanced generative models in the era of AI-Generated Content (AIGC) [77,132,163].Notably, the integration of retrieval models with language models has given rise to Retrieval-Augmented Generation (RAG) [74], which has emerged as one of the most representative techniques in the field of generative AI, aiming to enhance the quality of the generated text content with retrieved information [6,74,77].\n\nTo advance generation models and enhance the generated results, RAG incorporates information or knowledge from external data sources, which serves as supplementary for the input query or the generated output [62,103].Specifically, RAG first invokes the retriever to search and extract the relevant documents from external databases, which are then leveraged as the context to enhance the generation process [54].In practice, RAG techniques are feasible and efficient to apply in various generation tasks with simple adaptation of the retrieval component, requiring minimal or even no additional training [117].Recent studies have demonstrated the great potential of RAG not only for knowledge-intensive tasks such as the Open-domain Question Answering (OpenQA) [6,46,109,133], but also for general language tasks [48,62,170], and various downstream applications [90,163].",
            "score": 0.6633141642338874,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 175,
                    "end": 304
                },
                {
                    "start": 304,
                    "end": 601
                },
                {
                    "start": 601,
                    "end": 809
                },
                {
                    "start": 809,
                    "end": 987
                },
                {
                    "start": 987,
                    "end": 1305
                },
                {
                    "start": 1307,
                    "end": 1524
                },
                {
                    "start": 1524,
                    "end": 1719
                },
                {
                    "start": 1719,
                    "end": 1917
                },
                {
                    "start": 1917,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 164,
                    "matchedPaperCorpusId": "270350363"
                },
                {
                    "start": 164,
                    "end": 167,
                    "matchedPaperCorpusId": "224940171"
                },
                {
                    "start": 167,
                    "end": 170,
                    "matchedPaperCorpusId": "3710903"
                },
                {
                    "start": 170,
                    "end": 174,
                    "matchedPaperCorpusId": "260972090"
                },
                {
                    "start": 231,
                    "end": 234,
                    "matchedPaperCorpusId": "259521386"
                },
                {
                    "start": 234,
                    "end": 238,
                    "matchedPaperCorpusId": "13042931"
                },
                {
                    "start": 238,
                    "end": 242,
                    "matchedPaperCorpusId": "4230989"
                },
                {
                    "start": 536,
                    "end": 540,
                    "matchedPaperCorpusId": "4230989"
                },
                {
                    "start": 1108,
                    "end": 1112,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1295,
                    "end": 1298,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1298,
                    "end": 1301,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1515,
                    "end": 1519,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 1519,
                    "end": 1523,
                    "matchedPaperCorpusId": "216056269"
                },
                {
                    "start": 1714,
                    "end": 1718,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1911,
                    "end": 1916,
                    "matchedPaperCorpusId": "256459451"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79345703125
        },
        {
            "corpus_id": "272689170",
            "title": "SFR-RAG: Towards Contextually Faithful LLMs",
            "text": "Retrieval Augmented Generation (RAG), a paradigm that integrates external contextual information with large language models (LLMs) to enhance factual accuracy and relevance, has emerged as a pivotal area in generative AI. The LLMs used in RAG applications are required to faithfully and completely comprehend the provided context and users' questions, avoid hallucination, handle unanswerable, counterfactual or otherwise low-quality and irrelevant contexts, perform complex multi-hop reasoning and produce reliable citations. In this paper, we introduce SFR-RAG, a small LLM that is instruction-tuned with an emphasis on context-grounded generation and hallucination minimization. We also present ContextualBench, a new evaluation framework compiling multiple popular and diverse RAG benchmarks, such as HotpotQA and TriviaQA, with consistent RAG settings to ensure reproducibility and consistency in model assessments. Experimental results demonstrate that our SFR-RAG-9B model outperforms leading baselines such as Command-R+ (104B) and GPT-4o, achieving state-of-the-art results in 3 out of 7 benchmarks in ContextualBench with significantly fewer parameters. The model is also shown to be resilient to alteration in the contextual information and behave appropriately when relevant context is removed. Additionally, the SFR-RAG model maintains competitive performance in general instruction-following tasks and function-calling capabilities.",
            "score": 0.6618099679278825,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75
        },
        {
            "corpus_id": "275993994",
            "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects",
            "text": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.",
            "score": 0.6617351671827728,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9208984375
        },
        {
            "corpus_id": "268819582",
            "title": "RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation",
            "text": "Retrieval-augmented generation (RAG) represents an effective strategy for enhancing a model's capability to leverage non-parametric knowledge by retrieving external data resources, rather than relying solely on its intrinsic parametric knowledge during generation.This paradigm has garnered widespread attention across both industry and academia, proving its efficacy in a variety of scenarios such as question answering (Lewis et al., 2020), code generation (Zhou et al., 2022), alignment with human values (Xu et al., 2023a) and reducing hallucinations (Shuster et al., 2021).Recent advancements in RAG systems can generally be categorized into two areas: improvements in either the retrieval component or the generation component of the system.",
            "score": 0.6590745719834944,
            "section_title": "Related Works",
            "char_start_offset": 19385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 264,
                    "end": 578
                },
                {
                    "start": 578,
                    "end": 747
                }
            ],
            "ref_mentions": [
                {
                    "start": 421,
                    "end": 441,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82373046875
        },
        {
            "corpus_id": "274283400",
            "title": "Adaptive Control of Retrieval-Augmented Generation for Large Language Models Through Reflective Tags",
            "text": "LLMs have known weaknesses, such as producing answers that do not match or contradict the given context. This issue is particularly pronounced when addressing domain-specific or highly specialized queries. To mitigate these weaknesses, a technique called retrieval-augmented generation (RAG) was proposed in 2020 [1,2]. The core idea of RAG is to integrate data obtained from querying external knowledge sources into the generation process. By leveraging the accuracy and specificity of knowledge from external sources, RAG enhances the generative process, improving the ability to provide highly relevant and real-time responses to queries. This section provides an overview of RAG models and reviews existing research aimed at enhancing their performance.",
            "score": 0.6590745719834944,
            "section_title": "Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 6316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 757
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 316,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 316,
                    "end": 318,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82861328125
        },
        {
            "corpus_id": "269043117",
            "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
            "text": "Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the \u2018Blended RAG\u2019 method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a \u2018Blended Retriever\u2019 to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance.",
            "score": 0.658295778191916,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81005859375
        },
        {
            "corpus_id": "270357690",
            "title": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs",
            "text": "Large Language Models (LLMs) transformed many machine learning tasks using in-context learning abilities.They achieved such accuracy by leveraging an increasing number of parameters, which in recent models have grown to hundreds of billions, making LLM training expensive in terms of both time and resources.It also comes with the danger of leaking confidential data into model weights [28,33,40].Additionally, continuous training through fine-tuning is necessary to keep LLMs up-to-date.Even using the newest data, LLMs display an ongoing problem of hallucinations [13,38,44] by providing factually incorrect information.Retrieval Augmented Generation (RAG) was proposed [11,18] in order to address these issues as well as others and make LLMs more trustworthy.\n\nThe key idea behind RAG is to enhance the generative model's capabilities by integrating a retrieval system that can fetch relevant documents or passages from a large corpus of data.In this setting, when a query is received, the retrieval system first identifies and retrieves pertinent information, which is fed into the generative model's context for a more accurate and relevant response.Instead of the model storing information within its weights, RAG effectively leverages external knowledge, reducing Despite all these advances, we observe that no existing RAG scheme or evaluation methodology explicitly targets an important class of problems that come with a high degree of multi-aspectuality.These are problems that require combining several (potentially many) significantly different aspects in a single query.As a simple illustrative example of such a query, consider the question \"What car did Alexander the Great drive?\", and assume that the queried model has not been trained on history.When using RAG, to answer this question accurately, one would retrieve two documents, one describing Alexander the Great and one outlining the history of car manufacturing.However, the embeddings of these two documents could be far away from each other in the embedding space.At the same time, such queries are common in different industry settings, as indicated by extensive discussions with our industry collaborators.Imagine a chemical processing plant experiencing an equipment accident.One could use an LLM to find the accident cause, which might require the retrieval of multiple, potentially confidential documents to provide the necessary context.",
            "score": 0.6581314718280535,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 105,
                    "end": 308
                },
                {
                    "start": 308,
                    "end": 397
                },
                {
                    "start": 397,
                    "end": 488
                },
                {
                    "start": 488,
                    "end": 622
                },
                {
                    "start": 622,
                    "end": 762
                },
                {
                    "start": 764,
                    "end": 946
                },
                {
                    "start": 946,
                    "end": 1155
                },
                {
                    "start": 1155,
                    "end": 1465
                },
                {
                    "start": 1465,
                    "end": 1584
                },
                {
                    "start": 1584,
                    "end": 1765
                },
                {
                    "start": 1765,
                    "end": 1937
                },
                {
                    "start": 1937,
                    "end": 2041
                },
                {
                    "start": 2041,
                    "end": 2185
                },
                {
                    "start": 2185,
                    "end": 2256
                },
                {
                    "start": 2256,
                    "end": 2420
                }
            ],
            "ref_mentions": [
                {
                    "start": 386,
                    "end": 390,
                    "matchedPaperCorpusId": "263311025"
                },
                {
                    "start": 676,
                    "end": 679,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49560546875
        },
        {
            "corpus_id": "271270616",
            "title": "Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models",
            "text": "Retrieval-Augmented Generation (RAG) is applied to solve hallucination problems and real-time constraints of large language models, but it also induces vulnerabilities against retrieval corruption attacks. Existing research mainly explores the unreliability of RAG in white-box and closed-domain QA tasks. In this paper, we aim to reveal the vulnerabilities of Retrieval-Enhanced Generative (RAG) models when faced with black-box attacks for opinion manipulation. We explore the impact of such attacks on user cognition and decision-making, providing new insight to enhance the reliability and security of RAG models. We manipulate the ranking results of the retrieval model in RAG with instruction and use these results as data to train a surrogate model. By employing adversarial retrieval attack methods to the surrogate model, black-box transfer attacks on RAG are further realized. Experiments conducted on opinion datasets across multiple topics show that the proposed attack strategy can significantly alter the opinion polarity of the content generated by RAG. This demonstrates the model's vulnerability and, more importantly, reveals the potential negative impact on user cognition and decision-making, making it easier to mislead users into accepting incorrect or biased information.",
            "score": 0.6572877008776925,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.388427734375
        },
        {
            "corpus_id": "274283400",
            "title": "Adaptive Control of Retrieval-Augmented Generation for Large Language Models Through Reflective Tags",
            "text": "While retrieval-augmented generation (RAG) enhances large language models (LLMs), it also introduces challenges that can impact accuracy and performance. In practice, RAG can obscure the intrinsic strengths of LLMs. Firstly, LLMs may become too reliant on external retrieval, underutilizing their own knowledge and reasoning, which can diminish responsiveness. Secondly, RAG may introduce irrelevant or low-quality data, adding noise that disrupts generation, especially with complex tasks. This paper proposes an RAG framework that uses reflective tags to manage retrieval, evaluating documents in parallel and applying the chain-of-thought (CoT) technique for step-by-step generation. The model selects the highest quality content for final output. The key contributions are as follows: (1) reducing hallucinations by focusing on high-scoring documents; (2) improving real-time performance through efficient retrieval; and (3) mitigating negative effects by filtering out irrelevant information using parallel generation and reflective tagging. These innovations aim to optimize RAG for more reliable, high-quality results.",
            "score": 0.6568494606851605,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76953125
        },
        {
            "corpus_id": "270562128",
            "title": "Retrieval-Augmented Generation for Generative Artificial Intelligence in Medicine",
            "text": "Generative artificial intelligence (AI) has recently attracted widespread attention across various fields, including the GPT 1,2 and LLaMA [3][4][5] series for text generation; DALL-E 6 for image generation; as well as Sora 7 for video generation.In medicine, generative AI holds tremendous potential for applications in consulting, diagnosis, treatment, management, and education 8,9 .Additionally, the utilization of generative AI could enhance the quality of health services for patients while alleviating the workload for clinicians [9][10][11] .\n\nDespite this, we must consider the inherent limitations of generative AI models, which include susceptibility to biases from pre-training data 12 , lack of transparency, the potential to generate incorrect content, difficulty in maintaining up-to-date knowledge, among others 8 .For instance, large language models were shown to generate biased responses by adopting outdated race-based equations to estimate renal function 13 .In the process of image generation, biases related to gender, skin tone, and geo-cultural factors have been observed 14 .Similarly, for downstream tasks such as question answering and text summarization, the generated content is often factually inconsistent and lacks evidence for verification 15 .Moreover, due to their static knowledge and inability to access external data, generative AI models are unable to provide up to date clinical advice for physicians or effective personalized health management for patients 16 .\n\nIn tackling these challenges, retrieval-augmented generation (RAG) may provide a solution 17,18 .By providing models access to external data, RAG is capable of enhancing the accuracy of generated content.Specifically, a typical RAG framework consists of three parts (Figure 1): indexing, retrieval, and generation.In the indexing stage, external data is split into chunks, encoded into vectors, and stored into a vector database.In the retrieval stage, the user's query is encoded into a vector representation, and then the most relevant information is retrieved through similarity calculations between the query and the information in the vector database.In the generation stage, both the user's query and the retrieved relevant information are prompted to the model to generate content.",
            "score": 0.6567578294975183,
            "section_title": "Main Text",
            "char_start_offset": 12,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 247,
                    "end": 386
                },
                {
                    "start": 386,
                    "end": 550
                },
                {
                    "start": 552,
                    "end": 831
                },
                {
                    "start": 831,
                    "end": 980
                },
                {
                    "start": 980,
                    "end": 1101
                },
                {
                    "start": 1101,
                    "end": 1278
                },
                {
                    "start": 1278,
                    "end": 1503
                },
                {
                    "start": 1505,
                    "end": 1602
                },
                {
                    "start": 1602,
                    "end": 1709
                },
                {
                    "start": 1709,
                    "end": 1819
                },
                {
                    "start": 1819,
                    "end": 1934
                },
                {
                    "start": 1934,
                    "end": 2161
                },
                {
                    "start": 2161,
                    "end": 2293
                }
            ],
            "ref_mentions": [
                {
                    "start": 381,
                    "end": 383,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 383,
                    "end": 384,
                    "matchedPaperCorpusId": "260230800"
                },
                {
                    "start": 537,
                    "end": 540,
                    "matchedPaperCorpusId": "260230800"
                },
                {
                    "start": 540,
                    "end": 544,
                    "matchedPaperCorpusId": "268854975"
                },
                {
                    "start": 828,
                    "end": 829,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 976,
                    "end": 978,
                    "matchedPaperCorpusId": "264378028"
                },
                {
                    "start": 1499,
                    "end": 1501,
                    "matchedPaperCorpusId": "269348048"
                },
                {
                    "start": 1595,
                    "end": 1598,
                    "matchedPaperCorpusId": "269327466"
                },
                {
                    "start": 1598,
                    "end": 1600,
                    "matchedPaperCorpusId": "258740478"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67138671875
        },
        {
            "corpus_id": "267301416",
            "title": "The Power of Noise: Redefining Retrieval for RAG Systems",
            "text": "Retrieval-Augmented Generation (RAG) has recently emerged as a method to extend beyond the pre-trained knowledge of Large Language Models by augmenting the original prompt with relevant passages or documents retrieved by an Information Retrieval (IR) system. RAG has become increasingly important for Generative AI solutions, especially in enterprise settings or in any domain in which knowledge is constantly refreshed and cannot be memorized in the LLM. We argue here that the retrieval component of RAG systems, be it dense or sparse, deserves increased attention from the research community, and accordingly, we conduct the first comprehensive and systematic examination of the retrieval strategy of RAG systems. We focus, in particular, on the type of passages IR systems within a RAG solution should retrieve. Our analysis considers multiple factors, such as the relevance of the passages included in the prompt context, their position, and their number. One counter-intuitive finding of this work is that the retriever's highest-scoring documents that are not directly relevant to the query (e.g., do not contain the answer) negatively impact the effectiveness of the LLM. Even more surprising, we discovered that adding random documents in the prompt improves the LLM accuracy by up to 35%. These results highlight the need to investigate the appropriate strategies when integrating retrieval with LLMs, thereby laying the groundwork for future research in this area.",
            "score": 0.6566930050411388,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8203125
        },
        {
            "corpus_id": "272753572",
            "title": "MMSearch: Benchmarking the Potential of Large Models as Multi-modal Search Engines",
            "text": "RAG (Retrieval-Augmented Generation) is an effective strategy for enhancing model knowledge by retrieving relevant information from external sources (Fan et al., 2024). RAG has been leveraged in various scenarios including knowledge-intensive question answering (Borgeaud et al., 2022;Guu et al., 2020), machine translation (He et al., 2021), and hallucination elimination (B\u00e9chard & Ayala, 2024). Current works has focused on improving specific aspects of RAG. RG-RAG (Chan et al., 2024) proposes to refine the query for retrieval by decomposition and disambiguation. Self-RAG (Asai et al., 2023) incorporates the self-reflection of LLM to enhance the generation quality. The AI search engine could be viewed as a form of RAG with the Internet serving as the external knowledge source. Recently, MindSearch (Chen et al., 2024c) proposes an AI search engine framework to simulate the human minds in web information seeking. Meanwhile, multiple benchmarks of RAG (Yang et al., 2024b;Chen et al., 2024b) have been introduced to comprehensively evaluate a RAG system. However, both the current AI search engine and RAG benchmark are limited to the text-only setting, leaving the multimodal search engine and evaluation largely unexplored. To bridge this gap, we introduce MMSEARCH-ENGINE and MMSEARCH, a multimodal AI search engine pipeline and dataset designed to evaluate various multimodal scenarios.",
            "score": 0.6565912990637689,
            "section_title": "A RELATED WORK",
            "char_start_offset": 36563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1400
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 167,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 262,
                    "end": 285,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 285,
                    "end": 302,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 324,
                    "end": 341,
                    "matchedPaperCorpusId": "236460076"
                },
                {
                    "start": 808,
                    "end": 828,
                    "matchedPaperCorpusId": "269362546"
                },
                {
                    "start": 982,
                    "end": 1001,
                    "matchedPaperCorpusId": "261530434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7607421875
        },
        {
            "corpus_id": "271571143",
            "title": "Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework for Medical Applications",
            "text": "Retrieval-Augmented Generation (RAG) enhances the performance of large language models by leveraging external knowledge bases to retrieve relevant document segments through semantic similarity computations [23], [29]. This approach significantly reduces the incidence of hallucinations-defined as instances where generated content deviates from factual accuracy [38], [42]. Early research on RAG primarily focused on developing sparse or dense retrievers [19], [20], whereas contemporary studies have emphasized optimizing the inte-gration of RAG with Large Language Models (LLMs). These optimizations encompass the timing of retrieval, methodological enhancements, and refined utilization of contextual information to mitigate noise within retrieved documents [10], [12]. \n\nWith respect to adaptive retrieval strategies, the FLARE project has proposed two novel methods: proactive retrieval based on retrieval instructions and confidence-based proactive retrieval, both designed to mitigate unnecessary retrievals [18]. Furthermore, process optimization has evolved from the \"Rewrite-Retrieve-Read\" paradigm [24] to \"ITER-RETGEN\" implementing iterative retrieval to incrementally access more granular and comprehensive knowledge [30]. Concerning the management of noise within retrieved documents, studies conducted demonstrated that irrelevant noise documents do not necessarily deteriorate system performance; conversely, they can enhance accuracy by up to 35%. Conversely, documents incorrectly classified as relevant to the query introduce significant interference, substantially impacting the model's generative performance [9]. \n\nIn the medical domain, the application of RAG technology remains in its nascent stages. The MEDRAG system assessed performance variations across diverse retrievers and corpora in medical question-answering tasks [37]. The Self-BioRAG [16] project integrated Self-RAG [5] technology in medicine, optimizing it through reflective tokens that address retrieval timing, evaluate the relevance and supporting capacity of retrieved documents in answer generation, and assess the quality of generated outputs. Nevertheless, empirical evidence suggests that this approach fails to surpass the performance of models specifically optimized for medical datasets, potentially attributable to the limitations of smaller models in multitask integration, as demonstrated by the AUTOACT study [28].",
            "score": 0.6565256034082569,
            "section_title": "B. Retrieval-augmented Generation",
            "char_start_offset": 9292,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1634
                },
                {
                    "start": 1637,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2139
                },
                {
                    "start": 2140,
                    "end": 2419
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 210,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 455,
                    "end": 459,
                    "matchedPaperCorpusId": "259316759"
                },
                {
                    "start": 1871,
                    "end": 1875,
                    "matchedPaperCorpusId": "267312134"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8955078125
        },
        {
            "corpus_id": "273502291",
            "title": "Developing Retrieval Augmented Generation (RAG) based LLM Systems from PDFs: An Experience Report",
            "text": "This paper presents an experience report on the development of Retrieval Augmented Generation (RAG) systems using PDF documents as the primary data source. The RAG architecture combines generative capabilities of Large Language Models (LLMs) with the precision of information retrieval. This approach has the potential to redefine how we interact with and augment both structured and unstructured knowledge in generative models to enhance transparency, accuracy, and contextuality of responses. The paper details the end-to-end pipeline, from data collection, preprocessing, to retrieval indexing and response generation, highlighting technical challenges and practical solutions. We aim to offer insights to researchers and practitioners developing similar systems using two distinct approaches: OpenAI's Assistant API with GPT Series and Llama's open-source models. The practical implications of this research lie in enhancing the reliability of generative AI systems in various sectors where domain-specific knowledge and real-time information retrieval is important. The Python code used in this work is also available at: https://github.com/GPT-Laboratory/RAG-LLM-Development-Guidebook-from-PDFs.",
            "score": 0.6561795644753936,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50390625
        },
        {
            "corpus_id": "270688478",
            "title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems",
            "text": "Retrieval Augmented Generation (RAG) (Lewis et al., 2020) is an innovative approach that enhances the capabilities of Large Language Models (LLMs) by integrating retrieval mechanisms into the generative process.At its core, RAG operates by retrieving relevant information from a vast corpus of data and then generating coherent and contextually enriched responses based on this retrieved information.This dual process not only improves the accuracy and relevance of the generated content but also addresses some of the inherent limitations of standalone generative models, such as hallucinations (Huang et al., 2023) and context drift (Wang et al., 2022).The significance of RAG in natural language processing and artificial intelligence cannot be overstated.As the demand for more sophisticated and context-aware AI systems grows, the ability to generate information that is both accurate and contextually relevant becomes crucial (Gao et al., 2024).RAG achieves this by leveraging the vast amount of information available, ensuring that the outputs of the models are informed by up-todate and contextually appropriate data.This has profound implications for various applications, including conversational AI, information retrieval, and automated content generation (Shuster et al., 2021;Wang et al., 2024).Furthermore, RAG represents a paradigm shift in how we think about and utilize LLMs.Instead of relying solely on the generative power of these models, RAG harnesses the complementary strengths of retrieval systems.This synergy enables the creation of AI systems that are not only more knowledgeable but also more reliable and versatile in their applications (Izacard and Grave, 2021;Zhu et al., 2024).\n\nLLMs are the key component in RAG systems.They are initially pre-trained on the task of next token prediction (Radford et al., 2018), where the LLM learns to predict the next word in a sequence based on the context provided by the preceding words.This extensive pre-training phase involves processing vast amounts of text data, enabling the model to acquire a broad understanding of language, syntax, semantics, and general knowledge.We call this the \"base\" version.Following this pre-training phase, LLMs typically undergo two stages of refinement to enhance their performance and usability, whose output we call the \"instruct\".",
            "score": 0.6559800723402249,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 400
                },
                {
                    "start": 400,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 759
                },
                {
                    "start": 759,
                    "end": 951
                },
                {
                    "start": 951,
                    "end": 1125
                },
                {
                    "start": 1125,
                    "end": 1308
                },
                {
                    "start": 1308,
                    "end": 1392
                },
                {
                    "start": 1392,
                    "end": 1522
                },
                {
                    "start": 1522,
                    "end": 1709
                },
                {
                    "start": 1711,
                    "end": 1753
                },
                {
                    "start": 1753,
                    "end": 1958
                },
                {
                    "start": 1958,
                    "end": 2145
                },
                {
                    "start": 2145,
                    "end": 2177
                },
                {
                    "start": 2177,
                    "end": 2340
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68603515625
        },
        {
            "corpus_id": "276775668",
            "title": "SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity Reduction",
            "text": "Large Language Models (LLMs) have demonstrated improved generation performance by incorporating externally retrieved knowledge, a process known as retrieval-augmented generation (RAG). Despite the potential of this approach, existing studies evaluate RAG effectiveness by 1) assessing retrieval and generation components jointly, which obscures retrieval's distinct contribution, or 2) examining retrievers using traditional metrics such as NDCG, which creates a gap in understanding retrieval's true utility in the overall generation process. To address the above limitations, in this work, we introduce an automatic evaluation method that measures retrieval quality through the lens of information gain within the RAG framework. Specifically, we propose Semantic Perplexity (SePer), a metric that captures the LLM's internal belief about the correctness of the retrieved information. We quantify the utility of retrieval by the extent to which it reduces semantic perplexity post-retrieval. Extensive experiments demonstrate that SePer not only aligns closely with human preferences but also offers a more precise and efficient evaluation of retrieval utility across diverse RAG scenarios.",
            "score": 0.6558539948721187,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8896484375
        },
        {
            "corpus_id": "269148483",
            "title": "Introducing Super RAGs in Mistral 8x7B-v1",
            "text": "Retrieval-Augmented Generation (RAG) systems have emerged as pivotal tools in enhancing Language Model (LM) capabilities by incorporating external knowledge sources.Notably, RAG systems play a crucial role in mitigating inaccuracies inherent in LMs and facilitating real-time knowledge updates [6].These systems operate by retrieving pertinent information from extensive corpora, subsequently empowering the LM to produce outputs characterized by enhanced relevance and accuracy.However, the efficacy of RAGs hinges significantly upon the quality of the retrieved documents and their contextual relevance to the specific query under consideration [18].Thus, optimizing the selection and integration of retrieved documents is imperative to maximize the utility and effectiveness of RAG systems in augmenting LM performance.",
            "score": 0.6557354045362835,
            "section_title": "A. Background",
            "char_start_offset": 1137,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 165,
                    "end": 298
                },
                {
                    "start": 298,
                    "end": 479
                },
                {
                    "start": 479,
                    "end": 652
                },
                {
                    "start": 652,
                    "end": 822
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.609375
        },
        {
            "corpus_id": "267061013",
            "title": "Dynamic Q&A of Clinical Documents with Large Language Models",
            "text": "Retrieval Augmented Generation (RAG) has emerged as a transformative technique across diverse domains, revolutionizing information processing and decision-making processes. \n\nIn a work by Datta et al. [16], the authors propose two models, MAKG (Medical Appropriateness Knowledge Graph) and RAG-GPT (Retrieval Augmented Generation -Generative Pretrained Transformer). MAKG functions as an autonomous coarse-grained medical-inappropriateness vigilance model for payers and regulators, while RAG-GPT operates as a fine-grained LLM with human-in-the-loop for assessing medical appropriateness and inappropriateness. \n\nAnother work by Lewis et al. [34] introduces RAG models that combine parametric and non-parametric memory components to enhance sequence-to-sequence (seq2seq) models. By endowing pre-trained, parametric-memory generation models with a non-parametric memory, RAG models achieve state-of-the-art results in open-domain extractive question answering and knowledge-intensive generation tasks. \n\nIn the same vein of the previous works, our research focuses on RAG for medical question answering and exploring how RAG in this context is more efficient than traditional models fine-tuning.",
            "score": 0.655016150949804,
            "section_title": "Retrieval Augmented Generation (RAG) in Various Contexts",
            "char_start_offset": 11665,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 175,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 1002
                },
                {
                    "start": 1005,
                    "end": 1196
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "266126075"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81298828125
        },
        {
            "corpus_id": "276449952",
            "title": "DH-RAG: A Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue",
            "text": "Retrieval-Augmented Generation (RAG) \n\nRetrieval-Augmented Generation (RAG) systems significantly advance the capabilities of dialogue systems and question-answering tasks by amalgamating external knowledge bases with generative models. [22] introduces the RAG models, adeptly merging pre-trained parametric and nonparametric memories for enhanced language generation. Subsequent studies [23] introduce several enhancements to RAG models, focusing on refining retrieval [10,36] and enhancing generation capabilities [2,17]. Recent innovations include FLARE [43], which introduces a feedback loop augmented retrieval method to iteratively refine retrieval outcomes and bolster generation quality. Additionally, Sel-fRAG [3] presents a self-supervised retrieval-augmented framework that boosts both retrieval and generation processes through the strategic use of pseudo-labels generated by the model itself. Despite these significant advancements, the challenge of seamlessly integrating dynamic historical context in RAG models for multi-turn dialogues remains an elusive goal. Though achieve remarkable progress, most existing approaches continue to depend predominantly on static knowledge bases and do not adequately address the need to capture the evolving contextual nuances within conversations. This gap propels the development of DH-RAG in this paper, aimed at more effectively incorporating both static external knowledge and the transient context prevalent in ongoing dialogues, thereby enhancing the quality and coherence of multi-turn dialogue interactions.",
            "score": 0.6546536511376932,
            "section_title": "Related Work",
            "char_start_offset": 3838,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 39,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1568
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 241,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 470,
                    "end": 474,
                    "matchedPaperCorpusId": "258479968"
                },
                {
                    "start": 516,
                    "end": 519,
                    "matchedPaperCorpusId": "252186384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75146484375
        },
        {
            "corpus_id": "273654156",
            "title": "R3AG: First Workshop on Refined and Reliable Retrieval Augmented Generation",
            "text": "Retrieval-augmented generation (RAG) has gained wide attention as the key component to improve generative models with external knowledge augmentation from information retrieval. It has shown great prominence in enhancing the functionality and performance of large language model (LLM)-based applications. However, with the comprehensive application of RAG, more and more problems and limitations have been identified, thus urgently requiring further fundamental exploration to improve current RAG frameworks. This workshop aims to explore in depth how to conduct refined and reliable RAG for downstream AI tasks. To this end, we propose to organize the first R3AG workshop at SIGIR-AP 2024 to call for participants to re-examine and formulate the basic principles and practical implementation of refined and reliable RAG. The workshop serves as a platform for both academia and industry researchers to conduct discussions, share insights, and foster research to build the next generation of RAG systems. Participants will engage in discussions and presentations focusing on fundamental challenges, cutting-edge research, and potential pathways to improve RAG. At the end of the workshop, we aim to have a clearer understanding of how to improve the reliability and applicability of RAG with more robust information retrieval and language generation.",
            "score": 0.6546157031327288,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76611328125
        },
        {
            "corpus_id": "268819923",
            "title": "ARAGOG: Advanced RAG Output Grading",
            "text": "Large Language Models (LLMs) have significantly advanced the field of natural language processing, enabling a wide range of applications from text generation to question answering.However, integrating dynamic, external information remains a challenge for these models.Retrieval Augmented Generation (RAG) techniques address this limitation by incorporating external knowledge sources into the generation process, thus enhancing the models' ability to produce contextually relevant and informed outputs.This integration of retrieval mechanisms with generative models is a key development in improving the performance and versatility of LLMs, facilitating more accurate and context-aware responses.See Figure 1 for an overview of the standard RAG workflow.\n\nDespite the growing interest in RAG techniques within the domain of LLMs, the existing body of literature primarily consists of systematic reviews (Gao et al., 2024) and direct comparisons between successive state-of-the-art (SoTA) models (Gao et al., 2022;Jiang et al., 2023).This pattern reveals a notable gap: a comprehensive experimental comparison across a broad spectrum of advanced RAG techniques is missing.Such a comparison is crucial for understanding the relative strengths and weaknesses of these techniques in enhancing LLMs' performance across various tasks.This study seeks to contribute to bridging this gap by providing an extensive evaluation of multiple RAG techniques and their combinations, thereby offering insights into their efficacy and applicability in real-world scenarios.\n\nThe focus of this investigation is a spectrum of advanced RAG techniques aimed at optimizing the retrieval process.These techniques can be categorized into several areas: To evaluate the RAG techniques, this study leverages two metrics: Retrieval Precision and Answer Similarity (Tonic AI, 2023).Retrieval Precision measures the relevance of the retrieved context to the question asked, while Answer Similarity assesses how closely the system's answers align with reference responses, on a scale from 0 to 5.\n\nFigure 1: A high-level overview of the workflow within a Retrieval-Augmented Generation (RAG) system.This process diagram shows how a user query is processed by the system to retrieve relevant documents from a database and how these documents inform the generation of a response.",
            "score": 0.653401819039765,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 502
                },
                {
                    "start": 502,
                    "end": 696
                },
                {
                    "start": 696,
                    "end": 754
                },
                {
                    "start": 756,
                    "end": 1033
                },
                {
                    "start": 1033,
                    "end": 1171
                },
                {
                    "start": 1171,
                    "end": 1328
                },
                {
                    "start": 1328,
                    "end": 1556
                },
                {
                    "start": 1558,
                    "end": 1673
                },
                {
                    "start": 1673,
                    "end": 1854
                },
                {
                    "start": 1854,
                    "end": 2066
                },
                {
                    "start": 2068,
                    "end": 2169
                },
                {
                    "start": 2169,
                    "end": 2347
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94677734375
        },
        {
            "corpus_id": "277151020",
            "title": "Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval",
            "text": "Retrieval-Augmented Generation (RAG), initially introduced by [35], has rapidly become one of the most prominent methodologies for enhancing the generative capabilities of Large Language Models (LLMs) [11,45,47,51]. This approach significantly improves the accuracy and relevance of generated outputs by mitigating common issues such as \"hallucinations\" in LLMs [20,50]. One of RAG's distinctive features is its flexible architecture, enabling the interchange or update of its core components-the dataset, retriever, and LLM-without necessitating retraining or fine-tuning of the entire system [13,46]. Consequently, RAG has been widely adopted across various practical applications, including personal chatbots and specialized domain experts like medical diagnostic assistants [41]. \n\nThe increasing attention towards LLMs, both in industry and academia, underscores their remarkable ability to facilitate convincing linguistic interactions with humans [29,30,36,60]. However, adapting these models to new knowledge not available at training time poses significant challenges. For instance, in real-world scenarios involving virtual assistants [15,21,33], the knowledge base or tasks may evolve over time, requiring model adaptation through fine-tuning processes [2,17,58]. This can lead to catastrophic forgetting, where previously acquired knowledge is lost [38]. Alternatively, new knowledge can be appended to the input prompt via in-context learning (ICL) without altering the model parameters [6,19,37,53,55], a principle that underpins RAG systems. \n\nIn the context of RAG, a typical system comprises four principal components [45]: (i) a text embedder function , which maps textual information into a high-dimensional embedding space; (ii) a storage mechanism, often referred to as a vector store, that memorizes texts and their embedded representations; (iii) a similarity function, such as cosine similarity, used to evaluate the similarity between pairs of embedded text vectors; and (iv) a generative model, denoted as function  , typically an LLM, that produces output text based on input prompts and retrieved information.",
            "score": 0.6533909681456654,
            "section_title": "Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 11697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1556
                },
                {
                    "start": 1559,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 66,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 205,
                    "end": 208,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "261822526"
                },
                {
                    "start": 594,
                    "end": 598,
                    "matchedPaperCorpusId": "258479968"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "267112617"
                },
                {
                    "start": 961,
                    "end": 964,
                    "matchedPaperCorpusId": "220961531"
                },
                {
                    "start": 1149,
                    "end": 1152,
                    "matchedPaperCorpusId": "269914863"
                },
                {
                    "start": 1152,
                    "end": 1155,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 1500,
                    "end": 1503,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "273403982",
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "text": "Retrieval-augmented generation (RAG) is an advanced hybrid model architecture that augments natural language generation (NLG) with external retrieval mechanisms to enhance the model's knowledge base. Traditional large language models (LLMs) such as GPT-3 and BERT, which are pre-trained on vast corpora, rely entirely on their internal representations of knowledge, making them susceptible to issues like hallucinations-where the models generate plausible but incorrect information. These models cannot efficiently update their knowledge bases without retraining, making them less practical for dynamic, knowledge-intensive tasks like open-domain question answering and fact verification (Brown, T., et al. 2020). To overcome these limitations, the paper (Lewis et al. 2020) proposed the RAG architecture, which retrieves real-time, relevant external documents to ground the generated text in factual information. \n\nThe RAG model incorporates two key components: \n\n1. Retriever: This retrieves the most relevant documents from a corpus using techniques such as dense passage retrieval (DPR) (Karpukhin et. al. 2020) or traditional BM25 algorithms. 2. Generator: It synthesizes the retrieved documents into coherent, contextually relevant responses. \n\nRAG's strength lies in its ability to leverage external knowledge dynamically, allowing it to outperform generative models like GPT-3 and knowledge-grounded systems like BERT, which rely on static datasets. \n\nIn open-domain question answering, RAG has been demonstrated to be highly effective, consistently retrieving relevant information and improving the factual accuracy of the generated responses (Guu, K., et al. 2020). In addition to knowledge retrieval, RAG models excel at updating knowledge bases. Since the model fetches external documents for each query, it requires no retraining to incorporate the latest information. This flexibility makes RAG models particularly suitable for domains where information is constantly evolving, such as medical research, financial news, and legal proceedings. Furthermore, studies have shown that RAG models achieve superior results in a variety of knowledge-intensive tasks, including document summarization and, knowledge-grounded dialogues",
            "score": 0.65278911598739,
            "section_title": "Overview of RAG Models",
            "char_start_offset": 10564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 913
                },
                {
                    "start": 916,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1248
                },
                {
                    "start": 1251,
                    "end": 1457
                },
                {
                    "start": 1460,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2239
                }
            ],
            "ref_mentions": [
                {
                    "start": 755,
                    "end": 773,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87548828125
        },
        {
            "corpus_id": "271843432",
            "title": "A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning",
            "text": "Retrieval-augmented generation (RAG) is a framework enabling large language models (LLMs) to enhance their accuracy and reduce hallucinations by integrating external knowledge bases. In this paper, we introduce a hybrid RAG system enhanced through a comprehensive suite of optimizations that significantly improve retrieval quality, augment reasoning capabilities, and refine numerical computation ability. We refined the text chunks and tables in web pages, added attribute predictors to reduce hallucinations, conducted LLM Knowledge Extractor and Knowledge Graph Extractor, and finally built a reasoning strategy with all the references. We evaluated our system on the CRAG dataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and online evaluations demonstrate that our system significantly enhances complex reasoning capabilities. In local evaluations, we have significantly improved accuracy and reduced error rates compared to the baseline model, achieving a notable increase in scores. In the meanwhile, we have attained outstanding results in online assessments, demonstrating the performance and generalization capabilities of the proposed system. The source code for our system is released in \\url{https://gitlab.aicrowd.com/shizueyy/crag-new}.",
            "score": 0.6525139532981248,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69287109375
        },
        {
            "corpus_id": "277621182",
            "title": "Don't Lag, RAG: Training-Free Adversarial Detection Using RAG",
            "text": "Retrieval-Augmented Generation (RAG) integrates external knowledge into a generative model to improve both its generative capacity and semantic coherence. Formally, given a query Q, the model retrieves the top-k most relevant documents or embeddings R k from a database D: \n\nwhere S(\u2022, \u2022) is a similarity function. The query Q is then combined with R k within a generative function: \n\nIn our approach, this retrieval phase facilitates access to known adversarial patches, thereby enabling a more robust generative reasoning process. By incorporating historical data on diverse attack patterns, RAG-based defenses can dynamically adapt to novel threats while sustaining high efficacy against existing adversaries.",
            "score": 0.6523420744640105,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 9168,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 272
                },
                {
                    "start": 275,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 712
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65869140625
        },
        {
            "corpus_id": "272831924",
            "title": "IRSC: A Zero-shot Evaluation Benchmark for Information Retrieval through Semantic Comprehension in Retrieval-Augmented Generation Scenarios",
            "text": "The field of Retrieval-Augmented Generation (RAG) has gained significant attention, especially in addressing the limitations of Large Language models (LLMs) in providing accurate and contextually relevant information. This section reviews notable works in this domain and situates our contribution within the existing research. \n\nBenchmarking in RAG Chen et al. developed the Retrieval-Augmented Generation Benchmark (RGB) to evaluate LLMs on four abilities: noise robustness, negative rejection, information integration, and counterfactual robustness. Their findings highlight the need for nuanced evaluation metrics to improve RAG capabilities, as LLMs showed weaknesses in negative rejection, information integration, and handling false information (Chen et al., 2024b) . However, RGB primarily focuses on robustness aspects and does not provide comprehensive coverage of different retrieval tasks, which is crucial for real-world RAG applications.",
            "score": 0.6521627188465499,
            "section_title": "Related Work",
            "char_start_offset": 2956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 327
                },
                {
                    "start": 330,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 951
                }
            ],
            "ref_mentions": [
                {
                    "start": 752,
                    "end": 772,
                    "matchedPaperCorpusId": "261530434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86962890625
        },
        {
            "corpus_id": "276543876",
            "title": "Geographic Named Entity Matching and Evaluation Recommendation Using Multi-Objective Tasks: A Study Integrating a Large Language Model (LLM) and Retrieval-Augmented Generation (RAG)",
            "text": "Common deep learning models include LSTM and GRU [44,45]. With the development of large language model [46][47][48] technology, the Retrieval-Augmented Generation (RAG) framework was proposed initially by Patrick Lewis et al. [49] to address the limitations of large language models in handling domainspecific issues. Gao et al. reintroduced the RAG approach to mitigate the 'hallucination' phenomenon of large language models. Various RAG styles have since been proposed, generally categorized into Naive RAG, Advanced RAG, and Modular RAG, each focusing on different retrieval, generation, and enhancement techniques [50]. This framework enhances prediction capabilities by retrieving relevant information from a vast number of documents during the question-answering or text-generation stage and incorporating this information into the response. For example, Lin et al. [51] used the Retrieval-Augmented Dual Instruction Tuning (RA-DIT) method to enable pre-trained large language models with retrieval capabilities, significantly improving their performance in knowledge-intensive tasks. Wang-Chiew Tan et al. [52] innovated the RAG model architecture to improve the efficiency and accuracy of query answering tasks, while Lin et al. [53] enhanced retrieval performance and answer quality in visual question-answering tasks through fine-grained multimodal retrieval. Inspired by these developments, we propose an RAG-based address recommendation framework that uses new addresses and related information as an external knowledge base, combined with matching addresses from Geographical Named Entity Matching Models, to conduct address recommendation tasks.",
            "score": 0.65173250109841,
            "section_title": "Geographical Text Recommendation Based on Retrieval-Augmented Generation",
            "char_start_offset": 10342,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 58,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1660
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 53,
                    "matchedPaperCorpusId": "199510402"
                },
                {
                    "start": 53,
                    "end": 56,
                    "matchedPaperCorpusId": "233699286"
                },
                {
                    "start": 226,
                    "end": 230,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1238,
                    "end": 1242,
                    "matchedPaperCorpusId": "263310932"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7998046875
        },
        {
            "corpus_id": "272828029",
            "title": "Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support",
            "text": "Clients wishing to implement generative AI in the domain of IT Support and AIOps face two critical issues: domain coverage and model size constraints due to model choice limitations. Clients might choose to not use larger proprietary models such as GPT-4 due to cost and privacy concerns and so are limited to smaller models with potentially less domain coverage that do not generalize to the client's domain. Retrieval augmented generation is a common solution that addresses both of these issues: a retrieval system first retrieves the necessary domain knowledge which a smaller generative model leverages as context for generation. We present a system developed for a client in the IT Support domain for support case solution recommendation that combines retrieval augmented generation (RAG) for answer generation with an encoder-only model for classification and a generative large language model for query generation. We cover architecture details, data collection and annotation, development journey and preliminary validations, expected final deployment process and evaluation plans, and finally lessons learned.",
            "score": 0.6494820300784796,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49365234375
        },
        {
            "corpus_id": "270380174",
            "title": "Natural Language Interaction with a Household Electricity Knowledge-based Digital Twin",
            "text": "Retrieval Augmented Generation (RAG) is a technique that combines the capabilities of retrieval-based and generationbased models to improve information generation and accuracy. As depicted in Figure 1, in RAG, the LLM produces and answer based on three elements: a question, a query and context. The question is typically provided by a human in natural language. It is mapped by a transformation block to a query as depicted by steps 1 and 2 in Figure 1. The transformation step may embed the question and the embedding subsequently represents the query. It can also transform the question from natural language to an alternative query language such as SQL or SPARQL. The query resulting from the transformation block is then sent to a retrieval system that fetches relevant documents or pieces of information from a large dataset as represented by steps 3 and 4 in the figure. This system can use a vector database when the query is an embedding, and relational database in case of an SQL query or a knowledge graph in the case of a SPARQL query. The information retrieved by the system (i.e. embeddings, records, triples) is referred to as context that is send together with the question and query to the LLM that generates the answer as illustrated in steps 5 and 6. With this process, the generative model produces a coherent and contextually accurate response. This approach leverages the extensive factual knowledge in the retrieval database and the linguistic and contextual abilities of generative models. \n\nIn general, RAG is considered useful because it enhances the accuracy and reliability of generated content by grounding it in real, retrieved data, reducing the likelihood of generating hallucinations or incorrect information [18]. This makes RAG particularly valuable in applications requiring precise and factual responses, such as question answering, customer support, Fig. 2. Household Electricity Knowledge-based Digital Twin Visualization [12]. and content creation, where both relevance and correctness are critical. By combining retrieval and generation, RAG achieves a more robust and informed response generation process.",
            "score": 0.648358497231666,
            "section_title": "B. Background on RAG",
            "char_start_offset": 7482,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1513
                },
                {
                    "start": 1516,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2039
                },
                {
                    "start": 2040,
                    "end": 2147
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54638671875
        },
        {
            "corpus_id": "263835211",
            "title": "Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity",
            "text": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach to address certain limitations inherent to standalone LLMs, such as outdated information and the inability to memorize (Chase, 2022;Liu, 2022). These challenges are elaborated upon in Sec 4.2.1. Yet, while RAG offers solutions to some issues, it introduces its own set of challenges, including the potential for insufficient information and the misinterpretation of related data, as detailed in Sec 4.2.2. This subsection delves into various strategies devised to mitigate these challenges. Within the realm of retrieval-augmented generation, enhancement techniques can be broadly categorized into several pivotal areas: \n\n(1) The Normal Setting of Utilizing Retrieved Text for Generations (Sec 5.2.1): \n\n(2) Interactive Retrieval and Generation (Sec 5.2.2): Examples here include the integration of Chain-of-Thoughts steps into query retrieval (He et al., 2022) and the use of an LLMbased agent framework that taps into external knowledge APIs (Yao et al., 2023a). \n\n(3) Adapting LLMs to the RAG Setting (Sec 5.2.3): This involves methods like the one proposed by Peng et al. (2023), which combines a fixed LLM with a plug-and-play retrieval module. Another notable approach is REPLUG (Shi et al., 2023), a retrieval-augmented framework that treats the LLM as a black box and fine-tunes retrieval models using language modeling scores. \n\n(4) Retrieving from Additional Knowledge Bases (Sec 5.2.5 and Sec 5.2.4): This category includes methods that retrieve from external parametric memories (Chen et al., 2023a) or knowledge graphs (Zhang et al., 2023d) to enhance the model's knowledge base.",
            "score": 0.6483223318520104,
            "section_title": "On Retrieval-Augmented Generation",
            "char_start_offset": 107043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 780
                },
                {
                    "start": 783,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1414
                },
                {
                    "start": 1417,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1671
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 210,
                    "matchedPaperCorpusId": "253107178"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6943359375
        },
        {
            "corpus_id": "273375021",
            "title": "CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for Retrieval-Augmented Generation with Enhanced Data Diversity",
            "text": "Retrieval-Augmented Generation (RAG) aims to enhance large language models (LLMs) to generate more accurate and reliable answers with the help of the retrieved context from external knowledge sources, thereby reducing the incidence of hallucinations. Despite the advancements, evaluating these systems remains a crucial research area due to the following issues: (1) Limited data diversity: The insufficient diversity of knowledge sources and query types constrains the applicability of RAG systems; (2) Obscure problems location: Existing evaluation methods have difficulty in locating the stage of the RAG pipeline where problems occur; (3) Unstable retrieval evaluation: These methods often fail to effectively assess retrieval performance, particularly when the chunking strategy changes. To tackle these challenges, we propose a Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough evaluation across the entire RAG pipeline, including chunking, retrieval, reranking, and generation. To effectively evaluate the first three phases, we introduce multi-granularity keywords, including coarse-grained and fine-grained keywords, to assess the retrieved context instead of relying on the annotation of golden chunks. Moreover, we release a holistic benchmark dataset tailored for diverse data scenarios covering a wide range of document formats and query types. We demonstrate the utility of the CoFE-RAG framework by conducting experiments to evaluate each stage of RAG systems. Our evaluation method provides unique insights into the effectiveness of RAG systems in handling diverse data scenarios, offering a more nuanced understanding of their capabilities and limitations.",
            "score": 0.6458549456125813,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84765625
        },
        {
            "corpus_id": "265050661",
            "title": "Evaluating Generative Ad Hoc Information Retrieval",
            "text": "'Generative retrieval' or 'generative IR' are umbrella terms for a diversity of approaches that use generative models to solve retrieval tasks. 5 Following Arora et al. [6], Figure 3 categorizes these approaches into generation-augmented retrieval (GAR) and retrievalaugmented generation (RAG). Notably, GAR approaches create traditional list SERPs, while RAG approaches generate text SERPs. \n\nIn GAR approaches, generative models are used to enhance the traditional search architecture at indexing time or at query time. At indexing time, generative models can be used for augmenting documents [37,44,78,94,152] with confabulated or hallucinated content, or for replacing the standard indexing process with what are commonly termed 'differentiable indices' by, for instance, generating document identifiers like page titles [20,31,125], URLs [153], or (structured) string identifiers [124,128,148,151]. At query time, generative models can be used for augmenting queries [2,77], or for modeling relevance by, for instance, generating parts of existing documents from the query and retrieving the documents by string matching [11], by predicting a (re-)ranking directly [123], or by using special tokens as relevance signal [76,93,99,150].",
            "score": 0.6456899915756906,
            "section_title": "A Taxonomy of Generative Retrieval",
            "char_start_offset": 11570,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 391
                },
                {
                    "start": 394,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1239
                }
            ],
            "ref_mentions": [
                {
                    "start": 599,
                    "end": 602,
                    "matchedPaperCorpusId": "255545874"
                },
                {
                    "start": 602,
                    "end": 605,
                    "matchedPaperCorpusId": "216641912"
                },
                {
                    "start": 825,
                    "end": 829,
                    "matchedPaperCorpusId": "248118757"
                },
                {
                    "start": 829,
                    "end": 832,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 843,
                    "end": 848,
                    "matchedPaperCorpusId": "258714822"
                },
                {
                    "start": 885,
                    "end": 890,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 890,
                    "end": 894,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 894,
                    "end": 898,
                    "matchedPaperCorpusId": "255879096"
                },
                {
                    "start": 972,
                    "end": 975,
                    "matchedPaperCorpusId": "259123956"
                },
                {
                    "start": 1126,
                    "end": 1130,
                    "matchedPaperCorpusId": "248366293"
                },
                {
                    "start": 1170,
                    "end": 1175,
                    "matchedPaperCorpusId": "258212638"
                },
                {
                    "start": 1228,
                    "end": 1231,
                    "matchedPaperCorpusId": "212725651"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7021484375
        },
        {
            "corpus_id": "268513072",
            "title": "Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI",
            "text": "This method modifies only a small portion of the model's weights, reducing the number of parameters that need to be updated during fine-tuning.By focusing on these adaptable components, LoRA efficiently updates the model, maintaining performance while significantly lowering computational demands and memory usage.\n\nImproving the performance of Retrieval Augmented Generation (RAG) involves several strategic enhancements across data preparation, indexing, and query handling.To reduce computational time, we can explore various index types for better context retrieval.Additionally, we can also transform queries to better match the retrieval context.Each of these tactics aims at refining the interaction between the LLM and the data, ensuring more accurate, relevant, and efficient generation outcomes [20].",
            "score": 0.6443483513009005,
            "section_title": "B. Challenges in the Generative Models",
            "char_start_offset": 7772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 143,
                    "end": 314
                },
                {
                    "start": 316,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 570
                },
                {
                    "start": 570,
                    "end": 652
                },
                {
                    "start": 652,
                    "end": 810
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.458984375
        },
        {
            "corpus_id": "275119075",
            "title": "From Interests to Insights: An LLM Approach to Course Recommendations Using Natural Language Queries",
            "text": "Using a language model for course recommendation requires the model to access genuine course data. In recent times, language models are instructed to use data by generating a context and adding it to the prompt. It would be unrealistic to paste the entire collection of courses and course descriptions into a prompt. Instead, methods have been developed to provide an effective context from a body of knowledge. One popular paradigm for information retrieval is Retrieval Augmented Generation (RAG) [Lewis et al., 2021]. \n\nRAG has emerged as a powerful approach that combines information retrieval with generative AI to enhance the quality and reliability of generated content. The framework consists of two primary components: a retriever that identifies relevant information from data sources, and a generator that processes this information along with the input query [Guu et al., 2020][Lewis et al., 2021]. For retrieval, systems typically employ either sparse retrieval methods like BM25 [Robertson and Zaragoza, 2009] that analyze word statistics, or dense retrieval approaches that use embedding-based similarity [Karpukhin et al., 2020]. This retrieved information is then integrated with the original query through various augmentation strategies before being processed by the generator. \n\nThis approach offers several key advantages for course recommendation systems. First, RAG enables real-time access to the most current course information, unlike traditional language models with fixed knowledge [Mallen et al., 2023]. Second, it can effectively handle specialized course offerings and requirements by retrieving specific, relevant content on demand [Mallen et al., 2023]. Third, by grounding generation in retrieved course descriptions and data, RAG helps reduce model hallucination and improve recommendation accuracy [Carlini et al., 2021]. Recent developments in RAG architectures, such as Fusion-in-Decoder [Izacard and Grave, 2021] and RETRO [Borgeaud et al., 2022], have further enhanced the framework's ability to process and integrate retrieved information effectively.",
            "score": 0.6432726352722569,
            "section_title": "Retrieval Augmented Generation",
            "char_start_offset": 9376,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 520
                },
                {
                    "start": 523,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2092
                }
            ],
            "ref_mentions": [
                {
                    "start": 993,
                    "end": 1022,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1510,
                    "end": 1531,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 1664,
                    "end": 1685,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 1834,
                    "end": 1856,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1926,
                    "end": 1951,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69677734375
        },
        {
            "corpus_id": "269149146",
            "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
            "text": "Retrieval-Augmented Generation (RAG) [68] is a framework that combines the capabilities of large language models (LLMs) with external knowledge sources through a retrieval mechanism.Unlike traditional language models that generate text based solely on the text's internal representation, RAG models retrieve relevant information from a knowledge base (like a database or the internet) and integrate this into the generation process.The core components of RAG include the retrieval mechanism, which fetches relevant documents or data, and the generative model, which synthesizes the retrieved information into coherent and contextually relevant responses.The theoretical principles underpinning RAG stem from the need to enhance language models with the ability to access and utilize external, structured knowledge [69].This is in response to the limitations of traditional LLMs that rely solely on their pre-trained parameters for knowledge, which can be outdated or incomplete.The typical architecture of RAG systems involves a retriever that fetches relevant information from a database and a generator that incorporates this information into the final output, thus, the integration of these components allows the model to produce contextually enriched and factually accurate text [68].The incorporation of RAG into the workflow of LLMs, therefore, presents a sophisticated approach to augmenting the model's knowledge base beyond its pretraining, specifically tailored to the demands of the evolving nature of studies in SLRs.By allowing the model to access an external corpus of domain-specific literature the context within which the LLM operates, it attains the ability to be enriched while a critical countermeasure to the model's propensity for generating plausible yet factually incorrect information -hallucination -is mitigated.In the realm of SLRs, where the precision of synthesized knowledge is paramount, RAG's ability to draws upon relevant information from a targeted corpus helps ensure that the generative outputs of LLMs are anchored in verifiable data.\n\n2.4 Advancing LLMs for Specialized Domains: From Pretraining to Fine-Tuning\n\nIn training LLMs, they initially undergo pretraining on extensive, diverse datasets, acquiring a foundational grasp of both language and knowledge.",
            "score": 0.6431628849848846,
            "section_title": "RAG for Enhanced Factual Accuracy in SLRs",
            "char_start_offset": 18167,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 182,
                    "end": 432
                },
                {
                    "start": 432,
                    "end": 654
                },
                {
                    "start": 654,
                    "end": 819
                },
                {
                    "start": 819,
                    "end": 978
                },
                {
                    "start": 978,
                    "end": 1288
                },
                {
                    "start": 1288,
                    "end": 1529
                },
                {
                    "start": 1529,
                    "end": 1839
                },
                {
                    "start": 1839,
                    "end": 2073
                },
                {
                    "start": 2075,
                    "end": 2150
                },
                {
                    "start": 2152,
                    "end": 2299
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57958984375
        },
        {
            "corpus_id": "274788878",
            "title": "RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems",
            "text": "We present RAG Playground, an open-source framework for systematic evaluation of Retrieval-Augmented Generation (RAG) systems. The framework implements and compares three retrieval approaches: naive vector search, reranking, and hybrid vector-keyword search, combined with ReAct agents using different prompting strategies. We introduce a comprehensive evaluation framework with novel metrics and provide empirical results comparing different language models (Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our experiments demonstrate significant performance improvements through hybrid search methods and structured self-evaluation prompting, achieving up to 72.7% pass rate on our multi-metric evaluation framework. The results also highlight the importance of prompt engineering in RAG systems, with our custom-prompted agents showing consistent improvements in retrieval accuracy and response quality.",
            "score": 0.6410772020485098,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "278020653",
            "title": "ENHANCED RETRIEVAL-AUGMENTED GENERATION FOR STUDENT MENTAL HEALTH SUPPORT USING GENERATIVE ARTIFICIAL INTELLIGENCE",
            "text": "Generative AI, combined with Retrieval-Augmented Generation (RAG), enhances large language models (LLMs) by integrating dynamic information retrieval, significantly improving response accuracy and contextual relevance. This study explores the implementation of Generative AI-powered RAG in student mental health support systems, demonstrating how it mitigates hallucinations, incorporates real-time knowledge updates, and personalizes assistance. The proposed system integrates retrieval and generation components, leveraging vector-based search mechanisms to access domain-specific mental health knowledge. Comparative analysis with traditional LLMs highlights RAG\u2019s superior accuracy, reduced misinformation, and improved response reliability. Experimental evaluation using context precision, hit rate, faithfulness, and user satisfaction metrics validates the system's effectiveness. This research underscores the transformative potential of Generative AI-driven RAG in delivering scalable, evidence-based mental health support for students.",
            "score": 0.64075589794761,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9130859375
        },
        {
            "corpus_id": "270285886",
            "title": "XL-HeadTags: Leveraging Multimodal Retrieval Augmentation for the Multilingual Generation of News Headlines and Tags",
            "text": "Retrieval-Augmented Generation (RAG) represents a pivotal advancement in Natural Language Generation (NLG), addressing the issue of neural models' limited contextual understanding.Traditional neural models often falter when the input lacks comprehensive information for generating accurate outputs, particularly in complex realworld applications (Yu et al., 2022).To bridge this gap, KNNLM (Khandelwal et al., 2020) introduced a technique for augmenting language models with examples retrieved from a training text dataset, enhancing contextual relevance.Building on this, RETRO (Borgeaud et al., 2021) leveraged a vastly expanded text corpus, enabling models with a smaller footprint to achieve performance on par with GPT-3 (Brown et al., 2020).\n\nModels such as REALM (Guu et al., 2020) and RAG (Lewis et al., 2020b) incorporate Wikipedia passages as external knowledge bases, significantly boosting their efficacy in tasks like Question Answering.REALM focuses on encoding information through masked language modeling, whereas RAG employs an encoder-decoder structure for generative language tasks.\n\nExpanding on these concepts, MuRAG (Chen et al., 2022) stands out by integrating multimodal knowledge sources, encompassing both visual and textual data.This innovation extends the capabilities of knowledge-enhanced text generation, catering to the nuanced demands of intricate information landscapes.-w/I (K=15) 31.37 (-0.09) 12.63 (-0.10) 28.13 (-0.02) 8.58 (-0.17) 24.40 (-0.21) 0.71 (0.00) 70.84 (-0.03)Table 8: Headline Generation Evaluation.Selected Content (Only Important Sentences).The best results compared to their respective baseline models are marked in bold, and \u2206 gains are shown in round brackets and highlighted with green and red colors.",
            "score": 0.6405731566144804,
            "section_title": "G.4 Retrieval-Augmented Generation",
            "char_start_offset": 38590,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 364
                },
                {
                    "start": 364,
                    "end": 555
                },
                {
                    "start": 555,
                    "end": 747
                },
                {
                    "start": 749,
                    "end": 950
                },
                {
                    "start": 950,
                    "end": 1101
                },
                {
                    "start": 1103,
                    "end": 1256
                },
                {
                    "start": 1256,
                    "end": 1404
                },
                {
                    "start": 1404,
                    "end": 1510
                },
                {
                    "start": 1510,
                    "end": 1550
                },
                {
                    "start": 1550,
                    "end": 1594
                },
                {
                    "start": 1594,
                    "end": 1758
                }
            ],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 363,
                    "matchedPaperCorpusId": "222272210"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7392578125
        },
        {
            "corpus_id": "271244634",
            "title": "Exploring Advanced Large Language Models with LLMsuite",
            "text": "Despite advancements in training, tuning, and alignment techniques, large language models (LLMs) like ChatGPT and Gemini face inherent challenges that cannot be addressed by training alone.One significant issue is the temporal limitation of their knowledge base.This knowledge cutoff renders the model's information outdated.LLMs also struggle with complex mathematical tasks.When prompted to perform arithmetic operations, models may provide incorrect results as they attempt to predict the next token rather than execute precise calculations.Furthermore, LLMs are prone to hallucinations, generating plausible but incorrect information.To overcome these limitations, integrating external data sources and applications at inference time proves essential.Retrieval Augmented Generation (RAG) [23] is a framework designed to connect LLMs to external data, thus updating their knowledge base and improving accuracy without costly retraining.By accessing current information from external databases, vector stores, or APIs, models can generate more relevant and precise responses.RAG involves a retriever component that encodes user queries and searches an external data source for relevant information.This augmented query is then processed by the LLM to produce a more accurate completion.For example, in legal applications, RAG can enhance discovery phases by querying a corpus of legal documents, thus retrieving pertinent information efficiently.Figure 1 illustrates the architecture of an LLM-powered application using LangChain, demonstrating how various components such as tools, agents, and memory modules are integrated to facilitate complex task execution.This framework not only updates the model's knowledge but also mitigates hallucinations by grounding responses in external data.Implementing RAG requires careful handling of context window limitations and efficient data retrieval using techniques like vector stores, which facilitate rapid semantic searches.In Figure 2, we illustrate the Retrieval-Augmented Generation (RAG) framework, detailing its components and operational workflow to enhance the accuracy and reliability of generative AI models.\n\nIn conclusion, while LLMs face challenges like knowledge cutoffs, mathematical errors, and hallucinations, integrating external data sources through frameworks like RAG [23] can significantly enhance their performance, making them more reliable and useful in various applications (see slides at Tutorial LLMs Part 1).",
            "score": 0.6391011310708464,
            "section_title": "Retrieval-Augmented Generation (RAG) Framework",
            "char_start_offset": 9052,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 189,
                    "end": 262
                },
                {
                    "start": 262,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 544
                },
                {
                    "start": 544,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 755
                },
                {
                    "start": 755,
                    "end": 939
                },
                {
                    "start": 939,
                    "end": 1077
                },
                {
                    "start": 1077,
                    "end": 1200
                },
                {
                    "start": 1200,
                    "end": 1288
                },
                {
                    "start": 1288,
                    "end": 1448
                },
                {
                    "start": 1448,
                    "end": 1664
                },
                {
                    "start": 1664,
                    "end": 1792
                },
                {
                    "start": 1792,
                    "end": 1972
                },
                {
                    "start": 1972,
                    "end": 2165
                },
                {
                    "start": 2167,
                    "end": 2484
                }
            ],
            "ref_mentions": [
                {
                    "start": 792,
                    "end": 796,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 2336,
                    "end": 2340,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.396240234375
        },
        {
            "corpus_id": "270562128",
            "title": "Retrieval-Augmented Generation for Generative Artificial Intelligence in Medicine",
            "text": "Generative artificial intelligence (AI) has brought revolutionary innovations in various fields, including medicine. However, it also exhibits limitations. In response, retrieval-augmented generation (RAG) provides a potential solution, enabling models to generate more accurate contents by leveraging the retrieval of external knowledge. With the rapid advancement of generative AI, RAG can pave the way for connecting this transformative technology with medical applications and is expected to bring innovations in equity, reliability, and personalization to health care.",
            "score": 0.6390078327105704,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5166015625
        },
        {
            "corpus_id": "271212541",
            "title": "Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems",
            "text": "Retrieval Augmented Generation (RAG) [20] leverages a retriever that provides substantial external information to enhance the output of Large Language Models (LLMs).This strategy utilizes knowledge in a parameter-free manner and circumvents the high training costs associated with LLMs' parameterized knowledge.Furthermore, it alleviates hallucination issues, significantly enhancing the factual accuracy and relevance of the generated content.The concept of RAG is rooted in the DrQA framework [5], which marked the initial phase of integrating retrieval mechanisms with Language Models through heuristic retrievers like TF-IDF for sourcing evidence.Subsequently, RAG evolved with the introduction of Dense Passage Retrieval [15] and REALM [26].These methods utilize pre-trained transformers and are characterized by the joint optimization of retrieval and generation process.Recent advancements have extended RAG's capabilities by integrating Large Language Models (LLMs), with developments such as REPLUG [29] and IC-RALM [26] demonstrating the potent generalization abilities of LLMs in zero-shot or few-shot scenarios.These models can follow complex instructions, understand retrieved information, and utilize limited demonstrations for generating high-quality responses.",
            "score": 0.6386118488806979,
            "section_title": "Retrieval Augmented Generation",
            "char_start_offset": 27814,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 165,
                    "end": 311
                },
                {
                    "start": 311,
                    "end": 444
                },
                {
                    "start": 444,
                    "end": 651
                },
                {
                    "start": 651,
                    "end": 746
                },
                {
                    "start": 746,
                    "end": 877
                },
                {
                    "start": 877,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1276
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.654296875
        },
        {
            "corpus_id": "278170975",
            "title": "Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets",
            "text": "In recent years, Large Language Models (LLMs) have made significant progress in research and have grown increasingly popular [1]. However, LLMs face several challenges, including issues with hallucinations caused by insufficient context [2], as well as limitations in their learned content, which prevent them from addressing questions requiring specific or proprietary information [1]. To address these issues, [3] introduced Retrieval-Augmented Generation (RAG), which extends LLMs by integrating external knowledge sources for knowledge-intensive natural language processing (NLP) tasks. By incorporating domain-specific information, RAG systems enable tailored responses for specialized topics, improving accuracy, relevance, and contextual understanding. Since 2022, numerous RAG systems have demonstrated the effectiveness of this approach in overcoming some limitations of LLMs [1]. \n\nThese systems are applied across a wide range of NLP tasks and outperform in domain-specific scenarios by leveraging specialized knowledge to enhance performance and relevance [1]. RAG systems operate through three interconnected components: (1) indexing, which structures and organizes external knowledge bases; (2) retrieval, which identifies and extracts relevant documents from these sources; (3) and generation, which combines retrieved information with the input to produce a coherent, contextually relevant response using an LLM and prompt engineering techniques. These systems offer numerous customization options, including fine-tuning retrieval mechanisms, optimizing prompting techniques, refining generation models, and customizing knowledge base design [4], [5]. This flexibility raises the question of what settings should be used to configure RAG systems, and how these systems can be compared and evaluated to determine the most effective system for specific domains. This particular task of identifying optimal parameters in RAG systems is commonly referred to in the literature as RAG evaluation [6], [7]. During the RAG evaluation, each component of the RAG system is systematically evaluated within a predefined workflow to ensure that the system meets the overall quality standards [6]. The process begins with an input query from a prepared QA evaluation dataset, which is used to compute metrics such as retrieval accuracy and response relevance, providing statistical insight into the performance of the RAG components and thus a comprehensive understanding of their effectiveness [6].",
            "score": 0.6382037469774562,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 889
                },
                {
                    "start": 892,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2199
                },
                {
                    "start": 2200,
                    "end": 2501
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 240,
                    "matchedPaperCorpusId": "261530162"
                },
                {
                    "start": 1663,
                    "end": 1666,
                    "matchedPaperCorpusId": "271646356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8115234375
        },
        {
            "corpus_id": "277043297",
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "text": "With the ongoing advancements in the field of generative AI, specifically in RAG, numerous surveys have emerged. \n\nHowever, these surveys often focus on specific aspects of the field. They either concentrate solely on a single foundation of RAG or provide a concise overview of enhancement methods for RAG in limited scenarios. Most existing works emphasize text-related RAG tasks supported by large language models without delving into other modalities. The survey by Li et al. [131] offers a fundamental overview of RAG and discusses specific applications within the realm of text generation tasks. Recent surveys by Gao et al. and Fan et al. [63,73,94] explore RAG in the context of large language models, with a specific emphasis on query-oriented RAG enhancement methods. Wu et al. [246] delves into key technologies of RAG in the areas of retrieval, while simultaneously introducing its broad applications in natural language processing tasks. Several other works investigate a more general approach to RAG. Recent Works by Zhao et al. [282,283] extend RAG to multi-modal contexts, exploring its technologies and applications in broader AI-generated content (AIGC) scenarios. Another work by Peng et al. [184] examines how graph-structured information can aid more precise and comprehensive retrieval in RAG, enhancing relational knowledge acquisition and the generation of context-aware responses. In addition to concentrating on research pertaining to RAG technology, recent scholarship has increasingly directed its focus towards the assessment of RAG systems. A survey by Yu et al. [266] addresses the evaluation challenges Table 1. Comparison of Different RAG Surveys. LLM: whether the survey discusses RAG in the context of large language models; Multimodal: whether it covers multimodal RAG; Graph: whether it discusses graph-structured information in RAG; Advanced: the coverage of advanced RAG techniques; Evaluation: whether it addresses evaluation methods; Knowledge: whether it takes a knowledge-centric perspective.",
            "score": 0.6375238989754025,
            "section_title": "Related Surveys",
            "char_start_offset": 6672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 115,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 2034
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65966796875
        },
        {
            "corpus_id": "273532677",
            "title": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains",
            "text": "Retrieval-augmented generation (RAG) (Lewis et al., 2020;Gao et al., 2023;Guti\u00e9rrez et al., 2024;Asai et al., 2024) is a powerful technique that enhances large language models (LLMs) for various knowledge-intensive tasks such as question answering (QA) by incorporating external knowledge sources. This method not only customizes responses to handle long-tail knowledge but also avoids the need for costly model retraining (Ovadia et al., 2023). Additionally, RAG helps reduce the issue of LLM hallucination by ensuring responses are grounded in relevant evidence (Shuster et al., 2021), thereby improving the overall accuracy and reliability of LLM outputs. \n\n* Work done during an internship at Amazon. \n\nWhile extensive research has focused on developing effective (Asai et al., 2024;Lin et al., 2024;Liu et al., 2024) and efficient (Xu et al., 2024a) RAG systems for general-domain QA tasks, adapting RAG to specialized domains for LLMs poses significant challenges. These models often struggle with distribution shifts and fail to accurately extract information from domain-specific contexts (Miller et al., 2020;Liu et al., 2022). Moreover, directly using black-box LLMs (OpenAI, 2023;Anthropic, 2023;Wang et al., 2023b) in specialized domains raises concerns about privacy when dealing with sensitive proprietary data. It is essential to finetune LLMs on domain-relevant QA tasks to unlock the full potential of LLM-based RAG systems in specialized domains. \n\nDespite the critical need for domain-specific finetuning, the primary challenge lies in the acquisition of high-quality fine-tuning data towards RAG applications. Prior works rely on continuous pretraining (Chen et al., 2023;Zhang et al., 2024a) on specialized corpora or fine-tuning on domain-specific instruction-tuning data (Wu et al., 2024;Wadden et al., 2024). However, the mismatch between these general-purpose tasks and domain-specific QA hinders their effectiveness.",
            "score": 0.6374916953205679,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 704
                },
                {
                    "start": 707,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1464
                },
                {
                    "start": 1467,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1942
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 97,
                    "end": 115,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 564,
                    "end": 586,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 768,
                    "end": 787,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1097,
                    "end": 1118,
                    "matchedPaperCorpusId": "216867120"
                },
                {
                    "start": 1118,
                    "end": 1135,
                    "matchedPaperCorpusId": "237417170"
                },
                {
                    "start": 1794,
                    "end": 1811,
                    "matchedPaperCorpusId": "269136910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75634765625
        },
        {
            "corpus_id": "273229293",
            "title": "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation",
            "text": "In this paper, we introduce a novel retrieval augmented generation method, CoV-RAG. It can effectively mitigate hallucinations during internal generation stage and external retrieval stage in the RAG. Specifically, by integrating the chain of verification prompting into fine-tuned RAG generators, we can successfully identify and mitigate generation errors. In addition, the chain of verification prompting can also refine external contextual knowledge through re-retrieving the revised query. We conduct a various experiments to assess the effectiveness of CoV-RAG over different language model backbones. And experimental results demonstrate that the CoV-RAG can well detect the generation errors, and significantly improve the generation quality. Looking ahead, CoV-RAG paves the way for further research in refining knowledge augmentation strategies, contributing to the improvement of reliability and accuracy of RAG.",
            "score": 0.6360981696955473,
            "section_title": "Conclusion",
            "char_start_offset": 20200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 923
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.837890625
        },
        {
            "corpus_id": "273233795",
            "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed KV Caches for Chunked Text",
            "text": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) has achieved significant progress in natural language processing by integrating large language models (LLMs) with external knowledge databases. This integration enhances the ability of generative models to produce accurate, relevant, and context-rich responses. Recent studies (Borgeaud et al., 2022;Jiang et al., 2024;Trivedi et al., 2022;Ram et al., 2023) have demonstrated that RAG significantly outperforms pure generative models across various benchmarks, thereby gathering considerable amounts of research interests in various domains such as question answering (Siriwardhana et al., 2023;Han et al., 2024), code generation (Lu et al., 2022), and content creation (Khattab et al., 2022), etc. However, as a relative new research topic, the current RAG systems still suffer from some drawbacks, among which low performance and long latency are the most prominent ones. Addressing these problems would effectively make RAG more applicable to latency-sensitive LLM tasks. \n\nAs illustrated in Figure 1a, the workflow of a naive RAG system comprises two steps: retrieval and generation, combining offline preparation with online processing to enhance performance. In the offline phase, RAG utilizes embedding models such as BGE (Chen et al., 2024a)) and GTE (Li et al., 2023) to convert external knowledge sources (e.g., document chunks) into high-dimensional vectors, which are then indexed into a specialized vector database. Upon receiving a user request, RAG first accesses this vector database to perform a similarity search, retrieving documents that best match the request based on semantic content. Subsequently, RAG integrates the content of these retrieved documents with the original user request to form an augmented query, which is input into the LLM to generate a more informative and contextually relevant response (Topsakal & Akinci, 2023). Researchers have proposed various methods to optimize the performance of RAG systems. Some approaches modify the attention computation mechanism to reduce computational complexity (Wang et al., 2020;Choromanski et al., 2020;Monteiro et al., 2024).",
            "score": 0.6348296485145303,
            "section_title": "RELATED WORK",
            "char_start_offset": 4843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1032
                },
                {
                    "start": 1035,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2163
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 335,
                    "end": 358,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 398,
                    "end": 415,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 626,
                    "end": 653,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 1889,
                    "end": 1914,
                    "matchedPaperCorpusId": "260223847"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84912109375
        },
        {
            "corpus_id": "277994166",
            "title": "CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction",
            "text": "Recent advancements in AI infrastructure and methodologies have enabled training Large Language Models (LLMs) over internet-scale data. These models demonstrate impressive competence in answering a wide range of general queries. However, when applied to specialized domains such as addressing questions based on internal company documents, off-the-shelf LLMs exhibit significant limitations. They often lack access to latest information, have difficulty interpreting domain specific language, struggle with source attribution, are prone to hallucinations (Ji et al., 2023), and are prone to overly broad responses. \n\nTo overcome these challenges, two broad strategies have emerged. The first involves fine-tuning LLMs on domain-specific data. However, this approach is not only resource-intensive and requires frequent updates, but also risks unintended consequences such as catastrophic forgetting, where the model loses previously acquired general knowledge, thereby increasing the overall system complexity. The second, often more practical method is Retrieval-Augmented Generation (RAG). RAG is a process that combines information retrieval with text generation. It typically involves the following steps: (1) indexing a knowledge base of relevant information, (2) using a retrieval system to find content specifically relevant to a given user query, (3) providing the user query and the retrieved content to an LLM, instructing it to generate a response based on the retrieved content. RAG offers numerous benefits, including real-time access to up-to-date information, improved token generation (Khandelwal et al., 2019), reduced hallucinations, better source attribution (Gao et al., 2023a;Hsu et al., 2024) and overall superior response generation (Shuster et al., 2021;B\u00e9chard and Ayala, 2024). Additionally, RAG tends to be more cost-effective and transparent than full model fine-tuning. Examples of RAG-based products include Perplexity.ai (Perplexity AI, 2024), bing search, GPT Search etc. \n\nDespite enabling a novel information retrieval experience for users, RAG systems today face key limitations. Table 1 illustrates results of a Subject Matter Expert based auditing of a RAG based system. Shown is a metric \"Relative Mean Question Level Accuracy\", which captures relevancy of cited chunks, correctness and completeness of the answer(Sec.",
            "score": 0.6341158177680855,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 614
                },
                {
                    "start": 617,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2003
                },
                {
                    "start": 2006,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2356
                }
            ],
            "ref_mentions": [
                {
                    "start": 555,
                    "end": 572,
                    "matchedPaperCorpusId": "246652372"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8544921875
        },
        {
            "corpus_id": "277321530",
            "title": "What to Retrieve for Effective Retrieval-Augmented Code Generation? An Empirical Study and Beyond",
            "text": "Retrieval-augmented generation (RAG) enhances generative models by integrating external knowledge [25]. Traditional models, while coherent, often suffer from factual inaccuracies and knowledge gaps due to their reliance on fixed memory [49,57]. RAG addresses these limitations by incorporating a retrieval mechanism that fetches relevant information from large-scale knowledge sources, such as databases or document corpora, enabling more accurate and contextually grounded outputs. \n\nThe RAG framework includes a retriever, which ranks relevant information based on the query, and a generator, which conditions its output on both the query and retrieved content, ensuring linguistic fluency while incorporating external knowledge. This adaptability allows the model to stay current without expensive retraining. \n\nRAG has been successfully applied in domains requiring factual accuracy and knowledge grounding, including question answering [2,54], scientific text generation [14], and automated code synthesis [40]. By bridging retrieval-based and generative models, RAG advances the development of more reliable, context-aware AI systems. \n\nThe RAG process can be formulated as follows: \n\nwhere  represents the user query, { 1 , . . . ,   } denotes the candidate knowledge set, and  is the output generated by the language model. The function  (\u2022) retrieves relevant candidates from the knowledge base based on the given query, while  (\u2022) \n\ncombines the retrieved information with the original query and feeds it into the language model for response generation. Repository-level code generation is particularly well-suited for the RAG framework. Software repositories contain extensive historical code, library functions, API calls, and contextual information, making it impractical to rely solely on user queries for code generation. Moreover, due to the maximum input length constraints of LLMs, it is infeasible to import an entire repository into the model. RAG overcomes these limitations by retrieving relevant code snippets from the repository, enriching the model's context and improving the accuracy and coherence of generated code. Additionally, large codebases often contain implementations of similar functions. By retrieving and reusing high-quality existing code snippets, RAG ensures consistency in coding style and interface design while reducing redundancy and potential errors. Consequently, most LLM-based approaches for repository-level code generation incorporate RAG to enhance performance.",
            "score": 0.6339864404016826,
            "section_title": "Background 2.1 Retrieval-Augmented Generation",
            "char_start_offset": 6037,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 482
                },
                {
                    "start": 485,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1140
                },
                {
                    "start": 1143,
                    "end": 1188
                },
                {
                    "start": 1191,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2225
                },
                {
                    "start": 2226,
                    "end": 2397
                },
                {
                    "start": 2398,
                    "end": 2514
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 102,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 941,
                    "end": 944,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 976,
                    "end": 980,
                    "matchedPaperCorpusId": "273850363"
                },
                {
                    "start": 1011,
                    "end": 1015,
                    "matchedPaperCorpusId": "274895273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "267547568",
            "title": "Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs",
            "text": "The incorporation of large language models (LLMs) like GPT-4 into information retrieval represents a significant progress in the field. Retrieval Augmented Generation (RAG) is a crucial aspect of this integration, as it combines the capabilities of LLMs with dynamic information retrieval to improve the precision and pertinence of the generated responses. \n\nThe evolution from GPT to GPT-4 exemplifies major strides in natural language processing (NLP). These LLMs are built on sophisticated deep learning architectures, enabling them to comprehend and generate human-like text, significantly enriching user interaction with information retrieval systems [Brown et al., 2020]. \n\nRAG represents a novel approach in information retrieval, wherein the LLM not only generates responses but also retrieves and incorporates relevant external information in real-time. This mechanism allows the model to dynamically pull in data from a wide range of sources, ensuring that the answers provided are both current and contextually rich [Lewis et al., 2020]. RAG essentially combines the generative capabilities of models like GPT-4 with the information retrieval prowess of traditional search engines, leading to more accurate, informative, and up-to-date responses. \n\nIn practice, services such as Perplexity.ai [Per, 2023] and Bing AI Search [Bin, 2023] utilize LLMs enhanced with RAG to deliver a more sophisticated search experience. By leveraging RAG, these platforms are able to understand complex queries and generate responses that are not only contextually aware but also infused with the most relevant and recent information available across the web. This approach significantly surpasses traditional search methodologies, providing users with a comprehensive, concise, and highly informative answer [Fostikov, 2023]. \n\nCase studies focusing on the implementation of RAG in search technologies reveal a substantial improvement in user satisfaction. Users benefit from responses that are not only semantically aligned with their queries but also enriched with the latest information, offering a depth of understanding previously unattainable in standard search engines [Gao et al., 2023]. \n\nThe combination of LLMs, especially when combined with Retrieval Augmented Generation, signifies a significant transformation in the field of information retrieval.",
            "score": 0.6336816808055796,
            "section_title": "Large Language Models in Information Retrieval",
            "char_start_offset": 9096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 356
                },
                {
                    "start": 359,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 677
                },
                {
                    "start": 680,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1818
                },
                {
                    "start": 1821,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2188
                },
                {
                    "start": 2191,
                    "end": 2355
                }
            ],
            "ref_mentions": [
                {
                    "start": 656,
                    "end": 676,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64794921875
        },
        {
            "corpus_id": "271244429",
            "title": "Optimizing Query Generation for Enhanced Document Retrieval in RAG",
            "text": "In this paper, we tackled the issue of hallucinations in Retrieval-Augmented Generation (RAG) systems by optimizing query generation.Utilizing a top-k averaged query-document alignment score, we refined queries using Large Language Models (LLMs) to improve precision and computational efficiency in document retrieval.Our experiments demonstrated that these optimizations significantly reduce hallucinations and enhance document retrieval accuracy, achieving an average gain of 1.6%.This study highlights the significance of precise query generation in enhancing the dependability and effectiveness of RAG systems.Future work will focus on integrating more advanced query refinement techniques and applying our approach to a broader range of RAG applications.",
            "score": 0.6322767792476255,
            "section_title": "Conclusion",
            "char_start_offset": 11498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 133,
                    "end": 318
                },
                {
                    "start": 318,
                    "end": 483
                },
                {
                    "start": 483,
                    "end": 614
                },
                {
                    "start": 614,
                    "end": 759
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "269740933",
            "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
            "text": "Retrieval-augmented generation (RAG), a cutting-edge AI technique, has achieved remarkable success across various applications, including recommendation, molecule generation, protein representation, and software engineering, owing to the potent capabilities of retrieval in providing supplementary information to enhance generation performance.Recently, increasing efforts have been made to alleviate the limitations of large language models (LLMs), such as hallucination and out-of-date internal knowledge, by leveraging retrieval to provide the latest auxiliary information and teaching LLMs to harness the retrieved external knowledge.With the rapid advancements in retrieval-augmented large language models (RA-LLMs), there is a pressing need for a comprehensive and systematic overview.To bridge this gap, in this paper, we comprehensively review the RA-LLMs from the perspectives of morel architecture, training strategy, and application area, providing researchers with an in-depth understanding.Moreover, since the studies of RA-LLMs are still in the early stage, we also discuss the current limitations and several potential research directions for future research.",
            "score": 0.6321009623451715,
            "section_title": "CONCLUSION",
            "char_start_offset": 60360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 344
                },
                {
                    "start": 344,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 791
                },
                {
                    "start": 791,
                    "end": 1003
                },
                {
                    "start": 1003,
                    "end": 1174
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65869140625
        },
        {
            "corpus_id": "266977237",
            "title": "WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge",
            "text": "Retrieval-Augmented Generation (RAG) enhances Language Models by incorporating retrieved text, significantly improving performance in knowledgebased tasks, applicable in both fine-tuned and offthe-shelf scenarios (Gao et al., 2023;Gupta et al., 2024). Traditional RAG (Lewis et al., 2020), also known as Naive RAG, incorporates retrieval content to aid generation but faces key challenges: 1) varying retrieval quality, 2) generation of responses prone to inaccuracies, and 3) difficulties in coherently integrating retrieved-context with current tasks. To overcome the limitations of Naive RAG, advanced methods introduce more contextually rich information during inference. The DSP frame-  work (Khattab et al., 2022) facilitates an intricate exchange between frozen LMs and retrieval models, improving context richness, while PKG (Luo et al., 2023) allows LLMs to retrieve relevant information for complex tasks without altering their parameters. The working mechanism of WisdoM is similar to RAG, but different at the following aspects: \u2460 WisdoM utilizes LVLM to generate world knowledge to provide coherent and accurate context rather than retrieval, \u2461 WisdoM incorporates a contextual fusion mechanism to diminish noise within the context. For additional experimental analysis and discussion, please refer to \u00a7 5.3.",
            "score": 0.6317669737251208,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 7339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1321
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65673828125
        },
        {
            "corpus_id": "270870251",
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "text": "The contributions of this study are three-fold:\n\n\u2022 Through extensive experimentation, we thoroughly investigated existing RAG approaches and their combinations to identify and recommend optimal RAG practices.\n\n\u2022 We introduce a comprehensive framework of evaluation metrics and corresponding datasets to comprehensively assess the performance of retrieval-augmented generation models, covering general, specialized (or domain-specific), and RAG-related capabilities.\u2022 We demonstrate that the integration of multimodal retrieval techniques can substantially improve question-answering capabilities on visual inputs and speed up the generation of multimodal content through a strategy of \"retrieval as generation\".",
            "score": 0.6314901017668331,
            "section_title": "Retrieval Source",
            "char_start_offset": 4117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 49,
                    "end": 208
                },
                {
                    "start": 210,
                    "end": 465
                },
                {
                    "start": 465,
                    "end": 711
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "270620574",
            "title": "R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation",
            "text": "Retrieval augmented generation (RAG) (Lewis et al., 2020) significantly enhances the capabilities of large language models (LLMs) by integrating external, non-parametric knowledge provided by retrievers. In RAG framework, the retriever locates and looks up useful documents based on a given query, and then the LLM interacts with these retrieved results to generate a response. The coordination of retrieval and generation achieves impressive performance without additional training. Especially in domain-specific and knowledge-intensive Figure 1: A comparison between RAG and R 2 AG. R 2 AG employs a trainable R 2 -Former to bridge the semantic gap between retrievers and LLMs. Optionally, LLMs can be fine-tuned to understand the retrieval information further. \n\ntasks, RAG offers real-time knowledge with high interpretability to LLMs, effectively mitigating the hallucination problem (Mallen et al., 2023). \n\nHowever, there exists a semantic gap between retrievers and LLMs due to their vastly different training objectives and architectures (BehnamGhader et al., 2022). Specifically, retrievers, typically encoder architecture, are designed to retrieve the most relevant documents for a query (Zhu et al., 2023b). Conversely, LLMs, generally decoder architecture, are expected to answer questions based on their inherent knowledge or given documents. However, the interaction between retrievers and LLMs in RAG primarily relies on simple text concatenation (BehnamGhader et al., 2022). This poor communication strategy will lead to several challenges for LLMs. Externally, it is hard for LLMs to utilize more information from retrievers in separate processes. In RAG, the retrieved documents that only preserve sequential relationships are unidirectionally delivered to LLMs, and LLMs do not fully understand why retrievers provide the documents. \n\nParticularly, low-quality documents inevitably appear in retrieved results (Barnett et al., 2024), but LLMs have to accept this noise passively. Internally, it is hard for LLMs to handle all of the retrieved documents with their inherent knowledge. LLMs must process all the results and assess which documents are important, impacting their ability to generate accurate answers (Wu et al., 2024).",
            "score": 0.6312947114561412,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 889,
                    "end": 910,
                    "matchedPaperCorpusId": "254877603"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65283203125
        },
        {
            "corpus_id": "277104989",
            "title": "A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM",
            "text": "Different use cases for Generative AI and RAG are available in the literature. The use of vector-based databases also finds a place in the literature. LLMs have been shown to experience hallucinations in their responses to instruction-following tasks. Fine-tuning with new data may not prevent hallucinations and may be costly. Therefore, RAG was introduced to provide factual information and adapt to new information [17]. RAG uses a vector database. When a query is made to RAG, the most relevant data from this database is selected. This data is included in the prompt for LLMs. This process can combine existing information without requiring constant fine-tuning and provides contextbased information, increasing the clarity and relevance of responses. RAG provides the LLM with the ability to gain a focused understanding vis-\u00e0-vis a query. There are studies in the literature where RAG is used in NLP tasks [22][23][24][25]. Pal et al. sought to create unique and enjoyable music using Generative AI. When provided with a starting bar of music, the initial Discriminatory network, incorporating Support Vector Machines and Neural Nets, selects a note or chord to guide the subsequent bar. Building upon this chosen chord or note, a Generative Net, which includes Generative Pretrained Transformers (GPT-2) and LSTMs, then generates the entire bar of music. Their innovative two-step approach aims to closely emulate the process of real music composition to enhance the authenticity of the generated music [26]. \n\nKulkarni et al. proposed a Retrieval-Augmented Generator (RAG) approach for constructing a chatbot capable of addressing user queries using Frequently Asked Questions (FAQ) data. The chatbot employs an open API-based paid ChatGPT model as a Language Model (LLM), recognizing the potential to optimize LLM token usage and cost by leveraging previously retrieved context for specific query patterns. The authors employ Reinforcement Learning (RL) to optimize the number of LLM tokens within the constraints of a fixed retrieval model and LLM. The proposed RL-based optimization, coupled with a similarity threshold, achieves notable cost savings with a slight enhancement in accuracy.",
            "score": 0.6302146208494283,
            "section_title": "Related Works",
            "char_start_offset": 9365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1516
                },
                {
                    "start": 1519,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2201
                }
            ],
            "ref_mentions": [
                {
                    "start": 418,
                    "end": 422,
                    "matchedPaperCorpusId": "225930035"
                },
                {
                    "start": 913,
                    "end": 917,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 925,
                    "end": 929,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 1511,
                    "end": 1515,
                    "matchedPaperCorpusId": "241327053"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56494140625
        },
        {
            "corpus_id": "277272240",
            "title": "Retrieval Augmented Generation and Understanding in Vision: A Survey and New Outlook",
            "text": "Retrieval-augmented generation (RAG) has emerged as a pivotal technique in artificial intelligence (AI), particularly in enhancing the capabilities of large language models (LLMs) by enabling access to external, reliable, and up-to-date knowledge sources. In the context of AI-Generated Content (AIGC), RAG has proven invaluable by augmenting model outputs with supplementary, relevant information, thus improving their quality. Recently, the potential of RAG has extended beyond natural language processing, with emerging methods integrating retrieval-augmented strategies into the computer vision (CV) domain. These approaches aim to address the limitations of relying solely on internal model knowledge by incorporating authoritative external knowledge bases, thereby improving both the understanding and generation capabilities of vision models. This survey provides a comprehensive review of the current state of retrieval-augmented techniques in CV, focusing on two main areas: (I) visual understanding and (II) visual generation. In the realm of visual understanding, we systematically review tasks ranging from basic image recognition to complex applications such as medical report generation and multimodal question answering. For visual content generation, we examine the application of RAG in tasks related to image, video, and 3D generation. Furthermore, we explore recent advancements in RAG for embodied AI, with a particular focus on applications in planning, task execution, multimodal perception, interaction, and specialized domains. Given that the integration of retrieval-augmented techniques in CV is still in its early stages, we also highlight the key limitations of current approaches and propose future research directions to drive the development of this promising area.",
            "score": 0.6292202851007221,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.734375
        },
        {
            "corpus_id": "268509926",
            "title": "DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models",
            "text": "In recent years, large language models (LLMs) have made significant advancements across various natural language processing (NLP) tasks, quickly becoming a critical element in numerous AI applications (Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023a;Scao et al., 2022;Zhang et al., 2022). Despite their impressive capabilities, these models often produce text that seems coherent and plausible but factually incorrect, a problem commonly known as hallucination (Maynez et al., 2020;Zhou et al., 2020;Liu et al., 2021;Ji et al., 2023;Su et al., 2024). \n\nTo mitigate this issue, Retrieval-Augmented Generation (RAG) has emerged as a prominent solution. RAG enhances LLMs by retrieving and incorporating relevant information from external databases into the LLMs' inputs. It has demonstrated superior effectiveness across numerous NLP challenges (Khandelwal et al., 2019;Borgeaud et al., 2022;Lewis et al., 2020;Guu et al., 2020;Izacard and Grave, 2020;Jiang et al., 2022;Shi et al., 2023). Traditional methods of RAG typically rely on single-round retrieval, using the LLM's initial input to retrieve relevant information from external corpora. While this method is effective for straightforward tasks, it tends to fall short for complex multi-step tasks and long-form generation tasks (Jiang et al., 2023). In contrast, dynamic RAG (Trivedi et al., 2022;Borgeaud et al., 2022;Ram et al., 2023;Jiang et al., 2023) performs multiple times of retrieval during the generation process of LLMs. It includes two steps: identifying the optimal moment to activate the retrieval module (deciding when to retrieve), and crafting the appropriate query once retrieval is triggered (determining what to retrieve). Depending on when and what to retrieve, a variety types of methods have been proposed in this direction.",
            "score": 0.629085425220528,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1819
                }
            ],
            "ref_mentions": [
                {
                    "start": 533,
                    "end": 549,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 884,
                    "end": 906,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 906,
                    "end": 925,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 925,
                    "end": 942,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1369,
                    "end": 1391,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79638671875
        },
        {
            "corpus_id": "273345967",
            "title": "LoRE: Logit-Ranked Retriever Ensemble for Enhancing Open-Domain Question Answering",
            "text": "The development of retrieval-augmented LLMs has advanced significantly, focusing on improving the synergy between retrieval and generation processes. Shao et al. [17] introduced ITER-RETGEN, which iteratively enhances retrieval and generation for tasks like multi-hop QA and commonsense reasoning. It alternates between retrieval-augmented generation and generation-augmented retrieval, refining each step with the previous output. But, it faces challenges such as suboptimal utilization of retrieved information, with about 20% of contexts lacking actual answers, leading to inaccuracies or hallucinations due to positional bias and non-optimized iterative retrieval. \n\nAddressing similar issues, Sawarkar et al. [16] proposed Blended RAG, which incorporates semantic search and hybrid query-based retrievers to enhance the accuracy and relevancy of retrieved information. By using a combination of keyword-based, vector-based, and semantic-based searches, Blended RAG aims to mitigate the retrieval limitations observed in ITER-RETGEN by ensuring that more relevant documents are retrieved, thus improving the overall effectiveness of the RAG system. However, the complexity and computational intensity of implementing multiple retrieval strategies, as well as scalability concerns in real-world applications, remain significant challenges for Blended RAG. \n\nAnother approach by Ma et al. [9] focused on Query Rewriting within retrieval-augmented settings. Their \"Rewrite-Retrieve-Read\" framework aims to adapt the search query itself to better align with the information needs, thereby enhancing the relevancy and effectiveness of retrieval. This method reduces the dependency on extensive, domain-specific datasets by refining queries to work with existing open-domain data. Nonetheless, the system's reliance on high-quality training data and its limited ability to generalize across diverse datasets highlight persistent issues in the field. \n\nCheng et al. [3] introduced Lift Yourself Up, a retrieval-augmented text generation model using self-memory and external metrics like ROUGE/BLEU for more reliable and contextually relevant generation. \n\nThese studies highlight the need to refine retrieval and generation integration, addressing challenges like dynamic retrieval contexts, iterative process optimization, and scalability.",
            "score": 0.6290220721986022,
            "section_title": "Related Work",
            "char_start_offset": 4291,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 668
                },
                {
                    "start": 671,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1947
                },
                {
                    "start": 1950,
                    "end": 2150
                },
                {
                    "start": 2153,
                    "end": 2337
                }
            ],
            "ref_mentions": [
                {
                    "start": 1963,
                    "end": 1966,
                    "matchedPaperCorpusId": "258479968"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88916015625
        },
        {
            "corpus_id": "277065966",
            "title": "A Survey on the Optimization of Large Language Model-based Agents",
            "text": "RAG combines LLM with external information retrieval systems to enhance the relevance and accuracy of generated outputs. By retrieving relevant documents from external sources, RAG allows LLMs to address the knowledge constraints inherent in models. The evolution of RAG methods has been marked by significant advancements in retrieval and generation integration [44]. Early, Naive RAG methods focus on directly retrieving relevant documents to augment the generative process, improving the quality of responses in tasks requiring factual knowledge. To address the challenges of Naive RAG, Advanced RAG is introduced, refining the retrieval process by incorporating more effective ranking, filtering, and document selection strategies. Subsequently, Modular RAG introduces a modular framework that optimizes the retrieval and generative components independently. This modular approach enables task-specific optimizations, allowing for more flexibility and scalability in applications across different domains [8,193]. These advancements in RAG highlight its potential to enhance LLMs by enabling dynamic access to external knowledge, making them more adaptable and capable of addressing complex tasks in real-world scenarios.",
            "score": 0.6286856226502191,
            "section_title": "LLM-based RAG",
            "char_start_offset": 10969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1225
                }
            ],
            "ref_mentions": [
                {
                    "start": 1009,
                    "end": 1012,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1012,
                    "end": 1016,
                    "matchedPaperCorpusId": "252408513"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.703125
        },
        {
            "corpus_id": "277596113",
            "title": "OnRL-RAG: Real-Time Personalized Mental Health Dialogue System",
            "text": "Recent developments in Generative AI and large language models have made the approach of Retrieval-Augmented Generation, introduced by [Lewis et al., 2020], a favored one for answering complex questions with context. RAG combines a retrieval model that finds relevant data with the LLMs that use these data to generate responses, reducing the risk of generating inaccurate or unsupported information [Yang et al., 2019]. Despite the strengths, RAG still faces challenges, such as retrieving irrelevant or low-quality information that can lead to errors in the final response [Gao et al., 2023]. For instance, irrelevant information might skew the generation of answers, particularly in domains where the stakes are very high such as in medical or legal applications [Pipitone and Alami, 2024] Improving the retrieval component of the RAG is an important way to ensure that the RAG responses are accurate and contextually relevant [Sawarkar et al., 2024] [Mohsin et al., 2025a]. In this respect, hybrid retrieval methods provide a compelling example of improvement, combining dense and sparse retrieval techniques [Mandikal and Mooney, 2024]. The dense retrieval techniques use embeddings to capture semantic similarity, while the sparse retrieval methods ensure that keyword matching is exact. For instance, dense embeddings will find semantically similar phrases like \"climate change impact\" and \"global warming effects,\" whereas sparse embeddings will find exact matches for terms like \"CO2 emissions.\" The combination returns a balanced approach to the retrieval of relevant but precise data to improve the quality of the generated outputs. In addition, domain-specific retrievers trained on domain-specific datasets result in better performance [Chen et al., 2024a] [Mohsin et al., 2025b]. For example, in health care, domain-specific retrievers prove to be good at retrieving disease-specific studies to provide informa-tion very close to the needs in the specific domain. \n\nResearchers have developed various methods to offer improvements.",
            "score": 0.6270988476879793,
            "section_title": "Related Work",
            "char_start_offset": 7143,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1977
                },
                {
                    "start": 1980,
                    "end": 2045
                }
            ],
            "ref_mentions": [
                {
                    "start": 930,
                    "end": 952,
                    "matchedPaperCorpusId": "269043117"
                },
                {
                    "start": 952,
                    "end": 976,
                    "matchedPaperCorpusId": "266844268"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8076171875
        },
        {
            "corpus_id": "277349136",
            "title": "Unlocking the Potential of Past Research: Using Generative AI to Reconstruct Healthcare Simulation Models",
            "text": "The application of generative AI to practical domains, including computer simulation modeling, is an emerging and rapidly evolving field of research. Our study has been designed to account for the following lessons and challenges identified in the existing literature: \n\n\u2022 User Expertise: The effectiveness of generative AI can depend on the expertise and skill level of the user. \n\n\u2022 Mitigating Hallucination: Generative AI models are prone to hallucination (producing incorrect or fabricated outputs). This risk can be reduced through strategies such as prompt engineering, retrieval-augmented generation (RAG), and iterative refinement during interactions. \n\n\u2022 Model Validation: AI-generated models require thorough testing and validation at each stage of development to ensure reliability and accuracy. \n\n\u2022 Selection of Test Data: The choice of test data is critical to avoid data leakage, which can compromise the validity of results in generative AI studies. \n\n\u2022 Model Complexity: Current research has predominantly focused on applying generative AI to relatively simple models, leaving its performance with more complex systems largely unexplored. \n\n4 Methods overview \n\nOur study followed four stages: setup and model design (Stage 0); prompt engineering and code generation (Stage 1); internal replication (Stage 2); and evaluation and preservation (Stage 3). Figure 1 illustrates these stages and the activities carried out in each. For model generation, we used Perplexity.AI's standard model (free tier) that includes RAG from internet sources. The RAG functionality provides the model with up-to-date and new online sources about simulation and SimPy.",
            "score": 0.6259579579993808,
            "section_title": "Summary of lessons from the literature",
            "char_start_offset": 23516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 268
                },
                {
                    "start": 271,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 659
                },
                {
                    "start": 662,
                    "end": 806
                },
                {
                    "start": 809,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1154
                },
                {
                    "start": 1157,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1664
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.556640625
        },
        {
            "corpus_id": "270870251",
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "text": "In this study, we aim to identify optimal practices for implementing retrieval-augmented generation in order to improve the quality and reliability of content produced by large language models.We systematically assessed a range of potential solutions for each module within the RAG framework and recommended the most effective approach for each module.Furthermore, we introduced a comprehensive evaluation benchmark for RAG systems and conducted extensive experiments to determine the best practices among various alternatives.Our findings not only contribute to a deeper understanding of retrieval-augmented generation systems but also establish a foundation for future research.",
            "score": 0.6254129495633101,
            "section_title": "Conclusion",
            "char_start_offset": 32122,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 352
                },
                {
                    "start": 352,
                    "end": 527
                },
                {
                    "start": 527,
                    "end": 680
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8974609375
        },
        {
            "corpus_id": "275906944",
            "title": "Chain-of-Retrieval Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) integrates information retrieval techniques with generative models to enhance the quality and factual accuracy of generated content [18,19]. By equipping LLMs with the ability to browse the web [24], RAG systems can access real-time data, thereby providing responses that are both up-to-date and grounded. The relevance and quality of the retrieved information are pivotal for the efficacy of RAG systems. A substantial body of recent research has concentrated on developing better general-purpose text embeddings [16,33]. Nevertheless, text embeddings frequently face limitations in addressing complex queries due to their reliance on fixed-size vector representations for efficiency purposes. \n\nTo mitigate this constraint, contemporary research has extended the conventional paradigm of a single retrieval step followed by generation, advancing to multi-step iterative retrieval and generation [5]. FLARE [12] prompts an LLM to actively determine when and what to retrieve during the generation process. ITER-RETGEN [28] proposes to interleave retrieval-augmented generation with generation-augmented retrieval, demonstrating enhancements in multi-hop QA tasks. Similarly, IRCoT [31] employs a chain-of-thought methodology, which recursively refines the reasoning thought for subsequent retrieval steps. Self-RAG [1] empowers LLMs to adaptively retrieve, generate, and critique through self-reflection, thus improving factual accuracy and citation precision in open-domain QA and long-form generation tasks. Auto-RAG [38] utilizes heuristic rules and exact answer matching to construct intermediate retrieval steps, yet its performance remains significantly below that of state-of-the-art models. In this study, rather than exclusively on few-shot prompting or distillation from proprietary models, we propose a novel approach to explicitly train LLMs to iteratively retrieve and reason over relevant information. \n\nScaling Test-time Compute Instead of prompting LLMs to directly generate the final answer, Chainof-Thought (CoT) [34] demonstrates that letting the model to think step by step can drastically improve the performance on mathematical reasoning tasks. Tree-of-Thought (ToT) [37] extends the idea of CoT by adopting a tree structure, allowing the model to explore the search space more comprehensively.",
            "score": 0.6250435176301041,
            "section_title": "Related Work",
            "char_start_offset": 4679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 731
                },
                {
                    "start": 734,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1953
                },
                {
                    "start": 1956,
                    "end": 2204
                },
                {
                    "start": 2205,
                    "end": 2354
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 173,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 551,
                    "end": 555,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 945,
                    "end": 949,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 1056,
                    "end": 1060,
                    "matchedPaperCorpusId": "258866037"
                },
                {
                    "start": 1219,
                    "end": 1223,
                    "matchedPaperCorpusId": "254877499"
                },
                {
                    "start": 1353,
                    "end": 1356,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 2069,
                    "end": 2073,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75732421875
        },
        {
            "corpus_id": "268554288",
            "title": "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction",
            "text": "This paper focuses on Retrieval-Augmented Generation (RAG) system for black-box Large Language Models (LLMs), namely black-box RAG.In this section, we first give the definition of RAG and subsequently introduce the black-box RAG.\n\nRetrieval-Augmented Generation (RAG).Given a natural language question , an external knowledge corpus W and a generative language model M, a RAG system aims to help M generate more accurate and informative responses to  using a retrieval model R, which effectively retrieves relevant documents D = ( 1 ,  2 ,  3 , ...) from W. The form of introducing external knowledge to the language model varies, including modifying attention weights during generation, incorporating it into input prompts, or using it in post-calibration of the model output.Moreover, existing RAG methods typically require joint fine-tuning of the retriever and the language model (e.g.Atlas [15], REALM [9]).However, joint fine-tuning is unaffordable in amount of practical scenarios due to the extremely large parameter scale of LLMs.In these scenarios, we can alternatively treat an LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a RAG system, namely black-box RAG.Next, we introduce the definition of the black-box RAG.\n\nRetrieval-Augmented Generation System for Black-box LLM (Black-box RAG).A RAG system for blackbox LLM aims to enhance the generation capability of the black-box LLM M  by retrieving external knowledge without updating the LLM parameters.While the parameters of the black-box LLM M  are frozen, the parameters of the retrieval model R are learnable.Thus, the RAG system for black-box LLM only optimizes R to improve overall system performance, without modifying M  .Moreover, existing black-box RAG systems typically inject the retrieved documents D into M  by constructing an input prompt that concatenates the question  and documents D, which leverages the powerful in-context learning capabilities of the LLM.",
            "score": 0.624525636955708,
            "section_title": "Problem Formulation",
            "char_start_offset": 14673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 229
                },
                {
                    "start": 231,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 777
                },
                {
                    "start": 777,
                    "end": 889
                },
                {
                    "start": 889,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 1039
                },
                {
                    "start": 1039,
                    "end": 1204
                },
                {
                    "start": 1204,
                    "end": 1259
                },
                {
                    "start": 1261,
                    "end": 1333
                },
                {
                    "start": 1333,
                    "end": 1498
                },
                {
                    "start": 1498,
                    "end": 1609
                },
                {
                    "start": 1609,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1972
                }
            ],
            "ref_mentions": [
                {
                    "start": 895,
                    "end": 899,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 907,
                    "end": 910,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6015625
        },
        {
            "corpus_id": "273404015",
            "title": "FRAG: Toward Federated Vector Database Management for Collaborative and Secure Retrieval-Augmented Generation",
            "text": "In recent years, large-language models (LLMs) such as ChatGPT [36], LLaMA [47], BERT [11], and Transformer [40] have fundamentally transformed the landscape of natural language processing (NLP) and artificial intelligence (AI). These models, trained on vast amounts of textual data, have demonstrated unprecedented capabilities in generating human-like text, understanding complex queries, and performing tasks that require deep contextual understanding [7,11]. Applications leveraging LLMs span across numerous industries, from automated customer service and real-time translation to content creation and medical diagnostics. The flexibility and power of LLMs have made them central to many modern AI systems. \n\nA crucial enhancement in the effectiveness of LLMs has been the integration of Retrieval-Augmented Generation (RAG). Unlike purely generative models, RAG systems enhance their output by retrieving relevant external knowledge from vast databases or knowledge bases to augment their responses [27]. This mechanism allows LLMs to generate more informed, accurate, and contextually rich outputs by incorporating real-time or pre-stored information. For example, when answering factual queries, the model can retrieve up-to-date knowledge rather than relying solely on its pre-trained information. As a result, RAG has become a vital component in applications where the accuracy of the generated content is paramount, such as in recommendation systems, conversational agents, and information retrieval. \n\nAt the heart of RAG systems are vector embeddings, which represent pieces of data, such as words, sentences, or entire documents, as high-dimensional vectors. These embeddings capture semantic similarities between different entities, enabling efficient retrieval of related data. Vector databases like Faiss [23], Annoy [4], and HNSW [32] have been widely adopted for storing and querying these embeddings, offering optimized solutions for Approximate -Nearest Neighbor (ANN) searches on large-scale datasets [4,23,32]. These systems rely on specialized indexing and search algorithms to quickly identify the most relevant embeddings from millions or billions of records, making them indispensable in modern AI pipelines.",
            "score": 0.624168538616376,
            "section_title": "Introduction 1.Background",
            "char_start_offset": 28,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 710
                },
                {
                    "start": 713,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 111,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 454,
                    "end": 457,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1004,
                    "end": 1008,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1821,
                    "end": 1825,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1847,
                    "end": 1851,
                    "matchedPaperCorpusId": "8915893"
                },
                {
                    "start": 2025,
                    "end": 2028,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 2028,
                    "end": 2031,
                    "matchedPaperCorpusId": "8915893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65380859375
        },
        {
            "corpus_id": "273345235",
            "title": "Evaluating the Effectiveness and Efficiency of Demonstration Retrievers in RAG for Coding Tasks",
            "text": "Large language models (LLMs) have achieved remarkable success in natural language processing, but they still encounter significant limitations in the domain of knowledge-intensive tasks. In particular, LLMs are susceptible to \"hallucinations\" when confronted with queries that exceed the scope of their training data or necessitate the utilization of current information [1]. Retrieval-Augmented Generation (RAG) is the leading technique for improving LLMs by providing demonstrations from knowledge bases. By referencing external knowledge sources, RAG effectively mitigates the problem of generating factual inconsistency content and facilitates the continuous updating of knowledge [2]. \n\nRecently, the application of RAG has achieved promising results for various code-specific tasks such as Code Search [3], Program Synthesis [4], [5], and Assertion Generation [6]. In these tasks, LLM learns from contextual prompts consisting of task descriptions, queries, and additional demonstration examples without the need to fine-tune the model parameters. The retrieved demonstrations are typically used as the context to assist the pre-trained LLM in comprehending the task and regulating the generation behavior, which usually has a significant impact on the quality of the generated output. Therefore, it is important to retrieve appropriate demonstrations from a vast knowledge base. \n\nThe retriever plays a core role in retrieving relevant demonstrations from the external knowledge base and significantly affects the performance of RAG [7], [8]. Retrievers are typically classified into sparse and dense retrievers based on representation methods. Sparse retrievers operate at the token level, while dense retrievers operate at the level of latent semantics. The most widely-used sparse retriever, BM25 [9], ranks demonstrations based on term frequency (TF) and inverse document frequency (IDF) of the query. Dense retrievers perform retrieval by encoding the query and demonstrations into dense embedding representations and scoring each demonstration by its similarity with the query embedding. For instance, the popular RAG system Llamaindex [10] supports both BM25 and custom embedding encoders in RAG workflow.",
            "score": 0.6238041041616821,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1385
                },
                {
                    "start": 1388,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 808,
                    "end": 811,
                    "matchedPaperCorpusId": "269130676"
                },
                {
                    "start": 831,
                    "end": 834,
                    "matchedPaperCorpusId": "258180059"
                },
                {
                    "start": 836,
                    "end": 839,
                    "matchedPaperCorpusId": "252734952"
                },
                {
                    "start": 866,
                    "end": 869,
                    "matchedPaperCorpusId": "259860357"
                },
                {
                    "start": 1545,
                    "end": 1548,
                    "matchedPaperCorpusId": "269293655"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83447265625
        },
        {
            "corpus_id": "273023272",
            "title": "Quantifying reliance on external information over parametric knowledge during Retrieval Augmented Generation (RAG) using mechanistic analysis",
            "text": "Retrieval Augmented Generation (RAG) (Lewis et al., 2021)is a popular method to enhance a Language Model's (LLM) capability to reason and execute tasks by leveraging additional context provided during inference time (Shao et al., 2023) (Singh et al., 2023) (IngestAI, 2023). Additionally, researchers have also explored shortcomings of RAG systems, such as inconsistent responses (Liu et al., 2023) and only (Wu et al., 2024) delved into the balance between a model's internal knowledge and externally retrieved information, examining their practical value. \n\nSeveral research papers have proposed the approaches for editing knowledge in language model, including techniques like ROME (Meng et al., 2022a), MEMIT (Meng et al., 2022b) to update or correct facts. On the flip side, with the popularity of LLM integration for various tasks leveraging properitary, enterprise, and private data, the use of RAG framework has increased to tackle hallucinations while reasoning on new never seen before (out of distribution) knowledge tasks. However, a comprehensive study mechanistically probing of Langauge Model's behavior of choosing between information from RAG-generated context over intrinsic parametric knowledge has not been conducted to the best of our knowledge.",
            "score": 0.6236752455505116,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 557
                },
                {
                    "start": 560,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1266
                }
            ],
            "ref_mentions": [
                {
                    "start": 685,
                    "end": 705,
                    "matchedPaperCorpusId": "255825985"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65234375
        },
        {
            "corpus_id": "273403870",
            "title": "REFINE on Scarce Data: Retrieval Enhancement through Fine-Tuning via Model Fusion of Embedding Models",
            "text": "Retrieval augmented generation (RAG) pipelines are commonly used in tasks such as question-answering (QA), relying on retrieving relevant documents from a vector store computed using a pretrained embedding model. However, if the retrieved context is inaccurate, the answers generated using the large language model (LLM) may contain errors or hallucinations. Although pretrained embedding models have advanced, adapting them to new domains remains challenging. Fine-tuning is a potential solution, but industry settings often lack the necessary fine-tuning data. To address these challenges, we propose REFINE, a novel technique that generates synthetic data from available documents and then uses a model fusion approach to fine-tune embeddings for improved retrieval performance in new domains, while preserving out-of-domain capability. We conducted experiments on the two public datasets: SQUAD and RAG-12000 and a proprietary TOURISM dataset. Results demonstrate that even the standard fine-tuning with the proposed data augmentation technique outperforms the vanilla pretrained model. Furthermore, when combined with model fusion, the proposed approach achieves superior performance, with a 5.76% improvement in recall on the TOURISM dataset, and 6.58 % and 0.32% enhancement on SQUAD and RAG-12000 respectively.",
            "score": 0.6236259204481347,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80712890625
        },
        {
            "corpus_id": "271270246",
            "title": "Can Open-Source LLMs Compete with Commercial Models? Exploring the Few-Shot Performance of Current GPT Models in Biomedical Tasks",
            "text": "Retrieval augmented generation (RAG) is a technique [2] that combines information retrieval with language models to enhance their ability to generate relevant and factual text. In RAG, the language model is augmented with an external knowledge base or other information source, such as a collection of documents or web pages. When generating text, the model first retrieves relevant information, based on the input query, and then uses that information to guide the generation process. This process is applied in the BioASQ challenge, where the relevant information source is the annual baseline of PubMed. \n\nRAG has been shown to improve the factual accuracy of generated text compared to standalone language models [21]. It allows the model to access a vast amount of external knowledge and incorporate it into the generated output. RAG is particularly useful for tasks that require domain-specific knowledge or up-to-date information [3].",
            "score": 0.6232824541976582,
            "section_title": "Retrieval Augmented Generation (RAG)",
            "char_start_offset": 8906,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 941
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 55,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 717,
                    "end": 721,
                    "matchedPaperCorpusId": "233240939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6806640625
        },
        {
            "corpus_id": "269292881",
            "title": "Retrieval-Augmented Generation-based Relation Extraction",
            "text": "Retrieval-Augmented Generation (RAG) for large language models can be classified into two categories: i) naive RAG and ii) advanced RAG.Naive RAG has basic steps: retrieve, augmentation, and generation, while the advanced version includes a post-processing step before sending the retrieved information to a user [24].The concept of RAG has been suggested as a way to minimize the undesired alterations in Language Models (LLMs) when conversational systems built on LLMs generate arbitrary responses to a query [7].RAG is an example of open-book exams which are applied to the usage of LLMs.The retriever mechanism in RAG finds an example of the user query (prompt), and then the user query is regenerated along with the example by the data-augmentation module in RAG.Ovadia et al. [25] evaluates the knowledge injection capacities of both fine-tuning and the RAG approach and found that LLMs dealt with performance problems through unsupervised fine-tuning while RAG outperformed the fine-tuning approach in unsupervised learning.\n\nIn this work, we introduce a Retrieval-Augmented Generation-based Relation Extraction (RAG4RE) approach to identify the relationship between a pair of entities in a sentence.",
            "score": 0.6231551873965765,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 8393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 318
                },
                {
                    "start": 318,
                    "end": 515
                },
                {
                    "start": 515,
                    "end": 591
                },
                {
                    "start": 591,
                    "end": 768
                },
                {
                    "start": 768,
                    "end": 1031
                },
                {
                    "start": 1033,
                    "end": 1207
                }
            ],
            "ref_mentions": [
                {
                    "start": 511,
                    "end": 514,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93310546875
        },
        {
            "corpus_id": "269188036",
            "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
            "text": "To provide clarity and structure, this paper is organized as follows: Section 2 outlines the overall RAG workflow, dividing the methodologies into pre-retrieval, retrieval, post-retrieval, and generation phases. Sections 3 through 6 explore the core techniques within each phase. Section 7 focuses on the evaluation methodologies for RAG. Section 8 summarizes the reviewed studies, detailing the retrievers and generators used, while Section 9 discusses challenges and future research directions, extending beyond text-based studies to include multimodal data applications. The paper concludes with Section 10. \n\nOther related surveys provide valuable insights into the evolving RAG landscape from different angles. Gao et al. [38] identified three key stages in RAG development: pre-training enhancement, inference, and fine-tuning. Zhao et al. [162] focused on the diverse applications of RAG, including text, code, image, and video generation, emphasizing augmented intelligence in generative tasks. Meanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how interactions between retrievers, language models, and augmentations influence model architectures and applications. \n\nIn this paper, we aim to offer a comprehensive and unified framework for understanding RAG from an information retrieval (IR) perspective, identifying key challenges and areas for improvement. We delve into the core technologies that drive RAG, assessing their effectiveness in addressing retrieval and generation tasks. Additionally, this survey introduces the evaluation methods employed in RAG research, highlights current limitations, and proposes promising avenues for future exploration.",
            "score": 0.6225227730856241,
            "section_title": "Introduction",
            "char_start_offset": 2330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1212
                },
                {
                    "start": 1215,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1708
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89306640625
        },
        {
            "corpus_id": "269293007",
            "title": "A Survey on Efficient Inference for Large Language Models",
            "text": "Retrieval-Augmented Generation (RAG) [29] aims to improve the quality of LLMs' responses by incorporating external knowledge sources. RAG can be also viewed as a technique to improve the inference efficiency when handling a large amount of data. Instead of merging all information into an excessively long prompt, RAG only adds relevant retrieved information to the original prompt, ensuring that the model receives necessary information while reducing prompt length significantly. FLARE [30] uses predictions of upcoming sentences to proactively decide when and what information to retrieve. REPLUG [31] treats the LLM as a black box and augments it with a tuneable retrieval model. It prepends retrieved documents to the input for the frozen black-box LLM, and further utilizes the LLM to supervise the retrieval model. Self-RAG [32] enhances LLM's quality and factuality through retrieval and self-reflection. It introduces reflection tokens to make the LLM controllable during the inference phase.",
            "score": 0.6214238367617851,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 20663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1001
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61474609375
        },
        {
            "corpus_id": "268510182",
            "title": "Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases",
            "text": "We proposed an end-to-end system design towards utilizing Retrieval Augmented Generation (RAG) to improve the factual accuracy of Large Language Models (LLMs) for domain-specific and time-sensitive queries related to private knowledge-bases. Our system integrates RAG pipeline with upstream datasets processing and downstream performance evaluation. Addressing the challenge of LLM hallucinations, we finetune models with a curated dataset which originates from CMU's extensive resources and annotated with the teacher model. Our experiments demonstrate the system's effectiveness in generating more accurate answers to domain-specific and time-sensitive inquiries. The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets. This research highlights the potential of RAG systems in augmenting LLMs with external datasets for improved performance in knowledge-intensive tasks. Our code and models are available on Github.",
            "score": 0.6211288067422334,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80029296875
        },
        {
            "corpus_id": "271693376",
            "title": "A Survey of Mamba",
            "text": "Being among the most sophisticated techniques in AI, RAG can provide dependable and current external knowledge, offering significant utility for a multitude of tasks [30,100]. Large Language Models have recently showcased groundbreaking language comprehension and generation capabilities, despite encountering inherent limitations like hallucinations and outdated internal knowledge. In light of RAG's potent capacity to offer current and valuable supplementary information, Retrieval-Augmented LLMs have emerged to leverage extraneous knowledge databases for enhancing the generative quality of LLMs [22]. Similarly, RAG can be incorporated with Mamba language models to assist them in producing high-quality outputs, which is a promising future research direction.",
            "score": 0.6190896836743967,
            "section_title": "Retrieval-augmented Generation (RAG).",
            "char_start_offset": 85220,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 766
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 174,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 601,
                    "end": 605,
                    "matchedPaperCorpusId": "261530434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48876953125
        },
        {
            "corpus_id": "272600014",
            "title": "Experimenting with Legal AI Solutions: The Case of Question-Answering for Access to Justice",
            "text": "Retrieval Augmented Generation. Large language models often hallucinate and contain outdated information (Zhang et al., 2023). Retrieval augmented generation (RAG) is an emerging approach to reduce the prevalence of hallucinations by grounding a model's generations in a data source besides the model's weights. Retrieval has been used extensively in single-hop (Ke et al., 2024), multi-hop (Sun et al., 2023), and long-form open-ended question answering (Lin et al., 2023). With the rise of instruction-following language models (Touvron et al., 2023;Chung et al., 2022;Brown et al., 2020), retrieval methods often insert context directly into the context of the language model (Ma et al., 2023;Chen et al., 2023). We focus on this setting because it is possible to integrate with existing well-performing models (such as OpenAI's GPT-3.5), further supporting our humancentric goal of making legal AI more accessible. \n\nVery recently, commercial RAG efforts such as Cohere's Command R+ models, have been applied to legal domains with a focus on trustworthiness and data privacy (Gainer & Starostin, 2024). Their retrieval method passes the retrieved documents directly into the context of a language model, such that the model generations are grounded in the context provided. In this work, we build off this line of research by focusing on retrieval from a trusted source. \n\nDatasets and Legal AI Benchmarks. NLP has been applied to various fields in law, such as question answering, relation extraction, or text summarization (Zhong et al., 2020). \n\nPreviously, work was focused on domain-specific fine-tuned models (Chalkidis et al., 2020;Zheng et al., 2021). Recently, existing work has focused more on the ability of general LLMs to perform legal reasoning (Yu et al., 2022;Jiang & Yang, 2023;Blair-Stanek et al., 2023;Yu et al., 2023).",
            "score": 0.6183665268193367,
            "section_title": "Prior Work",
            "char_start_offset": 2956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 918
                },
                {
                    "start": 921,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1374
                },
                {
                    "start": 1377,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1550
                },
                {
                    "start": 1553,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1842
                }
            ],
            "ref_mentions": [
                {
                    "start": 391,
                    "end": 409,
                    "matchedPaperCorpusId": "252692968"
                },
                {
                    "start": 679,
                    "end": 696,
                    "matchedPaperCorpusId": "258841283"
                },
                {
                    "start": 1529,
                    "end": 1549,
                    "matchedPaperCorpusId": "216552897"
                },
                {
                    "start": 1643,
                    "end": 1662,
                    "matchedPaperCorpusId": "233296302"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3564453125
        },
        {
            "corpus_id": "276107971",
            "title": "Conversation AI Dialog for Medicare powered by Finetuning and Retrieval Augmented Generation",
            "text": "Retrieval Augmented Generation (RAG) is a frame-work that combines the strengths of retrieval sys-tems and generative language models. It aims to enhance the performance of language models by providing relevant contextual information from ex-ternal knowledge sources during the generation process. It consists of two main components: a re-triever and a generator. The retriever is responsible for retrieving relevant passages or documents from a knowledge base, given the input context. Vari-ous retrieval techniques can be used, such as sparse vector space models (e.g., BM25), dense vector rep-resentations (e.g., embeddings), or a combination of both. In our study we used embeddings. The retrieved passages are then fed into the genera-tor, which is typically a large pre-trained language model like GPT or BART. We used GPT in our study. The generator leverages the retrieved con-text to generate a more informed and knowledge-grounded output, drawing from the relevant information present in the retrieved passages. The retrieved passages are concatenated with the in-put context and provided as input to the genera-tor. Compared to traditional language models that rely solely on their pre-trained knowledge, RAG models can potentially generate more accurate and informative responses by dynamically retriev-ing and incorporating relevant external knowledge. This is particularly beneficial in domains where factual accuracy and knowledge grounding are crucial, such as question-answering, dialog systems, and knowledgeintensive applications. In our project, we use the RAG framework using GPT to enhance the performance of our",
            "score": 0.6169489594600877,
            "section_title": "RAG (Retreival Augmented Generation)",
            "char_start_offset": 4732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1634
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63134765625
        },
        {
            "corpus_id": "269149041",
            "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
            "text": "RAG-Memory-Finetuning is a technique to optimize the performance of large language models (LLMs) by combining Retrieval Augmented Generation (RAG) with fine-tuning.This approach aims to connect LLMs to external knowledge sources 56 AgentLM-70B:https://huggingface.co/THUDM/agentlm-70B 57 AgentLM-7B:https://huggingface.co/THUDM/agentlm-7b 58 AgentLM-7B:https://huggingface.co/THUDM/agentlm-13B 59 AgentTuning Github:https://github.com/THUDM/AgentTuningNavigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies 2023-12-20 through a retrieval mechanism and combine it with generative capabilities to search and integrate relevant information from knowledge bases.The goal of RAG-Memory-Finetuning is to improve the consistency and reliability of the output and reduce hallucination issues.Specifically,the RAG technique is implemented through the following steps:\n\n1) Chunking an external domain-specific knowledge base into small documents,each about 150 words.2) Creating embeddings using a pre-trained model and storing document vectors in a vector database.3) When an input query is passed to the LLM,the most relevant information is retrieved from the external database using metrics such as cosine similarity and combined with the LLM as additional context.4) This external context and the input prompt are passed together to the text generator to produce the output response.Responses generated by RAG are more factual,specific,and diverse.The parameter knowledge provided by traditional finetuning techniques is static,while RAG allows us to bypass retraining and obtain up-to-date information through retrievalbased generation to produce reliable outputs.M gen predicts IsUse given x, y t 12: end if through self-reflection and also addresses the hallucination issues present in large models.\n\nAlgorithm 2 present an overview of SELF-RAG [4] at inference.For every x and preceding generation y < t,the model decodes a retrieval token to evaluate the utility of retrieval.",
            "score": 0.6158534338107702,
            "section_title": "XIII. RAG-MEMORY-FINETUNING",
            "char_start_offset": 144027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 730
                },
                {
                    "start": 730,
                    "end": 856
                },
                {
                    "start": 856,
                    "end": 930
                },
                {
                    "start": 932,
                    "end": 1029
                },
                {
                    "start": 1029,
                    "end": 1128
                },
                {
                    "start": 1128,
                    "end": 1330
                },
                {
                    "start": 1330,
                    "end": 1449
                },
                {
                    "start": 1449,
                    "end": 1514
                },
                {
                    "start": 1514,
                    "end": 1731
                },
                {
                    "start": 1731,
                    "end": 1868
                },
                {
                    "start": 1870,
                    "end": 1931
                },
                {
                    "start": 1931,
                    "end": 2047
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66796875
        },
        {
            "corpus_id": "277955764",
            "title": "FinSage: A Multi-aspect RAG System for Financial Filings Question Answering",
            "text": "Despite their strong empirical performance, large language models (LLMs) are inherently bottlenecked by their fixed parametric size, limiting their ability to store factual knowledge, which can affect their performance on specific tasks. Retrieval-Augmented Generation (RAG) [9,27] emerged as a solution by integrating external knowledge that the LLM could access through retrieval within an external database. A retrieval module first selects relevant documents from sources like news, academic papers, or social media, which are then combined with the input query and fed into a language model to generate a response. This approach leverages both the model's internal memory and the retrieved corpus [50], ensuring more accurate and contextually grounded outputs. \n\nRecent studies have explored various improvements, such as feedback tokens for relevance [3], self-referential knowledge acquisition [40], iterative self-feedback [25], and domain-specific hierarchies [39]. In cases where initial queries lack sufficient detail, generative retrieval enhances information retrieval by generating queries or synthesizing passages beyond traditional keyword-based search [2,22,30]. Some studies further work to use multi-head RAG to extract information from diverse sources [6,41], aiming to maximize answer accuracy through quantity-based retrieval. Others focus on enhancing answer precision via quality retrieval, leveraging re-rankers [13,16,21] to preserve high answer relevancy.",
            "score": 0.6149592734967603,
            "section_title": "Related Work 2.1 Retrieval Augmented Generation",
            "char_start_offset": 4576,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1482
                }
            ],
            "ref_mentions": [
                {
                    "start": 275,
                    "end": 278,
                    "matchedPaperCorpusId": "250340214"
                },
                {
                    "start": 278,
                    "end": 281,
                    "matchedPaperCorpusId": "273185432"
                },
                {
                    "start": 857,
                    "end": 860,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 901,
                    "end": 905,
                    "matchedPaperCorpusId": "263828724"
                },
                {
                    "start": 1444,
                    "end": 1447,
                    "matchedPaperCorpusId": "269137220"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7880859375
        },
        {
            "corpus_id": "277043297",
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) [127] has emerged as a key approach that integrates information retrieval with generative models to enhance natural language processing tasks. By leveraging external knowledge sources, RAG systems can generate more accurate and contextually relevant outputs, addressing complex challenges in areas like question answering [119], summarization [85], and open-domain dialogue. In recent years, a variety of RAG methods have been proposed, ranging from basic retrieval-augmented models to more advanced architectures incorporating multi-hop [190] reasoning and memory-augmented techniques [67]. These developments have highlighted the potential of RAG to improve the performance of NLP systems by dynamically combining retrieval and generation in a unified framework. 1 https://github.com/USTCAGI/Awesome-Papers-Retrieval-Augmented-Generation Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \n\nRAG models augment traditional language models by incorporating external knowledge sources, such as documents, databases, or structured data [93,103]., during the generation process. Unlike conventional models that rely solely on pre-trained parameters, RAG systems dynamically retrieve relevant information at generation time, allowing them to produce more informed and contextually accurate outputs. This approach addresses key limitations of traditional language models, such as their inability to access real-time or domain-specific knowledge, and mitigates the challenge of handling out-of-vocabulary or rare entities. For example, in question answering tasks [62,192], RAG models retrieve relevant passages from large corpora to generate more precise and informative answers, while in summarization [85,172], they leverage external documents to provide richer and more comprehensive summaries.",
            "score": 0.614925080614174,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1462
                },
                {
                    "start": 1465,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2364
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 42,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 359,
                    "end": 364,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 380,
                    "end": 384,
                    "matchedPaperCorpusId": "226965071"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79931640625
        },
        {
            "corpus_id": "273374817",
            "title": "Evaluation of Attribution Bias in Retrieval-Augmented Large Language Models",
            "text": "Retrieval Augmented Generation. Given a question q and a set of top-k retrieved documents D ={d 1 , d 2 , . . . , d k } from a collection C, the goal of retrieval augmented generation (RAG) is to generate an answer for q using D as context. LLMs are currently an important component of RAG pipelines, acting as the generator. The generator is given q, D, and an instruction prompt on how to generate the answer (Jeong et al., 2024;Lee et al., 2024;Li et al., 2024a). Using top-k retrieved documents helps LLMs to be exposed to information that it might not have been trained/fine-tuned with during development. These documents are commonly retrieved using an effective sparse and/or dense retriever (Lewis et al., 2020). \n\nAttributive RAG. LLMs are prone to generate hallucinated (and even factually incorrect) answers (Ji et al., 2023;Rawte et al., 2023;Yue et al., 2024). \n\nAttributing answers in RAG with LLMs is an approach taken as a step towards ensuring the veracity of the output of these models (Bohnet et al., 2022;Hu et al., 2024;Kamalloo et al., 2023;Khalifa et al.;Li et al., 2024b). Menick et al. (2022) teach language models to support answers with verified quotes. Ye et al. (2024) propose a learning-based framework in which they fine-tune LLMs to generate citations, as opposed to prompting or relying on post-hoc attribution. Stolfo (2024) analyzes whether every generated sentence in the output of LLMs is grounded in the retrieved documents or the LLM's pre-training data.",
            "score": 0.6145933525803855,
            "section_title": "Background",
            "char_start_offset": 4962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1493
                }
            ],
            "ref_mentions": [
                {
                    "start": 411,
                    "end": 431,
                    "matchedPaperCorpusId": "268553748"
                },
                {
                    "start": 431,
                    "end": 448,
                    "matchedPaperCorpusId": "270514562"
                },
                {
                    "start": 448,
                    "end": 465,
                    "matchedPaperCorpusId": "259501744"
                },
                {
                    "start": 699,
                    "end": 719,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 819,
                    "end": 836,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 836,
                    "end": 855,
                    "matchedPaperCorpusId": "263831293"
                },
                {
                    "start": 855,
                    "end": 872,
                    "matchedPaperCorpusId": "268667523"
                },
                {
                    "start": 1063,
                    "end": 1078,
                    "matchedPaperCorpusId": "268819100"
                },
                {
                    "start": 1078,
                    "end": 1095,
                    "matchedPaperCorpusId": "263830219"
                },
                {
                    "start": 1181,
                    "end": 1197,
                    "matchedPaperCorpusId": "265220884"
                },
                {
                    "start": 1345,
                    "end": 1358,
                    "matchedPaperCorpusId": "269033410"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62744140625
        },
        {
            "corpus_id": "270620041",
            "title": "Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata",
            "text": "Large Language Models (LLMs) have shown remarkable language understanding and generation abilities [10,13]. However, there are two main challenges: static knowledge [8] and generative hallucination [5]. Retrieval-augmented generation [6] is an established process for answering user questions over entire datasets. RAG also helps mitigate generative hallucination and provides LLM with new information on which it was not trained [11]. Real-world RAG pipelines often need to retrieve evidence from multiple documents simultaneously, a procedure known as multi-hop querying. Nevertheless, existing RAG applications face challenges in answering multi-hop queries, requiring retrieval and reasoning over numerous pieces of evidence [12]. In this paper, we present Multi-Meta-RAG: an improved RAG using a database filtering approach with LLM-extracted metadata that significantly improves the results on the MultiHop-RAG benchmark.",
            "score": 0.6143224375196537,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 927
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 103,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 234,
                    "end": 237,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 430,
                    "end": 434,
                    "matchedPaperCorpusId": "233240939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57666015625
        },
        {
            "corpus_id": "273186680",
            "title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision",
            "text": "In this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences. The versatility of our approach allows it to be effectively applied across various domains through domain-specific fine-tuning. We evaluate Reward-RAG on publicly available benchmarks from multiple domains, comparing it to state-of-the-art methods. Our experimental results demonstrate significant improvements in performance, highlighting the effectiveness of Reward-RAG in improving the relevance and quality of generated responses. These findings underscore the potential of integrating reward models with RAG to achieve superior outcomes in natural language generation tasks.",
            "score": 0.6139348963513055,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "276576207",
            "title": "A Taxonomy of Generative AI in HEOR: Concepts, Emerging Applications, and Advanced Tools - An ISPOR Working Group Report.",
            "text": "Retrieval-Augmented Generation (RAG) is a sophisticated method that combines the broad knowledge base of FMs with precise, domain-specific data retrieval 73 . Conceptually a RAG system retrieves more up-to-date information, or task specific information from external knowledge or data sources than the FM was pre-trained on 74 . For example, a generative AI solution can employ RAG to verify facts by accessing external databases or websites in realtime, ensuring responses not only draw from a vast internal dataset but are also cross verified with the latest external references 75 . This capability significantly enhances the accuracy and reliability of the information provided, especially in rapidly evolving fields where the FM might not have been trained on the most up to date data 74,76 . Unless carefully managed, a limitation of RAG could be its lack of ability in handling conflicting information retrieval which can detrimentally affect RAG's output quality and is an active area of research 73,76,77 .",
            "score": 0.6125524835617523,
            "section_title": "Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 20067,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 1015
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57080078125
        },
        {
            "corpus_id": "270737995",
            "title": "Enhancing Commentary Strategies for Imperfect Information Card Games: A Study of Large Language Models in Guandan Commentary",
            "text": "In the field of Retrieval-Augmented Generation (RAG), significant progress has been made recently. One study [Shahul et al., 2023] introduces the RAGAs evaluation framework for rapid assessment of Retrieval-Augmented Generation system performance, particularly suitable for the rapid deployment of LLMs. \n\nAnother study investigates the domain adaptability of RAG models and proposes the RAG-end2end approach, enabling them to adapt to specific domain knowledge bases [Siriwardhana et al., 2022]. Additionally, researchers propose MuRAG, a multi-modal Retrieval-Augmented Generator. It leverages external non-parametric multi-modal memory for enhanced language generation, demonstrating outstanding cross-dataset performance in tasks involving image and text-based questions and answers [Chen et al., 2022]. Finally, another study develops the ARM-RAG system, which enhances problem-solving performance by storing and retrieving inference chains. This effectively boosts the intelligence of large language models while reducing training costs [Melz, 2023]. These advancements demonstrate the potential of RAG technology in enhancing the accuracy of knowledge access, the quality of generation, and the narrative ability. Our research explores the integration of RAG with domain-specific strategies to enhance the strategic depth of game commentary.",
            "score": 0.6125524835617523,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 4157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 303
                },
                {
                    "start": 306,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1348
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 130,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 468,
                    "end": 495,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 787,
                    "end": 806,
                    "matchedPaperCorpusId": "252735160"
                },
                {
                    "start": 1043,
                    "end": 1055,
                    "matchedPaperCorpusId": "265043634"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8291015625
        },
        {
            "corpus_id": "271218596",
            "title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems",
            "text": "Despite remarkable reasoning and conversational abilities, out-of-the-box pre-trained Large Language Models (LLMs) struggle to reason about out-of-domain, knowledge-intensive queries [21,14].In response, Retriever-Augmented Generation (RAG) systems [21,20] are becoming increasingly popular in user-facing dialogue applications [35].Generally, RAG systems comprise a retriever component that queries relevant documents from an in-domain corpus and a downstream LLM generator model that incorporates the retrieved documents along with the original user query to output an informed response.The additional context helps ground the LLM in factual information and has been shown to boost performance on knowledge-intensive tasks [21].\n\nStill, when used in production settings, RAG systems are prone to hallucinations as the generator model struggles to retrieve relevant information from the context [1,31,7].In the absence of a one-fits-all approach, application-specific RAG systems must be fine-tuned for optimal performance on domain-specific tasks.However, the choice of retriever and generator models for each application is complex and has serious implications on overall system quality and costs.With numerous commercial and open-source generative LLMs readily available 1 and many variable parameters in the RAG system design (Figure 1), tuning an optimal system for a particular RAG application involves iterative evaluation of multiple configurations.This motivates the need for automated RAG evaluation solutions.\n\nIn response, automated RAG evaluation systems like RAGAS [9] and TruLens [37] have emerged.These systems adopt a zero-shot LLM prompt-based approach to predict a set of curated RAG evaluation metrics.However, the lack of unified RAG benchmarks makes it difficult to compare approaches against each other.Each new study designs a new dataset, often employing LLMs as generators and labelers [9,33,4], which renders them irreproducible.A few benchmarks like RGB [4], AttributionBench [22] and RAGTruth [41] have been proposed recently, but they are small in size and target a disjoint set of labels.The exact RAG evaluation criteria also vary from study to study.",
            "score": 0.6122030192711347,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 191,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 589
                },
                {
                    "start": 589,
                    "end": 730
                },
                {
                    "start": 732,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 1049
                },
                {
                    "start": 1049,
                    "end": 1200
                },
                {
                    "start": 1200,
                    "end": 1458
                },
                {
                    "start": 1458,
                    "end": 1521
                },
                {
                    "start": 1523,
                    "end": 1614
                },
                {
                    "start": 1614,
                    "end": 1723
                },
                {
                    "start": 1723,
                    "end": 1827
                },
                {
                    "start": 1827,
                    "end": 1957
                },
                {
                    "start": 1957,
                    "end": 2120
                },
                {
                    "start": 2120,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 187,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 249,
                    "end": 253,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 328,
                    "end": 332,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 725,
                    "end": 729,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 899,
                    "end": 902,
                    "matchedPaperCorpusId": "235898896"
                },
                {
                    "start": 902,
                    "end": 904,
                    "matchedPaperCorpusId": "258947803"
                },
                {
                    "start": 1580,
                    "end": 1583,
                    "matchedPaperCorpusId": "263152733"
                },
                {
                    "start": 1913,
                    "end": 1916,
                    "matchedPaperCorpusId": "263152733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7314453125
        },
        {
            "corpus_id": "273403839",
            "title": "How Does Knowledge Selection Help Retrieval Augmented Generation?",
            "text": "Retrieval-augmented generation (RAG) is a powerful method for enhancing natural language generation by integrating external knowledge into a model's output. While prior work has demonstrated the importance of improving knowledge retrieval for boosting generation quality, the role of knowledge selection remains less clear. This paper empirically analyzes how knowledge selection influences downstream generation performance in RAG systems. By simulating different retrieval and selection conditions through a controlled mixture of gold and distractor knowledge, we assess the impact of these factors on generation outcomes. Our findings indicate that the downstream generator model's capability, as well as the complexity of the task and dataset, significantly influence the impact of knowledge selection on the overall RAG system performance. In typical scenarios, improving the knowledge recall score is key to enhancing generation outcomes, with the knowledge selector providing limited benefit when a strong generator model is used on clear, well-defined tasks. For weaker generator models or more ambiguous tasks and datasets, the knowledge F1 score becomes a critical factor, and the knowledge selector plays a more prominent role in improving overall performance.",
            "score": 0.6113333185323225,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90625
        },
        {
            "corpus_id": "273502659",
            "title": "Class-RAG: Real-Time Content Moderation with Retrieval Augmented Generation",
            "text": "Robust content moderation classifiers are essential for the safety of Generative AI systems. In this task, differences between safe and unsafe inputs are often extremely subtle, making it difficult for classifiers (and indeed, even humans) to properly distinguish violating vs. benign samples without context or explanation. Scaling risk discovery and mitigation through continuous model fine-tuning is also slow, challenging and costly, preventing developers from being able to respond quickly and effectively to emergent harms. We propose a Classification approach employing Retrieval-Augmented Generation (Class-RAG). Class-RAG extends the capability of its base LLM through access to a retrieval library which can be dynamically updated to enable semantic hotfixing for immediate, flexible risk mitigation. Compared to model fine-tuning, Class-RAG demonstrates flexibility and transparency in decision-making, outperforms on classification and is more robust against adversarial attack, as evidenced by empirical studies. Our findings also suggest that Class-RAG performance scales with retrieval library size, indicating that increasing the library size is a viable and low-cost approach to improve content moderation.",
            "score": 0.6110743234244418,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91845703125
        },
        {
            "corpus_id": "275820702",
            "title": "RAG-Reward: Optimizing RAG with Reward Modeling and RLHF",
            "text": "In this paper, we introduce RAG-Reward, a highquality preference dataset designed for Retrieval-Augmented Generation (RAG). Our dataset is generated through a novel automated AI annotation pipeline, leveraging both open-source and proprietary models to enhance generalization and versatility. To ensure fair and reliable evaluations, we use o3-mini to assess generation quality based on four key metrics carefully selected by human experts. The dataset spans multiple domains, including Question Answering, Data-to-Text, and Summarization, resulting in a large-scale and diverse benchmark. The experimental results show strong alignment with human evaluations, demonstrating the effectiveness of RAG-Reward in reward modeling and reinforcement learning. These findings highlight the potential of our dataset to advance both the evaluation and generation of RAG systems. To foster further research, we will publicly release the dataset to the community.",
            "score": 0.6097360027260988,
            "section_title": "Conclusion",
            "char_start_offset": 24695,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 952
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.830078125
        },
        {
            "corpus_id": "278165663",
            "title": "Building Scalable AI-Powered Applications with Cloud Databases: Architectures, Best Practices and Performance Considerations",
            "text": "To address hallucination and improve the accuracy of AI-generated responses, Retrieval-Augmented Generation (RAG) has emerged as a robust solution. RAG enhances LLMs by incorporating an external knowledge retrieval process before generating responses. Instead of relying solely on pre-trained information, the model dynamically retrieves relevant data from structured and unstructured sources such as vector databases, document repositories, and enterprise knowledge bases. This approach ensures that AI responses are factually accurate and contextually relevant, reducing misinformation and increasing trust in AI-driven applications. By integrating retrieval-based mechanisms, RAG enables AI systems to provide verifiable, real-time insights, making them more reliable for enterprise and mission-critical use cases.",
            "score": 0.6093192684215325,
            "section_title": "Addressing Hallucination with RAG",
            "char_start_offset": 2251,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 817
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.455810546875
        },
        {
            "corpus_id": "275357908",
            "title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG",
            "text": "As more and more Generative AI (GenAI) applications are integrated into real-world production systems, Retrieval-Augmented Generation (RAG) has been adopted in industry as a common technique to improve the output of Large Language Models (LLMs). RAG alleviates inherent LLM pitfalls such as propensity to hallucinate, generating outdated knowledge, and lack of traceability to data sources (Fan et al., 2024;Gao et al., 2024). \n\nIntroducing a retrieval step into the generation process introduces, however, several practical challenges. While an LLM with a large number of parameters, such as GPT-4 (OpenAI et al., 2024), can be prompted to work with any kind of input and generate any kind of textual output, the retriever needs to be small, fast, and perform well with data sources that tend to be domain-specific. : Given an ecosystem of RAG applications, how do we build a retriever that can adapt to a specific domain and to a variety of retrieval tasks? \n\nOff-the-shelf retrievers of different sizes are available to AI practitioners. Embedding services such as Voyage1 perform well on open-source benchmarks but they do not necessarily generalize to the kind of data seen in real-world settings, especially when this data is structured and comes from existing databases. \n\nAnother practical challenge is achieving scalability and generalization across different GenAI use cases that depend on retrieval. A crucial advantage of LLMs compared to traditional machine learning models is that they can generalize to a myriad of tasks due to vast amounts of pretraining data and instruction fine-tuning (Wei et al., 2022;Zhang et al., 2024a;Ouyang et al., 2022). But if the retriever does not perform well and fast across many retrieval tasks, the downstream generation will be negatively affected. \n\nThe problem we are trying to solve is then: how to adapt the retrieval step to a specific domain and to a variety of retrieval tasks? In this work, we are not interested in the choice of LLM, assuming that improvements in the retrieved results translate into improvements in the downstream generation task.",
            "score": 0.6091023412314337,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 959
                },
                {
                    "start": 962,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1799
                },
                {
                    "start": 1802,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2108
                }
            ],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 408,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 1604,
                    "end": 1622,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1642,
                    "end": 1662,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.771484375
        },
        {
            "corpus_id": "267412954",
            "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
            "text": "Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions. To address these challenges, a fine-tuning process is employed, involving feedback and examples to refine models. The objective is to enhance AI models through continuous feedback loops, utilizing metrics such as cosine similarity, LLM evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on financial datasets, including the FinanceBench and RAG Instruct Benchmark Tester Dataset, illustrating the necessity of fine-tuning. The results showcase the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs, providing superior question and answering capabilities. Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy.",
            "score": 0.6086768921072092,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88623046875
        },
        {
            "corpus_id": "270123034",
            "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
            "text": "Large language models (LLMs) have achieved remarkable performance across various natural language processing tasks (Brown et al., 2020;Ope-nAI, 2023;Touvron et al., 2023).Despite their extensive parameters enabling them to learn rich knowledge during pre-training, LLMs may still generate hallucinated, outdated, or inaccurate content, especially in scenarios requiring long-tail knowledge (Ji et al., 2023;Zhang et al., 2023b).\n\n* Correpsonding author.\n\nTo address this problem, retrieval-augmented generation (RAG) has emerged as a pivotal strategy.By explicitly decoupling knowledge retrieval from the backbone LLMs, such architectures have achieved more accurate and reliable content generation and shown particularly enhanced performance on knowledge-intensive tasks such as open-domain question answering (Lewis et al., 2020b;Petroni et al., 2021;Izacard et al., 2023;Tan et al., 2024;Jin et al., 2024).\n\nExisting efforts in RAG development can be roughly categorized into two groups (as illustrated in Figure 1).The first group leverages the incontext learning capabilities of LLMs by incorporating retrieved information into the input along with appropriate prompts (Shi et al., 2023;Ram et al., 2023).This allows for straightforward application to any off-the-shelf LLM without tuning its parameters.However, its effectiveness largely depends on the human experience in crafting effective prompts and the LLM's ability to interpret these prompts.The second group focuses on training LLMs to enhance their performance in RAG scenarios.This training might involve either endto-end pre-training (Guu et al., 2020;Borgeaud et al., 2022) or fine-tuning (Lin et al., 2023;Wang et al., 2023) for specific tasks.These approaches can often lead to better performance, but they require significant computational resources.Recently, parameter-efficient fine-tuning techniques, such as LoRA (Hu et al., 2022), have been widely studied, significantly reducing training costs.These methods can optimize the LLMs' parameters for RAG, but unfortunately compromise the model's general generation abilities in scenarios without retrieval.",
            "score": 0.608634916051781,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 171,
                    "end": 428
                },
                {
                    "start": 430,
                    "end": 453
                },
                {
                    "start": 455,
                    "end": 551
                },
                {
                    "start": 551,
                    "end": 909
                },
                {
                    "start": 911,
                    "end": 1019
                },
                {
                    "start": 1019,
                    "end": 1210
                },
                {
                    "start": 1210,
                    "end": 1309
                },
                {
                    "start": 1309,
                    "end": 1455
                },
                {
                    "start": 1455,
                    "end": 1543
                },
                {
                    "start": 1543,
                    "end": 1713
                },
                {
                    "start": 1713,
                    "end": 1821
                },
                {
                    "start": 1821,
                    "end": 1971
                },
                {
                    "start": 1971,
                    "end": 2129
                }
            ],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 407,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 811,
                    "end": 832,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 832,
                    "end": 853,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 853,
                    "end": 874,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1619,
                    "end": 1641,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1888,
                    "end": 1905,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66064453125
        },
        {
            "corpus_id": "274965455",
            "title": "Accelerating Retrieval-Augmented Generation",
            "text": "State-of-the-art natural language processing systems heavily rely on large language models (LLMs)-deep Transformer networks [93] with hundreds of millions of parameters. There is much evidence that information presented in the LLM training corpora is \"memorized\" in the LLM parameters, forming a parametric knowledge base that the model depends on for generating responses. A major challenge with parametric knowledge is its static nature; it cannot be updated unless the model undergoes retraining or fine-tuning, which is an extremely costly process. This creates a critical issue, especially when it comes to non-stationary domains where fresh content is constantly being produced [108]. Besides, previous studies have indicated that LLMs exhibit limited memorization for less frequent entities [29], are susceptible to hallucinations [83], and may experience temporal degradation [31]. \n\nTo overcome the challenges presented by LLMs, a potential solution is to enhance them with non-parametric knowledge, where the LLM is augmented with information retrieved from a knowledge source (e.g., text documents). These approaches have recently gained considerable attention in the machine learning communities [22; 26; 41; 49; 58; 73; 83], and have played key roles in some recent breakthrough applications in the tech industry, such as Google Gemini [90], Microsoft Copilot [86], and OpenAI ChatGPT with Retrieval Plugins [56]. \n\nRetrieval-Augmented Generation (RAG) is the term that is used to refer to systems that adopt this approach in the context of LLMs. \n\nA RAG application includes two key components: a retrieval model and an LLM for text generation, called the generative model. When a query is received, the retrieval model searches for relevant items (e.g., documents) and the top retrieved items, together with the input, are sent to the generative model. Current state-of-the-art retrieval approaches use bi-encoder neural networks (called dense retrieval) [30] for learning optimal embedding for queries and documents. Each item is then encoded into a high-dimension vector (called embedding vectors) and stored in a vector database. Such approaches use K-nearest neighbor algorithms for retrieving the top \"K\" items from the vector database.",
            "score": 0.6079096968362501,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 889
                },
                {
                    "start": 892,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1426
                },
                {
                    "start": 1429,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2147
                },
                {
                    "start": 2148,
                    "end": 2256
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 128,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 684,
                    "end": 689,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 798,
                    "end": 802,
                    "matchedPaperCorpusId": "246823128"
                },
                {
                    "start": 838,
                    "end": 842,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 884,
                    "end": 888,
                    "matchedPaperCorpusId": "251105205"
                },
                {
                    "start": 1970,
                    "end": 1974,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67529296875
        },
        {
            "corpus_id": "267412954",
            "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
            "text": "To enhance the accuracy of question and answer (Q&A) tasks, our approach included supervised finetuning on the zero-shot model for GPT-3.5 Turbo. This is needed because zero shot Retrieval-Augmented Generation (RAG) oftentimes retrieves irrelevant information or irrelevant embeddings. Subsequently, we utilized reprompting on GPT4All and Llama2 for further refinement. This process included feeding in example prompts with given questions, evidence text, and answers to guide the model in learning specific response patterns. Additionally, we benchmarked the fine-tuning process by incorporating the Retrieval-Augmented Generation (RAG) technique for Q&A tasks, thereby enhancing the evaluation and overall performance of the models. \n\nThere is an absence in standardized methodologies for evaluating model performance. We assessed the zero-shot model (LLM without fine-tuning) and subsequently evaluated the model with fine-tuning and reprompting. The approach involved iterative accuracy evaluation, incorporating fine-tuning or reprompting into models such as GPT-3.5 Turbo, GPT4All, Llama2, and Claude. The evaluation metrics used were cosine similarity and Rouge-L, measuring accuracy in each iteration",
            "score": 0.6075792215446476,
            "section_title": "Process and Work",
            "char_start_offset": 9452,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1208
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84130859375
        },
        {
            "corpus_id": "270560505",
            "title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability",
            "text": "This step involves incorporating LLMs to enhance the generative component of the Retrieval-Augmented Generation (RAG) system.For this study, we utilized open-source LLMs provided by Perplexity, integrating them into the langchain framework through a custom wrapper function (git link to wrapper code) .This integration was done by adapting the approach detailed in this github repository [8][9].",
            "score": 0.6074399757344824,
            "section_title": "LLM Integration",
            "char_start_offset": 9860,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 125,
                    "end": 302
                },
                {
                    "start": 302,
                    "end": 395
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5625
        },
        {
            "corpus_id": "269149041",
            "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
            "text": "For every x and preceding generation y < t,the model decodes a retrieval token to evaluate the utility of retrieval.If retrieval is not required,the model predicts the next output segment,as it does in a standard LM.If retrieval is needed,the model generates: a critique token to evaluate the retrieved passage's relevance,the next response segment,and a critique token to evaluate if the information in the response segment is supported by the passage.Finally,a new critique token evaluates the overall utility of the response.\n\n\"REPLUG: Retrieval-Augmented Black-Box Language Models [115]:\" This research explores how to optimize retrieval results in black-box language models,such as those that only expose APIs without revealing embeddings,using a retrieval-augmented approach.\n\n\"Atlas: Few-shot Learning with Retrieval Augmented Language Models [45]:\" The paper discusses methods for joint training of retrievers and language models,especially for knowledge-intensive tasks.\n\n\"RA-DIT: RETRIEVAL-AUGMENTED DUAL IN-STRUCTION TUNING [66]:\" A lightweight fine-tuning method that combines RAG and SFT is proposed to enhance the performance of retrieval-augmented language models.\n\n\"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering [118]:\" This paper assesses the domain adaptability of RAG models in open-domain question answering (ODQA) tasks and proposes RAG-end2end,an extension of RAG that can adapt to specific domain knowledge bases by updating all components during training.\n\n\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG,fine-tuning,and their combina-",
            "score": 0.6073776805593618,
            "section_title": "XIII. RAG-MEMORY-FINETUNING",
            "char_start_offset": 145958,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 116,
                    "end": 216
                },
                {
                    "start": 216,
                    "end": 453
                },
                {
                    "start": 453,
                    "end": 528
                },
                {
                    "start": 530,
                    "end": 781
                },
                {
                    "start": 783,
                    "end": 979
                },
                {
                    "start": 981,
                    "end": 1179
                },
                {
                    "start": 1181,
                    "end": 1547
                },
                {
                    "start": 1549,
                    "end": 1721
                }
            ],
            "ref_mentions": [
                {
                    "start": 1296,
                    "end": 1301,
                    "matchedPaperCorpusId": "252735056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7763671875
        },
        {
            "corpus_id": "269282856",
            "title": "Generating Test Scenarios from NL Requirements Using Retrieval-Augmented LLMs: An Industrial Study",
            "text": "Retrieval-Augmented Generation (RAG) is a hybrid approach that combines the capabilities of two major NLP techniques: information retrieval (IR) and generative LLMs.Generative LLMs like GPTx models [19] have revolutionized NLP domain.LLMs trained on vast datasets generate coherent and contextually relevant text.However, as discussed in Section I, LLMs' ability to generate accurate and specific information is often constrained by their training data scope and inherent limitations in accessing external, up-to-date domain information.Traditional IR systems, designed to search and retrieve information from large databases, provide the ability to access specific information.However, they lack LLMs' NL understanding and generation capabilities.RAG address these limitations by combining the generative capabilities of LLMs with IR efficiency.When a query is presented to a RAG model, the retrieval system fetches relevant documents or snippets.The retrieved snippets are fed into the generative model, which integrates them into its response generation process.This allows the model to produce responses that are not only contextually rich but also factually accurate.Next, we provide details on the RAG pipeline, visualized in Fig. 2. Indexing.In the RAG pipeline, the indexation process begins with the domain documentation loading, cleansing, and extracting the content.The documents can be loaded in different file formats, e.g., PDF, HTML, and Microsoft Word, and converted into standardized plain text.Thereafter, the text is split into smaller and more manageable passages or chunks (e.g., 512 tokens) to fit within the context limits of different LMs.These passages are subsequently transformed into vector representations through an embedding model, e.g., Sentence Bert (SBERT) [20].These embeddings are numerical representations that encapsulate the semantic content of the passage.An index is created to store the text passages and their vector embeddings as key-value pairs, which allows for retrieving contextually relevant data efficiently and accurately.These key-value pairs can be stored in a vector database to avoid re-indexing the documentation each time.A prompt for the RAG model is indexed using the same embedding model and passed onto the next step in the pipeline.Querying.",
            "score": 0.6066223918221638,
            "section_title": "C. Retrieval Augmented Generation (RAG)",
            "char_start_offset": 13536,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 165,
                    "end": 234
                },
                {
                    "start": 234,
                    "end": 313
                },
                {
                    "start": 313,
                    "end": 537
                },
                {
                    "start": 537,
                    "end": 678
                },
                {
                    "start": 678,
                    "end": 748
                },
                {
                    "start": 748,
                    "end": 846
                },
                {
                    "start": 846,
                    "end": 948
                },
                {
                    "start": 948,
                    "end": 1065
                },
                {
                    "start": 1065,
                    "end": 1172
                },
                {
                    "start": 1172,
                    "end": 1249
                },
                {
                    "start": 1249,
                    "end": 1377
                },
                {
                    "start": 1377,
                    "end": 1512
                },
                {
                    "start": 1512,
                    "end": 1663
                },
                {
                    "start": 1663,
                    "end": 1796
                },
                {
                    "start": 1796,
                    "end": 1896
                },
                {
                    "start": 1896,
                    "end": 2073
                },
                {
                    "start": 2073,
                    "end": 2179
                },
                {
                    "start": 2179,
                    "end": 2294
                },
                {
                    "start": 2294,
                    "end": 2303
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5400390625
        },
        {
            "corpus_id": "267750423",
            "title": "Foundation Models for Recommender Systems: A Survey and New Perspectives",
            "text": "Retrieval-Augmented Generation (RAG) is a technique used in FMs to enhance their generative capability by integrating external data retrieval into the generative process [Gao et al., 2023b]. This approach improves the accuracy, credibility, and relevance of FM outputs, particularly in knowledge-intensive tasks like information retrieval and RS. RAG aims to address outdated knowledge, the generation of incorrect information (hallucinations), and limited domain expertise by combining the FM's internal knowledge with dynamic external knowledge bases. RAG is suitable for enhancing the FM4RecSys, especially in modeling lifelong user behavior sequences in real-world RS environments [Lin et al., 2023c]. It could potentially ensure that the RecSys remains up-to-date with con-tinuous shifts in user preferences and trends, which is critical for precise identification and documentation of long-term behavioral patterns. For instance, considering the input token length restriction of FMs, RAG may be utilized to selectively extract pertinent portions of a user's interaction history and associated external knowledge, thereby conforming to the model's input constraint. Additionally, RAG may lessen the likelihood of producing irrelevant recommendations or nonexistent items (hallucinations), thereby enhancing the reliability of FM4RecSys.",
            "score": 0.6064400963583753,
            "section_title": "RAG meets RecSys",
            "char_start_offset": 28607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1342
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.611328125
        },
        {
            "corpus_id": "274788882",
            "title": "EXIT: Context-Aware Extractive Compression for Enhancing Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020;Khandelwal et al., 2020) is the task of enhancing Large Language Models (LLMs) responses with relevant external contexts or documents. By grounding answers in evidence, RAG systems have gained much attention for mitigating hallucination issues (Ram et al., 2023;Li et al., 2023b) and improving factual reliability (Jeong et al., 2024;Xia et al., 2024b",
            "score": 0.6062451212552559,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 408
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 81,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 301,
                    "end": 319,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 319,
                    "end": 336,
                    "matchedPaperCorpusId": "258832847"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66845703125
        },
        {
            "corpus_id": "269757277",
            "title": "ERAGent: Enhancing Retrieval-Augmented Language Models with Improved Accuracy, Efficiency, and Personalization",
            "text": "Retrieval Augmented Generation (RAG) (Lewis et al., 2020) leverages a retriever that provides substantial external information for improving the generated output of LLMs.This strategy utilizes knowledge in a parameter-free manner, circumvents the high training costs of LLMs' parameterized knowledge.Furthermore, it alleviates the hallucination issues in LLMs, significantly enhancing the factual accuracy and relevance of the generated content.The concept of RAG is rooted in the DrQA framework (Chen et al., 2017), which marked the initial phase of integrating retrieval mechanisms with Language Models (LMs) through heuristic retrievers like TF-IDF for sourcing evidence.Subsequently, RAG underwent evolution with the introduction of Dense Passage Retrieval (Karpukhin et al., 2020), and further advancements in RAG (Lewis et al., 2020) and REALM (Ram et al., 2023).These methods utilize pre-trained transformers and are characterized by the joint optimization of retrieval and generation components.Recent advancements have extended RAG's capabilities by integrating Large Language Models (LLMs).Exemplary developments such as RE-PLUG (Shi et al., 2023) and IC-RALM (Ram et al., 2023) demonstrate the potent generalization abilities of LLMs in zero-shot or few-shot scenarios.These models are capable of following complex instructions, understanding retrieved information, and utilizing limited demonstrations to generate high-quality responses.",
            "score": 0.6062451212552559,
            "section_title": "Retrieval Augmented Generation",
            "char_start_offset": 6872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 170,
                    "end": 300
                },
                {
                    "start": 300,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 674
                },
                {
                    "start": 674,
                    "end": 869
                },
                {
                    "start": 869,
                    "end": 1003
                },
                {
                    "start": 1003,
                    "end": 1100
                },
                {
                    "start": 1100,
                    "end": 1280
                },
                {
                    "start": 1280,
                    "end": 1449
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 819,
                    "end": 839,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69384765625
        },
        {
            "corpus_id": "270067523",
            "title": "Generation of Asset Administration Shell With Large Language Model Agents: Toward Semantic Interoperability in Digital Twins in the Context of Industry 4.0",
            "text": "As the name indicates, Retrieval-Augmented Generation (RAG) combines the generative capabilities of LLMs with a retrieval mechanism that queries helpful information from an external knowledge base, such as a database or a vast text corpus.This approach dynamically incorporates retrieved information into the generation process (cf. Figure 5).As a result, RAG enhances the LLM's ability to handle tasks that demand task-specific information, especially in scenarios where upto-date or specialized knowledge is required.",
            "score": 0.6062451212552559,
            "section_title": "E. RETRIEVAL-AUGMENTED GENERATION (RAG)",
            "char_start_offset": 18143,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 519
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44140625
        },
        {
            "corpus_id": "278020653",
            "title": "ENHANCED RETRIEVAL-AUGMENTED GENERATION FOR STUDENT MENTAL HEALTH SUPPORT USING GENERATIVE ARTIFICIAL INTELLIGENCE",
            "text": "Retrieval-Augmented Generation (RAG) has demonstrated significant potential across various domains, including mental health. By enhancing the accuracy and reliability of large language models (LLMs), RAG has proven particularly beneficial in fields such as healthcare (Xiong et al., 2024). In mental health applications, RAG can improve AI-powered support systems by providing patients and healthcare professionals with more accurate and up-to-date information. \n\nTigges-Limmer et al. (2018) highlighted the importance of mental health support in areas such as screening, diagnostics, assessment, and education for patients with ventricular assist devices, though their study did not explicitly employ RAG. However, RAG could enhance such interventions by retrieving relevant insights from specialized mental health knowledge bases, thereby improving the quality and personalization of support for patients. \n\nEnhanced Retrieval-Augmented Generation for Student Mental Health Support Using Generative Artificial Intelligence editor@iaeme.com \n\nGenerative AI has revolutionized mental health support by providing accessible, scalable, and personalized assistance. However, despite its potential, it faces several limitations that impact its reliability and effectiveness.",
            "score": 0.6060489900495682,
            "section_title": "Theoretical Background",
            "char_start_offset": 2028,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1270
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7421875
        },
        {
            "corpus_id": "271097348",
            "title": "Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting",
            "text": "Retrieval Augmented Generation Retrieval Augmented Generation (RAG) enhances LLMs by retrieving relevant documents from external databases and incorporating them into the generation process (Gao et al., 2023b;Lewis et al., 2020;Khandelwal et al., 2020;Izacard & Grave, 2021;Luo et al., 2023a;Xia et al., 2024b;Wang et al., 2024). Recent work has primarily focused on enabling LLMs to understand when and what to retrieve (Ma et al., 2023;Chen et al., 2023b;Jiang et al., 2023b;Schick et al., 2024), or designing approaches to better utilize contexts (Yu et al., 2023;Yoran et al., 2023;Wang et al., 2023b;Sarthi et al., 2024;Baek et al., 2023;Xu et al., 2023;Kim et al., 2024). Among them, SAIL (Luo et al., 2023a) fine-tunes a pre-trained LLM on web search data to filter irrelevant contents. Self-Reflective RAG (Asai et al., 2023) introduces reflection tokens to guide retrieval and annotation in instruction-tuning datasets. However, both approaches require additional instruction-tuning of generic LLMs, which is resource-intensive and may lead to forgetting or over-fitting (Luo et al., 2023b). Furthermore, long context with retrieved documents can suffer from computational inefficiency and position bias (Liu et al., 2024). Corrective RAG (Yan et al., 2024) on the other hand proposes a lightweight retrieval evaluator, but it lacks the capability for high-level reasoning. In contrast, our proposed SPECULATIVE RAG addresses these limitations by leveraging a smaller RAG drafter model to efficiently understand diverse perspectives in retrieval results and generate drafts for the generalist LMs to verify and integrate.",
            "score": 0.605851570607367,
            "section_title": "RELATED WORKS",
            "char_start_offset": 4900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1630
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 228,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 228,
                    "end": 252,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 457,
                    "end": 477,
                    "matchedPaperCorpusId": "15641339"
                },
                {
                    "start": 477,
                    "end": 497,
                    "matchedPaperCorpusId": "256697342"
                },
                {
                    "start": 625,
                    "end": 643,
                    "matchedPaperCorpusId": "264306280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.456298828125
        },
        {
            "corpus_id": "273502291",
            "title": "Developing Retrieval Augmented Generation (RAG) based LLM Systems from PDFs: An Experience Report",
            "text": "The development of Retrieval Augmented Generation (RAG) systems offers a new way to improve large language models by grounding their outputs in realtime, relevant information. This paper covers the main steps for building RAG systems that use PDF documents as the data source. With clear examples and code snippets, it connects theory with practice and highlights challenges like handling complex PDFs and extracting useful text. It also looks at the options available, with examples of using proprietary APIs like OpenAI's GPT and, as an alternative, open-source models like Llama 3.1, helping developers choose the best tools for their needs. \n\nBy following the recommendations in this guide, developers can avoid common mistakes and ensure their RAG systems retrieve relevant information and generate accurate, fact-based responses. As technology advances in adaptive learning, multi-modal capabilities, and retrieval methods, RAG systems will play a key role in industries like healthcare, legal research, and technical documentation. This guide offers a solid foundation for optimizing RAG systems and extending the potential of generative AI in practical applications.",
            "score": 0.6057612106402379,
            "section_title": "Conclusions",
            "char_start_offset": 34903,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 644
                },
                {
                    "start": 647,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1174
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37451171875
        },
        {
            "corpus_id": "274283400",
            "title": "Adaptive Control of Retrieval-Augmented Generation for Large Language Models Through Reflective Tags",
            "text": "This paper investigates retrieval-augmented generation (RAG) as a means to mitigate the hallucination and latency challenges inherent to large language models (LLMs). To address the limitations introduced by RAG, such as the potential masking of LLM capabilities, we propose a novel framework employing four reflective tags to control the retrieval and evaluation of external sources. A search tag enables adaptive search, mitigating the overreliance on irrelevant information. Evaluation tags facilitate a comprehensive assessment of retrieved documents based on relevance, support, and overall quality. The framework incorporates chain-of-thought (CoT) reasoning to decompose queries and generate responses incrementally, further enhancing output quality and reliability. \n\nTo evaluate the performance of the proposed framework, we conducted experiments on four benchmark datasets: ARC-Challenge, PubHealth, PopQA, and TriviaQA, where GPT-3.5 and Qwen served as baselines, with accuracy as the primary evaluation metric. The experimental results shown in Section 4 demonstrate the effectiveness of the proposed method, especially in improving the accuracy of the fact-checking benchmarks. The performance of the proposed method depends on the value of the hyperparameter k, and the optimal value of k depends on the baseline LLM. These findings collectively provide compelling evidence of the effectiveness of the proposed framework. \n\nFuture work includes comparisons with existing methods for improving the performance of RAGs and evaluation experiments using a wider range of datasets. We plan to compare our method with existing robust approaches, including RAG by Facebook AI [18], REALM by Google [1], and various domain-specific retrieval systems such as PubMed [19], BioBERT [20] PatentBERT [21], and FinancialBERT [22]. Additionally, we will evaluate against the latest state-of-the-art LLMs, such as Phi-3.5 [23], LLaMA 3.2 [24], and OLMo [25]. Specifically, our investigation will focus on the influence of prepared inputs, such as keywords, on generation performance, particularly as it relates to solving retrieval problems. Furthermore, it is essential to evaluate our method using domain-specific datasets in areas such as law, medicine, and science.",
            "score": 0.605711253434436,
            "section_title": "Concluding Remarks",
            "char_start_offset": 27075,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1435
                },
                {
                    "start": 1438,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2139
                },
                {
                    "start": 2140,
                    "end": 2267
                }
            ],
            "ref_mentions": [
                {
                    "start": 1705,
                    "end": 1708,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1771,
                    "end": 1775,
                    "matchedPaperCorpusId": "224820417"
                },
                {
                    "start": 1785,
                    "end": 1789,
                    "matchedPaperCorpusId": "59291975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.890625
        },
        {
            "corpus_id": "268248396",
            "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
            "text": "Interestingly, larger LMs generally do not benefit from finetuning, while smaller ones do. Therefore, a small fine-tuned LM with RAG can perform on par or better than a large LM; e.g., StableLM2 (1.6B) vs. Llama3 (8B) (Table 3). \u2022 Retrieval model: Comparing retrievers with varying performance in the RAG system, we observe that as the popularity of factual knowledge increases, the performance of the retriever decreases (Figure 7). Moreover, the performance of the RAG system increases by using higher performance retriever (Figures 1 and 8). \u2022 Fine-tuning vs. RAG: Comparing these two knowledge injection methods, RAG substantially outperforms fine-tuning. Fine-tuned LMs combined with RAG either outperform or perform on par with vanilla LMs with RAG in all but one case (Figure 1). \n\nWhile fine-tuning improves accuracy in answering factual questions, both with and without RAG, it demands a considerable amount of effort and resources. This leads us to our second research question: (RQ2): Can we avoid the cost of fine-tuning by developing an advanced RAG approach that surpass the performance of a fine-tuned LM with RAG? To answer this question, we develop Stimulus RAG (SRAG), a new RAG approach that stimulates an LM to generate the correct response based on the provided hint in the prompt. The hint is extracted from the top retrieved documents by the retrieval model. Our results demonstrate that Stimulus RAG outperforms all other combinations of fine-tuning, both with and without retrievethen-generate RAG. \n\nTo summarize, this paper makes the following contributions: \n\n\u2022 We study the effectiveness of fine-tuning and RAG approaches for question answering over less popular factual knowledge and compare the performance of these models across distinct setups: vanilla and fine-tuned models, both with and without RAG, using different data augmentation methods.",
            "score": 0.6056043704672425,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1523
                },
                {
                    "start": 1526,
                    "end": 1585
                },
                {
                    "start": 1588,
                    "end": 1878
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "273962778",
            "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
            "text": "Retrieval-augmented generation (RAG) has shown impressive capability in providing reliable answer predictions and addressing hallucination problems. A typical RAG implementation uses powerful retrieval models to extract external information and large language models (LLMs) to generate answers. In contrast, recent LLM-based retrieval has gained attention for its substantial improvements in information retrieval (IR) due to the LLMs' semantic understanding capability. However, directly applying LLM to RAG systems presents challenges. This may cause feature locality problems as massive parametric knowledge can hinder effective usage of global information across the corpus; for example, an LLM-based retriever often inputs document summaries instead of full documents. Moreover, various pre-trained tasks in LLMs introduce variance, further weakening performance as a retriever. To address these issues, we propose a novel two-stage fine-tuning architecture called Invar-RAG. In the retrieval stage, an LLM-based retriever is constructed by integrating LoRA-based representation learning to tackle feature locality issues. To enhance retrieval performance, we develop two patterns (invariant and variant patterns) and an invariance loss to reduce LLM variance. In the generation stage, a refined fine-tuning method is employed to improve LLM accuracy in generating answers based on retrieved information. Experimental results show that Invar-RAG significantly outperforms existing baselines across three open-domain question answering (ODQA) datasets. Code is available in the Supplementary Material for reproducibility.",
            "score": 0.6054507888648941,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "270286231",
            "title": "A + B: A General Generator-Reader Framework for Optimizing LLMs to Unleash Synergy Potential",
            "text": "Retrieval-Augmented Generation: Despite a lot of advancements, LLMs exhibit notable limitations, particularly in handling domain-specific or highly specialized queries (Kandpal et al., 2023).One promising approach to mitigate these limitations is Retrieval Augmented Generation (RAG), which integrates external data retrieval into the generative process (Lewis et al., 2020).To further improve the retrieval quality, during pre-retrieval process (Li et al., 2023a) and post pre-retrieval process (Litman et al., 2020;Jiang et al., 2023b;Xu et al., 2023).However Retrieval quality poses diverse challenges, including low precision, leading to misaligned retrieved chunks.Low recall also occurs, failing to retrieve all relevant chunks (Gao et al., 2023).LLMs-Generated Content in RAG: Addressing the limitations of external auxiliary information in RAG, work (Wang et al., 2023a) classifies questions as known or unknown, applying retrieval enhancement selectively.Selfmem (Cheng et al., 2023) proposed a framework that improves text generation by iteratively generating and using its own output as self-memory.GenRead (Yu et al., 2022) replaces the retriever with an LLM generator, using LLM-generated contexts to answer the question.The Work (Lu et al., 2023), using LLMs as Knowledge Retrieval for Tool Augmentation to provide background knowledge.",
            "score": 0.6053582519194496,
            "section_title": "Related Works",
            "char_start_offset": 27551,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 191,
                    "end": 375
                },
                {
                    "start": 375,
                    "end": 554
                },
                {
                    "start": 554,
                    "end": 670
                },
                {
                    "start": 670,
                    "end": 753
                },
                {
                    "start": 753,
                    "end": 964
                },
                {
                    "start": 964,
                    "end": 1110
                },
                {
                    "start": 1110,
                    "end": 1234
                },
                {
                    "start": 1234,
                    "end": 1350
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 190,
                    "matchedPaperCorpusId": "253522998"
                },
                {
                    "start": 354,
                    "end": 374,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 496,
                    "end": 517,
                    "matchedPaperCorpusId": "214641123"
                },
                {
                    "start": 1243,
                    "end": 1260,
                    "matchedPaperCorpusId": "258212542"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6552734375
        },
        {
            "corpus_id": "277999687",
            "title": "A Survey of Foundation Model-Powered Recommender Systems: From Feature-Based, Generative to Agentic Paradigms",
            "text": "Retrieval-augmented generation (RAG) is a technique used in FMs to enhance their generative capability by integrating external data retrieval into the generative process [253]. This approach improves the accuracy, credibility, and relevance of FM outputs, particularly in knowledge-intensive tasks like information retrieval and RS. RAG aims to address outdated knowledge, the generation of incorrect information (hallucinations), and limited domain expertise by combining the FM's internal knowledge with dynamic external knowledge bases. RAG is suitable for enhancing the FM4RecSys, especially in modeling lifelong user behavior sequences in real-world RS environments [254]. It could potentially ensure that the RSs remain up-to-date with continuous shifts in user preferences and trends, which is critical for precise identification and documentation of long-term behavioral patterns. For instance, considering the input token length restriction of FMs, RAG may be utilized to selectively extract pertinent portions of a user's interaction history and associated external knowledge, thereby conforming to the model's input constraint. Additionally, RAG may lessen the likelihood of producing irrelevant recommendations or non-existent items (hallucinations), thereby enhancing the reliability of FM4RecSys.",
            "score": 0.6047081219061472,
            "section_title": "RAG meets RecSys",
            "char_start_offset": 101817,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1310
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 175,
                    "matchedPaperCorpusId": "266359151"
                },
                {
                    "start": 671,
                    "end": 676,
                    "matchedPaperCorpusId": "261065228"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65576171875
        },
        {
            "corpus_id": "265128626",
            "title": "Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users",
            "text": "Research into methods for improving the performance of large language models (LLMs) through fine-tuning, retrieval-augmented generation (RAG) and soft-prompting has tended to focus on the use of highly technical or high-cost techniques, making many of the newly discovered approaches comparatively inaccessible to non-technical users. In this paper we tested an unmodified version of GPT 3.5, a fine-tuned version, and the same unmodified model when given access to a vectorised RAG database, both in isolation and in combination with a basic, non-algorithmic soft prompt. In each case we tested the model's ability to answer a set of 100 questions relating primarily to events that occurred after September 2021 (the point at which GPT 3.5's training data set ends). We found that if commercial platforms are used and default settings are applied with no iteration in order to establish a baseline set of outputs, a fine-tuned model outperforms GPT 3.5 Turbo, while the RAG approach out-performed both. The application of a soft prompt significantly improved the performance of each approach.",
            "score": 0.6042394976384142,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8095703125
        },
        {
            "corpus_id": "269214364",
            "title": "Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study",
            "text": "AI-powered question-answering (Q&A) systems have emerged as important tools, alongside established search technologies, to enable quick access to relevant information and knowledge from large digital sources that are complex and time-consuming for humans to navigate.Advancements in large language models (LLMs) have revolutionized the field of Q&A, with models like GPT-3 (Brown et al. 2020), BERT (Devlin et al. 2018), and RoBERTa (Liu et al. 2019) demonstrating remarkable abilities in understanding and generating human-like text.However, the effectiveness of such models in handling domain-specific questions that require specialized knowledge is limited.\n\nRetrieval-augmented generation (RAG) techniques, which combine information retrieval and generative models (Lewis et al. 2021), have shown promise in boosting the quality of LLM output in Q&A tasks.RAG systems leverage the strengths of both retrieval and generation components to provide contextually relevant and informative responses.While there is a lack of established quantification of RAG accuracy, early findings suggest that generic RAG does not perform well in complex domains such as finance.In one instance, RAG based on generic LLMs such as GPT-4-Turbo fails to answer 81% of the questions derived from Securities and Exchange Commission (SEC) financial filings (Islam et al. 2023).\n\nThe underperformance of generic LLMs and RAG in domain-specific Q&A has motivated us to research into and create methods to adapt and extend such models and techniques.In this paper, we describe and quantify the gains in accuracy from two major methods: model fine-tuning and iterative reasoning.\n\nFine-tuning is a way to adapt language models to specific domains and tasks (Devlin et al. 2018;Liu et al. 2019) by training them on domain-specific data and having them capture the nuances and intricacies of a particular field.In a typical RAG workflow, there are two principal models that can be considered for fine-tuning: the Embedding Model, whose tasks are indexing the information in the corpus and retrieving information relevant to the posed question, and the Generative Model, whose task is synthesizing an answer.",
            "score": 0.6027630799883393,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 267,
                    "end": 534
                },
                {
                    "start": 534,
                    "end": 660
                },
                {
                    "start": 662,
                    "end": 860
                },
                {
                    "start": 860,
                    "end": 998
                },
                {
                    "start": 998,
                    "end": 1164
                },
                {
                    "start": 1164,
                    "end": 1356
                },
                {
                    "start": 1358,
                    "end": 1526
                },
                {
                    "start": 1526,
                    "end": 1654
                },
                {
                    "start": 1656,
                    "end": 1884
                },
                {
                    "start": 1884,
                    "end": 2180
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8779296875
        },
        {
            "corpus_id": "273662368",
            "title": "LLM Robustness Against Misinformation in Biomedical Question Answering",
            "text": "In recent years, generative artificial intelligence (AI) has become increasingly ubiquitous, with a plethora of tools that allow even non-experts to easily access and use AI systems (for example, for natural language generation, image or video generation, etc.). Now, a wide range of individuals and organizations can almost effortlessly incorporate the capabilities of advanced AI into their workflows and products. However, this ease of access has also led to an increase in the misuse of generative AI, such as producing fake images or generating misinformation (i.e., the information that appears plausible but is factually incorrect) that often lands on the Internet. \n\nOne problem with incorrect synthetic information on the web is when the next generation of AI models (for example, large language models) is trained on this data. For instance, the work by Shumailov et al. [1] introduced a concept of a \"model collapse\" that is described as irreversible defects in the model trained on synthetic content leading to partial or complete disappearance of the original content distribution. Thus, (factually) incorrect content present in the training data may lead over time to the situation that AI models would \"memorize\" only the erroneous information. \n\nAnother problem can arise when web search is used to steer answer generation with a large language model using a retrieval-augmented generation approach [2]. The strengths of RAG-based approaches are that they allow: (1) to increase the LLM-generated answer accuracy by complementing or overriding the model's parametric (i.e., internal) knowledge with non-parametric (i.e., external) knowledge and (2) to provide the answer's provenance information, for example, by referencing retrieved information sources. However, the RAG bottleneck is its susceptibility to irrelevant context [3] or misinformation, i.e., incorrect evidence information [4], during the model prompting. \n\nIn this work, we investigate to what extent LLMs can be deceived by other LLMs into generating an incorrect answer in the RAG scenario in the domain of biomedical question answering. However, we do not address the retrieval aspects and focus only on the augmented generation (AG) part.",
            "score": 0.6020377484321998,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1259
                },
                {
                    "start": 1262,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1936
                },
                {
                    "start": 1939,
                    "end": 2121
                },
                {
                    "start": 2122,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 881,
                    "end": 884,
                    "matchedPaperCorpusId": "271448069"
                },
                {
                    "start": 1415,
                    "end": 1418,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1479,
                    "end": 1482,
                    "matchedPaperCorpusId": "271448069"
                },
                {
                    "start": 1661,
                    "end": 1664,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1844,
                    "end": 1847,
                    "matchedPaperCorpusId": "256459776"
                },
                {
                    "start": 1904,
                    "end": 1907,
                    "matchedPaperCorpusId": "266163801"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3056640625
        },
        {
            "corpus_id": "271571401",
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "text": "The development of RAG technology can be summarized in three stages. Initially, retrieval-augmented techniques were introduced to improve the performance of pre-trained language models on knowledge-intensive tasks [19], [20]. In specific implementations, Retro [21] optimized pre-trained autoregressive models through retrieval augmentation, while Atlas [22] utilized a retrieval-augmented few-shot fine-tuning method, enabling language models to adapt to diverse tasks. IRCOT [23] further enriched the reasoning process during the inference phase by combining chain-of-thought and multistep retrieval processes. Entering the second stage, as the language processing capabilities of LLMs significantly improved, retrieval-augmented techniques began to serve as a means of supplementing additional knowledge and providing references, aiming to reduce the hallucination. For instance, RRR [24] improved the rewriting phase, and LLMlingua [25] removed redundant tokens in retrieved document chunks. With the continuous progress of RAG technology, research has become more refined and focused, while also achieving innovative integration with other technologies such as graph neural networks [26] and fine-tuning techniques [27]. The overall pipeline has also become more flexible, such as using LLMs to proactively determine the timing of retrieval and generation [14], [28]. \n\nThe development of RAG technology has been accelerated by LLM technology and practical application needs. Researchers are examining and organizing the RAG framework and development pathways from different perspectives. Building upon the enhanced stages of RAG, Gao et al., [2] subdivided RAG into enhancement during pre-training, inference, and fine-tuning stages. Based on the main processes of RAG, relevant works on RAG were organized from the perspectives of retrieval, generation, and augmentation methods. Huang et al., [29] categorize RAG methods into four main classes: pre-retrieval, retrieval, post-retrieval, generation, and provide a detailed discussion of the methods and techniques within each class. Hu et al., [30] discuss Retrieval-Augmented Language Models (RALMs) form three key components, including retrievers, language models, augmentations, and how their interactions lead to different model structures and applications.",
            "score": 0.6019635585280025,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 8318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2318
                }
            ],
            "ref_mentions": [
                {
                    "start": 220,
                    "end": 224,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 261,
                    "end": 265,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 936,
                    "end": 940,
                    "matchedPaperCorpusId": "252186384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5849609375
        },
        {
            "corpus_id": "271533386",
            "title": "Introducing a new hyper-parameter for RAG: Context Window Utilization",
            "text": "This paper introduces a new hyper-parameter for Retrieval-Augmented Generation (RAG) systems called Context Window Utilization. RAG systems enhance generative models by incorporating relevant information retrieved from external knowledge bases, improving the factual accuracy and contextual relevance of generated responses. The size of the text chunks retrieved and processed is a critical factor influencing RAG performance. This study aims to identify the optimal chunk size that maximizes answer generation quality. Through systematic experimentation, we analyze the effects of varying chunk sizes on the efficiency and effectiveness of RAG frameworks. Our findings reveal that an optimal chunk size balances the trade-off between providing sufficient context and minimizing irrelevant information. These insights are crucial for enhancing the design and implementation of RAG systems, underscoring the importance of selecting an appropriate chunk size to achieve superior performance.",
            "score": 0.6019064033948461,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80712890625
        },
        {
            "corpus_id": "273812081",
            "title": "Ontology Population using LLMs",
            "text": "Retrieval Augmented Generation (RAG) is a machine learning approach used in NLP that integrates a retrieval system to improve the performance of a generative model [11]. \n\nThe core idea is to leverage the strengths of retrieval-based methods and generative models to produce more precise results. In this setup, the relevant information is retrieved based on the user's query from an external knowledge source, such as a database or a knowledge graph. Thus, to implement the RAG mechanism, all extracted text files from Wikipedia are first divided into smaller chunks to ensure efficient processing and then embedded into a vector database. This chunking process helps manage large documents by breaking them into manageable sections, each of which can be individually compared for relevance. Two distinct prompt models are employed during the process: the first model explicitly instructs the system to populate the ontology for the agent, indicating that the document begins with a specific name and that the content is primarily about that entity. The second model, in contrast, omits this specific instruction, allowing for a more general approach to content retrieval. \n\nIn the RAG mechanism, the input prompt (query) is also transformed into an embedding, which is then compared against the pre-embedded document chunks. The similarity between the prompt embedding and the document chunk embeddings determines which chunks are most relevant. These selected chunks, based on their high similarity scores, are retrieved and used in conjunction with the original query as input to the language models. This approach enhances the system's ability to generate informed and contextrich responses by integrating relevant document content directly into the model's output generation. \n\nFor the implementation of the RAG mechanism, LangChain7 was employed as the framework to handle the retrieval and augmentation processes. The embeddings used for both the documents and queries are generated using Hugging Face's pre-trained model, specifically the sentence-transformers/all-MiniLM-L6-v2, which is designed to efficiently capture semantic similarities between text chunks. This combination of tools ensures accurate retrieval and seamless integration of relevant information into the language model's output.",
            "score": 0.6018807473696719,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 18638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 172,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1173
                },
                {
                    "start": 1176,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1781
                },
                {
                    "start": 1784,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2171
                },
                {
                    "start": 2172,
                    "end": 2307
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51513671875
        },
        {
            "corpus_id": "277740883",
            "title": "DRAFT-ing Architectural Design Decisions using LLMs",
            "text": "Retrieval-Augmented Generation (RAG) introduced by Patrick et al. [13] is a hybrid approach that combines the generative capabilities of LLMs with an external retrieval mechanism. This hybrid approach enhances the contextual accuracy and factual reliability of generated responses by retrieving relevant information from an external knowledge base. RAG has been successfully employed in various applications, including question-answering systems and document summarization. Multiple studies have shown complex Retrieval architectures such as Knowledge Graphs [20], Re-ranking [21], etc. improving the performance of RAG models. \n\nIn this study we use Retrieval-Augmented Fewshot Generation, which is a combination of fewshot prompting and RAG [22]. Rather than relying on a static, predefined set of few-shot exemplars, this approach retrieves contextually similar examples from a structured knowledge base, such as a vector database (VDB). This method improves the model's ability to generate context-aware and semantically relevant responses. Here is a breakdown of how it works: \n\nEmbedding Representation: Textual data is transformed into high-dimensional vector representations, known as embeddings, which capture the semantic meaning of the content. These embeddings sre generated using an embedding function, which is a pretrained LLM such as BERT [16]. \n\nFormally, a given context C is converted into an embedding v C (of dimension d) using an embedding function: \n\nVector Database (VDB) Construction: A vector database (VDB) is a specialized type of database designed to store representations of data, such as sentences or documents, in the form of embeddings. A VDB performs efficient similarity searches by comparing the vector representations of queries with those of the data stored in the VDB, enabling quick retrieval of relevant or similar data. \n\nHere, given a dataset of context-decision pairs {(C i , D i )}, each context C i is converted into its embedding v C i and stored along with its corresponding {(C i , D i )} pair, forming the vector database: \n\nRetrieval mechanism: When a query is received, the top-k most similar documents are retrieved from the vector database. \n\nWhen a new Decision Context C is provided, its embedding is computed as:",
            "score": 0.6018332507668723,
            "section_title": "Retrieval-Augmented Few-shot Generation",
            "char_start_offset": 10745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1081
                },
                {
                    "start": 1084,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1360
                },
                {
                    "start": 1363,
                    "end": 1471
                },
                {
                    "start": 1474,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1861
                },
                {
                    "start": 1864,
                    "end": 2072
                },
                {
                    "start": 2075,
                    "end": 2194
                },
                {
                    "start": 2197,
                    "end": 2269
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84130859375
        },
        {
            "corpus_id": "269005050",
            "title": "Exploring Autonomous Agents through the Lens of Large Language Models: A Review",
            "text": "DPO obviates the need to train a reward model, the sample from the LLM during fine-tuning or conduct an extensive hyperparameter search.Figure 2 illustrates the pivotal procedures involved in RLHF and DPO.Retrieval Augmented Generation (RAG) has emerged as a favored paradigm for enabling Large Language Models (LLMs) to access external data, serving as a grounding mechanism to counter hallucinations.RAG models amalgamate pre-trained parametric and non-parametric memory for language generation.The parametric memory is a pre-trained seq2seq model, while the non-parametric memory is a dense vector index of Wikipedia accessed with a pre-trained neural retriever.With RAG, LLMs retrieve contextual documents from a database to enhance the accuracy of their responses.Frameworks such as LangChain, LlamaIndex, FastRAG, and others serve as orchestrators, connecting LLMs with tools, databases, memories, etc., thereby augmenting their capabilities.User instructions are not inherently optimized for retrieval.Various techniques, including multi-query retriever, HyDE, etc., can be employed to rephrase/expand them and enhance performance.Figure 3 elucidates the operational mechanism of the RAG.To recall previous instructions and responses, LLMs and chatbots like ChatGPT incorporate this history into their context window.This buffer can be enhanced with summarization (e.g., using a smaller LLM), a vector store + RAG, etc [71].Both the document retrieval (context precision and recall) and generation stages (faithfulness and answer relevancy) need to be evaluated.Tools like Ragas and DeepEval can simplify this process [72].\n\nAdvanced RAG techniques bolster retrieval techniques for RAG models and evaluate their performance using industry-standard metrics.Advanced RAG techniques systematize various approaches and provide a comprehensive examination of the progression of RAG paradigms.They encompass the Naive RAG, the Advanced RAG, and the Modular RAG.Pinecone's LLM Agent exemplifies an agent that can utilize tools like calculators, search, or executing code.Using agents, an LLM can write and execute Python code.It can search for information and even query a SQL database.",
            "score": 0.6016001179865826,
            "section_title": "The Art of Reasoning and Acting",
            "char_start_offset": 34328,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 205
                },
                {
                    "start": 205,
                    "end": 402
                },
                {
                    "start": 402,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 665
                },
                {
                    "start": 665,
                    "end": 769
                },
                {
                    "start": 769,
                    "end": 948
                },
                {
                    "start": 948,
                    "end": 1009
                },
                {
                    "start": 1009,
                    "end": 1138
                },
                {
                    "start": 1138,
                    "end": 1195
                },
                {
                    "start": 1195,
                    "end": 1324
                },
                {
                    "start": 1324,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1569
                },
                {
                    "start": 1569,
                    "end": 1630
                },
                {
                    "start": 1632,
                    "end": 1763
                },
                {
                    "start": 1763,
                    "end": 1894
                },
                {
                    "start": 1894,
                    "end": 1962
                },
                {
                    "start": 1962,
                    "end": 2071
                },
                {
                    "start": 2071,
                    "end": 2126
                },
                {
                    "start": 2126,
                    "end": 2186
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.325439453125
        },
        {
            "corpus_id": "275336185",
            "title": "Foundations of GenIR",
            "text": "In this chapter, we introduce the foundations and applications of generative AI models in information accessing. Instead of analyzing how generative AI models like LLMs could improve the existing modules of search engines and recommendation systems, we focus on how the they could revolutionize information access with new methodologies and system design. Particularly, we discuss two new paradigms brought by generaive AI models, namely information generation and information synthesis. \n\nInformation generation refers to the scenarios where users can use generative AI models to create information that directly satisfies their information needs. Here, we delved into the core components of generative models, including model architectures (with a focus on Transformers and their improvements), scaling laws, and training methodologies. We examined the debates surrounding continual model scaling, the importance of prompt optimization, and the extension of these models to multi-modal applications for information access. \n\nInformation synthesis refers to the paradigm that utilizes the superior instructionfollowing and logic-reasoning ability of LLMs to aggregate and synthesize existing information. We extensively discuss one of the most representative techniques, i.e., Retrieval Augmented Generation (RAG), on this direction, and introduce various approaches from naive implementations to more sophisticated modular systems. We describe the challenges and opportunities in optimizing RAG systems, highlighting the need for joint retrieval-generation optimization and the potential of several relevant research directions such as composite retrieval with planning. Besides RAG, we also discuss some alternative paradigms that use generative AI models to model corpus knowledge directly, such as generative retrieval, which aims to replace traditional indexing methods with neural network-based approaches, and domain-specific model training, which conducts continue pre-training or fine-tuning on LLMs with the target corpus. We discussed the potential and limitations of these approaches, including issues of system controllability and cost efficiency. \n\nOverall, research on how generative AI models could reshape modern information access systems is still at an early stage today. As discussed above, existing studies on information generation and information synthesis either focus on simple information tasks (such as writing a poem, answering a factoid question, etc.) or reply on simple system design (e.g., feeding all documents to LLMs as prompts) that obviously cannot fully exploit the power of modern retrieval and generation models.",
            "score": 0.6012024638944876,
            "section_title": "Summary and Future Directions",
            "char_start_offset": 58908,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2161
                },
                {
                    "start": 2164,
                    "end": 2291
                },
                {
                    "start": 2292,
                    "end": 2653
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.472900390625
        },
        {
            "corpus_id": "277814037",
            "title": "AI-driven FMEA: integration of large language models for faster and more accurate risk analysis",
            "text": "Generative AI refers to a subset of AI models that are able to generate new data instances that are similar to the training data. Unlike traditional AI systems that perform classification or prediction tasks based on existing data, generative models create new content, such as text, images or music, by learning the underlying patterns and structures in the training data. Techniques such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs) and transformer-based models are widely used in generative AI. These models have a wide range of applications, including content creation, data augmentation and simulation of environments for training other AI systems (Kingma & Welling 2013;Goodfellow et al. 2014). \n\nLLMs are a type of generative AI specifically designed to understand and generate human language. These models are trained on large amounts of text data and can perform a variety of tasks such as translation, summarization, question answering and text completion. Examples of LLMs include OpenAI's GPT-4, Google's Gemini and the open-source GPT-Neo model. They work by predicting the next word in a sequence so that they can produce coherent and contextually relevant text based on the input received (Brown et al. 2020;Rae et al. 2022). \n\nThere are several techniques to work with LLMs, including prompting, finetuning and Retrieval-Augmented Generation (RAG). Here you will find a brief explanation of each technique: a) Prompt Engineering Prompt engineering involves the creation of effective prompts to elicit the desired responses from LLMs. Since LLMs respond to the input text provided, the way a prompt is structured can significantly affect the output of the model. Effective prompt engineering can improve the accuracy and relevance of the content generated. Techniques include using specific keywords, providing detailed context and iteratively refining prompts based on the model's responses (Liu et al. 2021). b) Retrieval-Augmented Generation RAG combines the strengths of retrieval-based and generative models to improve the accuracy and relevance of AI-generated content. In a RAG system, a retrieval component first searches a large corpus of documents to find the most relevant information based on the input query. This information is then passed to a generative model that produces a coherent and contextually appropriate response.",
            "score": 0.6011487327227739,
            "section_title": "Generative AI and LLM",
            "char_start_offset": 9341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1270
                },
                {
                    "start": 1273,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2266
                },
                {
                    "start": 2267,
                    "end": 2384
                }
            ],
            "ref_mentions": [
                {
                    "start": 706,
                    "end": 729,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 1234,
                    "end": 1253,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60546875
        },
        {
            "corpus_id": "265308606",
            "title": "Adapting LLMs for Efficient, Personalized Information Retrieval: Methods and Implications",
            "text": "Foundation models, although powerful, encounter several limitations due to their intrinsic reliance on static, pre-trained knowledge, hampering their capability to stay abreast of contemporary developments and demonstrating difficulties in executing specialized, domain-specific tasks. Moreover, these models, while being vast reservoirs of general knowledge, sometimes fail to navigate the intricacies and depth required in specialized domains. This issue, paired with the risk of \"hallucination\" -generating plausible but incorrect or nonsensical information -underscores the importance of seeking enhancements to leverage these models effectively and safely. Retrieval-Augmented Generation (RAG) emerges as a pivotal technology to address some of these limitations, forging a connection between the foundational knowledge of large language models and the dynamic, up-to-date information contained in external repositories. Imagine a system that goes beyond just understanding a query, having the intelligence to know where to look for answers and the ability to form responses coherently and contextually. This vision, named Retrieval-Augmented Generation (RAG), was detailed by Lewis et al. [18], showing a close connection between retrieval systems and the generative capabilities of language models. An overview of teh RAG achitecture id depicted in Figure 1. This approach strengthens the model's ability to find relevant information and present it in a useful and contextually relevant way, enhancing the usefulness of machine learning models in real-world information query scenarios. By facilitating real-time retrieval of relevant information during the inference process, RAG circumvents the static knowledge limitation, providing outputs that are not only rich and contextually relevant but also verifiable against a known dataset. \n\nNavigating the practical implementation of RAG brings several key advantages to the forefront. The potential for cost and computational savings is evident, as the need for frequent, exhaustive retraining of the model is mitigated, supplanted by updates to the retrieval database. Interestingly, the approach of RAG also lends a hand in addressing privacy concerns. Since RAG pulls from an external retrieval database, instead of integrating data into the model, it aids in keeping sensitive information more secure. This characteristic ensures that the responses generated are current and relevant, while also providing a structured way to handle sensitive or private information within the retrieval data.",
            "score": 0.601060879733905,
            "section_title": "Enhancing the Information Retrieval Process",
            "char_start_offset": 6200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1844
                },
                {
                    "start": 1847,
                    "end": 1941
                },
                {
                    "start": 1942,
                    "end": 2126
                },
                {
                    "start": 2127,
                    "end": 2211
                },
                {
                    "start": 2212,
                    "end": 2362
                },
                {
                    "start": 2363,
                    "end": 2553
                }
            ],
            "ref_mentions": [
                {
                    "start": 1195,
                    "end": 1199,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62353515625
        },
        {
            "corpus_id": "277596427",
            "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented Generation with Reranker KV-Cache Reuse",
            "text": "Retrieval-Augmented Generation (RAG) (Ram et al., 2023;Lewis et al., 2021;Asai et al., 2023;Khandelwal et al., 2020;Jin et al., 2024a;Shao et al., 2024) is the process of optimizing the output of a large language model and it references an authoritative knowledge base outside of its training data sources before generating a response. This paradigm has gained traction for tasks requiring factual accuracy and up-to-date information, such as question answering, summarization, and dialogue generation. \n\nFigure 1: Classic RAG Workflow: The query is embedded and used to retrieve top-K documents. Then the reranker selects the most relevant ones which are combined with the query to generate the final response. \n\nTraditional RAG pipelines rely on retrievers which depend on embedding representations and cosine similarity to fetch relevant documents (Robertson & Zaragoza, 2009;Reimers & Gurevych, 2019;Wang et al., 2024). While this approach is straightforward, it often struggles to achieve optimal results in more complex scenarios. To solve the problem, advancements have introduced reranker mechanisms that refine the retrieved documents to improve relevance and contextuality before generation. These rerankers, often transformer-based, significantly boost the quality of generated content by ensuring the most pertinent documents are prioritized. \n\nEarly Rerankers were predominantly trained on encoder-only models such as BERT (Devlin et al., 2019) or XLM-RoBERTa (Conneau et al., 2020), leveraging their strong encoding capabilities to improve retrieval precision. However, recent advancements have demonstrated the growing dominance of decoder-based rerankers which capitalize on the powerful generative language capabilities of modern decoder models (Chen et al., 2024;Ma et al., 2023;Pradeep et al., 2023). By fine-tuning these decoder-based models on tasks originally designed for encoder-only rerankers, they achieve significant gains in performance, benefiting from both their inherent generative power and the fine-grained contextual understanding acquired during fine-tuning.",
            "score": 0.6005411858952102,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 3985,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 2093
                }
            ],
            "ref_mentions": [
                {
                    "start": 879,
                    "end": 904,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78271484375
        },
        {
            "corpus_id": "276249796",
            "title": "C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation",
            "text": "Recent advances in retrieval-augmented generation (RAG) for large language models (LLMs) have demonstrated remarkable capabilities in various tasks (Anthropic, 2024; Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). Hurst et al., 2024;Dubey et al., 2024;Yang et al., 2024a;Mesnard et al., 2024;Asai et al., 2024;Chen et al., 2024a;b;Wei et al., 2025;Sun et al., 2025b;a), empowering LLMs to acquire up-to-date or domain-specific knowledge while mitigating hallucinations (Gao et al., 2023;Fan et al., 2024;Qiao et al., 2024). The effectiveness of RAG systems, however, hinges on the alignment1 between the retriever and the LLM-an inherently challenging goal as these components are typically developed independently without co-training. This lack of co-training can result in semantic mismatch and suboptimal interactions: retrievers may fail to provide information tailored to the LLM's needs, while LLMs may struggle to generate effective queries or seamlessly incorporate retrieved content. Existing approaches address this misalignment through three main strategies: (1) fine-tuning retrievers to align with LLM preferences, (2) optimizing LLMs to adapt to retriever behavior, and (3) introducing intermediate modules to bridge the gap between them (Ma et al., 2023;Shi et al., 2024;Asai et al., 2024;Wei et al., 2025;Yu et al., 2024a;b). Despite progress, these methods face notable challenges: fine-tuning retrievers often requires carefully curated data and may not be feasible for commercial search engines (Schmidt, 2014;Nakano et al., 2021), while optimizing LLMs is resource-intensive and risks compromising their original capabilities (Zhou et al., 2024).",
            "score": 0.5992568854220418,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1757
                }
            ],
            "ref_mentions": [
                {
                    "start": 383,
                    "end": 401,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 401,
                    "end": 420,
                    "matchedPaperCorpusId": "270559546"
                },
                {
                    "start": 422,
                    "end": 439,
                    "matchedPaperCorpusId": "271909559"
                },
                {
                    "start": 439,
                    "end": 457,
                    "matchedPaperCorpusId": "270225999"
                },
                {
                    "start": 578,
                    "end": 595,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 1360,
                    "end": 1377,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 1377,
                    "end": 1395,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1395,
                    "end": 1412,
                    "matchedPaperCorpusId": "271909559"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5537109375
        },
        {
            "corpus_id": "271212697",
            "title": "Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis",
            "text": "The generation module serves as the primary functional component within this system, receiving outputs from the data retrieval module and invoking the LLM module to summarize and condense information, ultimately generating conclusions.\n\nRetrieval-augmented generation (RAG) represents an innovative approach amalgamating retrieval-based methods with generative models, typically Language Models (LMs), to enrich the quality and relevance of generated text (Ciuc\u0103 et al., 2023;Gao et al., 2024).This article's expert question and answer system embodies the concept of RAG.In this paradigm, LLMs retrieve input questions (segmented into subqueries via CoT) from the data retrieval module to acquire information for enhancing generation.Subsequently, LLMs reconsider both the acquired information and original questions.During the reconsideration phase, prompt engineering, leveraging task description, input data, contextual information, and prompt style, enriches and standardizes LLM output (Zhao et al., 2023), enabling LLM to produce answers meeting predefined standards, akin to those of domain experts or proficient students.\n\nIn the realm of RAG research, advanced and modular RAG methodologies are evolving from the original or naive RAG approach (Gao et al., 2024).Advanced RAG incorporates additional processing stages pre-and post-retrieval.Pre-retrieval processing concentrates on optimizing data indexing via various methods, including data granularity refinement, index structure optimization, and metadata incorporation to enhance retrieval content quality.Postretrieval processing involves reranking and prompt compression, with embedding playing a crucial role.Optimization strategies encompass fine-tuning embedding or employing dynamic embedding methods.Embracing the design principles of Advanced RAG, we enhance the pre-retrieval stage with query planning and expansion, while bolstering post-retrieval stages with reranking and summarization techniques.",
            "score": 0.5989953149161431,
            "section_title": "Generation module",
            "char_start_offset": 18381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 237,
                    "end": 494
                },
                {
                    "start": 494,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 734
                },
                {
                    "start": 734,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 1129
                },
                {
                    "start": 1131,
                    "end": 1272
                },
                {
                    "start": 1272,
                    "end": 1350
                },
                {
                    "start": 1350,
                    "end": 1570
                },
                {
                    "start": 1570,
                    "end": 1676
                },
                {
                    "start": 1676,
                    "end": 1771
                },
                {
                    "start": 1771,
                    "end": 1973
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4755859375
        },
        {
            "corpus_id": "270560495",
            "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
            "text": "Large Language Models (LLMs) have the unique capability to understand and generate human-like text from input queries. When fine-tuned, these models show enhanced performance on domain-specific queries. OpenAI highlights the process of fine-tuning, stating:\"To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples, but the right number varies greatly based on the exact use case.\"This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI. This study highlights the need for vigorous investigation and validation of fine-tuned models for domain-specific tasks.",
            "score": 0.5988536024455067,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9189453125
        },
        {
            "corpus_id": "271329372",
            "title": "Customized Retrieval Augmented Generation and Benchmarking for EDA Tool Documentation QA",
            "text": "Retrieval augmented generation (RAG) enhances the accuracy and reliability of generative AI models by sourcing factual information from external databases, which is extensively employed in document-grounded question-answering (QA) tasks. Off-the-shelf RAG flows are well pretrained on general-purpose documents, yet they encounter significant challenges when being applied to knowledge-intensive vertical domains, such as electronic design automation (EDA). This paper addresses such issue by proposing a customized RAG framework along with three domain-specific techniques for EDA tool documentation QA, including a contrastive learning scheme for text embedding model fine-tuning, a reranker distilled from proprietary LLM, and a generative LLM fine-tuned with high-quality domain corpus. Furthermore, we have developed and released a documentation QA evaluation benchmark, ORD-QA, for OpenROAD, an advanced RTL-to-GDSII design platform. Experimental results demonstrate that our proposed RAG flow and techniques have achieved superior performance on ORD-QA as well as on a commercial tool, compared with state-of-the-arts. The ORD-QA benchmark and the training dataset for our customized RAG flow are open-source at https://github.com/lesliepy99/RAG-EDA.",
            "score": 0.597843275363029,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57421875
        },
        {
            "corpus_id": "272689561",
            "title": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey",
            "text": "RAG is proposed to enhance generation quality by leveraging external knowledge bases. As research progresses, RAG technology has undergone three major developmental stages: Naive RAG, Advanced RAG, and Modular RAG. \n\nNaive RAG. Typically, naive RAG follows a \"Retrievalthen-Read\" process [21,26,27], consisting of a simple retriever and a pre-trained language model as the generator. Its workflow involves two simple steps: (1) retrieving relevant passages from a pre-constructed knowledge base based on the user query, and (2) combining the retrieved information with the input query to generate a response. \n\nEarly works primarily focused on optimizing the integration of retrievers and generators, including end-to-end joint training of retrievers and generators [21,26], separately training generators to better utilize retrieved documents with frozen retrievers [3,28], and modifying the model's decoding methods [10,29]. With the emergence of LLMs, the capabilities of generative models have significantly advanced. To further enhance the quality of generated context, prompt engineering have been proposed to optimize model outputs without additional training. To enhance the model's reasoning capabilities and the robustness of responses, various prompting techniques such as Chainof-Thought (CoT) [30], Tree-of-Thought (ToT) [31], and Self-Consistency [32] have been proposed. These methods extend the number of LLM's reasoning paths, thereby improving the likelihood of arriving at the correct result during the decoding process. However, Naive RAG also faces certain limitations. Firstly, the retrieved documents may contain noise or irrelevant information, which can interfere with the model's responses [5,33]. Secondly, the high reasoning cost inherent to large models is further exacerbated in the RAG process; the inclusion of lengthy retrieved documents can slow down the generation process and consume more computational resources. \n\nAdvanced RAG. To tackle the issues discussed earlier, additional components have been added to the RAG process, making it more complex. These enhanced systems, known as Advanced RAG, introduce specialized modules at different stages of the retrieval and generation pipeline, which can be categorized as pre-retrieval and post-retrieval components.",
            "score": 0.5974627028965923,
            "section_title": "Retrieval-augmented Generation System",
            "char_start_offset": 6549,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 214
                },
                {
                    "start": 217,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1949
                },
                {
                    "start": 1952,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 292,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 295,
                    "end": 298,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 766,
                    "end": 770,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 867,
                    "end": 870,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 870,
                    "end": 873,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 918,
                    "end": 922,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 922,
                    "end": 925,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 1716,
                    "end": 1719,
                    "matchedPaperCorpusId": "256662612"
                },
                {
                    "start": 1719,
                    "end": 1722,
                    "matchedPaperCorpusId": "256459776"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69775390625
        },
        {
            "corpus_id": "277994166",
            "title": "CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction",
            "text": "Retrieval Augmented Generation (RAG) has emerged as a powerful application of Large Language Models (LLMs), revolutionizing information search and consumption. RAG systems combine traditional search capabilities with LLMs to generate comprehensive answers to user queries, ideally with accurate citations. However, in our experience of developing a RAG product, LLMs often struggle with source attribution, aligning with other industry studies reporting citation accuracy rates of only about 74% for popular generative search engines. To address this, we present efficient post-processing algorithms to improve citation accuracy in LLM-generated responses, with minimal impact on latency and cost. Our approaches cross-check generated citations against retrieved articles using methods including keyword + semantic matching, fine tuned model with BERTScore, and a lightweight LLM-based technique. Our experimental results demonstrate a relative improvement of 15.46% in the overall accuracy metrics of our RAG system. This significant enhancement potentially enables a shift from our current larger language model to a relatively smaller model that is approximately 12x more cost-effective and 3x faster in inference time, while maintaining comparable performance. This research contributes to enhancing the reliability and trustworthiness of AI-generated content in information retrieval and summarization tasks which is critical to gain customer trust especially in commercial products.",
            "score": 0.5968074336874015,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83056640625
        },
        {
            "corpus_id": "271039616",
            "title": "Meta-prompting Optimized Retrieval-augmented Generation",
            "text": "Focusing on Retrieval-Augmented Generation (RAG), this approach seeks to enhance truthfulness and curb hallucinations by expanding the initial prompt, which contains the initial query, with additional content retrieved from sources that are external to the LLM.Such additional content is obtained with the help of an auxiliary Retrieval Model where the retrieval model may be a simple Jacquard model or a vector database that extracts relevant content from external sources and pass it on to a Large Language Model that generates an appropriate response given the original query and the extracted content.If this external content is unstructured text, it may be of different lengths, such as sentences, paragraphs or full documents, among others.\n\nBy feeding LLM's knowledge, and curbing its whim, with further knowledge from external sources, more accurate answers are likely to be provided.\n\nCompared with other techniques, such as fine-tuning or prompt-engineering, RAG key advantage is the ease with which newer, up-to-date content is taken advantage of, as this does not require the costly compute of re-training neural networks (as in fine-tuning) or the costly human labour for the creation of further manually designed prompts (as in prompt-engineering).To be sure, all these techniques can nevertheless be mixed and function together.",
            "score": 0.5967657687933355,
            "section_title": "Retrieval-augmented generation",
            "char_start_offset": 424,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 261,
                    "end": 605
                },
                {
                    "start": 605,
                    "end": 746
                },
                {
                    "start": 748,
                    "end": 892
                },
                {
                    "start": 894,
                    "end": 1262
                },
                {
                    "start": 1262,
                    "end": 1343
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55224609375
        },
        {
            "corpus_id": "268248396",
            "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
            "text": "Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LMs in handling low-frequency entities on question answering tasks. We conduct extensive experiments on twelve LMs of varying size and type and different FT methods, data augmentation, and retrieval models. Our findings indicate that while FT boosts the performance across entities of varying popularity, RAG surpasses FT by a large margin particularly for least popular factual knowledge. Additionally, the success of both RAG and FT approaches is amplified by improving retrieval and data augmentation techniques. Fine tuning, while beneficial for small LMs, requires extensive resources. To address this issue, we propose the new Stimulus RAG approach that surpasses the effectiveness of fine tuning based approaches, thereby eliminating the need for the costly data augmentation and fine tuning step for enriching LMs with less popular factual knowledge.",
            "score": 0.5966274498692409,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92822265625
        },
        {
            "corpus_id": "270371535",
            "title": "Retrieval & Fine-Tuning for In-Context Tabular Models",
            "text": "The idea of pre-training a model on corpora of text prior to fine-tuning has been explored in the Natural Language Processing domain for both classification and generation tasks [14,27,42].Later iterations refined this idea to train a model and use its in-context learning abilities for new tasks [10].This elicited research into prompt engineering to determine what to actually put in a model's context [39,51].Similar to prompt engineering, to better utilize the model's context, one can search for similar examples from a corpora and use them to facilitate the task; this is known as Retrieval-Augmented Generation (RAG) [31] in the generative context.Other variants of the idea include training jointly with retrieval [22,8] and augmenting the output of the model with kNN via interpolating [29].These ideas are analogous to our approach of (i) fine-tuning and retrieving jointly, and (ii) disjoint kNN and fine-tuning in our ablations, respectively.LLMs have also been directly applied to tabular data [16,25,18] however, due to the pre-training of these foundation models on large text corpora, there is the possibility of data leakage, which causes concern with evaluations [7].Note that this is not the case with TabPFN as it has been trained on synthetic data.",
            "score": 0.5960972606805802,
            "section_title": "Links with LLMs",
            "char_start_offset": 17004,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 189,
                    "end": 302
                },
                {
                    "start": 302,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 800
                },
                {
                    "start": 800,
                    "end": 954
                },
                {
                    "start": 954,
                    "end": 1185
                },
                {
                    "start": 1185,
                    "end": 1269
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 182,
                    "matchedPaperCorpusId": "7138078"
                },
                {
                    "start": 297,
                    "end": 301,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 408,
                    "end": 411,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 624,
                    "end": 628,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 722,
                    "end": 726,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 726,
                    "end": 728,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1007,
                    "end": 1011,
                    "matchedPaperCorpusId": "249642722"
                },
                {
                    "start": 1011,
                    "end": 1014,
                    "matchedPaperCorpusId": "252992811"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6767578125
        },
        {
            "corpus_id": "271915646",
            "title": "Xinyu: An Efficient LLM-based System for Commentary Generation",
            "text": "The domain of Natural Language Processing (NLP) has witnessed substantial progress [14,21,30,31,42], especially through the advent of Large Language Models (LLMs) [2,17,18,27,35]. These models show exceptional text generation proficiency, yielding high fluency and readability outputs [32,39]. Their ability to adapt to downstream tasks with minimal in-context examples is particularly noteworthy. To further augment the efficacy of LLMs in downstream tasks, two main methods have been identified: supervised fine-Tuning (SFT) and retrieval augmented generation (RAG). \n\nSupervised Fine-Tuning (SFT) entails the adaptation of an LLM to a specific downstream task. This process refines the model's parameters to align with the data distribution and task requirements, ensuring the model's behavior mirrors human behavior within the given domain. The topic of SFT has been extensively explored in numerous research. Ouyang et al. [18] pioneered the introduction of supervised fine-tuning and reinforcement learning to align language models with human intent. Zhou et al. [41] compiled a dataset of merely 1K examples for SFT, demonstrating that the success of SFT depends on the quality and diversity of data. \n\nRetrieval Augmented Generation (RAG) amalgamates LLMs with content retrieved from external databases. This approach offers a promising solution to the challenges encountered by LLMs, such as hallucination, outdated knowledge, and untraceable reasoning processes. The conventional RAG process encompasses indexing, retrieval, and generation [9,15]. RAG has been further enhanced by a range of innovative techniques: fine-tuning retrieval models to obtain precise semantic representations [11,28,33], reformulating queries to align with the semantic space of queries and documents [8,20,29], fine-tuning LLMs to harmonize the output of the retriever with the LLM's preference [10,22,34]. \n\nIn our work, we leverage the advances of both SFT and RAG to enhance the performance of the Xinyu.",
            "score": 0.5960385733466876,
            "section_title": "RELATED WORK 2.1 Large Language Models",
            "char_start_offset": 5295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1895
                },
                {
                    "start": 1898,
                    "end": 1996
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 87,
                    "matchedPaperCorpusId": "259950027"
                },
                {
                    "start": 90,
                    "end": 93,
                    "matchedPaperCorpusId": "226262321"
                },
                {
                    "start": 93,
                    "end": 96,
                    "matchedPaperCorpusId": "259858754"
                },
                {
                    "start": 96,
                    "end": 99,
                    "matchedPaperCorpusId": "254998782"
                },
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8486328125
        },
        {
            "corpus_id": "278284698",
            "title": "Leveraging long context in retrieval augmented language models for medical question answering",
            "text": "Despite the potential of LLMs, the deployment of LLMs in healthcare faces significant safety threats. LLMs struggle to generate accurate and up-to-date responses on current topics, due to outdated knowledge, lack of domain-specific expertise, or hallucination [13][14][15][16][17] . \n\nRetrieval-Augmented Generation (RAG) is a pivotal innovation to enhance the quality and relevance of responses in LLMs [18][19][20][21] . Typically, a RAG system consists of a retrieval module and a generative module. When a user query is provided as input, the system first uses the retrieval module to fetch relevant documents or data snippets by searching through external data sources. Next, the generative module takes the retrieved information as input and produces a response to the user query. With the help of the retrieval module, the generative module can provide more accurate and factual answers without the need for continual training or fine-tuning. As such, RAG poses a promising direction for applications requiring high factual accuracy and specificity 14,22 . \n\nHowever, prompting LLMs with contextual information has trade-offs. On the one hand, providing contextual information enhances the model's ability to perform the downstream tasks by augmenting LLMs with external domain-specific knowledge that is under-represented in their pretraining data. On the other hand, the input of LLMs is bounded by the limit of their context windows. Even though recently released models can process an increasing number of tokens, the increased amount of content to reason over can still hinder model performance 23 . The quality of RAG completion also depends on the retrieval results, such as the density or positions of queryrelevant information 14,22,[24][25][26] . As retrieval systems are still imperfect, it is inevitable to retrieve information irrelevant to the user query 14 . \n\nA recent study reports an issue of \"lost-in-the-middle\", i.e., the position of key information in the LLM context impacts the quality of the model completions 24 . This issue occurs when a lengthy context of information is retrieved, and the highly relevant information is not ranked at the top or bottom of the retrieval results.",
            "score": 0.5959491060800642,
            "section_title": "Check for updates",
            "char_start_offset": 2198,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1880
                },
                {
                    "start": 1883,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 264,
                    "matchedPaperCorpusId": "224706057"
                },
                {
                    "start": 264,
                    "end": 268,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 268,
                    "end": 272,
                    "matchedPaperCorpusId": "233231373"
                },
                {
                    "start": 272,
                    "end": 276,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 276,
                    "end": 280,
                    "matchedPaperCorpusId": "267301644"
                },
                {
                    "start": 404,
                    "end": 408,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 412,
                    "end": 416,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 416,
                    "end": 420,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1056,
                    "end": 1059,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 1743,
                    "end": 1746,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 1749,
                    "end": 1753,
                    "matchedPaperCorpusId": "259360665"
                },
                {
                    "start": 1753,
                    "end": 1757,
                    "matchedPaperCorpusId": "263830692"
                },
                {
                    "start": 1876,
                    "end": 1878,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 2042,
                    "end": 2044,
                    "matchedPaperCorpusId": "259360665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5537109375
        },
        {
            "corpus_id": "271909360",
            "title": "Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research",
            "text": "Retrieval Augmented Generation (RAG) \n\nIn our LLM based approaches, we experiment with four methods -zero-shot prompting, few-shot prompting, chain-of-thought reasoning, and Retrieval Augmented Generation based Question Answering. In zero-shot prompting we provide a single prompt to the model. In few-shot prompting, we provide a set of topics and anecdotes to the model as examples. In the chain of thought (COT) approach, we provide a set of instructions for the model to follow. Finally, for Retrieval Augmented Generation (RAG) we provide context and questions to the model, from which it extracts information. \n\nZero-shot prompts are simple instructions or tasks given to an LLM that have not been specifically trained on that task. It serves as a baseline because it demonstrates the model's fundamental ability to understand and respond to prompts based solely on its pre-training [19]. In few-shot prompting, a small set of examples illustrating the desired outcome are manually selected and provided to the LLM. These examples allow the model to understand the tasks at hand and generate similar results [5]. Chain-of-thought prompting provides a set of intermediate steps to guide the LLM to mimic human-like reasoning. This significantly improves the capability of the LLM to understand complex reasoning and generate better topics [36]. Retrieval-augmented generation (RAG) combines the capabilities of an LLM with a retrieval system to source and integrate additional information into its responses [23]. This effort provides contextually richer and ultimately more accurate outputs. We do this by providing all the interview transcripts to the LLM as a custom knowledge base. Two considerations helped the RAG approach outperform the other approaches: 4.1.1 Focused Analysis: In our approach, LLM searches the knowledge base to find and retrieve parts of documents that are most relevant to the question in the query. This narrows the focus to the most relevant information and ensures attention to critical topics and nuances.",
            "score": 0.5945851236049685,
            "section_title": "Thematic analysis enhanced through",
            "char_start_offset": 13846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 39,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2042
                }
            ],
            "ref_mentions": [
                {
                    "start": 1513,
                    "end": 1517,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.623046875
        },
        {
            "corpus_id": "269605438",
            "title": "Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization",
            "text": "Most machine learning systems, including large generative models, are self-contained systems, with both knowledge and reasoning encoded in model parameters.However, these models do not work effectively for tasks that require knowledge grounding [46], especially in case of non-stationary data where new information is actively being produced [47,52].As suggested by Zamani et al. [52], this issue can be addressed when machine learning systems are being enhanced with the capability of retrieving stored content.For example, in retrieval-augmented generation (RAG), as a special case of retrieval-enhanced machine learning (REML) [52], systems consume the responses provided by one or more retrieval models for the purpose of (text) generation [21,22].RAG models demonstrate substantial promise across various applications, including open-domain question answering [16,21,53], fact verification [44], dialogue systems [5,42,48], and personalized generation [36,37].\n\nMany prior studies on RAG use off-the-shelf retrieval models.For instance, Nakano et al. [25] used APIs from a commercial search engine for text generation.Glass et al. [9], on the other hand, used a term matching retrieval model.Neural ranking models trained based on human annotated data have also been used in the literature [12,21].There also exist methods that only optimize the retrieval model and keep the language model parameters frozen [40].A research direction in this area argues that optimizing retrieval models in RAG should depend on the downstream language model that consumes the retrieval results.This is also motivated by the findings presented by Salemi and Zamani [38] on evaluating retrieval quality in RAG systems.There exist solutions based on knowledge distillation [13] or end-to-end optimization based on some simplifying assumptions [35].One of these assumptions is marginalization via top  approximation [10,21].",
            "score": 0.5938922104275081,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 350
                },
                {
                    "start": 350,
                    "end": 512
                },
                {
                    "start": 512,
                    "end": 752
                },
                {
                    "start": 752,
                    "end": 965
                },
                {
                    "start": 967,
                    "end": 1028
                },
                {
                    "start": 1028,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1197
                },
                {
                    "start": 1197,
                    "end": 1303
                },
                {
                    "start": 1303,
                    "end": 1418
                },
                {
                    "start": 1418,
                    "end": 1582
                },
                {
                    "start": 1582,
                    "end": 1704
                },
                {
                    "start": 1704,
                    "end": 1833
                },
                {
                    "start": 1833,
                    "end": 1908
                }
            ],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 349,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 380,
                    "end": 384,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 630,
                    "end": 634,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 744,
                    "end": 748,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 869,
                    "end": 872,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 921,
                    "end": 924,
                    "matchedPaperCorpusId": "51879945"
                },
                {
                    "start": 924,
                    "end": 927,
                    "matchedPaperCorpusId": "52006529"
                },
                {
                    "start": 957,
                    "end": 961,
                    "matchedPaperCorpusId": "269009728"
                },
                {
                    "start": 1136,
                    "end": 1139,
                    "matchedPaperCorpusId": "250391085"
                },
                {
                    "start": 1295,
                    "end": 1299,
                    "matchedPaperCorpusId": "252568176"
                },
                {
                    "start": 1299,
                    "end": 1302,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1652,
                    "end": 1656,
                    "matchedPaperCorpusId": "269293655"
                },
                {
                    "start": 1828,
                    "end": 1832,
                    "matchedPaperCorpusId": "230437591"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79736328125
        },
        {
            "corpus_id": "267320876",
            "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
            "text": "Retrieval-augmented generation (RAG) is an advanced technique that leverages external knowledge sources to enhance the text generation capabilities of large language models(LLMs). It retrieves relevant paragraphs from a corpus based on the input, and feeds them to the LLMs along with the input. With the help of external knowledge, LLMs can generate more accurate and credible responses and effectively address challenges such as outdated knowledge [19], hallucinations [3,9,35,62], and lack of domain expertise [30,46]. Therefore, RAG technology is attracting increasing attention. \n\nAlthough the effectiveness of retrieval-augmented strategies has been proven through extensive practice, their implementation still requires a significant amount of tuning. The overall performance of the RAG system is affected by multiple factors, such as the retrieval model, construction of the external knowledge base, and language model. Therefore, automatic evaluation of RAG systems is crucial. Currently, there are only a few existing benchmarks for evaluating RAG performance, as creating high-quality datasets and experimenting with them entail significant costs. These benchmarks can be classified into two types: reference-required and reference-free evaluation. Reference-free evaluation frameworks, such as RAGAS [13] and ARES [44], use LLM-generated data to evaluate RAG systems on contextual relevance, faithfulness, and informativeness. These frameworks do not depend on ground truth references, but only assess the coherence of the generated text with the retrieved context. This approach may be unreliable if the retrieved external information is low-quality. \n\nConsequently, reference-required evaluations remain the predominant method for assessing RAG systems. Existing benchmarks for reference-required evaluations, such as RGB [8] and NQ [26]. do have their limitations. First, they all rely on question answering tasks to measure the performance of RAG systems. Question answering is not the only RAG application scenario, and an optimization strategy that works well for question answering may not be generalized to other scenarios. Thus, these benchmarks may not capture the full potential of RAG systems.",
            "score": 0.5938684103142063,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 471,
                    "end": 474,
                    "matchedPaperCorpusId": "259075950"
                },
                {
                    "start": 474,
                    "end": 476,
                    "matchedPaperCorpusId": "264350686"
                },
                {
                    "start": 476,
                    "end": 479,
                    "matchedPaperCorpusId": "252904927"
                },
                {
                    "start": 479,
                    "end": 482,
                    "matchedPaperCorpusId": "261891399"
                },
                {
                    "start": 513,
                    "end": 517,
                    "matchedPaperCorpusId": "263835169"
                },
                {
                    "start": 1847,
                    "end": 1851,
                    "matchedPaperCorpusId": "86611921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.875
        },
        {
            "corpus_id": "270688152",
            "title": "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation",
            "text": "Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; where plan and answer tasks use the same example data, different loss functions, and train two task-specific prompts simultaneously. The right shows the inference process, where the plan-answer process is repeated iteratively until completion. Guu et al., 2020) enhances LLMs by retrieving relevant passages, thereby improving both the quality and accuracy of generated content, particularly in knowledge-intensive tasks (Shen et al., 2023;Chen et al., 2023). Early works (Es et al., 2023;Lyu et al., 2024) chose to retrieve once, incorporating a fixed number of retrieved passages with a query into LLMs to generate a response. Recent research indicates that adaptive retrieval, tailored to the demands of LLMs, can further enhance generation. FLARE (Jiang et al., 2023b) uses the generated sentence with a low confidence score as the query to retrieve external knowledge adaptively and then regenerates the current sentence, while Self-RAG (Asai et al., 2023) introduces special tokens allowing the model to adaptively retrieve and reflect the quality of generated content. SuRe (Kim et al., 2024) generates conditional summarizations of retrieval and evaluating them with carefully designed prompts. However, existing approaches may not take full advantage of the planning capabilities of LLMs. Additionally, these methods may struggle to extract relevant content from retrieved passages and are easily influenced by irrelevant information. \n\nParameter-Efficient Fine-Tuning. Despite the powerful generative capabilities of LLMs, fine-tuning them requires substantial computational resources (Lester et al., 2021;Ding et al., 2022;Liu et al., 2023). To achieve more efficient fine-tuning, parameter-efficient tuning methods have emerged. These methods either fine-tune a small portion of the model parameters or introduce additional learnable parameters without fine-tuning the model itself (Hu et al., 2021;Liu et al., 2021;Ding et al., 2022;Wang et al., 2023).",
            "score": 0.5937075294140721,
            "section_title": "Related Work",
            "char_start_offset": 7176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1533
                },
                {
                    "start": 1536,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 2055
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 88,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 334,
                    "end": 351,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 530,
                    "end": 548,
                    "matchedPaperCorpusId": "264350686"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80029296875
        },
        {
            "corpus_id": "273403982",
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "text": "RAFT demonstrates consistent performance improvements in domain-specific RAG tasks, including PubMed, HotpotQA, and Gorilla datasets, serving as a post-training enhancement for LLMs. FILCO (Wang et. al. 2023) , a method designed to enhance the quality of context provided to generative models in tasks like open-domain question answering and fact verification, addresses issues of over-or under-reliance on retrieved passages, which can lead to problems such as hallucinations in the generated outputs. The method improves context quality by identifying useful context through lexical and information-theoretic approaches and training context filtering models to refine retrieved contexts during test time. Reflection Token is a key attribute of Self-reflective Retrieval Augmented-Generation (Self-RAG) (Asai et. al. 2023), a novel framework designed to improve the factual accuracy of large language models (LLMs) by combining retrieval with self-reflection. Unlike traditional methods that retrieve and incorporate a fixed number of passages, Self-RAG adaptively retrieves relevant passages and uses reflection tokens to evaluate and refine its responses, allowing the model to adjust its behavior according to task-specific needs and has shown superior performance in open-domain question-answering, reasoning, fact verification, and long-form generation tasks. Intelligence and effectiveness of RAG are dependent a lot on the quality of retrieval and more meta-data understanding of the repository would enhance the effectiveness of the RAG system. A novel data-centric Retrieval-Augmented Generation (RAG) workflow advances beyond the traditional retrieve-then-read mode and employs a prepare-then-rewrite-then-retrieve-then-read framework, enhancing LLMs by integrating contextually relevant, time-critical, or domain-specific information. Key innovations include generating metadata, synthetic Questions and Answers (QA), and introducing the Meta Knowledge Summary (MK Summary) for clusters of documents (Mombaerts et. al. 2024). A recent paper introduces CommunityKG-RAG (Chang et. al. 2024), a zero-shot framework that integrates community structures within Knowledge Graphs (KGs) into Retrieval-Augmented Generation (RAG) systems.",
            "score": 0.5935456070758318,
            "section_title": "Recent Advancement in the field:",
            "char_start_offset": 25127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2241
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90185546875
        },
        {
            "corpus_id": "270870251",
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "text": "Generative large language models are prone to producing outdated information or fabricating facts, although they were aligned with human preferences by reinforcement learning [1] or lightweight alternatives [2][3][4][5].Retrieval-augmented generation (RAG) techniques address these issues by combining the strengths of pretraining and retrieval-based models, thereby providing a robust framework for enhancing model performance [6].Furthermore, RAG enables rapid deployment of applications for specific organizations and domains without necessitating updates to the model parameters, as long as query-related documents are provided.\n\nMany RAG approaches have been proposed to enhance large language models (LLMs) through query-dependent retrievals [6][7][8].A typical RAG workflow usually contains multiple intervening processing steps: query classification (determining whether retrieval is necessary for a given input query), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the order of retrieved documents based on their relevance to the query), repacking (organizing the retrieved documents into a structured one for better generation), summarization (extracting key information for response generation from the repacked document and eliminating redundancies) modules.Implementing RAG also requires decisions on the ways to properly split documents into chunks, the types of embeddings to use for semantically representing these chunks, the choice of",
            "score": 0.593324261846757,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 220,
                    "end": 432
                },
                {
                    "start": 432,
                    "end": 632
                },
                {
                    "start": 634,
                    "end": 758
                },
                {
                    "start": 758,
                    "end": 1312
                },
                {
                    "start": 1312,
                    "end": 1494
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 178,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 754,
                    "end": 757,
                    "matchedPaperCorpusId": "250340214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.728515625
        },
        {
            "corpus_id": "275358357",
            "title": "Knowledge Retrieval Based on Generative AI",
            "text": "The study evaluated the effectiveness of Retrieval-Augmented Generation (RAG) in improving the accuracy of Large Language Models (LLMs) by answering 103 questions in the TTQA dataset using 14 different models. As Figure 3 showed, the results indicate that models with RAG (w/ RAG) consistently outperformed those without it (w/o RAG), demonstrating the significant impact of RAG on accuracy. Specifically, as Table I showed, Taiwan-LLM-8x7B-DPO saw an accuracy increase from 57.28% to 88.35% with RAG, while Taiwan-LLM-7B-v2-0-1-chat and Mistral-8x7B-Instruct-v0-1 improved from 41.75% to 69.9% and from 37.86% to 60.19%, respectively. ChatGPT 3.5 also showed notable accuracy enhancement from 74.76% to 88.35% when supplemented with system-generated reference materials. This trend underscores RAG's efficacy in enhancing model performance across diverse LLMs. In the evaluation of TMMLU+, covering 66 topics and 23,015 questions, four models were tested: Google's gemma-7b-it, Meta's Llama-2-13b-chat-hf, MediaTek's Breeze-7B-Instruct-v0 1, and TAIDE-LX-7B-Chat. Performance was assessed with and without the Retrieval-Augmented Generation (RAG) approach. \n\nAs Figure 4 showed, TAIDE-LX-7B-Chat demonstrated decreased accuracy when RAG was applied, performing better as a standalone LLM. As Figure 6 showed, the gemma-7b-it model performed comparably to Llama-2-13b-chat-hf despite its smaller parameter size, reflecting consistent results across topics. Fig. 6. Performance for gemma-7b-it As Figure 7 showed, Breeze-7B-Instruct-v0 1 achieved the highest overall accuracy among the models, outperforming others on most topics regardless of RAG usage.",
            "score": 0.5924712037159842,
            "section_title": "IV. EXPERIMENTS",
            "char_start_offset": 12257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1157
                },
                {
                    "start": 1160,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1653
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85888671875
        },
        {
            "corpus_id": "272911196",
            "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
            "text": "We release all models and generated datasets to facilitate further study on HuggingFace1 . In Section 2, we present some related work on RAG, fine tuning methods for RAG including RAFT, and PEFT techniques such as LoRA. In Section 3, we introduce our compute-efficient RAFT method, CRAFT, in detail. In Section 4, we present the experiment setup. In Section 5 we report the results, and in Section 6 we conclude the paper. \n\nRetrieval Augmented Generation (RAG) RAG (Lewis et al., 2021) enhances LLMs by retrieving relevant document chunks from external knowledge bases through semantic similarity calculations. This method mitigates the generation of factually incorrect content by referencing external knowledge rather than relying solely on knowledge the model learned during training, thereby improving the relevancy of the generated text while reducing \"hallucinations\". Despite its advantages, RAG faces challenges, particularly with domain-specific or knowledge-intensive tasks, particularly when handling queries beyond the scope of its retrieved data (Zhang et al., 2023), though at a lesser extent when compared to non-retrieval-augmented LLMs. Other major challenges with RAG includes requiring a high-performing retriever model to produce representative embeddings from the document chunks and retrieval system that balances scale and accuracy. Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024). RAG excels in dynamic environments by offering real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. However, it comes with higher latency and the possibility of added noise from extraneous contexts. \n\nFine tuning for RAG Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever. RAFT (Zhang et al., 2024b) includes a fine tuning strategy that generates training data from the QA target domain data for instruction fine tuning.",
            "score": 0.5924253676002214,
            "section_title": "Introduction",
            "char_start_offset": 4339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 422
                },
                {
                    "start": 425,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2235
                },
                {
                    "start": 2236,
                    "end": 2383
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79345703125
        },
        {
            "corpus_id": "272593221",
            "title": "What is the Role of Small Models in the LLM Era: A Survey",
            "text": "Retrieval Augmented Generation (RAG) can extract query-relevant knowledge from an external document collection or knowledge base, and thus enhance general LLMs by leveraging their incontext learning ability. It involves first using a lightweight retriever to find relevant content from the domain corpus, which is then incorporated into the LLM's input to improve its understanding of domain-specific knowledge (Siriwardhana et al., 2023;Shi et al., 2023;Gao et al., 2023). \n\nAnother approach employs small expert models to retrieve background knowledge for the base LLM in a generative manner. For example, BLADE (Li et al., 2024a) and Knowledge Card (Feng et al., 2024) first pre-train a small expert model on domain-specific data, which then generates expertise knowledge in response to a query, thereby enhancing the base LLM's performance. \n\nSummary and Future Directions Tuning large models for specific target domains is resourceintensive. To address this challenge, a more efficient approach is to fine-tune a small model on domain-specific data. This lightweight expert model can then guide the LLM either during decoding (white-box adaptation) or inference (blackbox adaptation), offering a cost-effective solution for domain adaptation.",
            "score": 0.5924021327457709,
            "section_title": "Domain Adaptation",
            "char_start_offset": 27955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1247
                }
            ],
            "ref_mentions": [
                {
                    "start": 652,
                    "end": 671,
                    "matchedPaperCorpusId": "258741298"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5908203125
        },
        {
            "corpus_id": "273186155",
            "title": "Enhancing Retrieval in QA Systems with Derived Feature Association",
            "text": "RAG is an extremely active area of AI research with the goal of improving LLM performance and safety by augmenting the generation context with useful cached information [17]. The modern approach to RAG is generator-agnostic [14]. This is convenient as it allows for the use of the latest and greatest LLMs without the need for retraining; however, it places a much greater burden on the retriever to be effective and efficient. \n\nIn addition to the rather expensive and unportable practice of training domain-specific query encoders [14] and rerankers [3], simple modifications to the retrieval mechanism can make a profound impact on the system's performance. For example, [1] demonstrated the benefits of shorter chunk sizes and more granular indexing, while [11] and [13] construct multi-resolution document stores which afford a balance of context and precision. Input transformation has also been shown to be an effective tool for improving dense retrieval. In this paradigm, we use an LLM to generate a \"pseudo-document\" from the query: a piece of text that looks like the document we want to retrieve but it is actually contrived [2], serving as a template for the target document. \n\nData augmentation is an increasingly popular solution for improving dense retrieval. [6] uses an RAG system to retrieve audio clips using a text query. To achieve such behavior, they generate text aliases for each audio clip and index the clips according to these aliases. This approach serves as [1] demonstrates the utility of transforming RAG documents into more digestible, concise, and explicit forms. They propose \"propositional retrieval\", a method quickly adopted by the AI agent community [5] due to its effectiveness, portability, and runtime efficiency. Their method preprocesses documents to extract individual propositions from the text and indexes them instead of chunks of the original text. This greatly improves retrieval of information which is explicitly stated, but it sacrifices nuance that may be necessary for answering more complex questions.",
            "score": 0.5922932012912588,
            "section_title": "Related Work",
            "char_start_offset": 3981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 427
                },
                {
                    "start": 430,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1188
                },
                {
                    "start": 1191,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2057
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61376953125
        },
        {
            "corpus_id": "274964905",
            "title": "Chinese SafetyQA: A Safety Short-form Factuality Benchmark for Large Language Models",
            "text": "Theoretically, Retrieval-Augmented Generation (RAG) contributes to the factuality of LLMs (Lewis et al., 2020). In our study, we also evaluate the effectiveness of different RAG approaches. Specifically, we employ two types of RAG triggering methods: \n\n\u2022 Passive RAG (Lewis et al., 2020;Fan et al., 2024): \n\nThe LLM invokes RAG during every inference. \u2022 Active RAG (Asai et al., 2023;Jiang et al., 2023b): The LLM assesses whether its understanding of the given question is clear and accurate; if not, it calls RAG for knowledge enhancement. \n\nSimilar to other experiments, we report the average accuracy, with the results presented in Figure 5. We find that RAG benefits the safety factuality of LLMs, although the improvement is less significant compared to the general knowledge domain, as observed in SimpleQA and Chinese SimpleQA. Furthermore, we identified two noteworthy findings from the results. Firstly, RAG substantially mitigates performance disparities among models, yielding greater accuracy improvements for smaller models (e.g., Qwen2.5-3B) compared to larger ones (e.g., Qwen2.5-72B). Secondly, the effectiveness of active RAG exhibits considerable variability across different LLMs, and its overall effectiveness is considerably inferior to passive RAG. We suggest that this is because LLMs exhibit significant hallucination with overconfidence in responses, and the proportion of instances where RAG is proactively requested is much lower than the actual incorrect (IN) rate.",
            "score": 0.5918341035599218,
            "section_title": "ANALYSIS ON RAG CONTRIBUTIONS",
            "char_start_offset": 13241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 250
                },
                {
                    "start": 253,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1494
                }
            ],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 110,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 267,
                    "end": 287,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 287,
                    "end": 304,
                    "matchedPaperCorpusId": "269740933"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.572265625
        },
        {
            "corpus_id": "270219294",
            "title": "BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models",
            "text": "Large Language Models (LLMs) are constrained by outdated information and a tendency to generate incorrect data, commonly referred to as\"hallucinations.\"Retrieval-Augmented Generation (RAG) addresses these limitations by combining the strengths of retrieval-based methods and generative models. This approach involves retrieving relevant information from a large, up-to-date dataset and using it to enhance the generation process, leading to more accurate and contextually appropriate responses. Despite its benefits, RAG introduces a new attack surface for LLMs, particularly because RAG databases are often sourced from public data, such as the web. In this paper, we propose \\TrojRAG{} to identify the vulnerabilities and attacks on retrieval parts (RAG database) and their indirect attacks on generative parts (LLMs). Specifically, we identify that poisoning several customized content passages could achieve a retrieval backdoor, where the retrieval works well for clean queries but always returns customized poisoned adversarial queries. Triggers and poisoned passages can be highly customized to implement various attacks. For example, a trigger could be a semantic group like\"The Republican Party, Donald Trump, etc.\"Adversarial passages can be tailored to different contents, not only linked to the triggers but also used to indirectly attack generative LLMs without modifying them. These attacks can include denial-of-service attacks on RAG and semantic steering attacks on LLM generations conditioned by the triggers. Our experiments demonstrate that by just poisoning 10 adversarial passages can induce 98.2\\% success rate to retrieve the adversarial passages. Then, these passages can increase the reject ratio of RAG-based GPT-4 from 0.01\\% to 74.6\\% or increase the rate of negative responses from 0.22\\% to 72\\% for targeted queries.",
            "score": 0.5908375989664244,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3642578125
        },
        {
            "corpus_id": "273346023",
            "title": "Honest AI: Fine-Tuning \"Small\" Language Models to Say \"I Don't Know\", and Reducing Hallucination in RAG",
            "text": "Hallucination is a key roadblock for applications of Large Language Models (LLMs), particularly for enterprise applications that are sensitive to information accuracy. To address this issue, two general approaches have been explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated information as context, and fine-tuning the LLMs with new information and desired output styles. In this paper, we propose Honest AI: a novel strategy to fine-tune\"small\"language models to say\"I don't know\"to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark. In addition, our approach emphasizes the use of relatively small models with fewer than 10 billion parameters, promoting resource efficiency.",
            "score": 0.5904858345520513,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8310546875
        },
        {
            "corpus_id": "276617519",
            "title": "Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World Spatial Reasoning Questions",
            "text": "Retrieval-Augmented Generation (RAG) is a hybrid approach that integrates retrieval systems and generative models to enhance factual accuracy and contextual relevance in natural language generation (Fan et al., 2024). Unlike conventional language models that rely solely on parametric memory, RAG dynamically retrieves relevant external knowledge before generating a response. One of the foundational works in RAG is Lewis et al. (2020), where a retrieval module fetches relevant passages from a large-scale knowledge corpus (e.g., Wikipedia), which are then fused with the question context to generate a more informed response. This technique has proven particularly effective in open-domain question answering (QA), fact verification, and contextaware text generation. RAG systems have expanded beyond text and document retrieval to incorporate a wide variety of data types (He et al., 2024) -tables, graphs, charts, and diagrams. While RAG has been widely explored, its application in spatial reasoning question answering remains an unexplored research area. Existing studies have primarily focused on knowledge-grounded dialogues (Yu et al., 2024) but often struggle with integrating spatial computation into the question-answering process effectively.",
            "score": 0.5903659854560677,
            "section_title": "Retrieval Augmented Generation",
            "char_start_offset": 5657,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1256
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 216,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 417,
                    "end": 436,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1134,
                    "end": 1151,
                    "matchedPaperCorpusId": "274060284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6875
        },
        {
            "corpus_id": "274141028",
            "title": "Deploying Large Language Models With Retrieval Augmented Generation",
            "text": "Knowing that the generative capabilities of large language models (LLM) are sometimes hampered by tendencies to hallucinate or create non-factual responses, researchers have increasingly focused on methods to ground generated outputs in factual data. Retrieval Augmented Generation (RAG) has emerged as a key approach for integrating knowledge from data sources outside of the LLM's training set, including proprietary and up-to-date information. While many research papers explore various RAG strategies, their true efficacy is tested in real-world applications with actual data. The journey from conceiving an idea to actualizing it in the real world is a lengthy process. We present insights from the development and field-testing of a pilot project that integrates LLMs with RAG for information retrieval. Additionally, we examine the impacts on the information value chain, encompassing people, processes, and technology. Our aim is to identify the opportunities and challenges of implementing this emerging technology, particularly within the context of behavioral research in the information systems (IS) field. The contributions of this work include the development of best practices and recommendations for adopting this promising technology while ensuring compliance with industry regulations through a proposed AI governance model.",
            "score": 0.5901926317450373,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82763671875
        },
        {
            "corpus_id": "277781671",
            "title": "DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation",
            "text": "In this paper, we investigate the effectiveness of Retrieval-Augmented Generation (RAG) techniques in mitigating hallucination issues in large language models (LLMs). However, existing dy-namic RAG methods face significant limitations in two key aspects, 1) Lack of an effective mechanism to control retrieval triggers, and 2) Lack of effective scrutiny of retrieval content. To address these limitations, we propose an innovative dynamic RAG approach, DioR (Adaptive Cognitive Detection and Contextual Retrieval Optimization), achieving when retrieval is needed and what to retrieve for LLMs is useful. Compared to existing popular dynamic RAG methods, DioR demonstrates superior performance across four knowledge-intensive generation datasets, proving the effectiveness of our method in improving. \n\nLooking ahead, we will refine DioR, especially in its ability to tackle complex problems in a stepby-step manner. By integrating more refined reasoning strategies, we aim to further elevate the model's performance on intricate tasks.",
            "score": 0.5900244408971249,
            "section_title": "Conclusion and Future work",
            "char_start_offset": 24646,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1035
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "270562128",
            "title": "Retrieval-Augmented Generation for Generative Artificial Intelligence in Medicine",
            "text": "In the generation stage, both the user's query and the retrieved relevant information are prompted to the model to generate content.Compared to fine-tuning a model for a specialized task, RAG has been shown to improve accuracy for knowledge-intensive tasks 19 , and is a more flexible paradigm for model updates.\n\nIn this comment, we emphasize the significance of RAG in the era of generative AI, particularly its potential applications within medicine and healthcare.We primarily analyze the breakthroughs that RAG could bring to medicine from three perspectives: equity, reliability, and personalization (Figure 2).Additionally, we explore the limitations of RAG in medical application scenarios.",
            "score": 0.5899654068761914,
            "section_title": "Main Text",
            "char_start_offset": 2173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 312
                },
                {
                    "start": 314,
                    "end": 468
                },
                {
                    "start": 468,
                    "end": 617
                },
                {
                    "start": 617,
                    "end": 698
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.818359375
        },
        {
            "corpus_id": "274776545",
            "title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation",
            "text": "Retrieval-augmented Generation Retrievalaugmented generation (RAG) improves generation quality by incorporating relevant context from external knowledge bases, which typically employ a separate dense retriever (Gao et al., 2024;Tan et al., 2024b;Jin et al., 2024b;Tan et al., 2024a;Zhou et al., 2024). Based on training approaches, current RAG systems fall into three categories: \n\n(1) Directly prompt of generative models with retrieved context (Press et al., 2023;Trivedi et al., et al., 2020;Singh et al., 2021). However, joint training faces challenges due to the architectural differences between retrieval and generation, as well as the need for updating document indices during training. Some approaches aim to unify dense retrieval and generation within a single model, including GritLM (Muennighoff et al., 2024) and OneGen (Zhang et al., 2024a). However, GritLM operates as two distinct models with separate attention mechanisms that share parameters, while OneGen still relies on retrieving passage chunks as input for subsequent generation. \n\nGenerative Retrieval Generative retrieval (GR) retrieves by directly generating document identifiers (DocIDs) without the need for traditional document indices (Metzler et al., 2021) (Li et al., 2023a;Tang et al., 2024), and learnable DocIDs (Sun et al., 2023;Wang et al., 2023;Yang et al., 2023). However, these methods mainly focus on optimizing retrieval tasks, without considering its connections with downstream tasks. Even though UniGen (Li et al., 2024c) and CorpusLM (Li et al., 2024a) address downstream tasks, they still require mapping the generated Do-cIDs to the corresponding documents before feeding them into the generator. While RICHES (Jain et al., 2024) attempts to streamline this process but fails to solve the false pruning issue, which leads to suboptimal downstream task performance.",
            "score": 0.5898483290105838,
            "section_title": "Related Work",
            "char_start_offset": 22502,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1862
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 246,
                    "matchedPaperCorpusId": "267750726"
                },
                {
                    "start": 246,
                    "end": 264,
                    "matchedPaperCorpusId": "267751485"
                },
                {
                    "start": 446,
                    "end": 466,
                    "matchedPaperCorpusId": "252762102"
                },
                {
                    "start": 495,
                    "end": 514,
                    "matchedPaperCorpusId": "235390519"
                },
                {
                    "start": 1297,
                    "end": 1315,
                    "matchedPaperCorpusId": "264305656"
                },
                {
                    "start": 1315,
                    "end": 1333,
                    "matchedPaperCorpusId": "264350310"
                },
                {
                    "start": 1333,
                    "end": 1351,
                    "matchedPaperCorpusId": "264305656"
                },
                {
                    "start": 1498,
                    "end": 1516,
                    "matchedPaperCorpusId": "266359654"
                },
                {
                    "start": 1530,
                    "end": 1548,
                    "matchedPaperCorpusId": "267406766"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6982421875
        },
        {
            "corpus_id": "273403480",
            "title": "RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards",
            "text": "Retrieval-Augmented Generation (RAG) has proven its effectiveness in mitigating hallucinations in Large Language Models (LLMs) by retrieving knowledge from external resources. To adapt LLMs for the RAG systems, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system. In this paper, we propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG systems by aligning data preferences between different RAG modules. DDR works by collecting the rewards to optimize each agent in the RAG system with the rollout method, which prompts agents to sample some potential responses as perturbations, evaluates the impact of these perturbations on the whole RAG system, and subsequently optimizes the agent to produce outputs that improve the performance of the RAG system. Our experiments on various knowledge-intensive tasks demonstrate that DDR significantly outperforms the SFT method, particularly for LLMs with smaller-scale parameters that depend more on the retrieved knowledge. Additionally, DDR exhibits a stronger capability to align the data preference between RAG modules. The DDR method makes the generation module more effective in extracting key information from documents and mitigating conflicts between parametric memory and external knowledge. All codes are available at https://github.com/OpenMatch/RAG-DDR.",
            "score": 0.5880863644434837,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91650390625
        },
        {
            "corpus_id": "273375021",
            "title": "CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for Retrieval-Augmented Generation with Enhanced Data Diversity",
            "text": "In recent years, Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for improving the performance of large language models (LLMs). By integrating the retrieved context with queries, RAG systems can generate more accurate and reliable answers, thereby mitigating the issue of hallucinations that often plagues standalone generative models (Izacard et al. 2023). With the development of this technology, comprehensively evaluating all stages of RAG systems becomes increasingly important as it offers cally factual queries, wherein the answers usually consist of specific entities. This narrows their applicability and hampers their ability to handle more complex analytical or tutorial queries. (2) Obscure problems location: Most previous methods predominantly evaluated the end-to-end results without performing step-by-step analysis. The RAG process can be divided into several stages: chunking, retrieval, reranking, and generation. By solely assessing the final generated outcomes, it becomes challenging to identify problems at specific stages within the RAG pipeline. Such approaches would result in poor interpretability and low optimization efficiency, hindering the ability to refine individual components effectively. (3) Unstable retrieval evaluation: Previous methods evaluate the retrieval stage relying on the annotation of golden chunks with metrics such as Mean Reciprocal Rank and Hit Rate. Annotating all chunks is a tedious and labor-intensive process, and relabeling is required when the chunking strategy is modified. \n\nTo systematically address these challenges, we propose a Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough evaluation across the entire RAG pipeline. We introduce multi-granularity keywords to effectively assess the chunking, retrieval, and reranking phases of RAG systems, which aims to address the dependency on golden chunk annotations for evaluation. The multigranularity keywords encompass coarse-grained and finegrained keywords. Specifically, coarse-grained keywords are the most representative and relevant words extracted from the query and context, serving as initial indicators for chunk relevance. Fine-grained keywords are formulated as a set of lists, where each list corresponds to an information point extracted from the context, providing detailed references for answering the query.",
            "score": 0.5877132350834163,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2201
                },
                {
                    "start": 2202,
                    "end": 2392
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 380,
                    "matchedPaperCorpusId": "251371732"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80517578125
        },
        {
            "corpus_id": "276903168",
            "title": "Approximate Hausdorff Distance for Multi-Vector Databases",
            "text": "Recent advances in large-scale machine learning have driven the development of Retrieval-Augmented Generation (RAG), where vector databases (VectorDBs) [17,2] play a crucial role in providing efficient and scalable retrieval mechanisms. Unlike traditional information retrieval systems that rely solely on term-based search (e.g., BM25 [20], TF-IDF [22]), VectorDBs support semantic search [23,19,14] by storing and retrieving high-dimensional embeddings [18,11,1], allowing for more accurate context retrieval in generative AI applications. \n\nThe RAG system converts raw data from multiple modalities, including text, images, and audio, into high-dimensional vector embeddings using pretrained models such as OpenAI's Ada [16], Cohere [3], Sentence-BERT [19], CLIP [18], and BLIP [11]. Unlike traditional keyword-based retrieval, which relies on exact term matching, vectorization allows for semantic similarity search by mapping data points into a shared continuous space where proximity reflects conceptual similarity [14]. Recent advancements in contrastive learning [5] and multimodal pretraining [1] have further improved the quality and generalizability of vector representations. These models leverage large-scale self-supervised objectives to produce embeddings that capture deeper semantic relationships across diverse data types. As a result, vectorization is now a fundamental component of modern retrieval-augmented generation (RAG) systems. \n\nWhen a user submits a query, it is vectorized using the same embedding model. The query vector is then used to retrieve the most relevant vectors from the database using approximate nearest neighbor (ANN) search [8,13,10]. Unlike exact k-nearest neighbor (k-NN) search [4], which requires a linear scan or an exact tree-based search over all data points, ANN algorithms trade off a small amount of retrieval accuracy for significant gains in efficiency.",
            "score": 0.5876996407702335,
            "section_title": "Vector Databases",
            "char_start_offset": 5723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1454
                },
                {
                    "start": 1457,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1910
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 156,
                    "matchedPaperCorpusId": "264426371"
                },
                {
                    "start": 156,
                    "end": 158,
                    "matchedPaperCorpusId": "265502503"
                },
                {
                    "start": 336,
                    "end": 340,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 390,
                    "end": 394,
                    "matchedPaperCorpusId": "252995741"
                },
                {
                    "start": 394,
                    "end": 397,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 397,
                    "end": 400,
                    "matchedPaperCorpusId": "5959482"
                },
                {
                    "start": 755,
                    "end": 759,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1021,
                    "end": 1025,
                    "matchedPaperCorpusId": "5959482"
                },
                {
                    "start": 1669,
                    "end": 1672,
                    "matchedPaperCorpusId": "6110572"
                },
                {
                    "start": 1672,
                    "end": 1675,
                    "matchedPaperCorpusId": "8915893"
                },
                {
                    "start": 1675,
                    "end": 1678,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57177734375
        },
        {
            "corpus_id": "269484332",
            "title": "Towards a Search Engine for Machines: Unified Ranking for Multiple Retrieval-Augmented Large Language Models",
            "text": "The vast majority of machine learning systems, including large generative models, are designed as self-contained systems, with both knowledge and reasoning encoded in model parameters.However, these models cannot work effectively for tasks that require knowledge grounding [1,48], especially in case of non-stationary Figure 1: A high-level overview of the uRAG ecosystem.The ecosystem consists of a shared search engine that serves multiple RAG models, each performing its own task.\n\ndata where new information is actively being produced [49,57].As suggested by Zamani et al. [57], this issue can be addressed when machine learning systems are being enhanced with the capability of retrieving stored content.For example, in retrieval-augmented generation (RAG), as a special case of retrieval-enhanced machine learning (REML) [57], systems consume the responses provided by a retrieval model for the purpose of text generation [25,26].RAG models demonstrate substantial promise across various applications, including open-domain question answering [6,20,25,44,60], fact verification [46], dialogue systems [8,45,53], machine translation [4], and personalized generation [37,38].\n\nIn the RAG literature, the retrieval component is often implemented using either of the following two approaches:\n\n(1) Employing an off-the-shelf retrieval model that does not require training for the downstream RAG system: in this category, RAG systems either use APIs from commercial web search engines [29], term matching retrieval models [11], such as TF-IDF and BM25, or neural ranking models trained on relevance annotations provided as an external resource [25]; (2) Training a retrieval model given the feedback from the downstream RAG system through knowledge distillation [15] or endto-end optimization [35].\n\nAs expected, the latter category offers the current state-of-theart performance for various tasks [17,52].From an IR perspective, in this category, the downstream RAG model is the only \"user\" of the search engine.",
            "score": 0.5876195041664507,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 483
                },
                {
                    "start": 485,
                    "end": 547
                },
                {
                    "start": 547,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 936
                },
                {
                    "start": 936,
                    "end": 1179
                },
                {
                    "start": 1181,
                    "end": 1294
                },
                {
                    "start": 1296,
                    "end": 1799
                },
                {
                    "start": 1801,
                    "end": 1907
                },
                {
                    "start": 1907,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 543,
                    "end": 546,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 577,
                    "end": 581,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 827,
                    "end": 831,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1049,
                    "end": 1052,
                    "matchedPaperCorpusId": "230437698"
                },
                {
                    "start": 1052,
                    "end": 1055,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1055,
                    "end": 1058,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1058,
                    "end": 1061,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 1084,
                    "end": 1088,
                    "matchedPaperCorpusId": "4711425"
                },
                {
                    "start": 1107,
                    "end": 1110,
                    "matchedPaperCorpusId": "53218829"
                },
                {
                    "start": 1110,
                    "end": 1113,
                    "matchedPaperCorpusId": "51879945"
                },
                {
                    "start": 1113,
                    "end": 1116,
                    "matchedPaperCorpusId": "52006529"
                },
                {
                    "start": 1138,
                    "end": 1141,
                    "matchedPaperCorpusId": "235166182"
                },
                {
                    "start": 1171,
                    "end": 1175,
                    "matchedPaperCorpusId": "269009728"
                },
                {
                    "start": 1523,
                    "end": 1527,
                    "matchedPaperCorpusId": "250391085"
                },
                {
                    "start": 1645,
                    "end": 1649,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1794,
                    "end": 1798,
                    "matchedPaperCorpusId": "230437591"
                },
                {
                    "start": 1899,
                    "end": 1903,
                    "matchedPaperCorpusId": "251371732"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59326171875
        },
        {
            "corpus_id": "273812427",
            "title": "Enhancing Question Answering Precision with Optimized Vector Retrieval and Instructions",
            "text": "Accurately answering queries utilizing large-scale datasets has become an important task in the fast evolving filed of natural language processing (NLP). Recent advancements in machine learning and artificial intelligence have led to the development of sophisticated models that can parse, follow, and respond to natural language queries with unprecedented accuracy. Among these, the Retrieval-Augmented Generation (RAG) architecture represents a significant leap forward, combining the strengths of information (vector) retrieval and generative models for QA applications [7]. This paper introduces a novel methodology that leverages the RAG architecture, aiming to enhance QA precision through optimized vector retrieval and improved context construction.",
            "score": 0.5874896749952587,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 757
                }
            ],
            "ref_mentions": [
                {
                    "start": 573,
                    "end": 576,
                    "matchedPaperCorpusId": "252735056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5791015625
        },
        {
            "corpus_id": "273798525",
            "title": "LLM-Ref: Enhancing Reference Handling in Technical Writing with Large Language Models",
            "text": "A.1 Basic Retrieval-Augmented Generation (RAG) \n\nRetrieval-Augmented Generation (RAG) is an advanced technique that combines information retrieval with text generation, making it particularly effective when generating responses that require specific contextual information from an external knowledge base. The process is typically divided into three main stages: Ingestion, retrieval, and response generation. Ingestion: Once an input file is read, the first stage in RAG involves chunking and embedding, where source texts are segmented into smaller, manageable units, which are then converted into embedding vectors for retrieval. Smaller chunks generally enhance query precision and relevance, while larger chunks may introduce noise, reducing accuracy. Effective chunk size management is crucial for balancing comprehensiveness and precision. Embedding transforms both the user's query and knowledge base documents into comparable formats, enabling the retrieval of the most relevant information. \n\nRetrieval: In the next stage, the relevant information is retrieved from a vector knowledge base such as FAISS. The retriever searches this vector store to find the most relevant chunks of information based on the user's query. This stage is crucial for ensuring that the model has access to the necessary context for generating accurate and contextually relevant responses. \n\nResponse Generation: In the final stage, the retrieved context is combined with the user's query and fed into the LLM, such as GPT-4, to generate a coherent and relevant response. The model uses the context provided by the retrieved documents to produce answers that are informed by the most pertinent information available. This step highlights the synergy between retrieval and generation, ensuring that the output is not only accurate but also contextually grounded. \n\nEach stage of the RAG process is designed to leverage the strengths of both retrieval and generation, enabling the creation of responses that are informed by specific and relevant external knowledge. By combining these components, RAG systems can significantly enhance the quality and relevance of generated content, making them a powerful tool for applications requiring precise and contextually aware responses.",
            "score": 0.586920830532143,
            "section_title": "A Appendix",
            "char_start_offset": 31515,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 49,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1377
                },
                {
                    "start": 1380,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1849
                },
                {
                    "start": 1852,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2265
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.603515625
        },
        {
            "corpus_id": "273229050",
            "title": "Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG",
            "text": "This paper investigates the impact of increasing the number of retrieved passages on the performance of long-context LLMs in retrieval-augmented generation (RAG) systems. Contrary to expectations, we observe that performance initially improve but then degrade as more passages are included. This phenomenon is attributed to the detrimental influence of retrieved \"hard negatives\". To mitigate this issue, we propose and evaluate three solutions: training-free retrieval reordering, RAG-specific implicit LLM fine-tuning, and RAG-oriented LLM fine-tuning with intermediate reasoning. A systematic analysis of the training-based methods explores the effects of data distribution, retriever for training, and training context length. Interesting future directions include exploring (automated) position optimization with more advanced retrieval ordering methods, and fine-tuning the LLMs for RAG with more fine-grained and multi-step reasoning chains. (2) Contriever is more similar to BM25, while bge is more similar to e5 (since their curves are closer respectively).",
            "score": 0.5869113262875749,
            "section_title": "Conclusions",
            "char_start_offset": 29954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1066
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7802734375
        },
        {
            "corpus_id": "276287820",
            "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG",
            "text": "Retrieval Augmented Generation: RAG enhances Large Language Models (LLMs) by integrating external data sources, such as knowledge bases, to improve relevance and accuracy (Lewis et al., 2020;Guu et al., 2020;Karpukhin et al., 2020). Recent advancements have extended its applicability across domains (Asai et al., 2024;Kim et al., 2024;Yan et al., 2024;Liu et al., 2024), but RAG systems still face key challenges: hallucinations due to mismatches between retrieved data and the LLM's pre-existing knowledge (Setty et al., 2024;Jin et al., 2024), difficulty with complex multi-document reasoning (Setty et al., 2024), and an inability to fully leverage fixed-domain settings where all domain-specific documents are available beforehand because typically neither the retriever nor the generator LLM are trained on the domain data. Domain-Aware Fine-Tuning for RAG: Joint training of the retriever and LLM has been proposed as a way to improve RAG's domain-specific performance (Guu et al., 2020;Sachan et al., 2021;Siriwardhana et al., 2023;Shi et al., 2024). By jointly training the retriever and LLM, the system can better adapt to domain-specific contexts. However, this approach introduces complexities, including the need for specialized loss functions and frequent retriever updates. \n\nAnother line of work (Mecklenburg et al., 2024;Zhang et al., 2024b) focuses solely on adding domain knowledge to LLMs as an alternative to RAG. These approaches fine-tune LLMs using questionanswer (QA) pairs derived from domain data and aim to answer any new test query without retrieving any document. As a result, they fail to leverage access to the domain documents during inference. \n\nRecently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts.",
            "score": 0.5865953407996722,
            "section_title": "Related Work",
            "char_start_offset": 5971,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1677
                },
                {
                    "start": 1680,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1980
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8310546875
        },
        {
            "corpus_id": "277113527",
            "title": "Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems",
            "text": "Retrieval-Augmented Generation (RAG) has emerged as a promising framework to mitigate hallucinations in Large Language Models (LLMs), yet its overall performance is dependent on the underlying retrieval system. In the finance domain, documents such as 10-K reports pose distinct challenges due to domain-specific vocabulary and multi-hierarchical tabular data. In this work, we introduce an efficient, end-to-end RAG pipeline that enhances retrieval for financial documents through a three-phase approach: pre-retrieval, retrieval, and post-retrieval. In the pre-retrieval phase, various query and corpus preprocessing techniques are employed to enrich input data. During the retrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with domain-specific knowledge and implemented a hybrid retrieval strategy that combines dense and sparse representations. Finally, the post-retrieval phase leverages Direct Preference Optimization (DPO) training and document selection methods to further refine the results. Evaluations on seven financial question answering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA, and MultiHiertt-demonstrate substantial improvements in retrieval performance, leading to more accurate and contextually appropriate generation. These findings highlight the critical role of tailored retrieval techniques in advancing the effectiveness of RAG systems for financial applications. A fully replicable pipeline is available on GitHub: https://github.com/seohyunwoo-0407/GAR.",
            "score": 0.5863270551833076,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76025390625
        },
        {
            "corpus_id": "278534742",
            "title": "Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency",
            "text": "The emergence of Large Language Models (LLMs) represents a paradigm shift in artificial intelligence, demonstrating unprecedented capabilities in text generation, summarization, translation and complex reasoning. Despite training on vast corpora, LLMs remain prone to hallucinations-plausible yet incorrect outputs-and their static knowledge cutoff renders them unaware of developments beyond their training data. Moreover, the opaque nature of their reasoning hinders verification and accountability in high-stakes domains such as healthcare, robotics, legal analysis, and scientific research [1]- [5]. \n\nRetrieval-Augmented Generation (RAG) integrates external knowledge retrieval with generative models to mitigate these issues. First introduced by Lewis et al. [6], RAG systems retrieve semantically relevant document fragments prior to generation, grounding outputs in verifiable sources and enabling continuous updates. This dynamic interface improves factual accuracy, relevance and trustworthiness without sacrificing generative flexibility. \n\nThe adoption of RAG has expanded LLM applications in conversational AI, enterprise knowledge management and specialized professional domains. By incorporating retrieval, these systems handle complex, knowledge-intensive queries more precisely. However, retrieval incurs increased latency, complicates system architecture and raises ethical concerns around data privacy, consent and information quality-factors that demand careful design and governance. \n\nDespite growing deployment, the impact of hyperparameter and configuration choices on RAG performance and efficiency remains underexplored. Real-world constraints-computational resources, latency requirements and accuracy thresholds-necessitate an empirical understanding of how implementation decisions affect the trade-off between responsiveness and output quality. \n\nTo fill this gap, we conducted a systematic investigation varying key components-vector stores (Chroma, Faiss), chunking strategies (naive, semantic), cross-encoder re-ranking and temperature settings-while monitoring six performance metrics: faithfulness, answer correctness, answer relevancy, context precision, context recall and answer similarity. This multidimensional analysis reveals the accuracy-latency tradeoffs inherent in each choice and provides actionable insights for practitioners. \n\nThe contributions of this paper are fourfold. First, we introduce a unified evaluation framework that jointly measures RAG quality along six dimensions and tracks computational efficiency.",
            "score": 0.586314129543506,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 603
                },
                {
                    "start": 606,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1049
                },
                {
                    "start": 1052,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1504
                },
                {
                    "start": 1507,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1874
                },
                {
                    "start": 1877,
                    "end": 2228
                },
                {
                    "start": 2229,
                    "end": 2374
                },
                {
                    "start": 2377,
                    "end": 2422
                },
                {
                    "start": 2423,
                    "end": 2565
                }
            ],
            "ref_mentions": [
                {
                    "start": 599,
                    "end": 602,
                    "matchedPaperCorpusId": "272957646"
                },
                {
                    "start": 765,
                    "end": 768,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.875
        },
        {
            "corpus_id": "271843111",
            "title": "Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors for a Robotics Course",
            "text": "The evaluation of a Large Language Model-based tutoring system for a University robotics course highlighted several insights into the application of advanced LLM techniques and the resulting performance in an educational setting. First, our findings underscored the positive impact of Retrieval-Augmented-Generation (RAG) and prompt engineering, which consistently improved model performance across similarity metrics. Particularly, the use of RAG demonstrated a considerable enhancement in providing factual answers and is consistent with the general belief that RAG is reducing hallucinations [Shuster et al., 2021]. Furthermore, even though our human evaluation is currently restricted to two test subjects, their answers already point out that added references increases trustworthiness. Therefore, RAG appears as a very valuable technique that should be-together with some form of prompt engineering-considered first. As further advantages, in our experience RAG is quite straight forward to realize, in particular in a course setting in which well-curated background material is readily available. Furthermore, from a teaching point of view a tutor should stick to PREPRINT the lecture material, e.g., when going over a concept the tutoring system should carefully choose examples and ideally stick-or at least start-with the ones provided in the lecture. This should positively affect the learning of students. \n\nFine-tuning has to be considered as a more involved technique. It requires additional effort in setting up a data set for training. As an advantage, in our case we saw that a quite small fine-tuned model (13 billion parameters) consistently performed on the same level-or better-as GPT-3.5 (175 billion parameters) when used without RAG. Fine-tuning produced a much more efficient expert which showed as quite capable. But, on the downside, the process of fine-tuning appeared as more delicate. In our data, we observed a curious drop-off when adding RAG to the fine-tuned model which was unexpected and would contradict our and others' experience with RAG. As an explanation, fine-tuning aims to specialize a model to a specific task and a specific type of interaction. A fine-tuned model might loose some of its general flexibility. As a consequence, when interacting very differently with the model, the model might produce worse results or even behave erratically.",
            "score": 0.5861447095111523,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 42602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2190
                },
                {
                    "start": 2191,
                    "end": 2254
                },
                {
                    "start": 2255,
                    "end": 2388
                }
            ],
            "ref_mentions": [
                {
                    "start": 595,
                    "end": 617,
                    "matchedPaperCorpusId": "233240939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83349609375
        },
        {
            "corpus_id": "277103711",
            "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
            "text": "Retrieval-Augmented Generation (RAG) combines traditional information retrieval with generative models to handle open-domain QA tasks, improving factual accuracy by retrieving relevant documents from external sources and generating answers. However, simple implementation of RAG, where often one model instance is responsible for query understanding, retrieval, and answer generation, often face significant challenges due to cascading errors, a phenomenon where errors made early in the retrieval or reasoning process propagate through later stages, compounding mistakes and reducing overall system accuracy. [9] empirically demonstrate that retrieval-augmented language models are highly Preprint sensitive to the relevance of the retrieved context: while relevant passages enhance performance, irrelevant ones can lead to cascading errors, particularly in multi-hop reasoning scenarios. To address this, they propose a modular, black-box solution that leverages a natural language inference (NLI) model to filter out irrelevant passages without altering the underlying LLM's parameters.",
            "score": 0.5860944708907657,
            "section_title": "Retrieval-augmented generation (RAG)",
            "char_start_offset": 3693,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1089
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8271484375
        },
        {
            "corpus_id": "271534082",
            "title": "A Study on the Implementation Method of an Agent-Based Advanced RAG System Using Graph",
            "text": "This study aims to improve knowledge-based question-answering (QA) systems by overcoming the limitations of existing Retrieval-Augmented Generation (RAG) models and implementing an advanced RAG system based on Graph technology to develop high-quality generative AI services. While existing RAG models demonstrate high accuracy and fluency by utilizing retrieved information, they may suffer from accuracy degradation as they generate responses using pre-loaded knowledge without reprocessing. Additionally, they cannot incorporate real-time data after the RAG configuration stage, leading to issues with contextual understanding and biased information. To address these limitations, this study implemented an enhanced RAG system utilizing Graph technology. This system is designed to efficiently search and utilize information. Specifically, it employs LangGraph to evaluate the reliability of retrieved information and synthesizes diverse data to generate more accurate and enhanced responses. Furthermore, the study provides a detailed explanation of the system's operation, key implementation steps, and examples through implementation code and validation results, thereby enhancing the understanding of advanced RAG technology. This approach offers practical guidelines for implementing advanced RAG systems in corporate services, making it a valuable resource for practical application.",
            "score": 0.5860891942717131,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7314453125
        },
        {
            "corpus_id": "271534082",
            "title": "A Study on the Implementation Method of an Agent-Based Advanced RAG System Using Graph",
            "text": "The RAG (Retrieval-Augmented Generation) model combines retrieval and generation to produce answers by integrating document retrieval and generation models (Lewis et al., 2020). To generate an answer to a question, the model first retrieves relevant documents and then uses them to produce the response. This process helps in generating accurate answers to questions. The RAG model can handle various types of questions effectively, even when there is a lack of specific domain knowledge. Consequently, it enhances the accuracy and consistency of information compared to traditional generative models. \n\nThe RAG model consists of two main stages: \n\n\u2022 Retrieval Stage: Information relevant to the given question is retrieved through a search engine. \u2022 Generation Stage: Answers are generated based on the retrieved information.",
            "score": 0.5860859664529184,
            "section_title": "Overview of the RAG Model",
            "char_start_offset": 4778,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 646
                },
                {
                    "start": 649,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 826
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 176,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6953125
        },
        {
            "corpus_id": "268253024",
            "title": "Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks",
            "text": "Retrieval Augmented Generation (RAG) [34] is the backbone of current LLM-integrated applications.At its core, the RAG framework combines language models with data retrieval techniques, enabling the model to dynamically pull in relevant information from a database or the internet during the generation process.This approach enhances the model's ability to provide more accurate, up-to-date, and contextually relevant responses on information outside its original training set.RAG serves as the primary method for operating over external (untrusted) data sources.The majority of LLM-integrated applications that process files, web content, or any other sizable inputs implement a form of RAG-based pipeline to enable LLMs to access such resources in an efficient and scalable way.",
            "score": 0.5858341844953945,
            "section_title": "Retrieval Augmented Generation (RAG).",
            "char_start_offset": 7508,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 97,
                    "end": 310
                },
                {
                    "start": 310,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 562
                },
                {
                    "start": 562,
                    "end": 779
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20263671875
        },
        {
            "corpus_id": "273346841",
            "title": "Parenting: Optimizing Knowledge Selection of Retrieval-Augmented Language Models with Parameter Decoupling and Tailored Tuning",
            "text": "Large Language Models (LLMs) have demonstrated exceptional capabilities, achieving stateof-the-art performance on various tasks (Brown, 2020;Chowdhery et al., 2023;Bubeck et al., 2023). Despite their success, they still face notable challenges, particularly in generating hallucinations and dealing with outdated knowledge (Gao et al., 2023). Retrieval-Augmented Generation (RAG) has emerged as a promising approach to mitigate these issues (Peng et al., 2023;Ren et al., 2023;Lewis et al., 2020). By integrating external information relevant to a specific query, RAG enhances the generative process with supplementary non-parametric knowledge. However, typical RAG * Equal contribution. \u2020 Corresponding author. frameworks lack effective control mechanisms for managing internal and external knowledge (Li et al., 2023a), which presents two main challenges: First, the conflicts between external knowledge and the internal memory of LLMs can, in certain cases, prevent the model from effectively following external evidence to produce accurate responses (Wu et al., 2024b;Li et al., 2023a). Secondly, the inherent imperfections of retrieval mechanisms mean that the retrieved contexts might include irrelevant noises (Creswell et al.), which can mislead the LLMs, leading to degraded performance (Fang et al., 2024;Xu et al., 2024). \n\nTo address the above issues, some approaches optimize knowledge selection in the RAG process through carefully designed prompts (Zhou et al., 2023). However, such methods do not fundamentally improve LLMs' ability to integrate external knowledge, leading to suboptimal outcomes in certain situations. Other methods focus on altering the behavioral patterns of LLMs through training techniques such as instruction tuning (Li et al., 2023a;Fang et al., 2024;Xu et al., 2024). Nevertheless, they lack differentiation in supervisory signals, leading to significant learning variance. On one hand, an excessive emphasis on adhering to context can lead models to pay attention to noisy information. On the other hand, prioritizing resistance to noise might cause models to overlook critical evidence present within the context (Wu et al., 2024a).",
            "score": 0.5846089326065522,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1332
                },
                {
                    "start": 1335,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2175
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 164,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 477,
                    "end": 496,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 802,
                    "end": 820,
                    "matchedPaperCorpusId": "253420654"
                },
                {
                    "start": 1072,
                    "end": 1089,
                    "matchedPaperCorpusId": "253420654"
                },
                {
                    "start": 1755,
                    "end": 1773,
                    "matchedPaperCorpusId": "253420654"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6875
        },
        {
            "corpus_id": "269009744",
            "title": "RAR-b: Reasoning as Retrieval Benchmark",
            "text": "Semantic textual similarity (STS) and information retrieval tasks (IR) have been two principle measures to record the progress of dense representation models (Agirre et al., 2013;2014;2015;2016;Cer et al., 2017;Thakur et al., 2021).Despite still heavily being evaluated in sentence representation research, STS is known for its limited alignment with real-world use cases (Neelakantan et al., 2022;Muennighoff et al., 2023b), ambiguity (Deshpande et al., 2023), and performance orthogonality with IR and other downstream tasks (Reimers et al., 2016;Wang et al., 2021;Xiao et al., 2023a).\n\nIn the LLM era, Retrieval-augmented Generation (RAG) (Lewis et al., 2020;Neelakantan et al., 2022;Xu et al., 2023;Gao et al., 2023) has become a go-to alternative method to vanilla end-toend generative language models (OpenAI, 2023;Touvron et al., 2023).This shift is motivated by the inherent weaknesses of LLMs towards factual errors, due to hallucinations (Ji et al., 2023), knowledge outdatedness (Vu et al., 2023), rarity in long-tailed knowledge (Kandpal et al., 2023;Malaviya et al., 2023), and reasoning failure such as on logical deduction (Berglund et al., 2023).\n\nRetrieval-Augmented Generation (RAG) is employed differently across various NLP tasks:\n\n\u2022 For knowledge-intensive tasks, RAG is employed to retrieve the most up-to-date and reliable knowledge references (Vu et al., 2023;Malaviya et al., 2023), serving as new prompts for LLMs to extract information and formulate responses.This method mitigates models' natural tendencies to hallucinate (Ji et al., 2023) and reduces the need for frequent fine-tuning of LLMs.",
            "score": 0.5845473094412524,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 232,
                    "end": 587
                },
                {
                    "start": 589,
                    "end": 843
                },
                {
                    "start": 843,
                    "end": 1162
                },
                {
                    "start": 1164,
                    "end": 1250
                },
                {
                    "start": 1252,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1623
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 179,
                    "matchedPaperCorpusId": "10241043"
                },
                {
                    "start": 179,
                    "end": 184,
                    "matchedPaperCorpusId": "11650107"
                },
                {
                    "start": 184,
                    "end": 189,
                    "matchedPaperCorpusId": "11879061"
                },
                {
                    "start": 194,
                    "end": 211,
                    "matchedPaperCorpusId": "4421747"
                },
                {
                    "start": 398,
                    "end": 424,
                    "matchedPaperCorpusId": "252907685"
                },
                {
                    "start": 527,
                    "end": 549,
                    "matchedPaperCorpusId": "18283203"
                },
                {
                    "start": 549,
                    "end": 567,
                    "matchedPaperCorpusId": "233231435"
                },
                {
                    "start": 567,
                    "end": 586,
                    "matchedPaperCorpusId": "264487241"
                },
                {
                    "start": 642,
                    "end": 662,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 948,
                    "end": 965,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 1041,
                    "end": 1063,
                    "matchedPaperCorpusId": "253522998"
                },
                {
                    "start": 1138,
                    "end": 1161,
                    "matchedPaperCorpusId": "262083829"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50341796875
        },
        {
            "corpus_id": "271039066",
            "title": "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning",
            "text": "This study explores Parameter-Efficient Fine-Tuning (PEFT) methods applied to Retrieval-Augmented Generation (RAG) models, comparing GPT and RETRO architectures. RETRO generally outperforms GPT in zero-shot settings due to their pre-training process that integrates external retrieval, enhancing contextual understanding. However, GPT models show a higher performance potential with PEFT, indicating more room for improvement during fine-tuning. Both RETRO and GPT models perform optimally around the 8B parameter mark, balancing cost and performance. While P-tuning is effective in larger models, it lags behind other methods in smaller models, particularly for RETRO. Applying PEFT to Instructiontuned RETRO yields limited improvement compared to base RETRO, suggesting a saturation point in leveraging pre-training and fine-tuning benefits. Our comprehensive analysis offers valuable insights for optimizing large language models with PEFT and RAG to the community.",
            "score": 0.5843186912469251,
            "section_title": "Conclusion",
            "char_start_offset": 13407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 968
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "268889623",
            "title": "How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?",
            "text": "Retrieval-Augmented Generation (RAG) demonstrates impressive abilities in a wide range of knowledge-intensive tasks (Lewis et al., 2020;Guu et al., 2020;Borgeaud et al., 2022;Izacard et al., 2023). LLMs utilize retrieval systems to navigate through external knowledge bases and identify a set of potentially relevant documents, thereby extending beyond the limitations of their parametric memory. Specifically, leveraging dense retriever models (Karpukhin et al., 2020;Gautier et al., 2022) and in-context learning (ICL) (Brown et al., 2020), retrieval-augmented approaches have shown to be remarkably effective in enhancing the capabilities of LLMs (Luan et al., 2021;Mallen et al., 2023;Ram et al., 2023;Shi et al., 2023b). Nonetheless, a challenge persists in the practical deployment of RAG systems, as they indiscriminately surface top-ranked documents that still include irrelevant distractions (BehnamGhader et al., 2023;Wang et al., 2023;Asai et al., 2024;Cuconasu et al., 2024). This issue undermines their utility in real-world applications, where precision and relevance in information retrieval are critical for decision-making processes, such as in medical diagnoses (Zhou et al., 2023). The presence of irrelevant information can lead to inaccurate outcomes, highlighting the need to enhance the reliability of RAG systems.",
            "score": 0.5836222614414159,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 3232,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1337
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 136,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 136,
                    "end": 153,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 153,
                    "end": 175,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 175,
                    "end": 196,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 445,
                    "end": 469,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 469,
                    "end": 490,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 521,
                    "end": 541,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 669,
                    "end": 689,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 901,
                    "end": 928,
                    "matchedPaperCorpusId": "254854344"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60107421875
        },
        {
            "corpus_id": "269983269",
            "title": "IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning Inner Monologues",
            "text": "Large Language Models (LLMs) have demonstrated impressive capabilities in language understanding and generation [5,30,44]; however, there are two major challenges: generative hallucination [50] and static knowledge [18].While LLMs possess a deep understanding of human language and can generate creative responses, they lack the ability to verify facts or access up-to-date information [1,28].To mitigate such issues, integrating Information Retrieval (IR) systems with LLMs has become an increasingly promising direction.IR systems complement LLM by retrieving timely and relevant information, enhancing the factuality of responses.The synergy between LLMs and the IR systems -Retrieval Augmented Generation (RAG) [28,40] improves the ability of LLMs and powers generative AI products like ChatGPT, Bard, and Bing, showcasing the power and future potential of the combining IR systems and LLMs for more accurate and reliable responses.\n\nThere are two typical paradigms to improve RAG systems: the joint training approach v.s.training different components separately.The first paradigm involves joint training of LLMs and retrievers on knowledge-intensive tasks, enhancing retrieval capabilities of language models [13].For example, Guu et al. [10] did joint training of LLM and a retriever's semantic embedding, and",
            "score": 0.5831613495168576,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 220,
                    "end": 393
                },
                {
                    "start": 393,
                    "end": 522
                },
                {
                    "start": 522,
                    "end": 633
                },
                {
                    "start": 633,
                    "end": 936
                },
                {
                    "start": 938,
                    "end": 1026
                },
                {
                    "start": 1026,
                    "end": 1067
                },
                {
                    "start": 1067,
                    "end": 1220
                },
                {
                    "start": 1220,
                    "end": 1316
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 118,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1244,
                    "end": 1248,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66650390625
        },
        {
            "corpus_id": "270214689",
            "title": "EnterpriseEM: Fine-tuned Embeddings for Enterprise Semantic Search",
            "text": "In the context of enterprises accumulating proprietary unstructured data, AI-driven information retrieval solutions have emerged as vital tools for extracting relevant answers to employee queries. Traditional methods for developing such solutions often involve choosing between Retrieval Augmented Generation (RAG) or fine-tuned Large Language Models (LLMs). However, fine-tuned LLMs, comprising only generative models, lack a guarantee of factual accuracy, while RAG, comprising an embedding model and a generative model, assures factual precision (Lewis at al., 2020 [1]). Despite their superior performance in general, RAG based solutions often rely on pre-trained models, potentially leading to suboptimal alignment with enterprise-specific data. \n\nAddressing this challenge entails exploring two potential avenues: Firstly, recent studies such as RAFT (Zhang et al., 2024 [2]) explore the integration of fine-tuned generative models within a RAG pipeline to enhance accuracy, albeit requiring substantial domain-specific data to fine-tune the generative models. Alternatively, leveraging domain-specific embedding models within a RAG pipeline to enhance accuracy remains an underexplored area. \n\nEarlier efforts, such as BioBERT (Lee et al., 2019 [3]), SciBERT (Beltagy et al., 2019 [4]), and LEGAL-BERT (Chalkidis et al., 2020 [5]) have effectively demonstrated the efficacy of domain-specific embeddings in information retrieval tasks. These endeavors primarily investigated two methodologies: (a) extending the pre-training of BERT and (b) pre-training BERT from scratch, both employing domain-specific corpora. Despite yielding commendable results, these methodologies necessitated substantial domainspecific corpora, with figures as staggering as 21.3B words for BioBERT, 3.17B tokens for SciBERT, and 11.5GB of text data for LEGAL-BERT, thereby posing significant challenges, particularly in lowresource domains like enterprises.",
            "score": 0.5830881355556112,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 750
                },
                {
                    "start": 753,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1940
                }
            ],
            "ref_mentions": [
                {
                    "start": 569,
                    "end": 572,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8759765625
        },
        {
            "corpus_id": "270123034",
            "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
            "text": "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capabilities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across 12 question-answering tasks demonstrate the superiority of our approach.",
            "score": 0.5829331542784741,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90380859375
        },
        {
            "corpus_id": "273850363",
            "title": "Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge Reasoning and Text Generation",
            "text": "We will select five different generative models to conduct comparative experiments with the RAG model based on graph structure optimization proposed in this paper (labeled as Ours). By comparing their performance on the same dataset, we can verify the superiority of our model. The evaluation indicators used in the comparative experiments include Quality, Knowledge Consistency (KC), and Reasoning Capability(RC). 1, the performance of each model in the three indicators of Quality, Knowledge Consistency, and Reasoning Capability varies. It can be observed that traditional generative models such as BART and T5 performed relatively weakly in the experiment, especially in terms of knowledge consistency and reasoning capability. BART's quality score is 0.74, KC score is 0.65, and reasoning capability is 0.68, reflecting that although its generated text is fluent, it cannot provide sufficient external support in knowledge-intensive tasks. The T5 model has a slight improvement in the three indicators, especially in reasoning capability (0.72), which shows that the T5 model can improve its generalization ability by unifying task processing, but it is still not enough to fully handle complex knowledge reasoning tasks. \n\nIn contrast, the RAG model and its improved version RAG+T (RAG+Text) perform significantly better than BART and T5 in these three indicators. The RAG model combines the retrieval module to enable it to dynamically access the external knowledge base during the generation process, thereby significantly improving knowledge consistency (0.73) and reasoning capability (0.80). The performance of RAG+T is further improved, especially in terms of reasoning ability, which reaches 0.84, which shows the great potential of retrieval-enhanced generative models in complex tasks. By incorporating more relevant knowledge into the generation process, RAG+T shows stronger ability to deal with complex background information and deep reasoning, further narrowing the limitations of generative models in dealing with knowledge-intensive tasks. \n\nFinally, the graph-based RAG optimization model (ours) proposed in this paper performs well in all indicators, with a quality score of 0.90, knowledge consistency of 0.85, and reasoning ability of 0.91, which is significantly better than other models. This result verifies the effectiveness of our introduction of graph structure information.",
            "score": 0.5826018312171315,
            "section_title": "B. Experimental Results",
            "char_start_offset": 11942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 2061
                },
                {
                    "start": 2064,
                    "end": 2315
                },
                {
                    "start": 2316,
                    "end": 2406
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89208984375
        },
        {
            "corpus_id": "271744975",
            "title": "A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel Chatbot Use Case",
            "text": "Furthermore, it incorporates paged optimizers to handle memory spikes. The second method implemented was a novel method called Retrieval Augmented Fine Tuning (RAFT), a training procedure for domain-specific Retrieval Augmented Generation (RAG), which can adapt pretrained LLMs like LLaMa 2 and Mistral for RAG in specific domains, such as ours in travel. [9] RAG is a text generation method for outsourcing relevant information, from a knowledge base, or a large corpus of relevant, factual and quality information, to supply an LLM with contextual clues for producing factual responses. Then, an RLHF training pipeline will be done for domain-specific LLM curation, aligned with human preferences, with reward model training. [10] II. RELATED WORK Through a comprehensive review of literature and existing research papers, we build an understanding for state-or-theart techniques and approaches aimed to achieve competitive performances. New innovations are expressed in different variations for enhancing large language models. Table II summarizes the selection of models, objective addressed, unique approach, performance results and ultimate findings. A survey of existing solutions show that initiatives have been developed to overcome shortcoming identified with public general-purpose and pretrained LLM. Some of the feature performance issues garnered from literature review are potential risks of hallucination, underdeveloped retrieval approaches, and inefficiencies in the use of computational resources. \n\nZhang et. al introduced Retrieval Augmented fine-tuning (RAFT) as a novel training strategy for fine-tuning LLMs to better perform on RAG tasks. The key concept is data augmentation to generate \"question, answer, document\" triplets before fine-tuning. This is done by generating realistic questions paired with elaborate chain of thought answering scheme and purposefully including relevant and irrelevant context documents. Through a chain of thought with the distractor documents, the model learns to extract the correct information from the entire chunk of context through reasoning, ignoring the distractors. RAFT operates by training the model to disregard any retrieved documents that do not contribute to answering a given question, thereby eliminating distractions. The optimal ratio of oracle to distractor documents during training varies across datasets, but including some distractors improves generalization. Finally, during RAG, RAFT retrieves the top-k documents from the database. With RAFT, Zhang et.",
            "score": 0.5823974876354114,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1516
                },
                {
                    "start": 1519,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2292
                },
                {
                    "start": 2293,
                    "end": 2440
                },
                {
                    "start": 2441,
                    "end": 2515
                },
                {
                    "start": 2516,
                    "end": 2536
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72607421875
        },
        {
            "corpus_id": "267320876",
            "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
            "text": "In this paper, we have introduced an innovative framework (CRUD-RAG) for evaluating retrievalaugmented generation (RAG) systems that is both comprehensive and scenario-specific. Our unique categorization of text generation tasks into the CRUD-Create, Read, Update, and Delete-types provides a structured approach to assess the capabilities and limitations of RAG systems in handling a variety of textual contexts. To facilitate this evaluation, we have meticulously constructed largescale datasets for each CRUD category, which are tailored to challenge and reflect the performance of RAG systems under different operational conditions. Through rigorous experimental comparisons, we have demonstrated that RAG systems can significantly enhance the quality of generated content by effectively incorporating information from external knowledge sources. \n\nOur study delves into the intricate balance required in the fine-tuning process of RAG systems, highlighting the importance of optimizing the retrieval model, context length, construction of the knowledge base, and the deployment of the underlying large language model to achieve the best results. The insights provided by our findings offer a valuable roadmap for researchers and practitioners in the field, guiding them in the development and refinement of RAG systems. We believe that the methodologies and results presented in this paper will spur further exploration and innovation in the realm of RAG technologies. Our work aims to catalyze advancements in text generation applications, pushing the envelope of what is possible with the integration of retrieval mechanisms and language models. We hope that this contribution will serve as a cornerstone for future research efforts, fostering the creation of more intelligent, adaptive, and context-aware generative systems.",
            "score": 0.5823002185682042,
            "section_title": "CONCLUSION",
            "char_start_offset": 67227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1832
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94677734375
        },
        {
            "corpus_id": "273963285",
            "title": "Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for Bengali Mathematical Olympiad Problem Solving",
            "text": "Initial experiments with Retrieval-Augmented Generation (RAG) aimed to enhance the model's responses by adding contextual information. However, RAG often introduced noise, leading to a decrease in performance. Removing RAG subsequently improved scores, indicating that the inherent capabilities of the larger Qwen models were sufficient without additional retrieved context for this task.",
            "score": 0.5817991952971578,
            "section_title": "C. RAG Limitations",
            "char_start_offset": 7990,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 388
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.477783203125
        },
        {
            "corpus_id": "271329405",
            "title": "MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) combines traditional language models with external databases to improve natural language processing (NLP) tasks [33], [34], [35].RAG models use a retriever to retrieve relevant information and a generator to generate answers based on the retrieved information.This improves accuracy and relevance, especially for domainspecific queries [36], [16].RAG's strengths lie in its ability to update knowledge bases without retraining and customise components for specific tasks such as cybersecurity [33], [34].However, RAG struggles with latency and scalability issues, especially when processing concurrent queries [33].Despite these limitations, RAG remains a versatile tool for a range of NLP applications, from chatbots to content creation.Ongoing research focuses on optimizing retrieval mechanisms and computational power [36], [16].",
            "score": 0.5817991952971578,
            "section_title": "B. Retrieval Augmented Generation",
            "char_start_offset": 7134,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 297
                },
                {
                    "start": 297,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 652
                },
                {
                    "start": 652,
                    "end": 775
                },
                {
                    "start": 775,
                    "end": 870
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 153,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 155,
                    "end": 159,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 373,
                    "end": 377,
                    "matchedPaperCorpusId": "3541996"
                },
                {
                    "start": 379,
                    "end": 383,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 536,
                    "end": 540,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 647,
                    "end": 651,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 859,
                    "end": 863,
                    "matchedPaperCorpusId": "3541996"
                },
                {
                    "start": 865,
                    "end": 869,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6201171875
        },
        {
            "corpus_id": "278033562",
            "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation",
            "text": "Retrieval-Augmented Generation (RAG) has garnered significant attention in the field of natural language processing, leading to the development of various tools, benchmarks, and datasets aimed at evaluating system performance (Lewis et al., 2020 Neelakantan et al., 2022). The current body of work primarily focuses on measuring the quality of retrieved context (Karpukhin et al., 2020). However, existing solutions often have limitations, such as incomplete datasets or a lack of dedicated benchmarks that comprehensively cover both retrieval and generation tasks (Fabbri et al., 2021). This section reviews relevant tools, QA datasets, and benchmarks that have contributed to the evaluation of RAG systems, highlighting their strengths and areas where improvements are needed (Yang et al., 2015).",
            "score": 0.5817991952971578,
            "section_title": "Related Work",
            "char_start_offset": 5095,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 798
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 245,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 565,
                    "end": 586,
                    "matchedPaperCorpusId": "220768873"
                },
                {
                    "start": 778,
                    "end": 797,
                    "matchedPaperCorpusId": "1373518"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8125
        },
        {
            "corpus_id": "277104712",
            "title": "RAG-KG-IL: A Multi-Agent Hybrid Framework for Reducing Hallucinations and Enhancing LLM Reasoning through RAG and Incremental Knowledge Graph Learning Integration",
            "text": "Jesson et al. presented a Bayesian model to estimate the probability of hallucination in ICL tasks by calculating response probabilities based on context [19]. \n\nAnother approach to measuring hallucination includes the use of a cross-encoder model and n-gram overlap metrics to promote more grounded responses [20]. These measurements provide an empirical framework to understand hallucination better and evaluate the mitigation approaches. \n\nAddressing hallucination in generative AI has received significant attention. Several strategies have been proposed to mitigate hallucinations, ranging from improvements in training methodologies to more advanced inference techniques. \n\nRetrieval-Augmented Generation (RAG): RAG combines generative models with information retrieval techniques to ensure that the generated output is grounded in real, verifiable data. B\u00e9chard et al. [21] demonstrated the effectiveness of RAG in reducing hallucinations in structured outputs like workflow generation from natural language instructions. \n\nChannel-Aware Domain-Adaptive Generative Adversarial Network (CADAGAN): Grayson et al. [22] proposed CADA-GAN, which modifies generative models by introducing channel-aware processing and domain-adaptive learning. This technique helps in mitigating hallucinations by aligning generated output with domain-specific knowledge while maintaining linguistic coherence. In addition, the research shows that selfrefining approach is another approach for training an agent incrementally rather than batch process. \n\nGenetic Algorithm for Grounded Answer Generation (GAuGE): Kulkarni et al. [20] introduced a genetic algorithmbased grounded answer generation method to minimize hallucination. This method effectively maintains high relevance by cross-checking with retrieved search engine results and encouraging grounding through a balanced fitness function. \n\nDomain-Specific Adaption and Knowledge Graph Utilization: Techniques such as domain adaptation and knowledge graph integration have shown promise in controlling hallucination. Towhidul Islam Tonmoy et al. [23] reviewed multiple mitigation strategies, including knowledge retrieval and domain adaptation to enhance fact consistency in generated texts.",
            "score": 0.5814121876542511,
            "section_title": "A. Hallucination in Generative AI",
            "char_start_offset": 4947,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 162,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 440
                },
                {
                    "start": 443,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 677
                },
                {
                    "start": 680,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1536
                },
                {
                    "start": 1539,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1881
                },
                {
                    "start": 1884,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "270379551"
                },
                {
                    "start": 310,
                    "end": 314,
                    "matchedPaperCorpusId": "272203021"
                },
                {
                    "start": 876,
                    "end": 880,
                    "matchedPaperCorpusId": "269137180"
                },
                {
                    "start": 1613,
                    "end": 1617,
                    "matchedPaperCorpusId": "272203021"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8974609375
        },
        {
            "corpus_id": "278339615",
            "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing",
            "text": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing language models' knowledge and reducing AI generative hallucinations, driving its widespread use. However, complex tasks requiring multi-round retrieval remain challenging, and early attempts tend to be overly optimistic without a good sense of self-skepticism. Current multi-round RAG systems may continue searching even when enough information has already been retrieved, or they may provide incorrect answers without having sufficient information or knowledge. Existing solutions either require large amounts of expensive human-labeled process supervision data or lead to subpar performance. This paper aims to address these limitations by introducing a new framework, \\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system self-practice multi-round retrieval, augmenting existing question-answer pairs with intermediate inner monologue reasoning steps to generate synthetic training data. For each pair, the system may explore multiple retrieval paths, which are labeled as successful if they reach the correct answer and unsuccessful otherwise. Using this data, we train a lightweight information sufficiency Critic. At inference time, the Critic evaluates whether the RAG system has retrieved sufficient information at each round, guiding retrieval decisions and improving system-level self-awareness through in-context reinforcement learning. Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an effective multi-round RAG solution. Furthermore, this framework is system-efficient, adding a lightweight component to RAG without requiring modifications to existing LLMs or search engines, and data-efficient, eliminating the need for costly human-annotated mid-step retrieval process supervision data.",
            "score": 0.5809594333626158,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72607421875
        },
        {
            "corpus_id": "269614233",
            "title": "A Method for Parsing and Vectorization of Semi-structured Data used in Retrieval Augmented Generation",
            "text": "Retrieval Augmented Generation (RAG) within large language models (LLMs) marks a significant stride in AI research, blending advanced knowledge retrieval with the generation capabilities of LLMs.This approach aims to boost the accuracy and relevance of the models' responses while preserving their contextual depth.Current research focuses on fine-tuning the retrieval process, ensuring that the information fetched aligns closely with user queries and enhances the quality of the model's output (Lewis et al., 2021.).A key challenge lies in integrating this retrieved information smoothly into the generation process, creating responses that are both coherent and contextually appropriate (Rohde et al., 2021).\n\nA significant area of exploration is in improving the retrieval phase to filter out irrelevant information or 'noise', ensuring that the data used by the model is of high quality and relevance (Karpukhin et al., 2020).Researchers are also working on making LLMs more adaptable in using this retrieved data across various topics, enhancing the algorithms that control how the model accesses and uses this information (Kalyan et al., 2021).\n\nCentral to RAG's function in LLMs is the creation of vector databases from unstructured or semi-structured data like texts and web pages.These databases store information in a format that LLMs can easily access and use.Current research, including work on Transformer-based models, is pivotal in developing methods to efficiently transform vast amounts of data into these useful vector formats (Devlin et al., 2019).\n\nHowever, a noticeable gap in this area is the lack of simple, efficient methods for creating these vector databases.Existing techniques, while effective, tend to be complex and resource-heavy, limiting their broader application.Addressing this challenge with more user-friendly vectorization methods is crucial.Such advancements would significantly widen the scope and effectiveness of LLMs, enabling them to process and generate more nuanced, context-rich language responses in a range of fields, thus enhancing the practical utility and reach of LLMs in various applications.",
            "score": 0.5808756409911888,
            "section_title": "Background and Related work",
            "char_start_offset": 3741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 195,
                    "end": 315
                },
                {
                    "start": 315,
                    "end": 518
                },
                {
                    "start": 518,
                    "end": 711
                },
                {
                    "start": 713,
                    "end": 931
                },
                {
                    "start": 931,
                    "end": 1151
                },
                {
                    "start": 1153,
                    "end": 1290
                },
                {
                    "start": 1290,
                    "end": 1372
                },
                {
                    "start": 1372,
                    "end": 1568
                },
                {
                    "start": 1570,
                    "end": 1686
                },
                {
                    "start": 1686,
                    "end": 1798
                },
                {
                    "start": 1798,
                    "end": 1881
                },
                {
                    "start": 1881,
                    "end": 2147
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66748046875
        },
        {
            "corpus_id": "275788867",
            "title": "ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation",
            "text": "Retrieval augmented generation (RAG) models are a subset of large language models (LLMs) which combine the generation capabilities of conventional LLMs with the factual grounding of information retrieval (IR) models to create more factually accurate outputs from LLMs (Lewis et al., 2020). RAG models work by taking a user question as input, and then selecting several reference texts with high semantic similarity (determined by an IR model) from a database. An LLM is then given these texts with the original question and is instructed to answer the question basing the answer on the relevant reference texts. \n\nRAG not only allows for more accurate answers to questions regarding general public knowledge (Guu et al., 2020;Ram et al., 2023), it also allows LLMs to generate responses based on locally available or domain specific information that it has not necessarily been trained upon (Gao et al., 2023;Zhang et al., 2024). \n\nHowever, the models that have exhibited the highest performance in RAG tasks are based on proprietary cloud-based LLMs, meaning that LLMs run locally are more likely to generate hallucinations or other untruthful outputs when being used for RAG (Hughes et al., 2023). Moreover, LLMs that are not trained using data from a specific domain exhibit lower RAG accuracy in that domain (Zhang et al., 2024). \n\nTo address this, we propose a framework called Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG). ALoF-TRAG improves the accuracy of base RAG systems by automatically training on the data which the system will later be used, all without using larger models or labelled data. \n\nWe demonstrate the effectiveness of ALoF-TRAG by performing experiments on 20 datasets in 26 languages across a variety of domains and comparing the accuracy to simply using the base LLM for RAG. We show that the ALoFTRAG approach improves both the citation accuracy and answer accuracy of RAG models across almost all datasets compared to the base RAG model. \n\nOur findings inform the future implementation of RAG systems, allowing users to fine-tune their RAG models on local data using modest hardware, enabling improved RAG accuracy while preserving data security.",
            "score": 0.5804842388852964,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 929
                },
                {
                    "start": 932,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1640
                },
                {
                    "start": 1643,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2002
                },
                {
                    "start": 2005,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 288,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 708,
                    "end": 726,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 726,
                    "end": 743,
                    "matchedPaperCorpusId": "256459451"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9072265625
        },
        {
            "corpus_id": "267406766",
            "title": "CorpusLM: Towards a Unified Language Model on Corpus for Knowledge-Intensive Tasks",
            "text": "Large language models (LLMs) have recently revolutionized fields such as question answering (QA), dialogue, and information retrieval, demonstrating impressive capabilities in a variety of language tasks [3,8,9,49,62]. However, LLMs often face the problem of \"hallucination\", where the generated text may contain misleading or false information [20]. This issue is particularly severe in knowledge-intensive (KI) tasks, such as slot filling [12] and opendomain question answering [24]. To address this, a popular approach is the use of retrieval-augmented generation (RAG), which involves a retriever to obtain relevant context from a large knowledge corpus, followed by a generator model that synthesizes the retrieved context into coherent responses [19,27,66]. \n\nTraditionally, the retrieval component follows an index-centric framework [22,23,42,56,61]. Although this approach is widely used, it has several drawbacks. Notably, the requirement for a large document index to search the entire corpus results in considerable memory footprint. Furthermore, during training, the disparate model structures for retrieval (matching similarity) and generation (auto-regressive) [2,27,31,36] hinder the joint optimization of both models. This limitation restricts the understanding of the relationship between both tasks. \n\nRecently, generative retrieval (GR) has emerged as a promising paradigm [4,35], which employs auto-regressive generative models to retrieve relevant documents by directly generating document identifiers (DocIDs) [47]. This method has shown improved performance in web search and question-answering (QA) scenarios. While prior research has focused on enhancing model training [46,53,63], DocID design [30,45,54,65], and task adaption [6,7] for better retrieval performance, the unification of generative retriever and downstream generator is often overlooked. Furthermore, the potential of LLMs in the field of generative retrieval remains unexplored.",
            "score": 0.5804637144634748,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1317
                },
                {
                    "start": 1320,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1970
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 207,
                    "end": 209,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 345,
                    "end": 349,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 441,
                    "end": 445,
                    "matchedPaperCorpusId": "4612975"
                },
                {
                    "start": 480,
                    "end": 484,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 752,
                    "end": 756,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 756,
                    "end": 759,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 844,
                    "end": 847,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 847,
                    "end": 850,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 850,
                    "end": 853,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1175,
                    "end": 1178,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1178,
                    "end": 1181,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1392,
                    "end": 1395,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 1532,
                    "end": 1536,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 1695,
                    "end": 1699,
                    "matchedPaperCorpusId": "258865792"
                },
                {
                    "start": 1702,
                    "end": 1705,
                    "matchedPaperCorpusId": "266163955"
                },
                {
                    "start": 1720,
                    "end": 1724,
                    "matchedPaperCorpusId": "258947148"
                },
                {
                    "start": 1727,
                    "end": 1730,
                    "matchedPaperCorpusId": "264350310"
                },
                {
                    "start": 1730,
                    "end": 1733,
                    "matchedPaperCorpusId": "265225868"
                },
                {
                    "start": 1753,
                    "end": 1756,
                    "matchedPaperCorpusId": "258418300"
                },
                {
                    "start": 1756,
                    "end": 1758,
                    "matchedPaperCorpusId": "251594672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68701171875
        },
        {
            "corpus_id": "272955388",
            "title": "Application of RAG Model Based on Retrieval Enhanced Generation Technique in Complex Query Processing",
            "text": "The RAG model, known as Retrieval-Augmented Generation (RAG), is a natural language processing framework that combines information retrieval with generative modeling [7].The core idea of the RAG model is to retrieve document fragments relevant to the query from a large-scale document repository before generating a natural language response, and these fragments are fed into the generative model as contextual information to make the generated response more relevant and accurate [8]. These snippets are fed into the generation model as contextual information to make the generated response more relevant and accurate. By combining retrieval and generation in this way, the RAG model is able to provide more accurate answers in complex query scenarios\uff0cshowed in Figure 1: Retrieval Augmented Generation (RAG) is a technique that deeply integrates traditional information retrieval methods with generative modeling. In the RAG model, the retriever is responsible for filtering out the most relevant documents or information fragments from a large-scale dataset, while the generator generates natural language responses based on these retrieved contents [9]. The advantage of this approach is that it can utilize the rich information in the external knowledge base to compensate for the knowledge blindness of the generative model due to insufficient training data when facing complex queries, thus improving the quality and accuracy of the response [10]. The Information Retrieval Step: \n\nThe workflow of a RAG model is usually divided into two main steps: retrieval and generation. In the retrieval phase, the model selects the document fragments with the highest relevance to the query from a pre-constructed document library based on the input query. These snippets are passed to the model in the generation phase as additional contextual information. The model in the generation phase then uses these information fragments to generate the final natural language response. Through this two-stage processing, the RAG model is able to better understand the deeper semantics behind complex queries and generate answers that match the user's needs. Response Generation Step: \n\nThe advantages of RAG models in complex query processing are mainly reflected in their efficient utilization of external knowledge and high relevance of generated content. Compared with traditional generative models, RAG models not only rely on pre-trained language models, but also dynamically acquire the most relevant knowledge to the query, which gives them an obvious advantage when facing diverse and high-complexity queries.",
            "score": 0.5802066283057636,
            "section_title": "Overview of the RAG model and its retrieval enhancement generation technique",
            "char_start_offset": 2174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1486
                },
                {
                    "start": 1489,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2147
                },
                {
                    "start": 2148,
                    "end": 2173
                },
                {
                    "start": 2176,
                    "end": 2347
                },
                {
                    "start": 2348,
                    "end": 2607
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 169,
                    "matchedPaperCorpusId": "3238904"
                },
                {
                    "start": 481,
                    "end": 484,
                    "matchedPaperCorpusId": "7883049"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81005859375
        },
        {
            "corpus_id": "268032903",
            "title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)",
            "text": "In our research, we concentrated primarily on the application of retrieval augmentation during the inference stage, without delving into its integration during pre-training or fine-tuning phases. Future work will aim to explore these compelling areas. Moreover, while our study has highlighted the privacy risks associated with commonly employed retrieval-augmented generation (RAG) systems, other retrieval-based language models (LMs) feature distinct components and architectures (Huang et al., 2023;Borgeaud et al., 2022) that warrant further investigation. In addition, developing effective strategies to protect retrieval data and leveraging RAG systems for the safeguarding of training data represent open research questions that we intend to pursue.",
            "score": 0.5797545466786015,
            "section_title": "Limitations",
            "char_start_offset": 29021,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 756
                }
            ],
            "ref_mentions": [
                {
                    "start": 502,
                    "end": 524,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41259765625
        },
        {
            "corpus_id": "273963747",
            "title": "AssistRAG: Boosting the Potential of Large Language Models with an Intelligent Information Assistant",
            "text": "The emergence of Large Language Models (LLMs) has significantly advanced the field of natural language processing, demonstrating an impressive ability to mimic human-like language patterns [1]. However, despite their extensive knowledge acquired during training, LLMs can occasionally generate factually incorrect information, a phenomenon referred to as \"hallucination\" [2,3]. To address this, the integration of retrieval systems with LLMs has been suggested, allowing these models to tap into external databases to generate more reliable responses [4]. \n\nInitially, retrieval-augmented generation (RAG) relied on a simple \"Retrieve-Read\" framework [5], which was adequate for basic question-answering but insufficient for complex, multi-step reasoning tasks. As language models advanced, various prompt-based RAG strategies emerged [6,7], incorporating pre-retrieval and post-retrieval prompts to refine the process. However, these strategies heavily relied on the foundational capabilities of the language models. Consequently, the focus shifted to Supervised Fine-Tuning (SFT)-based RAG methods [8], which involve fine-tuning language models specifically for RAG tasks to enhance their performance. \n\nWhile SFT-based methods have improved the quality of generated responses, they face two limitations that hinder their practical application. Firstly, these fine-tuned models are not easily adaptable to emerging LLMs, requiring retraining for each new foundational LLM. Secondly, directly fine-tuning a foundational LLM in the RAG scenario may change its innate abilities, potentially leading to negative impacts on the model's performance on other tasks. To address these challenges, we propose Assistant-based Retrieval-Augmented Generation (ASSISTRAG), which integrates an intelligent information assistant as a plugin within LLMs. This approach comprises a trainable assistant for information management and a static main LLM dedicated to task execution, as depicted in Figure 1. \n\nAs an intelligent information assistant, ASSISTRAG operates in two primary categories to handle complex tasks: memory management and knowledge management. Memory management involves integrating and analyzing content from internal memory, while knowledge management focuses on leveraging external knowledge. These two main functions are supported by four core capabilities of ASSISTRAG: (1) Tool usage, which involves recalling relevant information from both internal memory and external knowledge bases through a retriever;",
            "score": 0.579742578670666,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1988
                },
                {
                    "start": 1991,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2297
                },
                {
                    "start": 2298,
                    "end": 2514
                }
            ],
            "ref_mentions": [
                {
                    "start": 189,
                    "end": 192,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 371,
                    "end": 374,
                    "matchedPaperCorpusId": "226254579"
                },
                {
                    "start": 551,
                    "end": 554,
                    "matchedPaperCorpusId": "269762693"
                },
                {
                    "start": 651,
                    "end": 654,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78662109375
        },
        {
            "corpus_id": "264833257",
            "title": "ChipNeMo: Domain-Adapted LLMs for Chip Design",
            "text": "It is well known that LLMs can generate inaccurate text, so-called hallucination (Ji et al., 2023). Although the phenomenon is not completely understood, we still must mitigate hallucinations since they are particularly problematic in an engineering assistant chatbot context, where accuracy is critical. Our proposal is to leverage the retrieval augmented generation (RAG) method. RAG tries to re-trieve relevant passages from a database to be included in the prompt together with the question, which grounds the LLM to produce more accurate answers. We find that using a domain adapted language model for RAG significantly improves answer quality on our domain specific questions. Also, we find that fine-tuning an off-the-shelf unsupervised pre-trained dense retrieval model with a modest amount of domain specific training data significantly improves retrieval accuracy. Our domain-adapted RAG implementation diagram is illustrated on Figure 3. We created our domain adapted retrieval model by finetuning the e5 small unsupervised model (Wang et al., 2022) with 3000 domain specific auto-generated samples using the Tevatron framework (Gao et al., 2022). We refer readers to the details on the sample generation and training process in Appendix A.8. \n\nEven with the significant gains that come with fine-tuning a retrieval model, the fact remains that retrieval still struggles with queries that do not map directly to passages in the document corpus or require more context not present in the passage. Unfortunately, these queries are also more representative of queries that will be asked by engineers in real situations. Combining retrieval with a domain adapted language model is one way to address this issue.",
            "score": 0.5794674098921602,
            "section_title": "Domain-Adapted Retrieval Model",
            "char_start_offset": 13772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1253
                },
                {
                    "start": 1256,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1718
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 98,
                    "matchedPaperCorpusId": "246652372"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.599609375
        },
        {
            "corpus_id": "273323460",
            "title": "Do You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation",
            "text": "Recent progress on large-scale pre-trained language models (LMs) (Brown et al., 2020;Anil et al., 2023) has demonstrated great potential in revolutionizing a wide array of applications across fields, ranging from natural language understanding and generation to complex problem-solving in scientific research. Despite their remarkable abilities, generative LMs suffer from poor interpretability and transparency, as well as the intrinsic risk of hallucination and misinformation, which collectively prohibit them from being deployed in safety-critical domains such as healthcare (Wornow et al., 2023;D'Antonoli et al., 2024). \n\nRetrieval augmented generation (RAG) (Lewis et al., 2020) is a promising approach for enhancing language models (LMs) by incorporating verifiable, current information from external knowledge databases. Incorporating this external context to complement the inherent knowledge of LMs has demonstrated notable benefits in reducing occurrences of hallucination and misinformation, thereby improving the reliability of content produced (Shuster et al., 2021). Still, numerous studies (Karpukhin et al., 2020;Gao et al., 2022;Tan et al., 2022;Yan et al., 2024) show that the effectiveness of RAG is dependent on the relevance between the query and retrieved documents. In cases where documents of weak relevance are provided, they can become distractions for the LM (Shi et al., 2023), leading to the generation of incorrect answers. At the present moment in time, there is no viable solution for safety-critical RAG systems to possess mechanisms for (1) evaluating the relevance of queries submitted from users to the knowledge corpus and flagging low-relevance queries in real-time that cannot be adequately addressed using the available knowledge or (2) identifying significant shifts in query distribution that are indicative of a potential misalignment between the knowledge corpus and user interests, which would suggest an outdated knowledge database that requires updating. \n\nTo address these deficiencies, in this paper we establish a statistical framework for accurate assessment of the query-knowledge relevance in retrievalbased LMs through hypothesis testing.",
            "score": 0.5790392651069052,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 2003
                },
                {
                    "start": 2006,
                    "end": 2194
                }
            ],
            "ref_mentions": [
                {
                    "start": 65,
                    "end": 85,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 600,
                    "end": 624,
                    "matchedPaperCorpusId": "263621147"
                },
                {
                    "start": 665,
                    "end": 685,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1059,
                    "end": 1081,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 1107,
                    "end": 1131,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1148,
                    "end": 1165,
                    "matchedPaperCorpusId": "247475975"
                },
                {
                    "start": 1388,
                    "end": 1406,
                    "matchedPaperCorpusId": "256459776"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7666015625
        },
        {
            "corpus_id": "269484468",
            "title": "How Can I Improve? Using GPT to Highlight the Desired and Undesired Parts of Open-ended Responses",
            "text": "Additionally, while the prompting approach offers flexibility in testing different prompts to quickly gauge the model's capabilities on our task, its effectiveness heavily depends on the quality of the prompt design.As observed during our prompt engineering phase, inadequate prompts can lead to misleading outputs.\n\nOn the other hand, fine-tuning allows for deeper model customization by adjusting internal parameters to closely align with our task in identifying the components of praises from tutor responses, often resulting in superior performance measured by M-IOU scores, as observed in our study.Finetuning enables the GPT model to deeply integrate new knowledge and adjust its existing knowledge, better fitting the task requirements of identifying components of effort-and outcome-based praise.Despite these advantages, fine-tuning requires a substantial amount of relevant and high-quality data and significant computational resources.The data must be carefully annotated to guide the model effectively toward the desired behavior, which present a significant limitation if such data is scarce or difficult to collect.Additionally, finetuning involves updating the weights of a neural network based on a specific dataset, a process that can be resourceintensive and requires access to powerful hardware, especially for larger models.\n\nTo address some of these challenges and further enhance our highlighted feedback system, we are considering the integration of Retrieval-Augmented Generation (RAG).RAG combines the strengths of both retrieval and generation models to improve the performance of language models on specific tasks [35].RAG could enhance the performance of prompting LLMs by dynamically incorporating relevant external information into responses, providing more informed and contextually accurate outputs (e.g., [20]).Additionally, RAG can be integrated with the fine-tuning approach for providing highlighted feedback, potentially improving the model's accuracy in highlighting components of praise.This integration aims to create a model that not only leverages external data through RAG but also adapts more finely to specialized tasks through fine-tuning, demonstrating superior performance in contextually rich and dynamic environments.",
            "score": 0.5782143525018787,
            "section_title": "DISCUSSION",
            "char_start_offset": 42315,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 216,
                    "end": 315
                },
                {
                    "start": 317,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 804
                },
                {
                    "start": 804,
                    "end": 946
                },
                {
                    "start": 946,
                    "end": 1129
                },
                {
                    "start": 1129,
                    "end": 1344
                },
                {
                    "start": 1346,
                    "end": 1510
                },
                {
                    "start": 1510,
                    "end": 1646
                },
                {
                    "start": 1646,
                    "end": 1844
                },
                {
                    "start": 1844,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2267
                }
            ],
            "ref_mentions": [
                {
                    "start": 1641,
                    "end": 1645,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.748046875
        },
        {
            "paperId": "1236a748381656b262447b0e96c845c3875b6bc4",
            "corpusId": 275757903,
            "title": "GEC-RAG: Improving Generative Error Correction via Retrieval-Augmented Generation for Automatic Speech Recognition Systems",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 23,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.10734, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2341334971",
                    "name": "Amin Robatian"
                },
                {
                    "authorId": "2304545588",
                    "name": "Mohammadjavad Hajipour"
                },
                {
                    "authorId": "2341328685",
                    "name": "Mohammad Reza Peyghan"
                },
                {
                    "authorId": "2341322957",
                    "name": "Fatemeh Rajabi"
                },
                {
                    "authorId": "2140133",
                    "name": "Sajjad Amini"
                },
                {
                    "authorId": "145988166",
                    "name": "S. Ghaemmaghami"
                },
                {
                    "authorId": "2341335459",
                    "name": "Iman Gholampour"
                }
            ],
            "abstract": "Automatic Speech Recognition (ASR) systems have demonstrated remarkable performance across various applications. However, limited data and the unique language features of specific domains, such as low-resource languages, significantly degrade their performance and lead to higher Word Error Rates (WER). In this study, we propose Generative Error Correction via Retrieval-Augmented Generation (GEC-RAG), a novel approach designed to improve ASR accuracy for low-resource domains, like Persian. Our approach treats the ASR system as a black-box, a common practice in cloud-based services, and proposes a Retrieval-Augmented Generation (RAG) approach within the In-Context Learning (ICL) scheme to enhance the quality of ASR predictions. By constructing a knowledge base that pairs ASR predictions (1-best and 5-best hypotheses) with their corresponding ground truths, GEC-RAG retrieves lexically similar examples to the ASR transcription using the Term Frequency-Inverse Document Frequency (TF-IDF) measure. This process provides relevant error patterns of the system alongside the ASR transcription to the Generative Large Language Model (LLM), enabling targeted corrections. Our results demonstrate that this strategy significantly reduces WER in Persian and highlights a potential for domain adaptation and low-resource scenarios. This research underscores the effectiveness of using RAG in enhancing ASR systems without requiring direct model modification or fine-tuning, making it adaptable to any domain by simply updating the transcription knowledge base with domain-specific data.",
            "corpus_id": "275757903",
            "text": "Automatic Speech Recognition (ASR) systems have demonstrated remarkable performance across various applications. However, limited data and the unique language features of specific domains, such as low-resource languages, significantly degrade their performance and lead to higher Word Error Rates (WER). In this study, we propose Generative Error Correction via Retrieval-Augmented Generation (GEC-RAG), a novel approach designed to improve ASR accuracy for low-resource domains, like Persian. Our approach treats the ASR system as a black-box, a common practice in cloud-based services, and proposes a Retrieval-Augmented Generation (RAG) approach within the In-Context Learning (ICL) scheme to enhance the quality of ASR predictions. By constructing a knowledge base that pairs ASR predictions (1-best and 5-best hypotheses) with their corresponding ground truths, GEC-RAG retrieves lexically similar examples to the ASR transcription using the Term Frequency-Inverse Document Frequency (TF-IDF) measure. This process provides relevant error patterns of the system alongside the ASR transcription to the Generative Large Language Model (LLM), enabling targeted corrections. Our results demonstrate that this strategy significantly reduces WER in Persian and highlights a potential for domain adaptation and low-resource scenarios. This research underscores the effectiveness of using RAG in enhancing ASR systems without requiring direct model modification or fine-tuning, making it adaptable to any domain by simply updating the transcription knowledge base with domain-specific data.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8056640625
        },
        {
            "paperId": "bfdc1094eaf7be1037d66840d23eddfee91cb485",
            "corpusId": 271329121,
            "title": "An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought",
            "venue": "International Symposium on Chinese Spoken Language Processing",
            "year": 2024,
            "referenceCount": 21,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.15569, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2312343839",
                    "name": "Yuetong Zhao"
                },
                {
                    "authorId": "2312344958",
                    "name": "Hongyu Cao"
                },
                {
                    "authorId": "2312340862",
                    "name": "Xianyu Zhao"
                },
                {
                    "authorId": "2243267608",
                    "name": "Zhijian Ou"
                }
            ],
            "abstract": "Since the launch of ChatGPT at the end of 2022, generative dialogue models represented by ChatGPT have quickly become widely used. As user expectations increase, enhancing the capability of generative dialogue models to solve complex problems has become a focal point of current research. This paper delves into the effectiveness of the RAFT (Retrieval Aug-mented Fine-Tuning) method in improving the performance of Generative dialogue models. RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and retrieval augmented generation (RAG), which significantly enhanced the model's information extraction and logical reasoning abilities. We evaluated the RAFT method across multiple datasets and analysed its performance in various reasoning tasks, including long-form QA and short-form QA tasks, tasks in both Chinese and English, and supportive and comparison reasoning tasks. Notably, it addresses the gaps in previous research regarding long-form QA tasks and Chinese datasets. Moreover, we also evaluate the benefit of the chain-of-thought (CoT) in the RAFT method. This work offers valuable insights for studies focused on enhancing the performance of generative dialogue models.",
            "corpus_id": "271329121",
            "text": "Since the launch of ChatGPT at the end of 2022, generative dialogue models represented by ChatGPT have quickly become widely used. As user expectations increase, enhancing the capability of generative dialogue models to solve complex problems has become a focal point of current research. This paper delves into the effectiveness of the RAFT (Retrieval Aug-mented Fine-Tuning) method in improving the performance of Generative dialogue models. RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and retrieval augmented generation (RAG), which significantly enhanced the model's information extraction and logical reasoning abilities. We evaluated the RAFT method across multiple datasets and analysed its performance in various reasoning tasks, including long-form QA and short-form QA tasks, tasks in both Chinese and English, and supportive and comparison reasoning tasks. Notably, it addresses the gaps in previous research regarding long-form QA tasks and Chinese datasets. Moreover, we also evaluate the benefit of the chain-of-thought (CoT) in the RAFT method. This work offers valuable insights for studies focused on enhancing the performance of generative dialogue models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9560546875
        },
        {
            "paperId": "2d362392fb0e13baf77e8ee35b5543e08207e97e",
            "corpusId": 271798852,
            "title": "Causal Reasoning in Large Language Models using Causal Graph Retrieval Augmented Generation",
            "venue": "International Conference on Human System Interaction",
            "year": 2024,
            "referenceCount": 38,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/HSI61632.2024.10613566?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/HSI61632.2024.10613566, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51437913",
                    "name": "Chamod Samarajeewa"
                },
                {
                    "authorId": "144286739",
                    "name": "Daswin De Silva"
                },
                {
                    "authorId": "2276845387",
                    "name": "Evgeny Osipov"
                },
                {
                    "authorId": "143775049",
                    "name": "D. Alahakoon"
                },
                {
                    "authorId": "2185595070",
                    "name": "Milos Manic"
                }
            ],
            "abstract": "Large Language Models (LLMs) are leading the Generative Artificial Intelligence transformation in natural language understanding. Beyond language understanding, LLMs have demonstrated capabilities in reasoning tasks, including commonsense, logical, and mathematical reasoning. However, their proficiency in causal understanding has been limited due to the complex nature of causal reasoning. Several recent studies have discussed the role of external causal models for improved causal understanding. Building on the success of Retrieval-Augmented Generation (RAG) for factual reasoning in LLMs, this paper introduces a novel approach that utilizes Causal Graphs as external sources for establishing causal relationships between complex vectors. This method is empirically evaluated using two benchmark datasets across the metrics of Context Relevance, Answer Relevance, and Grounding, in its ability to retrieve relevant context with causal alignment. The retrieval effectiveness is further compared with traditional RAG methods that are based on semantic proximity.",
            "corpus_id": "271798852",
            "text": "Large Language Models (LLMs) are leading the Generative Artificial Intelligence transformation in natural language understanding. Beyond language understanding, LLMs have demonstrated capabilities in reasoning tasks, including commonsense, logical, and mathematical reasoning. However, their proficiency in causal understanding has been limited due to the complex nature of causal reasoning. Several recent studies have discussed the role of external causal models for improved causal understanding. Building on the success of Retrieval-Augmented Generation (RAG) for factual reasoning in LLMs, this paper introduces a novel approach that utilizes Causal Graphs as external sources for establishing causal relationships between complex vectors. This method is empirically evaluated using two benchmark datasets across the metrics of Context Relevance, Answer Relevance, and Grounding, in its ability to retrieve relevant context with causal alignment. The retrieval effectiveness is further compared with traditional RAG methods that are based on semantic proximity.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.81201171875
        },
        {
            "paperId": "73d9f6df2f92d35d2d3b58023cbb400da41c0d26",
            "corpusId": 276355526,
            "title": "Unveiling the Power of Large Language Models: A Comparative Study of Retrieval-Augmented Generation, Fine-Tuning, and Their Synergistic Fusion for Enhanced Performance",
            "venue": "IEEE Access",
            "year": 2025,
            "referenceCount": 36,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3542334?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3542334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2345470851",
                    "name": "G\u00fcls\u00fcm Budakoglu"
                },
                {
                    "authorId": "2345472890",
                    "name": "Hakan Emekci"
                }
            ],
            "abstract": "Large-language model optimization for a particular application is crucial and challenging in natural language processing. This study compares two salient techniques for retrieve-augmented generation (RAG) and fine-tuning along with a new hybrid method that combines both. In this study, we investigate the effectiveness of various methods using the Stanford Question Answering Dataset (SQuAD), Microsoft Machine Reading Comprehension (MS MARCO) and SQL CREATE TABLE statements. RAG is used because it enriches the model responses with external data without much computational load during the inference. Fine-tuning updates the model parameters to improve the contextual accuracy. Our hybrid model balances the accuracy and efficiency of the two techniques. While fine-tuning entails semantic precision, RAG is more resource efficient. The hybrid approach while it may not offer surpassing results over fine-tuning-offers a balanced solution in scenarios where the application demands both efficiency and accuracy. These findings represent the trade-off involved in LLM optimization and offers a scope for further studies and practical applications.",
            "corpus_id": "276355526",
            "text": "Large-language model optimization for a particular application is crucial and challenging in natural language processing. This study compares two salient techniques for retrieve-augmented generation (RAG) and fine-tuning along with a new hybrid method that combines both. In this study, we investigate the effectiveness of various methods using the Stanford Question Answering Dataset (SQuAD), Microsoft Machine Reading Comprehension (MS MARCO) and SQL CREATE TABLE statements. RAG is used because it enriches the model responses with external data without much computational load during the inference. Fine-tuning updates the model parameters to improve the contextual accuracy. Our hybrid model balances the accuracy and efficiency of the two techniques. While fine-tuning entails semantic precision, RAG is more resource efficient. The hybrid approach while it may not offer surpassing results over fine-tuning-offers a balanced solution in scenarios where the application demands both efficiency and accuracy. These findings represent the trade-off involved in LLM optimization and offers a scope for further studies and practical applications.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.970703125
        },
        {
            "paperId": "77d16c4fd4392ad6a8a59f553fc25cca38a84e15",
            "corpusId": 273162404,
            "title": "Auto-GDA: Automatic Domain Adaptation for Efficient Grounding Verification in Retrieval Augmented Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 53,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03461, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324579020",
                    "name": "Tobias Leemann"
                },
                {
                    "authorId": "2189080460",
                    "name": "Periklis Petridis"
                },
                {
                    "authorId": "2072372278",
                    "name": "Giuseppe Vietri"
                },
                {
                    "authorId": "41073684",
                    "name": "Dionysis Manousakas"
                },
                {
                    "authorId": "2272478086",
                    "name": "Aaron Roth"
                },
                {
                    "authorId": "120169766",
                    "name": "Serg\u00fcl Ayd\u00f6re"
                }
            ],
            "abstract": "While retrieval-augmented generation (RAG) has been shown to enhance factuality of large language model (LLM) outputs, LLMs still suffer from hallucination, generating incorrect or irrelevant information. A common detection strategy involves prompting the LLM again to assess whether its response is grounded in the retrieved evidence, but this approach is costly. Alternatively, lightweight natural language inference (NLI) models for efficient grounding verification can be used at inference time. While existing pre-trained NLI models offer potential solutions, their performance remains subpar compared to larger models on realistic RAG inputs. RAG inputs are more complex than most datasets used for training NLI models and have characteristics specific to the underlying knowledge base, requiring adaptation of the NLI models to a specific target domain. Additionally, the lack of labeled instances in the target domain makes supervised domain adaptation, e.g., through fine-tuning, infeasible. To address these challenges, we introduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework enables unsupervised domain adaptation through synthetic data generation. Unlike previous methods that rely on handcrafted filtering and augmentation strategies, Auto-GDA employs an iterative process to continuously improve the quality of generated samples using weak labels from less efficient teacher models and discrete optimization to select the most promising augmented samples. Experimental results demonstrate the effectiveness of our approach, with models fine-tuned on synthetic data using Auto-GDA often surpassing the performance of the teacher model and reaching the performance level of LLMs at 10% of their computational cost.",
            "corpus_id": "273162404",
            "text": "While retrieval-augmented generation (RAG) has been shown to enhance factuality of large language model (LLM) outputs, LLMs still suffer from hallucination, generating incorrect or irrelevant information. A common detection strategy involves prompting the LLM again to assess whether its response is grounded in the retrieved evidence, but this approach is costly. Alternatively, lightweight natural language inference (NLI) models for efficient grounding verification can be used at inference time. While existing pre-trained NLI models offer potential solutions, their performance remains subpar compared to larger models on realistic RAG inputs. RAG inputs are more complex than most datasets used for training NLI models and have characteristics specific to the underlying knowledge base, requiring adaptation of the NLI models to a specific target domain. Additionally, the lack of labeled instances in the target domain makes supervised domain adaptation, e.g., through fine-tuning, infeasible. To address these challenges, we introduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework enables unsupervised domain adaptation through synthetic data generation. Unlike previous methods that rely on handcrafted filtering and augmentation strategies, Auto-GDA employs an iterative process to continuously improve the quality of generated samples using weak labels from less efficient teacher models and discrete optimization to select the most promising augmented samples. Experimental results demonstrate the effectiveness of our approach, with models fine-tuned on synthetic data using Auto-GDA often surpassing the performance of the teacher model and reaching the performance level of LLMs at 10% of their computational cost.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.87255859375
        },
        {
            "paperId": "46b26abc9db27c72a17420d346c86c23ccb5f970",
            "corpusId": 278096418,
            "title": "Tailoring Large Language Models for Drilling Applications: A Comparative Study of Retrieval-Augmented Generation and Fine-Tuning",
            "venue": "SPE Western Regional Meeting",
            "year": 2025,
            "referenceCount": 9,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2118/224128-ms?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2118/224128-ms, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2357517547",
                    "name": "Odai A. Elyas"
                },
                {
                    "authorId": "2357517375",
                    "name": "Hassan W. Al Hashim"
                },
                {
                    "authorId": "2357867008",
                    "name": "John R. Williams"
                }
            ],
            "abstract": "\n The Fourth Industrial Revolution (4IR) is defined by its pure data-driven nature, leading to advancements in computing, sensors, and machine learning to enhance operational efficiency through data-driven approaches. However, a significant amount of data remains underutilized due to its complex and unstructured formats. It is estimated that 50% of large enterprises manage at least 5 Petabytes (PB) of data, with 80% being unstructured (Mcdowell, 2023). Some examples of these datasets in the drilling industry are derived from daily operation remarks, engineering programs, and domain literature, all containing valuable insights.\n While significant effort is being made in the drilling industry through Artificial Intelligence (AI) systems, these models often depend on data gathered from over 80,000 sensors deployed on drilling platforms (Patel, 2024), limiting them to quantitative, single-modality datasets. Emerging technologies in Natural Language Processing (NLP), and particularly Large Language Models (LLMs) offer promising solutions to processing unstructured data, thereby unlocking additional insights from untapped resources. Nonetheless, developing domain-specific language models from scratch is a challenging endeavor. For reference, it is estimated that the smaller 175 billion parameters GPT-3 was trained using 1024 Graphics Processing Units (GPUs) for 34 days (Narayanan et al., 2021). While the larger 1.8 trillion parameters GPT-4 was trained with approximately 25,000 GPUs over 90 days (Treiber, 2023).\n This paper demonstrates the potential of adapting current Generative Pre-trained Transformer models (GPT) for drilling applications. The first objective is to improve drilling knowledge of base GPT models by analyzing the effectiveness of Retrieval Augmented Generation (RAG), and Low-Rank Adaptive (LoRA) fine-tuning. The second objective is to utilize LoRA fine-tuning for Lost Time Incident (LTI) prediction using a curated dataset from operational reports. The approaches highlighted in this paper will showcase how LLMs can be tailored to the drilling domain for various tasks, cost-effectively and without extensive computational resources.",
            "corpus_id": "278096418",
            "text": "\n The Fourth Industrial Revolution (4IR) is defined by its pure data-driven nature, leading to advancements in computing, sensors, and machine learning to enhance operational efficiency through data-driven approaches. However, a significant amount of data remains underutilized due to its complex and unstructured formats. It is estimated that 50% of large enterprises manage at least 5 Petabytes (PB) of data, with 80% being unstructured (Mcdowell, 2023). Some examples of these datasets in the drilling industry are derived from daily operation remarks, engineering programs, and domain literature, all containing valuable insights.\n While significant effort is being made in the drilling industry through Artificial Intelligence (AI) systems, these models often depend on data gathered from over 80,000 sensors deployed on drilling platforms (Patel, 2024), limiting them to quantitative, single-modality datasets. Emerging technologies in Natural Language Processing (NLP), and particularly Large Language Models (LLMs) offer promising solutions to processing unstructured data, thereby unlocking additional insights from untapped resources. Nonetheless, developing domain-specific language models from scratch is a challenging endeavor. For reference, it is estimated that the smaller 175 billion parameters GPT-3 was trained using 1024 Graphics Processing Units (GPUs) for 34 days (Narayanan et al., 2021). While the larger 1.8 trillion parameters GPT-4 was trained with approximately 25,000 GPUs over 90 days (Treiber, 2023).\n This paper demonstrates the potential of adapting current Generative Pre-trained Transformer models (GPT) for drilling applications. The first objective is to improve drilling knowledge of base GPT models by analyzing the effectiveness of Retrieval Augmented Generation (RAG), and Low-Rank Adaptive (LoRA) fine-tuning. The second objective is to utilize LoRA fine-tuning for Lost Time Incident (LTI) prediction using a curated dataset from operational reports. The approaches highlighted in this paper will showcase how LLMs can be tailored to the drilling domain for various tasks, cost-effectively and without extensive computational resources.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.49951171875
        },
        {
            "paperId": "50d31439390b3853d8b36f2e2adb5d145b56f700",
            "corpusId": 272562963,
            "title": "REMED: Retrieval-Augmented Medical Document Query Responding with Embedding Fine-Tuning",
            "venue": "IEEE International Joint Conference on Neural Network",
            "year": 2024,
            "referenceCount": 34,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IJCNN60899.2024.10651011?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IJCNN60899.2024.10651011, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2215451922",
                    "name": "Tianqi Pang"
                },
                {
                    "authorId": "2216509779",
                    "name": "Kehui Tan"
                },
                {
                    "authorId": "2320838988",
                    "name": "Yujun Yao"
                },
                {
                    "authorId": "2320579279",
                    "name": "Xiangyang Liu"
                },
                {
                    "authorId": "2320779859",
                    "name": "Fanlong Meng"
                },
                {
                    "authorId": "2320813945",
                    "name": "Chenyou Fan"
                },
                {
                    "authorId": "2320708996",
                    "name": "Xiaofan Zhang"
                }
            ],
            "abstract": "While advanced Large Language Models (LLMs) exhibit considerable promise, their tendency to generate unreliable information poses significant challenges, particularly in high-risk domains like healthcare. However, the advent of Retrieval-Augmented Generation (RAG) offers a novel solution tailored for the medical realm. This study further enhances retrieval accuracy by introducing REMED, a specialized medical document retrieval framework designed to address the hallucination problem prevalent in LLMs. The REMED framework integrates dataset construction, an efficient embedding fine-tuning EM-FT model, retrieval-augmented generation, and human evaluation of LLM responses. The EM-FT model can end-to-end fine-tune the medical sentence representations in large pre-trained models through an efficient embedding fine-tuning method, thereby enhancing the performance of medical retrieval. We adopt contrastive learning as the loss function to optimize the performance of the EM-FT model, enabling it to accurately capture the similarity between query and relevant documents. This approach not only improves the retrieval accuracy of positively related contents but also effectively reduces the matching with negatively related contents. Compared to direct dense vector retrieval, fine-tuning query and content vectors first and then performing dense retrieval tasks significantly improved the performance. Through validation on two datasets, we demonstrate that our EM-FT method improves recall and precision on MMD by 3.2%-6.0% and on MPD by 14.4%-42.6% compared to using the embedding model directly for retrieval. Furthermore, through human evaluation on the PULSE-7Bv5 model, we further confirm the effectiveness of our retrieval results in improving the quality of generated text.",
            "corpus_id": "272562963",
            "text": "While advanced Large Language Models (LLMs) exhibit considerable promise, their tendency to generate unreliable information poses significant challenges, particularly in high-risk domains like healthcare. However, the advent of Retrieval-Augmented Generation (RAG) offers a novel solution tailored for the medical realm. This study further enhances retrieval accuracy by introducing REMED, a specialized medical document retrieval framework designed to address the hallucination problem prevalent in LLMs. The REMED framework integrates dataset construction, an efficient embedding fine-tuning EM-FT model, retrieval-augmented generation, and human evaluation of LLM responses. The EM-FT model can end-to-end fine-tune the medical sentence representations in large pre-trained models through an efficient embedding fine-tuning method, thereby enhancing the performance of medical retrieval. We adopt contrastive learning as the loss function to optimize the performance of the EM-FT model, enabling it to accurately capture the similarity between query and relevant documents. This approach not only improves the retrieval accuracy of positively related contents but also effectively reduces the matching with negatively related contents. Compared to direct dense vector retrieval, fine-tuning query and content vectors first and then performing dense retrieval tasks significantly improved the performance. Through validation on two datasets, we demonstrate that our EM-FT method improves recall and precision on MMD by 3.2%-6.0% and on MPD by 14.4%-42.6% compared to using the embedding model directly for retrieval. Furthermore, through human evaluation on the PULSE-7Bv5 model, we further confirm the effectiveness of our retrieval results in improving the quality of generated text.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8642578125
        },
        {
            "paperId": "013f0ab2ce809dd4f402c606c5a4e0356851ad8b",
            "corpusId": 272191656,
            "title": "Leveraging generative AI for knowledge-driven information retrieval in the energy sector",
            "venue": "MATEC Web of Conferences",
            "year": 2024,
            "referenceCount": 11,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1051/matecconf/202440110008",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1051/matecconf/202440110008?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1051/matecconf/202440110008, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2318200821",
                    "name": "Ali Alsayegh"
                },
                {
                    "authorId": "2318206563",
                    "name": "Tariq Masood"
                }
            ],
            "abstract": "This paper presents an innovative approach to knowledge management in the energy sector through the development of the Advanced Agent Architecture (AAA). AAA integrates Retrieval-Augmented Generation (RAG) techniques with a tailored local knowledge base (LKM) and web search functionalities, aiming to enhance the accuracy, robustness, and flexibility of information retrieval. We conducted a detailed case study involving a solar power system to evaluate the effectiveness of AAA compared to traditional Large Language Models (LLMs) such as Llama 3. Our results demonstrate that AAA significantly outperforms conventional methods in delivering accurate and relevant answers to complex domain-specific queries. However, the system also shows higher energy consumption and slower response times, identifying critical areas for future research. This study sets the stage for further exploration into optimizing AAA\u2019s energy efficiency and processing speed, expanding the range of queries, and providing a more comprehensive benchmarking against traditional systems. Our findings indicate that AAA has the potential to substantially improve knowledge management practices, facilitating more informed decision-making and operational efficiencies in the energy sector.",
            "corpus_id": "272191656",
            "text": "This paper presents an innovative approach to knowledge management in the energy sector through the development of the Advanced Agent Architecture (AAA). AAA integrates Retrieval-Augmented Generation (RAG) techniques with a tailored local knowledge base (LKM) and web search functionalities, aiming to enhance the accuracy, robustness, and flexibility of information retrieval. We conducted a detailed case study involving a solar power system to evaluate the effectiveness of AAA compared to traditional Large Language Models (LLMs) such as Llama 3. Our results demonstrate that AAA significantly outperforms conventional methods in delivering accurate and relevant answers to complex domain-specific queries. However, the system also shows higher energy consumption and slower response times, identifying critical areas for future research. This study sets the stage for further exploration into optimizing AAA\u2019s energy efficiency and processing speed, expanding the range of queries, and providing a more comprehensive benchmarking against traditional systems. Our findings indicate that AAA has the potential to substantially improve knowledge management practices, facilitating more informed decision-making and operational efficiencies in the energy sector.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.69140625
        },
        {
            "paperId": "57fc6d44578d072f997ceff69d7b0d003b91fded",
            "corpusId": 278782961,
            "title": "RAG or Fine-tuning? A Comparative Study on LCMs-based Code Completion in Industry",
            "venue": "",
            "year": 2025,
            "referenceCount": 72,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.15179, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2135764153",
                    "name": "Chaozheng Wang"
                },
                {
                    "authorId": "2155450982",
                    "name": "Zezhou Yang"
                },
                {
                    "authorId": "2112314113",
                    "name": "Shuzheng Gao"
                },
                {
                    "authorId": "2267893922",
                    "name": "Cuiyun Gao"
                },
                {
                    "authorId": "2299029254",
                    "name": "Ting Peng"
                },
                {
                    "authorId": "2265772572",
                    "name": "Hailiang Huang"
                },
                {
                    "authorId": "2299160326",
                    "name": "Yuetang Deng"
                },
                {
                    "authorId": "2338266828",
                    "name": "Michael R. Lyu"
                }
            ],
            "abstract": "Code completion, a crucial practice in industrial settings, helps developers improve programming efficiency by automatically suggesting code snippets during development. With the emergence of Large Code Models (LCMs), this field has witnessed significant advancements. Due to the natural differences between open-source and industrial codebases, such as coding patterns and unique internal dependencies, it is a common practice for developers to conduct domain adaptation when adopting LCMs in industry. There exist multiple adaptation approaches, among which retrieval-augmented generation (RAG) and fine-tuning are the two most popular paradigms. However, no prior research has explored the trade-off of the two approaches in industrial scenarios. To mitigate the gap, we comprehensively compare the two paradigms including Retrieval-Augmented Generation (RAG) and Fine-tuning (FT), for industrial code completion in this paper. In collaboration with Tencent's WXG department, we collect over 160,000 internal C++ files as our codebase. We then compare the two types of adaptation approaches from three dimensions that are concerned by industrial practitioners, including effectiveness, efficiency, and parameter sensitivity, using six LCMs. Our findings reveal that RAG, when implemented with appropriate embedding models that map code snippets into dense vector representations, can achieve higher accuracy than fine-tuning alone. Specifically, BM25 presents superior retrieval effectiveness and efficiency among studied RAG methods. Moreover, RAG and fine-tuning are orthogonal and their combination leads to further improvement. We also observe that RAG demonstrates better scalability than FT, showing more sustained performance gains with larger scales of codebase.",
            "corpus_id": "278782961",
            "text": "Code completion, a crucial practice in industrial settings, helps developers improve programming efficiency by automatically suggesting code snippets during development. With the emergence of Large Code Models (LCMs), this field has witnessed significant advancements. Due to the natural differences between open-source and industrial codebases, such as coding patterns and unique internal dependencies, it is a common practice for developers to conduct domain adaptation when adopting LCMs in industry. There exist multiple adaptation approaches, among which retrieval-augmented generation (RAG) and fine-tuning are the two most popular paradigms. However, no prior research has explored the trade-off of the two approaches in industrial scenarios. To mitigate the gap, we comprehensively compare the two paradigms including Retrieval-Augmented Generation (RAG) and Fine-tuning (FT), for industrial code completion in this paper. In collaboration with Tencent's WXG department, we collect over 160,000 internal C++ files as our codebase. We then compare the two types of adaptation approaches from three dimensions that are concerned by industrial practitioners, including effectiveness, efficiency, and parameter sensitivity, using six LCMs. Our findings reveal that RAG, when implemented with appropriate embedding models that map code snippets into dense vector representations, can achieve higher accuracy than fine-tuning alone. Specifically, BM25 presents superior retrieval effectiveness and efficiency among studied RAG methods. Moreover, RAG and fine-tuning are orthogonal and their combination leads to further improvement. We also observe that RAG demonstrates better scalability than FT, showing more sustained performance gains with larger scales of codebase.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.970703125
        },
        {
            "paperId": "8f14ba1ce827a174916098d78b396c863d2ee7d5",
            "corpusId": 276526965,
            "title": "Adapting Generative Large Language Models for Information Extraction from Unstructured Electronic Health Records in Residential Aged Care: A Comparative Analysis of Training Approaches",
            "venue": "J. Heal. Informatics Res.",
            "year": 2025,
            "referenceCount": 69,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1007/s41666-025-00190-z",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12037947, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277877538",
                    "name": "D. Vithanage"
                },
                {
                    "authorId": "144551284",
                    "name": "C. Deng"
                },
                {
                    "authorId": "2278214328",
                    "name": "Lei Wang"
                },
                {
                    "authorId": "1972725075",
                    "name": "M. Yin"
                },
                {
                    "authorId": "2069756289",
                    "name": "M. Alkhalaf"
                },
                {
                    "authorId": "2109338789",
                    "name": "Zhenyu Zhang"
                },
                {
                    "authorId": "2117078401",
                    "name": "Yunshu Zhu"
                },
                {
                    "authorId": "2231452266",
                    "name": "P. Yu"
                }
            ],
            "abstract": "Information extraction (IE) of unstructured electronic health records is challenging due to the semantic complexity of textual data. Generative large language models (LLMs) offer promising solutions to address this challenge. However, identifying the best training methods to adapt LLMs for IE in residential aged care settings remains underexplored. This research addresses this challenge by evaluating the effects of zero-shot and few-shot learning, both with and without parameter-efficient fine-tuning (PEFT) and retrieval-augmented generation (RAG) using Llama 3.1-8B. The study performed named entity recognition (NER) to nursing notes from Australian aged care facilities (RACFs), focusing on agitation in dementia and malnutrition risk factors. Performance evaluation includes accuracy, macro-averaged precision, recall, and F1 score. We used non-parametric statistical methods to compare if the differences were statistically significant. Results show that zero-shot and few-shot learning, whether combined with PEFT or RAG, achieve comparable performance across the clinical domains when the same prompting template is used. Few-shot learning significantly outperforms zero-shot learning when neither PEFT nor RAG is applied. Notably, PEFT significantly improves model performance in both zero-shot and few-shot learning; however, RAG significantly improves performance only in few-shot learning. After PEFT, the performance of zero-shot learning reaches a comparable level with few-shot learning. However, few-shot learning with RAG significantly outperforms zero-shot learning with RAG. We also found a similar level of performance between few-shot learning with RAG and zero-shot learning with PEFT. These findings provide valuable insights for researchers, practitioners, and stakeholders to optimize the use of generative LLMs in clinical IE. Supplementary Information The online version contains supplementary material available at 10.1007/s41666-025-00190-z.",
            "corpus_id": "276526965",
            "text": "Information extraction (IE) of unstructured electronic health records is challenging due to the semantic complexity of textual data. Generative large language models (LLMs) offer promising solutions to address this challenge. However, identifying the best training methods to adapt LLMs for IE in residential aged care settings remains underexplored. This research addresses this challenge by evaluating the effects of zero-shot and few-shot learning, both with and without parameter-efficient fine-tuning (PEFT) and retrieval-augmented generation (RAG) using Llama 3.1-8B. The study performed named entity recognition (NER) to nursing notes from Australian aged care facilities (RACFs), focusing on agitation in dementia and malnutrition risk factors. Performance evaluation includes accuracy, macro-averaged precision, recall, and F1 score. We used non-parametric statistical methods to compare if the differences were statistically significant. Results show that zero-shot and few-shot learning, whether combined with PEFT or RAG, achieve comparable performance across the clinical domains when the same prompting template is used. Few-shot learning significantly outperforms zero-shot learning when neither PEFT nor RAG is applied. Notably, PEFT significantly improves model performance in both zero-shot and few-shot learning; however, RAG significantly improves performance only in few-shot learning. After PEFT, the performance of zero-shot learning reaches a comparable level with few-shot learning. However, few-shot learning with RAG significantly outperforms zero-shot learning with RAG. We also found a similar level of performance between few-shot learning with RAG and zero-shot learning with PEFT. These findings provide valuable insights for researchers, practitioners, and stakeholders to optimize the use of generative LLMs in clinical IE. Supplementary Information The online version contains supplementary material available at 10.1007/s41666-025-00190-z.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9130859375
        },
        {
            "paperId": "92b2ea3c244855235d10ca270dea7348fae07a25",
            "corpusId": 278327536,
            "title": "Enhancing LLM Code Generation: A Systematic Evaluation of Multi-Agent Collaboration and Runtime Debugging for Improved Accuracy, Reliability, and Latency",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 45,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.02133, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2359150080",
                    "name": "Nazmus Ashrafi"
                },
                {
                    "authorId": "2359148921",
                    "name": "Salah Bouktif"
                },
                {
                    "authorId": "2329167767",
                    "name": "Mohammed Mediani"
                }
            ],
            "abstract": "The use of large language models (LLMs) for automated code generation has emerged as a significant focus within AI research. As these pretrained models continue to evolve, their ability to understand and generate complex code structures has opened new possibilities for automating intricate programming tasks for the sake of accurate code generation. Although contemporary foundational models demonstrate promoting results, researchers continue to explore optimal post-training strategies to enhance code quality. These include supervised fine-tuning, retrieval-augmented generation (RAG), debugging, and many others. In this paper, we combine two widely used approaches namely multi-agent collaboration and runtime execution information-based debugging, for improving code generation functionality, reliability, and practical applicability. We perform an empirical study in order to extend the evaluation of the individual strategies as well as the proposed composition of the activities of both strategies. Our study use 19 LLMs to examines the performance of individual and the proposed strategies, offering comprehensive insights into how different programming activities compositions and training paradigms influence code generation effectiveness. In particular, we implement a chained system that combines both strategies to assess their combined impact on functional accuracy, code reliability, and generation latency using two benchmark datasets commonly used for code generation. Our findings provide valuable insights for organizations seeking robust AI-driven coding solutions by guiding them in selecting models that can better adapt to complex post-training strategies, ultimately fostering the adoption of more effective and reliable code generation technologies.",
            "corpus_id": "278327536",
            "text": "The use of large language models (LLMs) for automated code generation has emerged as a significant focus within AI research. As these pretrained models continue to evolve, their ability to understand and generate complex code structures has opened new possibilities for automating intricate programming tasks for the sake of accurate code generation. Although contemporary foundational models demonstrate promoting results, researchers continue to explore optimal post-training strategies to enhance code quality. These include supervised fine-tuning, retrieval-augmented generation (RAG), debugging, and many others. In this paper, we combine two widely used approaches namely multi-agent collaboration and runtime execution information-based debugging, for improving code generation functionality, reliability, and practical applicability. We perform an empirical study in order to extend the evaluation of the individual strategies as well as the proposed composition of the activities of both strategies. Our study use 19 LLMs to examines the performance of individual and the proposed strategies, offering comprehensive insights into how different programming activities compositions and training paradigms influence code generation effectiveness. In particular, we implement a chained system that combines both strategies to assess their combined impact on functional accuracy, code reliability, and generation latency using two benchmark datasets commonly used for code generation. Our findings provide valuable insights for organizations seeking robust AI-driven coding solutions by guiding them in selecting models that can better adapt to complex post-training strategies, ultimately fostering the adoption of more effective and reliable code generation technologies.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.90576171875
        },
        {
            "paperId": "337c0e4ff5c516fbb513a3d6ce752cab444e94c2",
            "corpusId": 277940163,
            "title": "Detecting Malicious Source Code in PyPI Packages with LLMs: Does RAG Come in Handy?",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 32,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.13769, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1412620999",
                    "name": "Motunrayo O. Ibiyo"
                },
                {
                    "authorId": "2356401662",
                    "name": "Thinakone Louangdy"
                },
                {
                    "authorId": "145431495",
                    "name": "P. T. Nguyen"
                },
                {
                    "authorId": "1644891552",
                    "name": "Claudio Di Sipio"
                },
                {
                    "authorId": "2133181",
                    "name": "D. D. Ruscio"
                }
            ],
            "abstract": "Malicious software packages in open-source ecosystems, such as PyPI, pose growing security risks. Unlike traditional vulnerabilities, these packages are intentionally designed to deceive users, making detection challenging due to evolving attack methods and the lack of structured datasets. In this work, we empirically evaluate the effectiveness of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and few-shot learning for detecting malicious source code. We fine-tune LLMs on curated datasets and integrate YARA rules, GitHub Security Advisories, and malicious code snippets with the aim of enhancing classification accuracy. We came across a counterintuitive outcome: While RAG is expected to boost up the prediction performance, it fails in the performed evaluation, obtaining a mediocre accuracy. In contrast, few-shot learning is more effective as it significantly improves the detection of malicious code, achieving 97% accuracy and 95% balanced accuracy, outperforming traditional RAG approaches. Thus, future work should expand structured knowledge bases, refine retrieval models, and explore hybrid AI-driven cybersecurity solutions.",
            "corpus_id": "277940163",
            "text": "Malicious software packages in open-source ecosystems, such as PyPI, pose growing security risks. Unlike traditional vulnerabilities, these packages are intentionally designed to deceive users, making detection challenging due to evolving attack methods and the lack of structured datasets. In this work, we empirically evaluate the effectiveness of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and few-shot learning for detecting malicious source code. We fine-tune LLMs on curated datasets and integrate YARA rules, GitHub Security Advisories, and malicious code snippets with the aim of enhancing classification accuracy. We came across a counterintuitive outcome: While RAG is expected to boost up the prediction performance, it fails in the performed evaluation, obtaining a mediocre accuracy. In contrast, few-shot learning is more effective as it significantly improves the detection of malicious code, achieving 97% accuracy and 95% balanced accuracy, outperforming traditional RAG approaches. Thus, future work should expand structured knowledge bases, refine retrieval models, and explore hybrid AI-driven cybersecurity solutions.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7373046875
        },
        {
            "paperId": "b2ae456b03249401ebd7b21f6afc262fb99bfffc",
            "corpusId": 277231843,
            "title": "Enhancing Policy Generation with GraphRAG and YouTube Data: A Logistics Case Study",
            "venue": "Electronics",
            "year": 2025,
            "referenceCount": 36,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics14071241?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics14071241, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2295941874",
                    "name": "Hisatoshi Naganawa"
                },
                {
                    "authorId": "120962479",
                    "name": "Enna Hirata"
                }
            ],
            "abstract": "Graph-based retrieval-augmented generation (GraphRAG) represents an innovative advancement in natural language processing, leveraging the power of large language models (LLMs) for complex tasks such as policy generation. This research presents a GraphRAG model trained on YouTube data containing keywords related to logistics issues to generate policy proposals addressing these challenges. The collected data include both video subtitles and user comments, which are used to fine-tune the GraphRAG model. To evaluate the effectiveness of this approach, the performance of the proposed model is compared to a standard generative pre-trained transformer (GPT) model. The results show that the GraphRAG model outperforms the GPT model in most prompts, highlighting its potential to generate more accurate and contextually relevant policy recommendations. This study not only contributes to the evolving field of LLM-based natural language processing (NLP) applications but also explores new methods for improving model efficiency and scalability in real-world domains like logistics policy making.",
            "corpus_id": "277231843",
            "text": "Graph-based retrieval-augmented generation (GraphRAG) represents an innovative advancement in natural language processing, leveraging the power of large language models (LLMs) for complex tasks such as policy generation. This research presents a GraphRAG model trained on YouTube data containing keywords related to logistics issues to generate policy proposals addressing these challenges. The collected data include both video subtitles and user comments, which are used to fine-tune the GraphRAG model. To evaluate the effectiveness of this approach, the performance of the proposed model is compared to a standard generative pre-trained transformer (GPT) model. The results show that the GraphRAG model outperforms the GPT model in most prompts, highlighting its potential to generate more accurate and contextually relevant policy recommendations. This study not only contributes to the evolving field of LLM-based natural language processing (NLP) applications but also explores new methods for improving model efficiency and scalability in real-world domains like logistics policy making.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8818359375
        },
        {
            "paperId": "e0ab1e4f24ae040cea90e2a2ac623acc4087d9f2",
            "corpusId": 273014623,
            "title": "Research on the Adaptability of Generative Algorithm in Generative Landscape Design",
            "venue": "Landscape architecture",
            "year": 2024,
            "referenceCount": 15,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3724/j.fjyl.202404120207?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3724/j.fjyl.202404120207, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275183603",
                    "name": "Ran Chen"
                },
                {
                    "authorId": "2213522531",
                    "name": "Xiaomin Luo"
                },
                {
                    "authorId": "2275226787",
                    "name": "Yueheng He"
                },
                {
                    "authorId": "2275676213",
                    "name": "Jing Zhao"
                }
            ],
            "abstract": ": [Objective] In recent years, groundbreaking generative algorithms such as GPT-4 and Diffusion have propelled a new wave of technological revolution, significantly impacting various fields, including landscape architecture. This research reviews the integration of these advanced algorithms into landscape architecture, with a focus on their adaptability across different stages of design. These algorithms, known for their capability to generate texts and images, are poised to revolutionize design methodologies by offering innovative solutions that can transform traditional practices. [Methods] The methodology of this research involves a systematic exploration of generative algorithms applied in a structured framework within the landscape architecture domain. The process is divided into four distinct stages: text generation, layout generation, master plan rendering, and effect visualization. Each stage tests different algorithms to evaluate their practicality and effectiveness and comprehensively assess their capabilities and limitations in real-world design scenarios. [Results] 1) Text generation: The initial stage of the design process involves generating descriptive texts based on input queries. Traditional LLMs like GPT-4 show robust capabilities in general text generation but often lack the nuanced understanding required for specialized fields such as landscape architecture. To address this, the research employs techniques such as fine-tuning and retrieval-augmented generation (RAG) to enhance the specificity and relevance of the outputs to landscape architecture. Despite these efforts, the adaptability of LLMs to generate contextually rich and technically accurate descriptions remains a significant challenge. The research suggests that integrating domain-specific knowledge bases and employing advanced tuning methods may improve the performance of LLMs in generating more relevant design descriptions.Layout generation. 2 \uff09 Layout generation: The research explores the use of generative adversarial network (GAN), specifically CycleGAN and Pix2Pix, which can adapt source domain images to target domain layouts. These models excel in identifying and translating underlying design patterns without the need for direct supervision, which aligns well with creative design practices that value innovation over replication. The research highlights the potential of these algorithms to understand and reinterpret spatial data into feasible design layouts, showcasing their capability to innovate within the predefined norms of landscape architecture. 3 \uff09 Master plan rendering: The master plan rendering stage is critical for producing detailed and accurate architectural drawings. The research tests the efficacy of large pre-trained models like Stable Diffusion and examines their integration with traditional GAN for enhanced precision. The findings indicate that while Stable Diffusion provides high-quality image outputs, its application in producing detailed technical drawings is limited. The research introduces a hybrid approach, combining the strengths of GAN for structural accuracy and the image quality of Stable Diffusion, to produce renderings that are both aesthetically pleasing and technically detailed. 4 \uff09 Effect visualization: The final stage involves creating detailed three-dimensional visual effects from the two-dimensional plans. This stage tests the adaptability of algorithms to translate flat designs into vivid, multi-dimensional landscapes. Techniques such as ControlNet and specialized tuning methods like LoRA are used to fine-tune the visual outputs to meet specific aesthetic and functional requirements. The research delves into the challenges of maintaining the fidelity of the original design while enhancing the visual representation, which emphasizes the need for sophisticated control mechanisms to achieve high-quality visualizations. [Conclusion] The research concludes that while generative algorithms hold significant promise for the field of landscape architecture, their success is contingent upon targeted adaptations and enhancements tailored to specific design tasks. The complexities of integrating these technologies into a coherent design process highlight the necessity for a multidisciplinary approach that leverages both technological innovations and traditional design principles. Future research should aim to develop an integrated system that combines various AI technologies, potentially transforming the landscape architecture field by streamlining and enhancing the design process. This integrated approach could pave the way for new methodologies that seamlessly merge theoretical and practical aspects of landscape design, thus fostering innovation and efficiency.",
            "corpus_id": "273014623",
            "text": ": [Objective] In recent years, groundbreaking generative algorithms such as GPT-4 and Diffusion have propelled a new wave of technological revolution, significantly impacting various fields, including landscape architecture. This research reviews the integration of these advanced algorithms into landscape architecture, with a focus on their adaptability across different stages of design. These algorithms, known for their capability to generate texts and images, are poised to revolutionize design methodologies by offering innovative solutions that can transform traditional practices. [Methods] The methodology of this research involves a systematic exploration of generative algorithms applied in a structured framework within the landscape architecture domain. The process is divided into four distinct stages: text generation, layout generation, master plan rendering, and effect visualization. Each stage tests different algorithms to evaluate their practicality and effectiveness and comprehensively assess their capabilities and limitations in real-world design scenarios. [Results] 1) Text generation: The initial stage of the design process involves generating descriptive texts based on input queries. Traditional LLMs like GPT-4 show robust capabilities in general text generation but often lack the nuanced understanding required for specialized fields such as landscape architecture. To address this, the research employs techniques such as fine-tuning and retrieval-augmented generation (RAG) to enhance the specificity and relevance of the outputs to landscape architecture. Despite these efforts, the adaptability of LLMs to generate contextually rich and technically accurate descriptions remains a significant challenge. The research suggests that integrating domain-specific knowledge bases and employing advanced tuning methods may improve the performance of LLMs in generating more relevant design descriptions.Layout generation. 2 \uff09 Layout generation: The research explores the use of generative adversarial network (GAN), specifically CycleGAN and Pix2Pix, which can adapt source domain images to target domain layouts. These models excel in identifying and translating underlying design patterns without the need for direct supervision, which aligns well with creative design practices that value innovation over replication. The research highlights the potential of these algorithms to understand and reinterpret spatial data into feasible design layouts, showcasing their capability to innovate within the predefined norms of landscape architecture. 3 \uff09 Master plan rendering: The master plan rendering stage is critical for producing detailed and accurate architectural drawings. The research tests the efficacy of large pre-trained models like Stable Diffusion and examines their integration with traditional GAN for enhanced precision. The findings indicate that while Stable Diffusion provides high-quality image outputs, its application in producing detailed technical drawings is limited. The research introduces a hybrid approach, combining the strengths of GAN for structural accuracy and the image quality of Stable Diffusion, to produce renderings that are both aesthetically pleasing and technically detailed. 4 \uff09 Effect visualization: The final stage involves creating detailed three-dimensional visual effects from the two-dimensional plans. This stage tests the adaptability of algorithms to translate flat designs into vivid, multi-dimensional landscapes. Techniques such as ControlNet and specialized tuning methods like LoRA are used to fine-tune the visual outputs to meet specific aesthetic and functional requirements. The research delves into the challenges of maintaining the fidelity of the original design while enhancing the visual representation, which emphasizes the need for sophisticated control mechanisms to achieve high-quality visualizations. [Conclusion] The research concludes that while generative algorithms hold significant promise for the field of landscape architecture, their success is contingent upon targeted adaptations and enhancements tailored to specific design tasks. The complexities of integrating these technologies into a coherent design process highlight the necessity for a multidisciplinary approach that leverages both technological innovations and traditional design principles. Future research should aim to develop an integrated system that combines various AI technologies, potentially transforming the landscape architecture field by streamlining and enhancing the design process. This integrated approach could pave the way for new methodologies that seamlessly merge theoretical and practical aspects of landscape design, thus fostering innovation and efficiency.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.56640625
        }
    ],
    "quotes": {
        "cost": 0.17778899999999997,
        "quotes": [
            {
                "idx": 0,
                "key": "[267320876 | Lyu et al. | 2024 | Citations: 40]",
                "snippets": "Through rigorous experimental comparisons, we have demonstrated that RAG systems can significantly enhance the quality of generated content by effectively incorporating information from external knowledge sources.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "CONCLUSION",
                        "pdf_hash": "",
                        "start": 637,
                        "end": 850,
                        "sentence_offsets": [
                            {
                                "start": 637,
                                "end": 850
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Through rigorous experimental comparisons, we have demonstrated that RAG systems can significantly enhance the quality of generated content by effectively incorporating information from external knowledge sources."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[267412954 | Zhang et al. | 2024 | Citations: 20]",
                "snippets": "Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 885,
                        "end": 1046,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[268248396 | Soudani et al. | 2024 | Citations: 37]",
                "snippets": "We study the effectiveness of fine-tuning and RAG approaches for question answering over less popular factual knowledge and compare the performance of these models across distinct setups: vanilla and fine-tuned models, both with and without RAG, using different data augmentation methods.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 1590,
                        "end": 1878,
                        "sentence_offsets": [
                            {
                                "start": 1588,
                                "end": 1878
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We study the effectiveness of fine-tuning and RAG approaches for question answering over less popular factual knowledge and compare the performance of these models across distinct setups: vanilla and fine-tuned models, both with and without RAG, using different data augmentation methods."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[268819923 | Eibich et al. | 2024 | Citations: 4]",
                "snippets": "Despite the growing interest in RAG techniques within the domain of LLMs, the existing body of literature primarily consists of systematic reviews (Gao et al., 2024) and direct comparisons between successive state-of-the-art (SoTA) models (Gao et al., 2022;Jiang et al., 2023).This pattern reveals a notable gap: a comprehensive experimental comparison across a broad spectrum of advanced RAG techniques is missing.Such a comparison is crucial for understanding the relative strengths and weaknesses of these techniques in enhancing LLMs' performance across various tasks.This study seeks to contribute to bridging this gap by providing an extensive evaluation of multiple RAG techniques and their combinations, thereby offering insights into their efficacy and applicability in real-world scenarios.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 756,
                        "end": 1556,
                        "sentence_offsets": [
                            {
                                "start": 756,
                                "end": 1033
                            },
                            {
                                "start": 1033,
                                "end": 1171
                            },
                            {
                                "start": 1171,
                                "end": 1328
                            },
                            {
                                "start": 1328,
                                "end": 1556
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Despite the growing interest in RAG techniques within the domain of LLMs, the existing body of literature primarily consists of systematic reviews (Gao et al., 2024) and direct comparisons between successive state-of-the-art (SoTA) models (Gao et al., 2022;Jiang et al., 2023).This pattern reveals a notable gap: a comprehensive experimental comparison across a broad spectrum of advanced RAG techniques is missing.Such a comparison is crucial for understanding the relative strengths and weaknesses of these techniques in enhancing LLMs' performance across various tasks.This study seeks to contribute to bridging this gap by providing an extensive evaluation of multiple RAG techniques and their combinations, thereby offering insights into their efficacy and applicability in real-world scenarios."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[269214364 | Nguyen et al. | 2024 | Citations: 4]",
                "snippets": "This paper investigates the impact of domain-specific model fine-tuning and of reasoning mechanisms on the performance of question-answering (Q&A) systems powered by large language models (LLMs) and Retrieval-Augmented Generation (RAG). Using the FinanceBench SEC financial filings dataset, we observe that, for RAG, combining a fine-tuned embedding model with a fine-tuned LLM achieves better accuracy than generic models, with relatively greater gains attributable to fine-tuned embedding models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "This paper investigates the impact of domain-specific model fine-tuning and of reasoning mechanisms on the performance of question-answering (Q&A) systems powered by large language models (LLMs) and Retrieval-Augmented Generation (RAG). Using the FinanceBench SEC financial filings dataset, we observe that, for RAG, combining a fine-tuned embedding model with a fine-tuned LLM achieves better accuracy than generic models, with relatively greater gains attributable to fine-tuned embedding models.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[269292881 | Efeoglu et al. | 2024 | Citations: 9]",
                "snippets": "Ovadia et al. [25] evaluates the knowledge injection capacities of both fine-tuning and the RAG approach and found that LLMs dealt with performance problems through unsupervised fine-tuning while RAG outperformed the fine-tuning approach in unsupervised learning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Retrieval-Augmented Generation",
                        "pdf_hash": "",
                        "start": 768,
                        "end": 1031,
                        "sentence_offsets": [
                            {
                                "start": 768,
                                "end": 1031
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Ovadia et al. [25] evaluates the knowledge injection capacities of both fine-tuning and the RAG approach and found that LLMs dealt with performance problems through unsupervised fine-tuning while RAG outperformed the fine-tuning approach in unsupervised learning."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[269502216 | Mayfield et al. | 2024 | Citations: 16]",
                "snippets": "Most approaches to automated evaluation aim to estimate the effectiveness of RAG systems across desirable dimensions (e.g., faithfulness, answer relevance, and context relevance). Techniques include prompting LLMs to evaluate generated summaries [76], and fine-tuning lightweight models on synthetic data [73]. Downstream applications, such as question answering, can also be used to evaluate the effectiveness of RAG systems [74].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258587884 | Yue et al. | 2023 | Citations: 58]": "A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support its claims. However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution given by LLMs. We begin by defining different types of attribution errors, and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks such as question answering, fact-checking, natural language inference, and summarization. We manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on this curated test set and simulated examples from existing benchmarks highlight both promising signals and challenges. We hope our problem formulation, testbeds, and findings will help lay the foundation for future studies on this important problem."
                },
                "metadata": [
                    {
                        "section_title": "3.2.4",
                        "pdf_hash": "",
                        "start": 1078,
                        "end": 1509,
                        "sentence_offsets": [
                            {
                                "start": 999,
                                "end": 1105
                            },
                            {
                                "start": 1105,
                                "end": 1346
                            },
                            {
                                "start": 1348,
                                "end": 1527
                            }
                        ],
                        "ref_mentions": [
                            "258587884"
                        ],
                        "quote": "Most approaches to automated evaluation aim to estimate the effectiveness of RAG systems across desirable dimensions (e.g., faithfulness, answer relevance, and context relevance). Techniques include prompting LLMs to evaluate generated summaries [76], and fine-tuning lightweight models on synthetic data [73]. Downstream applications, such as question answering, can also be used to evaluate the effectiveness of RAG systems [74]."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[270123034 | Zhu et al. | 2024 | Citations: 6]",
                "snippets": "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 448,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[270214689 | Rathinasamy et al. | 2024 | Citations: 1]",
                "snippets": "In the context of enterprises accumulating proprietary unstructured data, AI-driven information retrieval solutions have emerged as vital tools for extracting relevant answers to employee queries. Traditional methods for developing such solutions often involve choosing between Retrieval Augmented Generation (RAG) or fine-tuned Large Language Models (LLMs). However, fine-tuned LLMs, comprising only generative models, lack a guarantee of factual accuracy, while RAG, comprising an embedding model and a generative model, assures factual precision (Lewis at al., 2020 [1]). Despite their superior performance in general, RAG based solutions often rely on pre-trained models, potentially leading to suboptimal alignment with enterprise-specific data.\n\nAddressing this challenge entails exploring two potential avenues: Firstly, recent studies such as RAFT (Zhang et al., 2024 [2]) explore the integration of fine-tuned generative models within a RAG pipeline to enhance accuracy, albeit requiring substantial domain-specific data to fine-tune the generative models. Alternatively, leveraging domain-specific embedding models within a RAG pipeline to enhance accuracy remains an underexplored area.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218869575 | Lewis et al. | 2020 | Citations: 6476]": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1197,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 196
                            },
                            {
                                "start": 197,
                                "end": 358
                            },
                            {
                                "start": 359,
                                "end": 574
                            },
                            {
                                "start": 575,
                                "end": 750
                            },
                            {
                                "start": 753,
                                "end": 1066
                            },
                            {
                                "start": 1067,
                                "end": 1198
                            }
                        ],
                        "ref_mentions": [
                            "218869575"
                        ],
                        "quote": "In the context of enterprises accumulating proprietary unstructured data, AI-driven information retrieval solutions have emerged as vital tools for extracting relevant answers to employee queries. Traditional methods for developing such solutions often involve choosing between Retrieval Augmented Generation (RAG) or fine-tuned Large Language Models (LLMs). However, fine-tuned LLMs, comprising only generative models, lack a guarantee of factual accuracy, while RAG, comprising an embedding model and a generative model, assures factual precision (Lewis at al., 2020 [1]). Despite their superior performance in general, RAG based solutions often rely on pre-trained models, potentially leading to suboptimal alignment with enterprise-specific data.\n\nAddressing this challenge entails exploring two potential avenues: Firstly, recent studies such as RAFT (Zhang et al., 2024 [2]) explore the integration of fine-tuned generative models within a RAG pipeline to enhance accuracy, albeit requiring substantial domain-specific data to fine-tune the generative models. Alternatively, leveraging domain-specific embedding models within a RAG pipeline to enhance accuracy remains an underexplored area."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[270560505 | Balakrishnan et al. | 2024 | Citations: 14]",
                "snippets": "More recent work further advances the field by introducing novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments [24].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 1246,
                        "end": 1414,
                        "sentence_offsets": [
                            {
                                "start": 1040,
                                "end": 1414
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "More recent work further advances the field by introducing novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments [24]."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[271039066 | Ficek et al. | 2024 | Citations: 1]",
                "snippets": "This study explores Parameter-Efficient Fine-Tuning (PEFT) methods applied to Retrieval-Augmented Generation (RAG) models, comparing GPT and RETRO architectures. RETRO generally outperforms GPT in zero-shot settings due to their pre-training process that integrates external retrieval, enhancing contextual understanding. However, GPT models show a higher performance potential with PEFT, indicating more room for improvement during fine-tuning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 445,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 161
                            },
                            {
                                "start": 162,
                                "end": 321
                            },
                            {
                                "start": 322,
                                "end": 445
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "This study explores Parameter-Efficient Fine-Tuning (PEFT) methods applied to Retrieval-Augmented Generation (RAG) models, comparing GPT and RETRO architectures. RETRO generally outperforms GPT in zero-shot settings due to their pre-training process that integrates external retrieval, enhancing contextual understanding. However, GPT models show a higher performance potential with PEFT, indicating more room for improvement during fine-tuning."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[271571143 | Long et al. | 2024 | Citations: 5]",
                "snippets": "In the medical domain, the application of RAG technology remains in its nascent stages. The MEDRAG system assessed performance variations across diverse retrievers and corpora in medical question-answering tasks [37]. The Self-BioRAG (Jeong et al., 2024) project integrated Self-RAG [5] technology in medicine, optimizing it through reflective tokens that address retrieval timing, evaluate the relevance and supporting capacity of retrieved documents in answer generation, and assess the quality of generated outputs. Nevertheless, empirical evidence suggests that this approach fails to surpass the performance of models specifically optimized for medical datasets, potentially attributable to the limitations of smaller models in multitask integration, as demonstrated by the AUTOACT study [28].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[267312134 | Jeong et al. | 2024 | Citations: 33]": "Abstract Summary Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens. Our work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions. Using three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less. Similarly, Self-BioRAG outperforms RAG by 8% Rouge-1 score in generating more proficient answers on two long-form question-answering benchmarks on average. Overall, we analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does. We release our data and code for training our framework components and model weights (7B and 13B) to enhance capabilities in biomedical and clinical domains. Availability and implementation Self-BioRAG is available at https://github.com/dmis-lab/self-biorag."
                },
                "metadata": [
                    {
                        "section_title": "B. Retrieval-augmented Generation",
                        "pdf_hash": "",
                        "start": 1637,
                        "end": 2419,
                        "sentence_offsets": [
                            {
                                "start": 1637,
                                "end": 1724
                            },
                            {
                                "start": 1725,
                                "end": 1854
                            },
                            {
                                "start": 1855,
                                "end": 2139
                            },
                            {
                                "start": 2140,
                                "end": 2419
                            }
                        ],
                        "ref_mentions": [
                            "267312134"
                        ],
                        "quote": "In the medical domain, the application of RAG technology remains in its nascent stages. The MEDRAG system assessed performance variations across diverse retrievers and corpora in medical question-answering tasks [37]. The Self-BioRAG (Jeong et al., 2024) project integrated Self-RAG [5] technology in medicine, optimizing it through reflective tokens that address retrieval timing, evaluate the relevance and supporting capacity of retrieved documents in answer generation, and assess the quality of generated outputs. Nevertheless, empirical evidence suggests that this approach fails to surpass the performance of models specifically optimized for medical datasets, potentially attributable to the limitations of smaller models in multitask integration, as demonstrated by the AUTOACT study [28]."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[273345967 | Sanniboina et al. | 2024 | Citations: 1]",
                "snippets": "The development of retrieval-augmented LLMs has advanced significantly, focusing on improving the synergy between retrieval and generation processes. Shao et al. [17] introduced ITER-RETGEN, which iteratively enhances retrieval and generation for tasks like multi-hop QA and commonsense reasoning. It alternates between retrieval-augmented generation and generation-augmented retrieval, refining each step with the previous output. But, it faces challenges such as suboptimal utilization of retrieved information, with about 20% of contexts lacking actual answers, leading to inaccuracies or hallucinations due to positional bias and non-optimized iterative retrieval.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 668,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 149
                            },
                            {
                                "start": 150,
                                "end": 297
                            },
                            {
                                "start": 298,
                                "end": 431
                            },
                            {
                                "start": 432,
                                "end": 668
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The development of retrieval-augmented LLMs has advanced significantly, focusing on improving the synergy between retrieval and generation processes. Shao et al. [17] introduced ITER-RETGEN, which iteratively enhances retrieval and generation for tasks like multi-hop QA and commonsense reasoning. It alternates between retrieval-augmented generation and generation-augmented retrieval, refining each step with the previous output. But, it faces challenges such as suboptimal utilization of retrieved information, with about 20% of contexts lacking actual answers, leading to inaccuracies or hallucinations due to positional bias and non-optimized iterative retrieval."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[273403982 | Gupta et al. | 2024 | Citations: 23]",
                "snippets": "In addition to knowledge retrieval, RAG models excel at updating knowledge bases. Since the model fetches external documents for each query, it requires no retraining to incorporate the latest information. This flexibility makes RAG models particularly suitable for domains where information is constantly evolving, such as medical research, financial news, and legal proceedings. Furthermore, studies have shown that RAG models achieve superior results in a variety of knowledge-intensive tasks, including document summarization and, knowledge-grounded dialogues",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Overview of RAG Models",
                        "pdf_hash": "",
                        "start": 1676,
                        "end": 2239,
                        "sentence_offsets": [
                            {
                                "start": 1676,
                                "end": 1757
                            },
                            {
                                "start": 1758,
                                "end": 1881
                            },
                            {
                                "start": 1882,
                                "end": 2056
                            },
                            {
                                "start": 2057,
                                "end": 2239
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In addition to knowledge retrieval, RAG models excel at updating knowledge bases. Since the model fetches external documents for each query, it requires no retraining to incorporate the latest information. This flexibility makes RAG models particularly suitable for domains where information is constantly evolving, such as medical research, financial news, and legal proceedings. Furthermore, studies have shown that RAG models achieve superior results in a variety of knowledge-intensive tasks, including document summarization and, knowledge-grounded dialogues"
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[273502659 | Chen et al. | 2024 | Citations: 2]",
                "snippets": "Compared to model fine-tuning, Class-RAG demonstrates flexibility and transparency in decision-making, outperforms on classification and is more robust against adversarial attack, as evidenced by empirical studies.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 811,
                        "end": 1025,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Compared to model fine-tuning, Class-RAG demonstrates flexibility and transparency in decision-making, outperforms on classification and is more robust against adversarial attack, as evidenced by empirical studies."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[273850363 | Dong et al. | 2024 | Citations: 13]",
                "snippets": "In contrast, the RAG model and its improved version RAG+T (RAG+Text) perform significantly better than BART and T5 in these three indicators. The RAG model combines the retrieval module to enable it to dynamically access the external knowledge base during the generation process, thereby significantly improving knowledge consistency (0.73) and reasoning capability (0.80). The performance of RAG+T is further improved, especially in terms of reasoning ability, which reaches 0.84, which shows the great potential of retrieval-enhanced generative models in complex tasks. By incorporating more relevant knowledge into the generation process, RAG+T shows stronger ability to deal with complex background information and deep reasoning, further narrowing the limitations of generative models in dealing with knowledge-intensive tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Experimental Results",
                        "pdf_hash": "",
                        "start": 1229,
                        "end": 2061,
                        "sentence_offsets": [
                            {
                                "start": 1229,
                                "end": 1370
                            },
                            {
                                "start": 1371,
                                "end": 1602
                            },
                            {
                                "start": 1603,
                                "end": 1800
                            },
                            {
                                "start": 1801,
                                "end": 2061
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In contrast, the RAG model and its improved version RAG+T (RAG+Text) perform significantly better than BART and T5 in these three indicators. The RAG model combines the retrieval module to enable it to dynamically access the external knowledge base during the generation process, thereby significantly improving knowledge consistency (0.73) and reasoning capability (0.80). The performance of RAG+T is further improved, especially in terms of reasoning ability, which reaches 0.84, which shows the great potential of retrieval-enhanced generative models in complex tasks. By incorporating more relevant knowledge into the generation process, RAG+T shows stronger ability to deal with complex background information and deep reasoning, further narrowing the limitations of generative models in dealing with knowledge-intensive tasks."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[273969615 | Zhang et al. | 2024 | Citations: 3]",
                "snippets": "A comparative study by Ovadia et al. [98] shows that RAG outperforms unsupervised fine-tuning, particularly in scenarios involving new or unseen knowledge, underscoring its superiority in knowledge injection and model adaptation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C. Retrieval-Augmented Generation",
                        "pdf_hash": "",
                        "start": 636,
                        "end": 865,
                        "sentence_offsets": [
                            {
                                "start": 636,
                                "end": 865
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "A comparative study by Ovadia et al. [98] shows that RAG outperforms unsupervised fine-tuning, particularly in scenarios involving new or unseen knowledge, underscoring its superiority in knowledge injection and model adaptation."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[274283400 | Yao et al. | 2024 | Citations: 4]",
                "snippets": "Future work includes comparisons with existing methods for improving the performance of RAGs and evaluation experiments using a wider range of datasets. We plan to compare our method with existing robust approaches, including RAG by Facebook AI [18], REALM by Google (Guu et al., 2020), and various domain-specific retrieval systems such as PubMed (White, 2020), BioBERT (Lee et al., 2019) PatentBERT [21], and FinancialBERT [22]. Additionally, we will evaluate against the latest state-of-the-art LLMs, such as Phi-3.5 [23], LLaMA 3.2 [24], and OLMo [25].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[211204736 | Guu et al. | 2020 | Citations: 2119]": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",
                    "[224820417 | White | 2020 | Citations: 127]": "Abstract After years of strategic planning, the National Library of Medicine has introduced an updated and redesigned version of its PubMed health sciences research website. The new website features a more modern and responsive interface, especially on mobile devices. Tools and features have been relocated to make them more intuitive for new users. While not without some turbulence and slight discomfort for long-time users adjusting to the modernized interface and search engine, the new version of the PubMed website introduced in 2020 succeeds in the website\u2019s time-honored task of collecting and making freely accessible high-quality health sciences information and resources.",
                    "[59291975 | Lee et al. | 2019 | Citations: 5673]": "Abstract Motivation Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. Results We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. Availability and implementation We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert."
                },
                "metadata": [
                    {
                        "section_title": "Concluding Remarks",
                        "pdf_hash": "",
                        "start": 1438,
                        "end": 1956,
                        "sentence_offsets": [
                            {
                                "start": 1438,
                                "end": 1590
                            },
                            {
                                "start": 1591,
                                "end": 1830
                            },
                            {
                                "start": 1831,
                                "end": 1956
                            }
                        ],
                        "ref_mentions": [
                            "211204736",
                            "224820417",
                            "59291975"
                        ],
                        "quote": "Future work includes comparisons with existing methods for improving the performance of RAGs and evaluation experiments using a wider range of datasets. We plan to compare our method with existing robust approaches, including RAG by Facebook AI [18], REALM by Google (Guu et al., 2020), and various domain-specific retrieval systems such as PubMed (White, 2020), BioBERT (Lee et al., 2019) PatentBERT [21], and FinancialBERT [22]. Additionally, we will evaluate against the latest state-of-the-art LLMs, such as Phi-3.5 [23], LLaMA 3.2 [24], and OLMo [25]."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[274788878 | Papadimitriou et al. | 2024 | Citations: 1]",
                "snippets": "We present RAG Playground, an open-source framework for systematic evaluation of Retrieval-Augmented Generation (RAG) systems. The framework implements and compares three retrieval approaches: naive vector search, reranking, and hybrid vector-keyword search, combined with ReAct agents using different prompting strategies. We introduce a comprehensive evaluation framework with novel metrics and provide empirical results comparing different language models (Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our experiments demonstrate significant performance improvements through hybrid search methods and structured self-evaluation prompting, achieving up to 72.7% pass rate on our multi-metric evaluation framework. The results also highlight the importance of prompt engineering in RAG systems, with our custom-prompted agents showing consistent improvements in retrieval accuracy and response quality.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 923,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We present RAG Playground, an open-source framework for systematic evaluation of Retrieval-Augmented Generation (RAG) systems. The framework implements and compares three retrieval approaches: naive vector search, reranking, and hybrid vector-keyword search, combined with ReAct agents using different prompting strategies. We introduce a comprehensive evaluation framework with novel metrics and provide empirical results comparing different language models (Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our experiments demonstrate significant performance improvements through hybrid search methods and structured self-evaluation prompting, achieving up to 72.7% pass rate on our multi-metric evaluation framework. The results also highlight the importance of prompt engineering in RAG systems, with our custom-prompted agents showing consistent improvements in retrieval accuracy and response quality."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[275788867 | Devine | 2025 | Citations: 0]",
                "snippets": "We demonstrate the effectiveness of ALoF-TRAG by performing experiments on 20 datasets in 26 languages across a variety of domains and comparing the accuracy to simply using the base LLM for RAG. We show that the ALoFTRAG approach improves both the citation accuracy and answer accuracy of RAG models across almost all datasets compared to the base RAG model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1643,
                        "end": 2002,
                        "sentence_offsets": [
                            {
                                "start": 1643,
                                "end": 1838
                            },
                            {
                                "start": 1839,
                                "end": 2002
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We demonstrate the effectiveness of ALoF-TRAG by performing experiments on 20 datasets in 26 languages across a variety of domains and comparing the accuracy to simply using the base LLM for RAG. We show that the ALoFTRAG approach improves both the citation accuracy and answer accuracy of RAG models across almost all datasets compared to the base RAG model."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[276355526 | Budakoglu et al. | 2025 | Citations: 0]",
                "snippets": "This study compares two salient techniques for retrieve-augmented generation (RAG) and fine-tuning along with a new hybrid method that combines both. In this study, we investigate the effectiveness of various methods using the Stanford Question Answering Dataset (SQuAD), Microsoft Machine Reading Comprehension (MS MARCO) and SQL CREATE TABLE statements.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "This study compares two salient techniques for retrieve-augmented generation (RAG) and fine-tuning along with a new hybrid method that combines both. In this study, we investigate the effectiveness of various methods using the Stanford Question Answering Dataset (SQuAD), Microsoft Machine Reading Comprehension (MS MARCO) and SQL CREATE TABLE statements.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[276408622 | Han et al. | 2025 | Citations: 3]",
                "snippets": "Several studies have evaluated the effectiveness of RAG systems across various tasks (Yu et al., 2024;(Chen et al., 2023)Es et al., 2023), such as multi-hop question answering (Tian et al., 2023), biomedical question answering (Xiong et al., 2024), and text generation (Liu et al., 2023). However, no existing study has simultaneously and systematically evaluated and compared RAG and GraphRAG on these general text-based tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[261530434 | Chen et al. | 2023 | Citations: 307]": "Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.",
                    "[263152125 | Tian et al. | 2023 | Citations: 49]": "Large language models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs (KGs) to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. Therefore, how to enhance pre-trained LLMs using grounded knowledge, e.g., retrieval-augmented generation, remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple datasets demonstrate the superiority of GNP on both commonsense and biomedical reasoning tasks across different LLM sizes and settings. Code is available at https://github.com/meettyj/GNP."
                },
                "metadata": [
                    {
                        "section_title": "Retrieval-Augmented Generation",
                        "pdf_hash": "",
                        "start": 1079,
                        "end": 1508,
                        "sentence_offsets": [
                            {
                                "start": 1079,
                                "end": 1368
                            },
                            {
                                "start": 1369,
                                "end": 1508
                            }
                        ],
                        "ref_mentions": [
                            "261530434",
                            "263152125"
                        ],
                        "quote": "Several studies have evaluated the effectiveness of RAG systems across various tasks (Yu et al., 2024;(Chen et al., 2023)Es et al., 2023), such as multi-hop question answering (Tian et al., 2023), biomedical question answering (Xiong et al., 2024), and text generation (Liu et al., 2023). However, no existing study has simultaneously and systematically evaluated and compared RAG and GraphRAG on these general text-based tasks."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[276408784 | Baqar et al. | 2025 | Citations: 1]",
                "snippets": "This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 655,
                        "end": 1200,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[276526965 | Vithanage et al. | 2025 | Citations: 0]",
                "snippets": "Results show that zero-shot and few-shot learning, whether combined with PEFT or RAG, achieve comparable performance across the clinical domains when the same prompting template is used. Few-shot learning significantly outperforms zero-shot learning when neither PEFT nor RAG is applied. Notably, PEFT significantly improves model performance in both zero-shot and few-shot learning; however, RAG significantly improves performance only in few-shot learning. After PEFT, the performance of zero-shot learning reaches a comparable level with few-shot learning. However, few-shot learning with RAG significantly outperforms zero-shot learning with RAG. We also found a similar level of performance between few-shot learning with RAG and zero-shot learning with PEFT.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Results show that zero-shot and few-shot learning, whether combined with PEFT or RAG, achieve comparable performance across the clinical domains when the same prompting template is used. Few-shot learning significantly outperforms zero-shot learning when neither PEFT nor RAG is applied. Notably, PEFT significantly improves model performance in both zero-shot and few-shot learning; however, RAG significantly improves performance only in few-shot learning. After PEFT, the performance of zero-shot learning reaches a comparable level with few-shot learning. However, few-shot learning with RAG significantly outperforms zero-shot learning with RAG. We also found a similar level of performance between few-shot learning with RAG and zero-shot learning with PEFT.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[277104712 | Yu et al. | 2025 | Citations: 3]",
                "snippets": "B\u00e9chard et al. (B'echard et al., 2024) demonstrated the effectiveness of RAG in reducing hallucinations in structured outputs like workflow generation from natural language instructions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[269137180 | B'echard et al. | 2024 | Citations: 60]": "A common and fundamental limitation of Generative AI (GenAI) is its propensity to hallucinate. While large language models (LLM) have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging Retrieval Augmented Generation (RAG) to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucinations in the output and improves the generalization of our LLM in out-of-domain settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive."
                },
                "metadata": [
                    {
                        "section_title": "A. Hallucination in Generative AI",
                        "pdf_hash": "",
                        "start": 861,
                        "end": 1028,
                        "sentence_offsets": [
                            {
                                "start": 861,
                                "end": 1028
                            }
                        ],
                        "ref_mentions": [
                            "269137180"
                        ],
                        "quote": "B\u00e9chard et al. (B'echard et al., 2024) demonstrated the effectiveness of RAG in reducing hallucinations in structured outputs like workflow generation from natural language instructions."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[278782961 | Wang et al. | 2025 | Citations: 0]",
                "snippets": "No prior research has explored the trade-off of the two approaches in industrial scenarios. To mitigate the gap, we comprehensively compare the two paradigms including Retrieval-Augmented Generation (RAG) and Fine-tuning (FT), for industrial code completion in this paper...Our findings reveal that RAG, when implemented with appropriate embedding models that map code snippets into dense vector representations, can achieve higher accuracy than fine-tuning alone. Specifically, BM25 presents superior retrieval effectiveness and efficiency among studied RAG methods. Moreover, RAG and fine-tuning are orthogonal and their combination leads to further improvement. We also observe that RAG demonstrates better scalability than FT, showing more sustained performance gains with larger scales of codebase.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "No prior research has explored the trade-off of the two approaches in industrial scenarios. To mitigate the gap, we comprehensively compare the two paradigms including Retrieval-Augmented Generation (RAG) and Fine-tuning (FT), for industrial code completion in this paper",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    },
                    {
                        "quote": "Our findings reveal that RAG, when implemented with appropriate embedding models that map code snippets into dense vector representations, can achieve higher accuracy than fine-tuning alone. Specifically, BM25 presents superior retrieval effectiveness and efficiency among studied RAG methods. Moreover, RAG and fine-tuning are orthogonal and their combination leads to further improvement. We also observe that RAG demonstrates better scalability than FT, showing more sustained performance gains with larger scales of codebase.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.022617,
        "cot": "The user is asking about empirical studies comparing RAG and fine-tuning approaches for improving generative AI models. I need to structure this information in a way that clearly addresses the query.\n\nFor this topic, I should start with an introduction section that provides background on both RAG and fine-tuning approaches to set the context. \n\nThen, I need to organize the empirical studies that compare these approaches. Looking at the quotes, I can identify several dimensions:\n\n1. \"Comparative Performance Studies\" - This dimension will synthesize the empirical studies that directly compare RAG and fine-tuning performance across different metrics and scenarios. This should be in synthesis format as it requires bringing together multiple findings to present a coherent picture of how these approaches compare.\n\n2. \"Domain-Specific Comparisons\" - Many quotes discuss how RAG and fine-tuning perform in specific domains (medical, financial, industrial code, etc.). This warrants its own dimension to organize findings by domain, and should be in synthesis format to properly connect related information.\n\n3. \"Hybrid Approaches\" - Several quotes mention combining RAG and fine-tuning. This dimension will explore how hybrid approaches perform compared to either approach alone. Synthesis format is appropriate to explain these nuanced findings.\n\n4. \"Evaluation Methodologies\" - Some quotes discuss frameworks and methodologies for evaluating RAG and fine-tuning approaches. This dimension will help understand how researchers are measuring effectiveness. Synthesis format works best to explain the evaluation approaches.\n\n5. \"Performance Factors and Trade-offs\" - This dimension will explore the various factors that affect the performance of each approach and the trade-offs between them. Synthesis format is appropriate to explain these complex relationships.\n\nThis structure will comprehensively cover the empirical studies comparing RAG and fine-tuning approaches while making the information easily digestible for the reader.",
        "plan": {
            "Introduction to RAG and Fine-tuning (synthesis)": [],
            "Comparative Performance Studies (synthesis)": [
                5,
                7,
                14,
                15,
                16,
                22,
                25
            ],
            "Domain-Specific Comparisons (synthesis)": [
                2,
                4,
                8,
                11,
                13,
                18,
                19,
                20,
                21,
                23,
                24
            ],
            "Hybrid Approaches (synthesis)": [
                1,
                9,
                10,
                12,
                17
            ],
            "Evaluation Methodologies (synthesis)": [
                0,
                3,
                6
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to RAG and Fine-tuning",
                "tldr": "Retrieval-Augmented Generation (RAG) and fine-tuning represent two distinct approaches to enhancing the performance of large language models. While RAG improves model outputs by retrieving relevant external knowledge during inference, fine-tuning adapts the model's parameters through additional training on specific datasets. (LLM Memory)",
                "text": "\nRetrieval-Augmented Generation (RAG) and fine-tuning are two primary methods for improving the capabilities of generative AI models, each with distinct approaches and tradeoffs. RAG works by combining a retrieval system with a generation model, allowing the model to access and leverage external knowledge sources during inference without modifying the model's parameters. This approach enables models to incorporate up-to-date or domain-specific information that wasn't available during their training, improving factuality and reducing hallucinations. The retrieval component searches through a knowledge base (such as documents, databases, or the internet) to find relevant information that can inform the generation process. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nFine-tuning, in contrast, involves additional training of a pre-trained model on specific datasets to adapt its parameters for particular tasks or domains. This process modifies the model's internal knowledge and behavior patterns through gradient-based optimization. Fine-tuning can be performed using various techniques, including full fine-tuning (updating all model parameters), parameter-efficient fine-tuning (PEFT) methods like LoRA that update only a subset of parameters, or instruction tuning to align the model with specific formats and requirements. Fine-tuning generally requires computational resources for training and a high-quality dataset that represents the target domain or task. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThese approaches serve different purposes and can be selected based on specific requirements: RAG is particularly useful when up-to-date or specialized information is needed without retraining, while fine-tuning may be more appropriate when the goal is to systematically adapt the model's behavior for a particular domain or task format. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Comparative Performance Studies",
                "tldr": "Empirical studies comparing RAG and fine-tuning show that each approach excels in different scenarios, with RAG often demonstrating superior performance in knowledge-intensive tasks and handling unseen information, while fine-tuning methods like DoRA can achieve higher accuracy in specific domains. (7 sources)",
                "text": "\nSeveral comparative studies have evaluated the performance of Retrieval-Augmented Generation (RAG) against fine-tuning approaches across various tasks and domains. Research by Ovadia et al. demonstrates that RAG outperforms unsupervised fine-tuning, particularly when dealing with new or previously unseen knowledge, highlighting its effectiveness for knowledge injection and model adaptation <Paper corpusId=\"269292881\" paperTitle=\"(Efeoglu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This advantage is especially pronounced in situations requiring up-to-date or specialized information that wasn't included in the original training data.\n\nIn knowledge-intensive tasks, RAG models have shown significant improvements over traditional fine-tuned models like BART and T5. The combination of retrieval mechanisms with generation capabilities enables RAG to dynamically access external knowledge bases during inference, substantially enhancing knowledge consistency (0.73) and reasoning capability (0.80). Further improvements in RAG architectures, such as RAG+T (RAG+Text), have pushed reasoning abilities even higher (0.84), demonstrating the potential of retrieval-enhanced generative models to overcome limitations in complex reasoning tasks <Paper corpusId=\"273850363\" paperTitle=\"(Dong et al., 2024)\" isShortName></Paper>.\n\nFor classification tasks, Class-RAG has been shown to outperform model fine-tuning approaches, offering greater flexibility, transparency in decision-making, and increased robustness against adversarial attacks <Paper corpusId=\"273502659\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>. However, the performance tradeoffs between these approaches can vary significantly based on the specific application context.\n\nA large-scale empirical evaluation comparing RAG with fine-tuning methods such as LoRA and DoRA on FAQ-based queries revealed that DoRA achieved the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query) in domain-specific applications <Paper corpusId=\"276408784\" paperTitle=\"(Baqar et al., 2025)\" isShortName></Paper>. This suggests that advanced fine-tuning techniques can outperform RAG in certain specialized scenarios where performance metrics like accuracy and latency are critical.\n\nIn industrial code completion tasks, comprehensive comparisons between RAG and fine-tuning have shown that RAG implementations with appropriate embedding models can achieve higher accuracy than fine-tuning alone. Notably, BM25 demonstrated superior retrieval effectiveness and efficiency among the RAG methods studied. The research also found that RAG exhibits better scalability than fine-tuning, with more sustained performance gains as the codebase size increases <Paper corpusId=\"278782961\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.\n\nAn important consideration when implementing RAG is that while it generally improves factuality and accuracy without modifying model parameters, fine-tuning may compromise the model's general generation capabilities by altering its parameters <Paper corpusId=\"270123034\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>. This suggests that RAG may be preferable in scenarios where maintaining the model's general capabilities is important alongside enhancing performance on specific tasks.\n\nInterestingly, research indicates that RAG and fine-tuning approaches are not mutually exclusive but rather complementary, with their combination leading to further performance improvements <Paper corpusId=\"278782961\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>. This points to the potential benefits of hybrid approaches that leverage the strengths of both methodologies.",
                "citations": [
                    {
                        "id": "(Efeoglu et al., 2024)",
                        "snippets": [
                            "Ovadia et al. [25] evaluates the knowledge injection capacities of both fine-tuning and the RAG approach and found that LLMs dealt with performance problems through unsupervised fine-tuning while RAG outperformed the fine-tuning approach in unsupervised learning."
                        ],
                        "paper": {
                            "corpus_id": 269292881,
                            "title": "Retrieval-Augmented Generation-based Relation Extraction",
                            "authors": [
                                {
                                    "authorId": "2189018699",
                                    "name": "Sefika Efeoglu"
                                },
                                {
                                    "authorId": "2259621860",
                                    "name": "Adrian Paschke"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.93310546875
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "A comparative study by Ovadia et al. [98] shows that RAG outperforms unsupervised fine-tuning, particularly in scenarios involving new or unseen knowledge, underscoring its superiority in knowledge injection and model adaptation."
                        ],
                        "paper": {
                            "corpus_id": 273969615,
                            "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
                            "authors": [
                                {
                                    "authorId": "2270181751",
                                    "name": "Zilun Zhang"
                                },
                                {
                                    "authorId": "2174678931",
                                    "name": "Haozhan Shen"
                                },
                                {
                                    "authorId": "8200875",
                                    "name": "Tiancheng Zhao"
                                },
                                {
                                    "authorId": "2330774884",
                                    "name": "Yuhao Wang"
                                },
                                {
                                    "authorId": "2330612748",
                                    "name": "Bin Chen"
                                },
                                {
                                    "authorId": "2149196373",
                                    "name": "Yuxiang Cai"
                                },
                                {
                                    "authorId": "2093090552",
                                    "name": "Yongheng Shang"
                                },
                                {
                                    "authorId": "2111612160",
                                    "name": "Jianwei Yin"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.9248046875
                    },
                    {
                        "id": "(Dong et al., 2024)",
                        "snippets": [
                            "In contrast, the RAG model and its improved version RAG+T (RAG+Text) perform significantly better than BART and T5 in these three indicators. The RAG model combines the retrieval module to enable it to dynamically access the external knowledge base during the generation process, thereby significantly improving knowledge consistency (0.73) and reasoning capability (0.80). The performance of RAG+T is further improved, especially in terms of reasoning ability, which reaches 0.84, which shows the great potential of retrieval-enhanced generative models in complex tasks. By incorporating more relevant knowledge into the generation process, RAG+T shows stronger ability to deal with complex background information and deep reasoning, further narrowing the limitations of generative models in dealing with knowledge-intensive tasks."
                        ],
                        "paper": {
                            "corpus_id": 273850363,
                            "title": "Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge Reasoning and Text Generation",
                            "authors": [
                                {
                                    "authorId": "2326920545",
                                    "name": "Yuxin Dong"
                                },
                                {
                                    "authorId": "2327007198",
                                    "name": "Shuo Wang"
                                },
                                {
                                    "authorId": "2327003963",
                                    "name": "Hongye Zheng"
                                },
                                {
                                    "authorId": "2322450076",
                                    "name": "Jiajing Chen"
                                },
                                {
                                    "authorId": "2322450970",
                                    "name": "Zhenhong Zhang"
                                },
                                {
                                    "authorId": "2322612972",
                                    "name": "Chihang Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "2024 5th International Symposium on Computer Engineering and Intelligent Communications (ISCEIC)",
                            "n_citations": 13
                        },
                        "score": 0.89208984375
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "Compared to model fine-tuning, Class-RAG demonstrates flexibility and transparency in decision-making, outperforms on classification and is more robust against adversarial attack, as evidenced by empirical studies."
                        ],
                        "paper": {
                            "corpus_id": 273502659,
                            "title": "Class-RAG: Real-Time Content Moderation with Retrieval Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2327003851",
                                    "name": "Jianfa Chen"
                                },
                                {
                                    "authorId": "2326992786",
                                    "name": "Emily Shen"
                                },
                                {
                                    "authorId": "2297187181",
                                    "name": "Trupti Bavalatti"
                                },
                                {
                                    "authorId": "2327028660",
                                    "name": "Xiaowen Lin"
                                },
                                {
                                    "authorId": "2326986310",
                                    "name": "Yongkai Wang"
                                },
                                {
                                    "authorId": "2327158340",
                                    "name": "Shuming Hu"
                                },
                                {
                                    "authorId": "2322094813",
                                    "name": "Harihar Subramanyam"
                                },
                                {
                                    "authorId": "2149726609",
                                    "name": "Ksheeraj Sai Vepuri"
                                },
                                {
                                    "authorId": "2327303021",
                                    "name": "Ming Jiang"
                                },
                                {
                                    "authorId": "2327505613",
                                    "name": "Ji Qi"
                                },
                                {
                                    "authorId": "2287762612",
                                    "name": "Li Chen"
                                },
                                {
                                    "authorId": "2326964342",
                                    "name": "Nan Jiang"
                                },
                                {
                                    "authorId": "2287848816",
                                    "name": "Ankit Jain"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.91845703125
                    },
                    {
                        "id": "(Baqar et al., 2025)",
                        "snippets": [
                            "This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications."
                        ],
                        "paper": {
                            "corpus_id": 276408784,
                            "title": "Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA",
                            "authors": [
                                {
                                    "authorId": "2316485338",
                                    "name": "Mohammad Baqar"
                                },
                                {
                                    "authorId": "69923048",
                                    "name": "Rajat Khanda"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9873046875
                    },
                    {
                        "id": "(Wang et al., 2025)",
                        "snippets": [
                            "No prior research has explored the trade-off of the two approaches in industrial scenarios. To mitigate the gap, we comprehensively compare the two paradigms including Retrieval-Augmented Generation (RAG) and Fine-tuning (FT), for industrial code completion in this paper",
                            "Our findings reveal that RAG, when implemented with appropriate embedding models that map code snippets into dense vector representations, can achieve higher accuracy than fine-tuning alone. Specifically, BM25 presents superior retrieval effectiveness and efficiency among studied RAG methods. Moreover, RAG and fine-tuning are orthogonal and their combination leads to further improvement. We also observe that RAG demonstrates better scalability than FT, showing more sustained performance gains with larger scales of codebase."
                        ],
                        "paper": {
                            "corpus_id": 278782961,
                            "title": "RAG or Fine-tuning? A Comparative Study on LCMs-based Code Completion in Industry",
                            "authors": [
                                {
                                    "authorId": "2135764153",
                                    "name": "Chaozheng Wang"
                                },
                                {
                                    "authorId": "2155450982",
                                    "name": "Zezhou Yang"
                                },
                                {
                                    "authorId": "2112314113",
                                    "name": "Shuzheng Gao"
                                },
                                {
                                    "authorId": "2267893922",
                                    "name": "Cuiyun Gao"
                                },
                                {
                                    "authorId": "2299029254",
                                    "name": "Ting Peng"
                                },
                                {
                                    "authorId": "2265772572",
                                    "name": "Hailiang Huang"
                                },
                                {
                                    "authorId": "2299160326",
                                    "name": "Yuetang Deng"
                                },
                                {
                                    "authorId": "2338266828",
                                    "name": "Michael R. Lyu"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.970703125
                    },
                    {
                        "id": "(Zhu et al., 2024)",
                        "snippets": [
                            "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters."
                        ],
                        "paper": {
                            "corpus_id": 270123034,
                            "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
                            "authors": [
                                {
                                    "authorId": "1900406",
                                    "name": "Yutao Zhu"
                                },
                                {
                                    "authorId": "2187935160",
                                    "name": "Zhaoheng Huang"
                                },
                                {
                                    "authorId": "1897235",
                                    "name": "Zhicheng Dou"
                                },
                                {
                                    "authorId": "2186578511",
                                    "name": "Ji-Rong Wen"
                                }
                            ],
                            "year": 2024,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 6
                        },
                        "score": 0.90380859375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Domain-Specific Comparisons",
                "tldr": "Empirical studies across various domains show that RAG and fine-tuning exhibit different performance characteristics depending on the specific field; in financial, medical, and enterprise settings, domain-adapted RAG implementations generally outperform generic approaches, while hybrid methods combining both techniques often yield the best results. (16 sources)",
                "text": "\nDomain-specific evaluations of RAG and fine-tuning approaches reveal important insights about their relative effectiveness across different fields. In the financial domain, research using the FinanceBench SEC financial filings dataset demonstrated that combining a fine-tuned embedding model with a fine-tuned LLM achieved better accuracy than generic models within RAG systems. Notably, the researchers found that greater performance gains were attributable to fine-tuned embedding models than to fine-tuned language models <Paper corpusId=\"269214364\" paperTitle=\"(Nguyen et al., 2024)\" isShortName></Paper>.\n\nFor enterprise knowledge management, traditional approaches typically involve choosing between RAG or fine-tuned LLMs. While fine-tuned LLMs can be effective, they lack guarantees of factual accuracy. Conversely, RAG solutions ensure factual precision by retrieving relevant information from knowledge bases, but may perform suboptimally when relying on pre-trained models that aren't aligned with enterprise-specific data <Paper corpusId=\"270214689\" paperTitle=\"(Rathinasamy et al., 2024)\" isShortName></Paper> <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>. This finding highlights the importance of domain adaptation in retrieval components.\n\nIn the medical domain, RAG technology applications remain relatively new, with systems like MEDRAG and Self-BioRAG showing promise. Self-BioRAG integrates reflective tokens to evaluate retrieval timing, document relevance, and generation quality. However, empirical evidence suggests that these RAG approaches still struggle to surpass models specifically optimized for medical datasets, possibly due to limitations of smaller models in multitask integration <Paper corpusId=\"271571143\" paperTitle=\"(Long et al., 2024)\" isShortName></Paper> <Paper corpusId=\"267312134\" paperTitle=\"(Jeong et al., 2024)\" isShortName></Paper>.\n\nRAG models demonstrate particular advantages in domains where information evolves rapidly, such as medical research, financial news, and legal proceedings. Their ability to update knowledge bases without retraining makes them especially suitable for these dynamic fields <Paper corpusId=\"273403982\" paperTitle=\"(Gupta et al., 2024)\" isShortName></Paper>. In clinical applications, comparative studies show that Parameter-Efficient Fine-Tuning (PEFT) significantly improves model performance in both zero-shot and few-shot scenarios, while RAG significantly enhances performance only in few-shot learning. Interestingly, few-shot learning with RAG significantly outperforms zero-shot learning with RAG, suggesting that the combination of techniques can be particularly powerful <Paper corpusId=\"276526965\" paperTitle=\"(Vithanage et al., 2025)\" isShortName></Paper>.\n\nFor workflow generation from natural language instructions, RAG has proven effective in reducing hallucinations in structured outputs <Paper corpusId=\"277104712\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"269137180\" paperTitle=\"(B'echard et al., 2024)\" isShortName></Paper>. The improved quality of structured outputs demonstrates RAG's value in enterprise applications requiring high precision.\n\nRecent innovations in RAG implementation show promising results across various domains. The RAG Playground framework, which implements and compares three retrieval approaches (naive vector search, reranking, and hybrid vector-keyword search), has demonstrated significant performance improvements through hybrid search methods and structured self-evaluation prompting, achieving up to 72.7% pass rate on multi-metric evaluations <Paper corpusId=\"274788878\" paperTitle=\"(Papadimitriou et al., 2024)\" isShortName></Paper>. Similarly, the ALoF-TRAG approach has been shown to improve both citation accuracy and answer accuracy across almost all datasets compared to base RAG models in experiments spanning 20 datasets in 26 languages across various domains <Paper corpusId=\"275788867\" paperTitle=\"(Devine, 2025)\" isShortName></Paper>.\n\nComparative studies examining the effectiveness of RAG and fine-tuning for question answering over less popular factual knowledge have found that performance varies significantly depending on the specific setup used <Paper corpusId=\"268248396\" paperTitle=\"(Soudani et al., 2024)\" isShortName></Paper>. Research using benchmark datasets like SQuAD, MS MARCO, and SQL CREATE TABLE statements further validates the complementary nature of these approaches <Paper corpusId=\"276355526\" paperTitle=\"(Budakoglu et al., 2025)\" isShortName></Paper>.\n\nWhile multiple studies have evaluated RAG systems across various tasks, including multi-hop question answering, biomedical question answering, and text generation, systematic comparisons between traditional RAG and newer variants like GraphRAG remain limited <Paper corpusId=\"276408622\" paperTitle=\"(Han et al., 2025)\" isShortName></Paper> <Paper corpusId=\"261530434\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper> <Paper corpusId=\"263152125\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper>. This suggests an opportunity for more comprehensive cross-domain evaluations to better understand the relative strengths of different approaches.",
                "citations": [
                    {
                        "id": "(Nguyen et al., 2024)",
                        "snippets": [
                            "This paper investigates the impact of domain-specific model fine-tuning and of reasoning mechanisms on the performance of question-answering (Q&A) systems powered by large language models (LLMs) and Retrieval-Augmented Generation (RAG). Using the FinanceBench SEC financial filings dataset, we observe that, for RAG, combining a fine-tuned embedding model with a fine-tuned LLM achieves better accuracy than generic models, with relatively greater gains attributable to fine-tuned embedding models."
                        ],
                        "paper": {
                            "corpus_id": 269214364,
                            "title": "Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study",
                            "authors": [
                                {
                                    "authorId": "2297189569",
                                    "name": "Zooey Nguyen"
                                },
                                {
                                    "authorId": "2297188041",
                                    "name": "Anthony Annunziata"
                                },
                                {
                                    "authorId": "69442223",
                                    "name": "Vinh Luong"
                                },
                                {
                                    "authorId": "2297188221",
                                    "name": "Sang Dinh"
                                },
                                {
                                    "authorId": "2297190249",
                                    "name": "Quynh Le"
                                },
                                {
                                    "authorId": "2297189614",
                                    "name": "A. Ha"
                                },
                                {
                                    "authorId": "2297189915",
                                    "name": "Chanh Le"
                                },
                                {
                                    "authorId": "2297189697",
                                    "name": "Hong An Phan"
                                },
                                {
                                    "authorId": "2058395065",
                                    "name": "Shruti Raghavan"
                                },
                                {
                                    "authorId": "2297324474",
                                    "name": "Christopher Nguyen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.8779296875
                    },
                    {
                        "id": "(Rathinasamy et al., 2024)",
                        "snippets": [
                            "In the context of enterprises accumulating proprietary unstructured data, AI-driven information retrieval solutions have emerged as vital tools for extracting relevant answers to employee queries. Traditional methods for developing such solutions often involve choosing between Retrieval Augmented Generation (RAG) or fine-tuned Large Language Models (LLMs). However, fine-tuned LLMs, comprising only generative models, lack a guarantee of factual accuracy, while RAG, comprising an embedding model and a generative model, assures factual precision (Lewis at al., 2020 [1]). Despite their superior performance in general, RAG based solutions often rely on pre-trained models, potentially leading to suboptimal alignment with enterprise-specific data.\n\nAddressing this challenge entails exploring two potential avenues: Firstly, recent studies such as RAFT (Zhang et al., 2024 [2]) explore the integration of fine-tuned generative models within a RAG pipeline to enhance accuracy, albeit requiring substantial domain-specific data to fine-tune the generative models. Alternatively, leveraging domain-specific embedding models within a RAG pipeline to enhance accuracy remains an underexplored area."
                        ],
                        "paper": {
                            "corpus_id": 270214689,
                            "title": "EnterpriseEM: Fine-tuned Embeddings for Enterprise Semantic Search",
                            "authors": [
                                {
                                    "authorId": "66945149",
                                    "name": "Kamalkumar Rathinasamy"
                                },
                                {
                                    "authorId": "2304474062",
                                    "name": "Jayarama Nettar"
                                },
                                {
                                    "authorId": "2304471263",
                                    "name": "Amit Kumar"
                                },
                                {
                                    "authorId": "2304472624",
                                    "name": "Vishal Manchanda"
                                },
                                {
                                    "authorId": "2304472592",
                                    "name": "Arun Vijayakumar"
                                },
                                {
                                    "authorId": "2275256433",
                                    "name": "Ayush Kataria"
                                },
                                {
                                    "authorId": "2304471029",
                                    "name": "Venkateshprasanna Manjunath"
                                },
                                {
                                    "authorId": "2304472432",
                                    "name": "GS Chidambaram"
                                },
                                {
                                    "authorId": "31722494",
                                    "name": "Jaskirat Sodhi"
                                },
                                {
                                    "authorId": "2304453176",
                                    "name": "Shoeb Shaikh"
                                },
                                {
                                    "authorId": "2304474234",
                                    "name": "Wasim Akhtar Khan"
                                },
                                {
                                    "authorId": "2304542089",
                                    "name": "Prashant Singh"
                                },
                                {
                                    "authorId": "2304475433",
                                    "name": "Tanishq Dattatray Ige"
                                },
                                {
                                    "authorId": "2334485471",
                                    "name": "V. Tiwari"
                                },
                                {
                                    "authorId": "2304471138",
                                    "name": "Rajab Ali Mondal"
                                },
                                {
                                    "authorId": "2304472540",
                                    "name": "K. Harshini"
                                },
                                {
                                    "authorId": "2129197171",
                                    "name": "S. Reka"
                                },
                                {
                                    "authorId": "2304472698",
                                    "name": "Chetana Amancharla"
                                },
                                {
                                    "authorId": "2302622929",
                                    "name": "Faiz ur Rahman"
                                },
                                {
                                    "authorId": "2304471871",
                                    "name": "A. HarikrishnanP"
                                },
                                {
                                    "authorId": "2304475424",
                                    "name": "Indraneel Saha"
                                },
                                {
                                    "authorId": "2304471930",
                                    "name": "Bhavya Tiwary"
                                },
                                {
                                    "authorId": "2305129400",
                                    "name": "Navin Shankar Patel"
                                },
                                {
                                    "authorId": "2304470943",
                                    "name": "S. PradeepT"
                                },
                                {
                                    "authorId": "2304475392",
                                    "name": "J. BalajiA"
                                },
                                {
                                    "authorId": "2304472627",
                                    "name": "Priyapravas"
                                },
                                {
                                    "authorId": "2304472453",
                                    "name": "Mohammed Rafee Tarafdar"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.8759765625
                    },
                    {
                        "id": "(Lewis et al., 2020)",
                        "snippets": [
                            "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
                        ],
                        "paper": {
                            "corpus_id": 218869575,
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3439053",
                                    "name": "Ethan Perez"
                                },
                                {
                                    "authorId": "1716179427",
                                    "name": "Aleksandara Piktus"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "103131985",
                                    "name": "Heinrich Kuttler"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "2620211",
                                    "name": "Tim Rockt\u00e4schel"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "1743722",
                                    "name": "Douwe Kiela"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6476
                        },
                        "score": 0
                    },
                    {
                        "id": "(Long et al., 2024)",
                        "snippets": [
                            "In the medical domain, the application of RAG technology remains in its nascent stages. The MEDRAG system assessed performance variations across diverse retrievers and corpora in medical question-answering tasks [37]. The Self-BioRAG (Jeong et al., 2024) project integrated Self-RAG [5] technology in medicine, optimizing it through reflective tokens that address retrieval timing, evaluate the relevance and supporting capacity of retrieved documents in answer generation, and assess the quality of generated outputs. Nevertheless, empirical evidence suggests that this approach fails to surpass the performance of models specifically optimized for medical datasets, potentially attributable to the limitations of smaller models in multitask integration, as demonstrated by the AUTOACT study [28]."
                        ],
                        "paper": {
                            "corpus_id": 271571143,
                            "title": "Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework for Medical Applications",
                            "authors": [
                                {
                                    "authorId": "2313916812",
                                    "name": "Cui Long"
                                },
                                {
                                    "authorId": "47909171",
                                    "name": "Yongbin Liu"
                                },
                                {
                                    "authorId": "16318808",
                                    "name": "Chunping Ouyang"
                                },
                                {
                                    "authorId": "2296267575",
                                    "name": "Ying Yu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.8955078125
                    },
                    {
                        "id": "(Jeong et al., 2024)",
                        "snippets": [
                            "Abstract Summary Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens. Our work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions. Using three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less. Similarly, Self-BioRAG outperforms RAG by 8% Rouge-1 score in generating more proficient answers on two long-form question-answering benchmarks on average. Overall, we analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does. We release our data and code for training our framework components and model weights (7B and 13B) to enhance capabilities in biomedical and clinical domains. Availability and implementation Self-BioRAG is available at https://github.com/dmis-lab/self-biorag."
                        ],
                        "paper": {
                            "corpus_id": 267312134,
                            "title": "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
                            "authors": [
                                {
                                    "authorId": "2281744951",
                                    "name": "Minbyul Jeong"
                                },
                                {
                                    "authorId": "2281744575",
                                    "name": "Jiwoong Sohn"
                                },
                                {
                                    "authorId": "147610425",
                                    "name": "Mujeen Sung"
                                },
                                {
                                    "authorId": "2281792202",
                                    "name": "Jaewoo Kang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Bioinform.",
                            "n_citations": 33
                        },
                        "score": 0
                    },
                    {
                        "id": "(Gupta et al., 2024)",
                        "snippets": [
                            "In addition to knowledge retrieval, RAG models excel at updating knowledge bases. Since the model fetches external documents for each query, it requires no retraining to incorporate the latest information. This flexibility makes RAG models particularly suitable for domains where information is constantly evolving, such as medical research, financial news, and legal proceedings. Furthermore, studies have shown that RAG models achieve superior results in a variety of knowledge-intensive tasks, including document summarization and, knowledge-grounded dialogues"
                        ],
                        "paper": {
                            "corpus_id": 273403982,
                            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
                            "authors": [
                                {
                                    "authorId": "2311997786",
                                    "name": "Shailja Gupta"
                                },
                                {
                                    "authorId": "2311893279",
                                    "name": "Rajesh Ranjan"
                                },
                                {
                                    "authorId": "2321535962",
                                    "name": "Surya Narayan Singh"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 23
                        },
                        "score": 0.90185546875
                    },
                    {
                        "id": "(Vithanage et al., 2025)",
                        "snippets": [
                            "Results show that zero-shot and few-shot learning, whether combined with PEFT or RAG, achieve comparable performance across the clinical domains when the same prompting template is used. Few-shot learning significantly outperforms zero-shot learning when neither PEFT nor RAG is applied. Notably, PEFT significantly improves model performance in both zero-shot and few-shot learning; however, RAG significantly improves performance only in few-shot learning. After PEFT, the performance of zero-shot learning reaches a comparable level with few-shot learning. However, few-shot learning with RAG significantly outperforms zero-shot learning with RAG. We also found a similar level of performance between few-shot learning with RAG and zero-shot learning with PEFT."
                        ],
                        "paper": {
                            "corpus_id": 276526965,
                            "title": "Adapting Generative Large Language Models for Information Extraction from Unstructured Electronic Health Records in Residential Aged Care: A Comparative Analysis of Training Approaches",
                            "authors": [
                                {
                                    "authorId": "2277877538",
                                    "name": "D. Vithanage"
                                },
                                {
                                    "authorId": "144551284",
                                    "name": "C. Deng"
                                },
                                {
                                    "authorId": "2278214328",
                                    "name": "Lei Wang"
                                },
                                {
                                    "authorId": "1972725075",
                                    "name": "M. Yin"
                                },
                                {
                                    "authorId": "2069756289",
                                    "name": "M. Alkhalaf"
                                },
                                {
                                    "authorId": "2109338789",
                                    "name": "Zhenyu Zhang"
                                },
                                {
                                    "authorId": "2117078401",
                                    "name": "Yunshu Zhu"
                                },
                                {
                                    "authorId": "2231452266",
                                    "name": "P. Yu"
                                }
                            ],
                            "year": 2025,
                            "venue": "J. Heal. Informatics Res.",
                            "n_citations": 0
                        },
                        "score": 0.9130859375
                    },
                    {
                        "id": "(Yu et al., 2025)",
                        "snippets": [
                            "B\u00e9chard et al. (B'echard et al., 2024) demonstrated the effectiveness of RAG in reducing hallucinations in structured outputs like workflow generation from natural language instructions."
                        ],
                        "paper": {
                            "corpus_id": 277104712,
                            "title": "RAG-KG-IL: A Multi-Agent Hybrid Framework for Reducing Hallucinations and Enhancing LLM Reasoning through RAG and Incremental Knowledge Graph Learning Integration",
                            "authors": [
                                {
                                    "authorId": "2351728696",
                                    "name": "Hong Qing Yu"
                                },
                                {
                                    "authorId": "2350756862",
                                    "name": "Frank McQuade"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.8974609375
                    },
                    {
                        "id": "(B'echard et al., 2024)",
                        "snippets": [
                            "A common and fundamental limitation of Generative AI (GenAI) is its propensity to hallucinate. While large language models (LLM) have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging Retrieval Augmented Generation (RAG) to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucinations in the output and improves the generalization of our LLM in out-of-domain settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive."
                        ],
                        "paper": {
                            "corpus_id": 269137180,
                            "title": "Reducing hallucination in structured outputs via Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2296597690",
                                    "name": "Patrice B'echard"
                                },
                                {
                                    "authorId": "2296597772",
                                    "name": "Orlando Marquez Ayala"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 60
                        },
                        "score": 0
                    },
                    {
                        "id": "(Papadimitriou et al., 2024)",
                        "snippets": [
                            "We present RAG Playground, an open-source framework for systematic evaluation of Retrieval-Augmented Generation (RAG) systems. The framework implements and compares three retrieval approaches: naive vector search, reranking, and hybrid vector-keyword search, combined with ReAct agents using different prompting strategies. We introduce a comprehensive evaluation framework with novel metrics and provide empirical results comparing different language models (Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our experiments demonstrate significant performance improvements through hybrid search methods and structured self-evaluation prompting, achieving up to 72.7% pass rate on our multi-metric evaluation framework. The results also highlight the importance of prompt engineering in RAG systems, with our custom-prompted agents showing consistent improvements in retrieval accuracy and response quality."
                        ],
                        "paper": {
                            "corpus_id": 274788878,
                            "title": "RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems",
                            "authors": [
                                {
                                    "authorId": "2280268220",
                                    "name": "Ioannis Papadimitriou"
                                },
                                {
                                    "authorId": "1988554",
                                    "name": "Ilias Gialampoukidis"
                                },
                                {
                                    "authorId": "3019137",
                                    "name": "S. Vrochidis"
                                },
                                {
                                    "authorId": "1715604",
                                    "name": "Y. Kompatsiaris"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9375
                    },
                    {
                        "id": "(Devine, 2025)",
                        "snippets": [
                            "We demonstrate the effectiveness of ALoF-TRAG by performing experiments on 20 datasets in 26 languages across a variety of domains and comparing the accuracy to simply using the base LLM for RAG. We show that the ALoFTRAG approach improves both the citation accuracy and answer accuracy of RAG models across almost all datasets compared to the base RAG model."
                        ],
                        "paper": {
                            "corpus_id": 275788867,
                            "title": "ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2341534946",
                                    "name": "Peter Devine"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9072265625
                    },
                    {
                        "id": "(Soudani et al., 2024)",
                        "snippets": [
                            "We study the effectiveness of fine-tuning and RAG approaches for question answering over less popular factual knowledge and compare the performance of these models across distinct setups: vanilla and fine-tuned models, both with and without RAG, using different data augmentation methods."
                        ],
                        "paper": {
                            "corpus_id": 268248396,
                            "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
                            "authors": [
                                {
                                    "authorId": "2165569122",
                                    "name": "Heydar Soudani"
                                },
                                {
                                    "authorId": "1713134",
                                    "name": "E. Kanoulas"
                                },
                                {
                                    "authorId": "1951737",
                                    "name": "Faegheh Hasibi"
                                }
                            ],
                            "year": 2024,
                            "venue": "SIGIR-AP",
                            "n_citations": 37
                        },
                        "score": 0.92822265625
                    },
                    {
                        "id": "(Budakoglu et al., 2025)",
                        "snippets": [
                            "This study compares two salient techniques for retrieve-augmented generation (RAG) and fine-tuning along with a new hybrid method that combines both. In this study, we investigate the effectiveness of various methods using the Stanford Question Answering Dataset (SQuAD), Microsoft Machine Reading Comprehension (MS MARCO) and SQL CREATE TABLE statements."
                        ],
                        "paper": {
                            "corpus_id": 276355526,
                            "title": "Unveiling the Power of Large Language Models: A Comparative Study of Retrieval-Augmented Generation, Fine-Tuning, and Their Synergistic Fusion for Enhanced Performance",
                            "authors": [
                                {
                                    "authorId": "2345470851",
                                    "name": "G\u00fcls\u00fcm Budakoglu"
                                },
                                {
                                    "authorId": "2345472890",
                                    "name": "Hakan Emekci"
                                }
                            ],
                            "year": 2025,
                            "venue": "IEEE Access",
                            "n_citations": 0
                        },
                        "score": 0.970703125
                    },
                    {
                        "id": "(Han et al., 2025)",
                        "snippets": [
                            "Several studies have evaluated the effectiveness of RAG systems across various tasks (Yu et al., 2024;(Chen et al., 2023)Es et al., 2023), such as multi-hop question answering (Tian et al., 2023), biomedical question answering (Xiong et al., 2024), and text generation (Liu et al., 2023). However, no existing study has simultaneously and systematically evaluated and compared RAG and GraphRAG on these general text-based tasks."
                        ],
                        "paper": {
                            "corpus_id": 276408622,
                            "title": "RAG vs. GraphRAG: A Systematic Evaluation and Key Insights",
                            "authors": [
                                {
                                    "authorId": "2049039664",
                                    "name": "Haoyu Han"
                                },
                                {
                                    "authorId": "2220302956",
                                    "name": "Harry Shomer"
                                },
                                {
                                    "authorId": "2346107355",
                                    "name": "Yu Wang"
                                },
                                {
                                    "authorId": "2338562947",
                                    "name": "Yongjia Lei"
                                },
                                {
                                    "authorId": "2338271219",
                                    "name": "Kai Guo"
                                },
                                {
                                    "authorId": "2293482433",
                                    "name": "Zhigang Hua"
                                },
                                {
                                    "authorId": "2338267824",
                                    "name": "Bo Long"
                                },
                                {
                                    "authorId": "2298005501",
                                    "name": "Hui Liu"
                                },
                                {
                                    "authorId": "2330147642",
                                    "name": "Jiliang Tang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.88916015625
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs."
                        ],
                        "paper": {
                            "corpus_id": 261530434,
                            "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2115448879",
                                    "name": "Jiawei Chen"
                                },
                                {
                                    "authorId": "2116455765",
                                    "name": "Hongyu Lin"
                                },
                                {
                                    "authorId": "2118233348",
                                    "name": "Xianpei Han"
                                },
                                {
                                    "authorId": "2110832778",
                                    "name": "Le Sun"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 307
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tian et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs (KGs) to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. Therefore, how to enhance pre-trained LLMs using grounded knowledge, e.g., retrieval-augmented generation, remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple datasets demonstrate the superiority of GNP on both commonsense and biomedical reasoning tasks across different LLM sizes and settings. Code is available at https://github.com/meettyj/GNP."
                        ],
                        "paper": {
                            "corpus_id": 263152125,
                            "title": "Graph Neural Prompting with Large Language Models",
                            "authors": [
                                {
                                    "authorId": "46879986",
                                    "name": "Yijun Tian"
                                },
                                {
                                    "authorId": "2248096816",
                                    "name": "Huan Song"
                                },
                                {
                                    "authorId": "2249432235",
                                    "name": "Zichen Wang"
                                },
                                {
                                    "authorId": "2256768980",
                                    "name": "Haozhu Wang"
                                },
                                {
                                    "authorId": "2248753090",
                                    "name": "Ziqing Hu"
                                },
                                {
                                    "authorId": "2262512203",
                                    "name": "Fang Wang"
                                },
                                {
                                    "authorId": "144539424",
                                    "name": "N. Chawla"
                                },
                                {
                                    "authorId": "2248954229",
                                    "name": "Panpan Xu"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 49
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Hybrid Approaches",
                "tldr": "Hybrid approaches that combine RAG with fine-tuning techniques demonstrate superior performance compared to either method alone across various tasks. These combined methodologies leverage the complementary strengths of both approaches, with recent innovations exploring parameter-efficient fine-tuning specifically optimized for retrieval-augmented systems. (8 sources)",
                "text": "\nResearch increasingly shows that combining fine-tuning with Retrieval Augmented Generation (RAG) produces responses with significantly improved accuracy compared to either approach used independently <Paper corpusId=\"267412954\" paperTitle=\"(Zhang et al._1, 2024)\" isShortName></Paper>. This synergistic effect leverages the complementary strengths of both methodologies\u2014the knowledge adaptation capabilities of fine-tuning with the external information access of RAG.\n\nRecent advances have introduced novel methodologies specifically designed for fine-tuning language models in RAG-based environments, particularly for knowledge-intensive tasks <Paper corpusId=\"270560505\" paperTitle=\"(Balakrishnan et al., 2024)\" isShortName></Paper>. These specialized fine-tuning approaches optimize the model's ability to effectively utilize retrieved information rather than treating RAG as merely an add-on to a standard fine-tuned model.\n\nComparative studies exploring Parameter-Efficient Fine-Tuning (PEFT) methods applied to RAG systems have revealed interesting performance differences between model architectures. Research shows that RETRO architectures (which integrate external retrieval during pre-training) generally outperform GPT models in zero-shot settings due to their enhanced contextual understanding capabilities. However, GPT models demonstrate greater performance gains during fine-tuning, suggesting they have more room for improvement when adapted using PEFT techniques <Paper corpusId=\"271039066\" paperTitle=\"(Ficek et al., 2024)\" isShortName></Paper>.\n\nIterative approaches that combine retrieval and generation have shown promise in addressing complex reasoning tasks. For example, ITER-RETGEN alternates between retrieval-augmented generation and generation-augmented retrieval, with each step refining results based on previous outputs. While effective for multi-hop question answering and commonsense reasoning, these methods still face challenges in optimal utilization of retrieved information, with studies showing approximately 20% of retrieved contexts lacking actual answers <Paper corpusId=\"273345967\" paperTitle=\"(Sanniboina et al., 2024)\" isShortName></Paper>.\n\nThe research community continues to develop more sophisticated hybrid approaches, with ongoing work comparing RAG implementations against established retrieval-enhanced models like Facebook AI's RAG and Google's REALM <Paper corpusId=\"274283400\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"211204736\" paperTitle=\"(Guu et al., 2020)\" isShortName></Paper>. Domain-specific hybrid systems that combine retrieval with specialized language models are also showing promise, with systems like PubMed <Paper corpusId=\"224820417\" paperTitle=\"(White, 2020)\" isShortName></Paper> and BioBERT demonstrating significant improvements in biomedical applications. BioBERT, for instance, achieves substantial performance gains across various biomedical text mining tasks, including named entity recognition (0.62% F1 score improvement), relation extraction (2.80% F1 score improvement), and question answering (12.24% MRR improvement) <Paper corpusId=\"59291975\" paperTitle=\"(Lee et al., 2019)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Zhang et al._1, 2024)",
                        "snippets": [
                            "Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy."
                        ],
                        "paper": {
                            "corpus_id": 267412954,
                            "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
                            "authors": [
                                {
                                    "authorId": "2279813822",
                                    "name": "Liang Zhang"
                                },
                                {
                                    "authorId": "2279831793",
                                    "name": "Katherine Jijo"
                                },
                                {
                                    "authorId": "2282528163",
                                    "name": "Spurthi Setty"
                                },
                                {
                                    "authorId": "2279830841",
                                    "name": "Eden Chung"
                                },
                                {
                                    "authorId": "2282539958",
                                    "name": "Fatima Javid"
                                },
                                {
                                    "authorId": "2279830757",
                                    "name": "Natan Vidra"
                                },
                                {
                                    "authorId": "2279838243",
                                    "name": "Thomas Clifford"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 20
                        },
                        "score": 0.88623046875
                    },
                    {
                        "id": "(Balakrishnan et al., 2024)",
                        "snippets": [
                            "More recent work further advances the field by introducing novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments [24]."
                        ],
                        "paper": {
                            "corpus_id": 270560505,
                            "title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability",
                            "authors": [
                                {
                                    "authorId": "2356633197",
                                    "name": "Gautam Balakrishnan"
                                },
                                {
                                    "authorId": "33856997",
                                    "name": "A. Purwar"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE India Conference",
                            "n_citations": 14
                        },
                        "score": 0.87158203125
                    },
                    {
                        "id": "(Ficek et al., 2024)",
                        "snippets": [
                            "This study explores Parameter-Efficient Fine-Tuning (PEFT) methods applied to Retrieval-Augmented Generation (RAG) models, comparing GPT and RETRO architectures. RETRO generally outperforms GPT in zero-shot settings due to their pre-training process that integrates external retrieval, enhancing contextual understanding. However, GPT models show a higher performance potential with PEFT, indicating more room for improvement during fine-tuning."
                        ],
                        "paper": {
                            "corpus_id": 271039066,
                            "title": "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning",
                            "authors": [
                                {
                                    "authorId": "2186740325",
                                    "name": "Aleksander Ficek"
                                },
                                {
                                    "authorId": "2266881428",
                                    "name": "Jiaqi Zeng"
                                },
                                {
                                    "authorId": "2787022",
                                    "name": "Oleksii Kuchaiev"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 1
                        },
                        "score": 0.9375
                    },
                    {
                        "id": "(Sanniboina et al., 2024)",
                        "snippets": [
                            "The development of retrieval-augmented LLMs has advanced significantly, focusing on improving the synergy between retrieval and generation processes. Shao et al. [17] introduced ITER-RETGEN, which iteratively enhances retrieval and generation for tasks like multi-hop QA and commonsense reasoning. It alternates between retrieval-augmented generation and generation-augmented retrieval, refining each step with the previous output. But, it faces challenges such as suboptimal utilization of retrieved information, with about 20% of contexts lacking actual answers, leading to inaccuracies or hallucinations due to positional bias and non-optimized iterative retrieval."
                        ],
                        "paper": {
                            "corpus_id": 273345967,
                            "title": "LoRE: Logit-Ranked Retriever Ensemble for Enhancing Open-Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "2325905180",
                                    "name": "Saikrishna Sanniboina"
                                },
                                {
                                    "authorId": "2325907509",
                                    "name": "Shiv Trivedi"
                                },
                                {
                                    "authorId": "2325903482",
                                    "name": "Sreenidhi Vijayaraghavan"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.88916015625
                    },
                    {
                        "id": "(Yao et al., 2024)",
                        "snippets": [
                            "Future work includes comparisons with existing methods for improving the performance of RAGs and evaluation experiments using a wider range of datasets. We plan to compare our method with existing robust approaches, including RAG by Facebook AI [18], REALM by Google (Guu et al., 2020), and various domain-specific retrieval systems such as PubMed (White, 2020), BioBERT (Lee et al., 2019) PatentBERT [21], and FinancialBERT [22]. Additionally, we will evaluate against the latest state-of-the-art LLMs, such as Phi-3.5 [23], LLaMA 3.2 [24], and OLMo [25]."
                        ],
                        "paper": {
                            "corpus_id": 274283400,
                            "title": "Adaptive Control of Retrieval-Augmented Generation for Large Language Models Through Reflective Tags",
                            "authors": [
                                {
                                    "authorId": "2332684729",
                                    "name": "Chengyuan Yao"
                                },
                                {
                                    "authorId": "2275134724",
                                    "name": "Satoshi Fujita"
                                }
                            ],
                            "year": 2024,
                            "venue": "Electronics",
                            "n_citations": 4
                        },
                        "score": 0.890625
                    },
                    {
                        "id": "(Guu et al., 2020)",
                        "snippets": [
                            "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."
                        ],
                        "paper": {
                            "corpus_id": 211204736,
                            "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                            "authors": [
                                {
                                    "authorId": "2091768",
                                    "name": "Kelvin Guu"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "9941702",
                                    "name": "Zora Tung"
                                },
                                {
                                    "authorId": "2616463",
                                    "name": "Panupong Pasupat"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 2119
                        },
                        "score": 0
                    },
                    {
                        "id": "(White, 2020)",
                        "snippets": [
                            "Abstract After years of strategic planning, the National Library of Medicine has introduced an updated and redesigned version of its PubMed health sciences research website. The new website features a more modern and responsive interface, especially on mobile devices. Tools and features have been relocated to make them more intuitive for new users. While not without some turbulence and slight discomfort for long-time users adjusting to the modernized interface and search engine, the new version of the PubMed website introduced in 2020 succeeds in the website\u2019s time-honored task of collecting and making freely accessible high-quality health sciences information and resources."
                        ],
                        "paper": {
                            "corpus_id": 224820417,
                            "title": "PubMed 2.0",
                            "authors": [
                                {
                                    "authorId": "2111356742",
                                    "name": "Jacob White"
                                }
                            ],
                            "year": 2020,
                            "venue": "Medical Reference Services Quarterly",
                            "n_citations": 127
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2019)",
                        "snippets": [
                            "Abstract Motivation Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. Results We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. Availability and implementation We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert."
                        ],
                        "paper": {
                            "corpus_id": 59291975,
                            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
                            "authors": [
                                {
                                    "authorId": "46664096",
                                    "name": "Jinhyuk Lee"
                                },
                                {
                                    "authorId": "51433082",
                                    "name": "WonJin Yoon"
                                },
                                {
                                    "authorId": "2829848",
                                    "name": "Sungdong Kim"
                                },
                                {
                                    "authorId": "2145183568",
                                    "name": "Donghyeon Kim"
                                },
                                {
                                    "authorId": "2144247125",
                                    "name": "Sunkyu Kim"
                                },
                                {
                                    "authorId": "51435068",
                                    "name": "Chan Ho So"
                                },
                                {
                                    "authorId": "144323862",
                                    "name": "Jaewoo Kang"
                                }
                            ],
                            "year": 2019,
                            "venue": "Bioinform.",
                            "n_citations": 5673
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Evaluation Methodologies",
                "tldr": "Evaluating the effectiveness of RAG versus fine-tuning approaches requires robust methodologies that assess multiple dimensions including faithfulness, relevance, and factual accuracy. Current evaluation frameworks range from automated LLM-based assessments to comprehensive benchmark comparisons, though a significant research gap exists in standardized evaluation protocols across diverse RAG implementations. (4 sources)",
                "text": "\nThe assessment of Retrieval-Augmented Generation (RAG) systems against fine-tuning approaches presents unique methodological challenges that researchers continue to address. Rigorous experimental comparisons have demonstrated that RAG systems can significantly enhance content quality by effectively incorporating information from external knowledge sources <Paper corpusId=\"267320876\" paperTitle=\"(Lyu et al., 2024)\" isShortName></Paper>. These evaluations typically focus on multiple dimensions of performance, including faithfulness to source material, answer relevance, and context relevance.\n\nCurrent evaluation methodologies employ various techniques to comprehensively assess RAG systems. These include prompting large language models to evaluate generated summaries, fine-tuning lightweight models on synthetic data, and using downstream applications like question answering to measure system effectiveness <Paper corpusId=\"269502216\" paperTitle=\"(Mayfield et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258587884\" paperTitle=\"(Yue et al., 2023)\" isShortName></Paper>. For instance, researchers have developed specialized approaches for evaluating attribution in generative systems, which verify whether generated statements are fully supported by cited references\u2014a critical aspect of RAG evaluation <Paper corpusId=\"269502216\" paperTitle=\"(Mayfield et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258587884\" paperTitle=\"(Yue et al., 2023)\" isShortName></Paper>.\n\nDespite growing interest in RAG techniques within the LLM domain, the research community has identified a significant gap in evaluation methodologies. While systematic reviews and direct comparisons between successive state-of-the-art models exist, comprehensive experimental comparisons across a broad spectrum of advanced RAG techniques remain limited <Paper corpusId=\"268819923\" paperTitle=\"(Eibich et al., 2024)\" isShortName></Paper>. This gap highlights the need for more standardized evaluation frameworks that can provide consistent metrics across diverse implementations of both RAG and fine-tuning approaches.\n\nRecent research efforts have begun addressing this gap by providing more extensive evaluations of multiple RAG techniques and their combinations, offering insights into their relative strengths and weaknesses across various tasks and real-world scenarios <Paper corpusId=\"268819923\" paperTitle=\"(Eibich et al., 2024)\" isShortName></Paper>. These comparative studies are essential for developing a more nuanced understanding of when and how to apply RAG versus fine-tuning approaches in different contexts.\n\nAs the field continues to evolve, establishing robust, standardized evaluation methodologies will be crucial for making meaningful comparisons between RAG and fine-tuning approaches. These methodologies will need to account for the multifaceted nature of performance in generative AI systems, including factual accuracy, relevance, coherence, and computational efficiency <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Lyu et al., 2024)",
                        "snippets": [
                            "Through rigorous experimental comparisons, we have demonstrated that RAG systems can significantly enhance the quality of generated content by effectively incorporating information from external knowledge sources."
                        ],
                        "paper": {
                            "corpus_id": 267320876,
                            "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2187857206",
                                    "name": "Yuanjie Lyu"
                                },
                                {
                                    "authorId": "2268429641",
                                    "name": "Zhiyu Li"
                                },
                                {
                                    "authorId": "2268393907",
                                    "name": "Simin Niu"
                                },
                                {
                                    "authorId": "2268399953",
                                    "name": "Feiyu Xiong"
                                },
                                {
                                    "authorId": "2268400606",
                                    "name": "Bo Tang"
                                },
                                {
                                    "authorId": "2117833477",
                                    "name": "Wenjin Wang"
                                },
                                {
                                    "authorId": "2282083454",
                                    "name": "Hao Wu"
                                },
                                {
                                    "authorId": "2304320758",
                                    "name": "Huan Liu"
                                },
                                {
                                    "authorId": "2277237058",
                                    "name": "Tong Xu"
                                },
                                {
                                    "authorId": "2265580543",
                                    "name": "Enhong Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "ACM Trans. Inf. Syst.",
                            "n_citations": 40
                        },
                        "score": 0.94677734375
                    },
                    {
                        "id": "(Mayfield et al., 2024)",
                        "snippets": [
                            "Most approaches to automated evaluation aim to estimate the effectiveness of RAG systems across desirable dimensions (e.g., faithfulness, answer relevance, and context relevance). Techniques include prompting LLMs to evaluate generated summaries [76], and fine-tuning lightweight models on synthetic data [73]. Downstream applications, such as question answering, can also be used to evaluate the effectiveness of RAG systems [74]."
                        ],
                        "paper": {
                            "corpus_id": 269502216,
                            "title": "On the Evaluation of Machine-Generated Reports",
                            "authors": [
                                {
                                    "authorId": "2298905578",
                                    "name": "James Mayfield"
                                },
                                {
                                    "authorId": "2296599846",
                                    "name": "Eugene Yang"
                                },
                                {
                                    "authorId": "2539674",
                                    "name": "Dawn J Lawrie"
                                },
                                {
                                    "authorId": "2290916915",
                                    "name": "Sean MacAvaney"
                                },
                                {
                                    "authorId": "145324163",
                                    "name": "Paul McNamee"
                                },
                                {
                                    "authorId": "1737250",
                                    "name": "Douglas W. Oard"
                                },
                                {
                                    "authorId": "3328733",
                                    "name": "Luca Soldaini"
                                },
                                {
                                    "authorId": "2299328116",
                                    "name": "Ian Soboroff"
                                },
                                {
                                    "authorId": "47433471",
                                    "name": "Orion Weller"
                                },
                                {
                                    "authorId": "1396112798",
                                    "name": "Efsun Kayi"
                                },
                                {
                                    "authorId": "2187060946",
                                    "name": "Kate Sanders"
                                },
                                {
                                    "authorId": "2215823149",
                                    "name": "Marc Mason"
                                },
                                {
                                    "authorId": "2299332019",
                                    "name": "Noah Hibbler"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 16
                        },
                        "score": 0.892578125
                    },
                    {
                        "id": "(Yue et al., 2023)",
                        "snippets": [
                            "A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support its claims. However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution given by LLMs. We begin by defining different types of attribution errors, and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks such as question answering, fact-checking, natural language inference, and summarization. We manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on this curated test set and simulated examples from existing benchmarks highlight both promising signals and challenges. We hope our problem formulation, testbeds, and findings will help lay the foundation for future studies on this important problem."
                        ],
                        "paper": {
                            "corpus_id": 258587884,
                            "title": "Automatic Evaluation of Attribution by Large Language Models",
                            "authors": [
                                {
                                    "authorId": "145548079",
                                    "name": "Xiang Yue"
                                },
                                {
                                    "authorId": "7425689",
                                    "name": "Boshi Wang"
                                },
                                {
                                    "authorId": "145086492",
                                    "name": "Kai Zhang"
                                },
                                {
                                    "authorId": "11832104",
                                    "name": "Ziru Chen"
                                },
                                {
                                    "authorId": "1758652",
                                    "name": "Yu Su"
                                },
                                {
                                    "authorId": "1515546612",
                                    "name": "Huan Sun"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 58
                        },
                        "score": 0
                    },
                    {
                        "id": "(Eibich et al., 2024)",
                        "snippets": [
                            "Despite the growing interest in RAG techniques within the domain of LLMs, the existing body of literature primarily consists of systematic reviews (Gao et al., 2024) and direct comparisons between successive state-of-the-art (SoTA) models (Gao et al., 2022;Jiang et al., 2023).This pattern reveals a notable gap: a comprehensive experimental comparison across a broad spectrum of advanced RAG techniques is missing.Such a comparison is crucial for understanding the relative strengths and weaknesses of these techniques in enhancing LLMs' performance across various tasks.This study seeks to contribute to bridging this gap by providing an extensive evaluation of multiple RAG techniques and their combinations, thereby offering insights into their efficacy and applicability in real-world scenarios."
                        ],
                        "paper": {
                            "corpus_id": 268819923,
                            "title": "ARAGOG: Advanced RAG Output Grading",
                            "authors": [
                                {
                                    "authorId": "2294361167",
                                    "name": "Matouvs Eibich"
                                },
                                {
                                    "authorId": "2294361283",
                                    "name": "Shivay Nagpal"
                                },
                                {
                                    "authorId": "2294362877",
                                    "name": "Alexander Fred-Ojala"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.94677734375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.112545
    }
}